Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.535s
user	0m0.863s
sys	0m1.230s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 35%] Built target test-c
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Built target llava_shared
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-sampling
[ 49%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Built target test-arg-parser
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Built target test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-chat-template
[ 61%] Built target test-gguf
[ 62%] Built target test-barrier
[ 62%] Built target test-autorelease
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Built target test-quantize-perf
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Built target llama-batched-bench
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Built target llama-eval-callback
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gbnf-validator
[ 74%] Built target llama-imatrix
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Built target llama-gritlm
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Built target llama-infill
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-lookahead
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-lookup-create
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-passkey
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Built target llama-perplexity
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Built target llama-quantize
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Built target llama-retrieval
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 93%] Built target llama-save-load-state
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.037s
user	0m5.920s
sys	0m9.148s

main: quantize time =  3097.62 ms
main:    total time =  3097.62 ms

main: quantize time =  1369.08 ms
main:    total time =  1369.08 ms

main: quantize time =  1457.72 ms
main:    total time =  1457.72 ms

main: quantize time =  1418.86 ms
main:    total time =  1418.86 ms

main: quantize time =  2767.09 ms
main:    total time =  2767.09 ms

main: quantize time =  5005.87 ms
main:    total time =  5005.87 ms

main: quantize time =  5685.24 ms
main:    total time =  5685.24 ms

main: quantize time =  7113.60 ms
main:    total time =  7113.60 ms

main: quantize time =  5845.17 ms
main:    total time =  5845.17 ms

main: quantize time =  4556.44 ms
main:    total time =  4556.44 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.178 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.296 I main: llama backend init
0.00.000.302 I main: load the model and apply lora adapter, if any
0.00.029.493 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.289 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.312 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.313 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.316 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.317 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.317 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.319 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.319 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.324 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.878 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.891 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.896 I llama_model_loader: - type  f32:  194 tensors
0.00.058.897 I llama_model_loader: - type  f16:   98 tensors
0.00.090.645 I load_vocab: special tokens cache size = 25
0.00.098.076 I load_vocab: token to piece cache size = 0.2984 MB
0.00.098.079 I print_meta: format           = GGUF V3 (latest)
0.00.098.079 I print_meta: arch             = gptneox
0.00.098.080 I print_meta: vocab type       = BPE
0.00.098.080 I print_meta: n_vocab          = 50304
0.00.098.080 I print_meta: n_merges         = 50009
0.00.098.080 I print_meta: vocab_only       = 0
0.00.098.080 I print_meta: n_ctx_train      = 2048
0.00.098.081 I print_meta: n_embd           = 2048
0.00.098.081 I print_meta: n_layer          = 24
0.00.098.083 I print_meta: n_head           = 16
0.00.098.084 I print_meta: n_head_kv        = 16
0.00.098.084 I print_meta: n_rot            = 32
0.00.098.084 I print_meta: n_swa            = 0
0.00.098.086 I print_meta: n_embd_head_k    = 128
0.00.098.086 I print_meta: n_embd_head_v    = 128
0.00.098.087 I print_meta: n_gqa            = 1
0.00.098.087 I print_meta: n_embd_k_gqa     = 2048
0.00.098.088 I print_meta: n_embd_v_gqa     = 2048
0.00.098.089 I print_meta: f_norm_eps       = 1.0e-05
0.00.098.089 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.089 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.089 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.089 I print_meta: f_logit_scale    = 0.0e+00
0.00.098.090 I print_meta: n_ff             = 8192
0.00.098.090 I print_meta: n_expert         = 0
0.00.098.091 I print_meta: n_expert_used    = 0
0.00.098.091 I print_meta: causal attn      = 1
0.00.098.091 I print_meta: pooling type     = 0
0.00.098.091 I print_meta: rope type        = 2
0.00.098.091 I print_meta: rope scaling     = linear
0.00.098.092 I print_meta: freq_base_train  = 10000.0
0.00.098.092 I print_meta: freq_scale_train = 1
0.00.098.092 I print_meta: n_ctx_orig_yarn  = 2048
0.00.098.092 I print_meta: rope_finetuned   = unknown
0.00.098.094 I print_meta: ssm_d_conv       = 0
0.00.098.094 I print_meta: ssm_d_inner      = 0
0.00.098.094 I print_meta: ssm_d_state      = 0
0.00.098.094 I print_meta: ssm_dt_rank      = 0
0.00.098.094 I print_meta: ssm_dt_b_c_rms   = 0
0.00.098.095 I print_meta: model type       = 1.4B
0.00.098.095 I print_meta: model ftype      = all F32 (guessed)
0.00.098.096 I print_meta: model params     = 1.41 B
0.00.098.096 I print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.096 I print_meta: general.name     = 1.4B
0.00.098.100 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.100 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.100 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.101 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.101 I print_meta: LF token         = 128 'Ä'
0.00.098.101 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.101 I print_meta: max token length = 1024
0.00.100.715 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.715 I llm_load_tensors: offloading output layer to GPU
0.00.100.715 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.733 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.735 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.737 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.738 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.739 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.739 I llama_new_context_with_model: n_batch       = 2048
0.00.101.739 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.739 I llama_new_context_with_model: flash_attn    = 0
0.00.101.740 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.740 I llama_new_context_with_model: freq_scale    = 1
0.00.101.740 I ggml_metal_init: allocating
0.00.101.748 I ggml_metal_init: found device: Apple M4
0.00.101.751 I ggml_metal_init: picking default device: Apple M4
0.00.102.428 I ggml_metal_init: using embedded metal library
0.00.113.133 I ggml_metal_init: GPU name:   Apple M4
0.00.113.135 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.136 I ggml_metal_init: simdgroup reduction   = true
0.00.113.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.137 I ggml_metal_init: has bfloat            = true
0.00.113.137 I ggml_metal_init: use bfloat            = true
0.00.113.137 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.707 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.156.701 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.156.707 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.156.729 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.157.654 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.157.655 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.157.655 I llama_new_context_with_model: graph nodes  = 967
0.00.157.656 I llama_new_context_with_model: graph splits = 2
0.00.157.658 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.157.781 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.157.782 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.243.497 I main: llama threadpool init, n_threads = 4
0.00.243.537 I 
0.00.243.566 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.243.568 I 
0.00.243.647 I sampler seed: 1234
0.00.243.652 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.243.687 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.243.688 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.243.688 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.090.905 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.02.090.906 I llama_perf_context_print:        load time =     213.99 ms
0.02.090.907 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.48 tokens per second)
0.02.090.907 I llama_perf_context_print:        eval time =    1800.81 ms /    63 runs   (   28.58 ms per token,    34.98 tokens per second)
0.02.090.908 I llama_perf_context_print:       total time =    1847.41 ms /    70 tokens
0.02.091.098 I ggml_metal_free: deallocating

real	0m2.379s
user	0m0.145s
sys	0m0.105s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.829 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.542 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.544 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.544 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.545 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.545 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.545 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.546 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.549 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.543 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.544 I llama_model_loader: - type  f32:  194 tensors
0.00.027.544 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.525 I load_vocab: special tokens cache size = 25
0.00.055.603 I load_vocab: token to piece cache size = 0.2984 MB
0.00.055.608 I print_meta: format           = GGUF V3 (latest)
0.00.055.609 I print_meta: arch             = gptneox
0.00.055.609 I print_meta: vocab type       = BPE
0.00.055.610 I print_meta: n_vocab          = 50304
0.00.055.611 I print_meta: n_merges         = 50009
0.00.055.617 I print_meta: vocab_only       = 0
0.00.055.617 I print_meta: n_ctx_train      = 2048
0.00.055.617 I print_meta: n_embd           = 2048
0.00.055.617 I print_meta: n_layer          = 24
0.00.055.623 I print_meta: n_head           = 16
0.00.055.625 I print_meta: n_head_kv        = 16
0.00.055.626 I print_meta: n_rot            = 32
0.00.055.626 I print_meta: n_swa            = 0
0.00.055.626 I print_meta: n_embd_head_k    = 128
0.00.055.627 I print_meta: n_embd_head_v    = 128
0.00.055.628 I print_meta: n_gqa            = 1
0.00.055.629 I print_meta: n_embd_k_gqa     = 2048
0.00.055.630 I print_meta: n_embd_v_gqa     = 2048
0.00.055.630 I print_meta: f_norm_eps       = 1.0e-05
0.00.055.631 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.631 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.631 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.631 I print_meta: f_logit_scale    = 0.0e+00
0.00.055.632 I print_meta: n_ff             = 8192
0.00.055.632 I print_meta: n_expert         = 0
0.00.055.632 I print_meta: n_expert_used    = 0
0.00.055.632 I print_meta: causal attn      = 1
0.00.055.632 I print_meta: pooling type     = 0
0.00.055.633 I print_meta: rope type        = 2
0.00.055.633 I print_meta: rope scaling     = linear
0.00.055.633 I print_meta: freq_base_train  = 10000.0
0.00.055.634 I print_meta: freq_scale_train = 1
0.00.055.634 I print_meta: n_ctx_orig_yarn  = 2048
0.00.055.634 I print_meta: rope_finetuned   = unknown
0.00.055.634 I print_meta: ssm_d_conv       = 0
0.00.055.634 I print_meta: ssm_d_inner      = 0
0.00.055.634 I print_meta: ssm_d_state      = 0
0.00.055.634 I print_meta: ssm_dt_rank      = 0
0.00.055.634 I print_meta: ssm_dt_b_c_rms   = 0
0.00.055.635 I print_meta: model type       = 1.4B
0.00.055.635 I print_meta: model ftype      = Q8_0
0.00.055.636 I print_meta: model params     = 1.41 B
0.00.055.636 I print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.636 I print_meta: general.name     = 1.4B
0.00.055.636 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.637 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.637 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.637 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.637 I print_meta: LF token         = 128 'Ä'
0.00.055.637 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.638 I print_meta: max token length = 1024
0.00.058.092 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.092 I llm_load_tensors: offloading output layer to GPU
0.00.058.093 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.104 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.106 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.059.061 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.062 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.062 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.062 I llama_new_context_with_model: n_batch       = 2048
0.00.059.062 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.062 I llama_new_context_with_model: flash_attn    = 0
0.00.059.063 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.063 I llama_new_context_with_model: freq_scale    = 1
0.00.059.064 I ggml_metal_init: allocating
0.00.059.071 I ggml_metal_init: found device: Apple M4
0.00.059.073 I ggml_metal_init: picking default device: Apple M4
0.00.059.814 I ggml_metal_init: using embedded metal library
0.00.062.340 I ggml_metal_init: GPU name:   Apple M4
0.00.062.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.343 I ggml_metal_init: simdgroup reduction   = true
0.00.062.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.343 I ggml_metal_init: has bfloat            = true
0.00.062.343 I ggml_metal_init: use bfloat            = true
0.00.062.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.010 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.191 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.206 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.231 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.337 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.340 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.340 I llama_new_context_with_model: graph nodes  = 967
0.00.098.340 I llama_new_context_with_model: graph splits = 2
0.00.098.344 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.195.141 I main: llama threadpool init, n_threads = 4
0.01.195.182 I 
0.01.195.206 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.195.206 I 
0.01.195.441 I sampler seed: 1234
0.01.195.446 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.195.485 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.195.497 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.195.498 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.283.512 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.02.283.513 I llama_perf_context_print:        load time =    1185.31 ms
0.02.283.513 I llama_perf_context_print: prompt eval time =      39.84 ms /     7 tokens (    5.69 ms per token,   175.69 tokens per second)
0.02.283.514 I llama_perf_context_print:        eval time =    1045.24 ms /    63 runs   (   16.59 ms per token,    60.27 tokens per second)
0.02.283.516 I llama_perf_context_print:       total time =    1088.37 ms /    70 tokens
0.02.283.773 I ggml_metal_free: deallocating

real	0m2.303s
user	0m0.113s
sys	0m0.230s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.019.538 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.456 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.469 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.215 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.215 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.216 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.216 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.217 I llama_model_loader: - type  f32:  194 tensors
0.00.043.217 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.218 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.993 I load_vocab: special tokens cache size = 25
0.00.071.137 I load_vocab: token to piece cache size = 0.2984 MB
0.00.071.144 I print_meta: format           = GGUF V3 (latest)
0.00.071.145 I print_meta: arch             = gptneox
0.00.071.145 I print_meta: vocab type       = BPE
0.00.071.145 I print_meta: n_vocab          = 50304
0.00.071.145 I print_meta: n_merges         = 50009
0.00.071.146 I print_meta: vocab_only       = 0
0.00.071.146 I print_meta: n_ctx_train      = 2048
0.00.071.146 I print_meta: n_embd           = 2048
0.00.071.146 I print_meta: n_layer          = 24
0.00.071.152 I print_meta: n_head           = 16
0.00.071.153 I print_meta: n_head_kv        = 16
0.00.071.153 I print_meta: n_rot            = 32
0.00.071.153 I print_meta: n_swa            = 0
0.00.071.154 I print_meta: n_embd_head_k    = 128
0.00.071.154 I print_meta: n_embd_head_v    = 128
0.00.071.154 I print_meta: n_gqa            = 1
0.00.071.155 I print_meta: n_embd_k_gqa     = 2048
0.00.071.155 I print_meta: n_embd_v_gqa     = 2048
0.00.071.156 I print_meta: f_norm_eps       = 1.0e-05
0.00.071.159 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.159 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.159 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.159 I print_meta: f_logit_scale    = 0.0e+00
0.00.071.160 I print_meta: n_ff             = 8192
0.00.071.160 I print_meta: n_expert         = 0
0.00.071.160 I print_meta: n_expert_used    = 0
0.00.071.160 I print_meta: causal attn      = 1
0.00.071.160 I print_meta: pooling type     = 0
0.00.071.161 I print_meta: rope type        = 2
0.00.071.161 I print_meta: rope scaling     = linear
0.00.071.161 I print_meta: freq_base_train  = 10000.0
0.00.071.161 I print_meta: freq_scale_train = 1
0.00.071.162 I print_meta: n_ctx_orig_yarn  = 2048
0.00.071.162 I print_meta: rope_finetuned   = unknown
0.00.071.162 I print_meta: ssm_d_conv       = 0
0.00.071.162 I print_meta: ssm_d_inner      = 0
0.00.071.162 I print_meta: ssm_d_state      = 0
0.00.071.162 I print_meta: ssm_dt_rank      = 0
0.00.071.162 I print_meta: ssm_dt_b_c_rms   = 0
0.00.071.162 I print_meta: model type       = 1.4B
0.00.071.163 I print_meta: model ftype      = Q4_0
0.00.071.163 I print_meta: model params     = 1.41 B
0.00.071.164 I print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.071.164 I print_meta: general.name     = 1.4B
0.00.071.164 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.164 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.164 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.164 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.165 I print_meta: LF token         = 128 'Ä'
0.00.071.165 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.165 I print_meta: max token length = 1024
0.00.073.465 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.465 I llm_load_tensors: offloading output layer to GPU
0.00.073.466 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.478 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.073.479 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.074.613 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.614 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.614 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.615 I llama_new_context_with_model: n_batch       = 2048
0.00.074.615 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.615 I llama_new_context_with_model: flash_attn    = 0
0.00.074.616 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.616 I llama_new_context_with_model: freq_scale    = 1
0.00.074.617 I ggml_metal_init: allocating
0.00.074.620 I ggml_metal_init: found device: Apple M4
0.00.074.623 I ggml_metal_init: picking default device: Apple M4
0.00.075.393 I ggml_metal_init: using embedded metal library
0.00.077.930 I ggml_metal_init: GPU name:   Apple M4
0.00.077.932 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.933 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.933 I ggml_metal_init: simdgroup reduction   = true
0.00.077.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.933 I ggml_metal_init: has bfloat            = true
0.00.077.934 I ggml_metal_init: use bfloat            = true
0.00.077.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.935 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.321 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.114.686 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.693 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.717 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.885 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.887 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.887 I llama_new_context_with_model: graph nodes  = 967
0.00.115.887 I llama_new_context_with_model: graph splits = 2
0.00.115.892 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.116.033 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.034 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.957.647 I main: llama threadpool init, n_threads = 4
0.00.957.701 I 
0.00.957.733 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.957.734 I 
0.00.957.954 I sampler seed: 1234
0.00.957.960 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.957.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.957.999 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.958.000 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.647.995 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47491.64 tokens per second)
0.01.647.995 I llama_perf_context_print:        load time =     938.10 ms
0.01.647.996 I llama_perf_context_print: prompt eval time =      50.82 ms /     7 tokens (    7.26 ms per token,   137.74 tokens per second)
0.01.647.997 I llama_perf_context_print:        eval time =     636.37 ms /    63 runs   (   10.10 ms per token,    99.00 tokens per second)
0.01.647.998 I llama_perf_context_print:       total time =     690.35 ms /    70 tokens
0.01.648.259 I ggml_metal_free: deallocating

real	0m1.683s
user	0m0.112s
sys	0m0.147s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.699 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.679 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.680 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.680 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.680 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.681 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.682 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.682 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.682 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.683 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.683 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.683 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.684 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.686 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.687 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.687 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.650 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.670 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.514 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.515 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.516 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.516 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.517 I llama_model_loader: - type  f32:  194 tensors
0.00.024.517 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.517 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.608 I load_vocab: special tokens cache size = 25
0.00.050.682 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.685 I print_meta: format           = GGUF V3 (latest)
0.00.050.686 I print_meta: arch             = gptneox
0.00.050.686 I print_meta: vocab type       = BPE
0.00.050.686 I print_meta: n_vocab          = 50304
0.00.050.686 I print_meta: n_merges         = 50009
0.00.050.687 I print_meta: vocab_only       = 0
0.00.050.687 I print_meta: n_ctx_train      = 2048
0.00.050.687 I print_meta: n_embd           = 2048
0.00.050.687 I print_meta: n_layer          = 24
0.00.050.690 I print_meta: n_head           = 16
0.00.050.691 I print_meta: n_head_kv        = 16
0.00.050.691 I print_meta: n_rot            = 32
0.00.050.692 I print_meta: n_swa            = 0
0.00.050.692 I print_meta: n_embd_head_k    = 128
0.00.050.692 I print_meta: n_embd_head_v    = 128
0.00.050.693 I print_meta: n_gqa            = 1
0.00.050.693 I print_meta: n_embd_k_gqa     = 2048
0.00.050.694 I print_meta: n_embd_v_gqa     = 2048
0.00.050.695 I print_meta: f_norm_eps       = 1.0e-05
0.00.050.695 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.695 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.697 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.697 I print_meta: f_logit_scale    = 0.0e+00
0.00.050.698 I print_meta: n_ff             = 8192
0.00.050.698 I print_meta: n_expert         = 0
0.00.050.698 I print_meta: n_expert_used    = 0
0.00.050.700 I print_meta: causal attn      = 1
0.00.050.700 I print_meta: pooling type     = 0
0.00.050.700 I print_meta: rope type        = 2
0.00.050.700 I print_meta: rope scaling     = linear
0.00.050.701 I print_meta: freq_base_train  = 10000.0
0.00.050.701 I print_meta: freq_scale_train = 1
0.00.050.701 I print_meta: n_ctx_orig_yarn  = 2048
0.00.050.701 I print_meta: rope_finetuned   = unknown
0.00.050.702 I print_meta: ssm_d_conv       = 0
0.00.050.702 I print_meta: ssm_d_inner      = 0
0.00.050.702 I print_meta: ssm_d_state      = 0
0.00.050.702 I print_meta: ssm_dt_rank      = 0
0.00.050.702 I print_meta: ssm_dt_b_c_rms   = 0
0.00.050.702 I print_meta: model type       = 1.4B
0.00.050.703 I print_meta: model ftype      = Q4_1
0.00.050.703 I print_meta: model params     = 1.41 B
0.00.050.704 I print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.704 I print_meta: general.name     = 1.4B
0.00.050.704 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.704 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.705 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.705 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.705 I print_meta: LF token         = 128 'Ä'
0.00.050.705 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.706 I print_meta: max token length = 1024
0.00.052.692 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.692 I llm_load_tensors: offloading output layer to GPU
0.00.052.692 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.703 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.704 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.590 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.591 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.591 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.592 I llama_new_context_with_model: n_batch       = 2048
0.00.053.592 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.592 I llama_new_context_with_model: flash_attn    = 0
0.00.053.592 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.593 I llama_new_context_with_model: freq_scale    = 1
0.00.053.593 I ggml_metal_init: allocating
0.00.053.596 I ggml_metal_init: found device: Apple M4
0.00.053.598 I ggml_metal_init: picking default device: Apple M4
0.00.054.199 I ggml_metal_init: using embedded metal library
0.00.056.506 I ggml_metal_init: GPU name:   Apple M4
0.00.056.508 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.508 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.509 I ggml_metal_init: simdgroup reduction   = true
0.00.056.509 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.509 I ggml_metal_init: has bfloat            = true
0.00.056.510 I ggml_metal_init: use bfloat            = true
0.00.056.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.146 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.700 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.706 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.726 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.782 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.783 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.783 I llama_new_context_with_model: graph nodes  = 967
0.00.086.784 I llama_new_context_with_model: graph splits = 2
0.00.086.786 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.927 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.820 I main: llama threadpool init, n_threads = 4
0.00.743.861 I 
0.00.743.884 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.886 I 
0.00.744.130 I sampler seed: 1234
0.00.744.134 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.149 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.150 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.150 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.466.205 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.466.206 I llama_perf_context_print:        load time =     735.12 ms
0.01.466.207 I llama_perf_context_print: prompt eval time =      43.54 ms /     7 tokens (    6.22 ms per token,   160.78 tokens per second)
0.01.466.207 I llama_perf_context_print:        eval time =     675.75 ms /    63 runs   (   10.73 ms per token,    93.23 tokens per second)
0.01.466.208 I llama_perf_context_print:       total time =     722.39 ms /    70 tokens
0.01.466.450 I ggml_metal_free: deallocating

real	0m1.482s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.695 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.775 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.776 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.777 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.777 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.782 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.782 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.783 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.783 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.784 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.785 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.785 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.788 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.788 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.789 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.422 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.423 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.424 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.424 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.425 I llama_model_loader: - type  f32:  194 tensors
0.00.024.425 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.425 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.557 I load_vocab: special tokens cache size = 25
0.00.050.544 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.547 I print_meta: format           = GGUF V3 (latest)
0.00.050.547 I print_meta: arch             = gptneox
0.00.050.548 I print_meta: vocab type       = BPE
0.00.050.548 I print_meta: n_vocab          = 50304
0.00.050.548 I print_meta: n_merges         = 50009
0.00.050.548 I print_meta: vocab_only       = 0
0.00.050.548 I print_meta: n_ctx_train      = 2048
0.00.050.549 I print_meta: n_embd           = 2048
0.00.050.549 I print_meta: n_layer          = 24
0.00.050.551 I print_meta: n_head           = 16
0.00.050.552 I print_meta: n_head_kv        = 16
0.00.050.552 I print_meta: n_rot            = 32
0.00.050.552 I print_meta: n_swa            = 0
0.00.050.553 I print_meta: n_embd_head_k    = 128
0.00.050.553 I print_meta: n_embd_head_v    = 128
0.00.050.554 I print_meta: n_gqa            = 1
0.00.050.554 I print_meta: n_embd_k_gqa     = 2048
0.00.050.557 I print_meta: n_embd_v_gqa     = 2048
0.00.050.558 I print_meta: f_norm_eps       = 1.0e-05
0.00.050.558 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.559 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.559 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.561 I print_meta: f_logit_scale    = 0.0e+00
0.00.050.561 I print_meta: n_ff             = 8192
0.00.050.562 I print_meta: n_expert         = 0
0.00.050.562 I print_meta: n_expert_used    = 0
0.00.050.563 I print_meta: causal attn      = 1
0.00.050.564 I print_meta: pooling type     = 0
0.00.050.564 I print_meta: rope type        = 2
0.00.050.564 I print_meta: rope scaling     = linear
0.00.050.566 I print_meta: freq_base_train  = 10000.0
0.00.050.566 I print_meta: freq_scale_train = 1
0.00.050.567 I print_meta: n_ctx_orig_yarn  = 2048
0.00.050.567 I print_meta: rope_finetuned   = unknown
0.00.050.567 I print_meta: ssm_d_conv       = 0
0.00.050.567 I print_meta: ssm_d_inner      = 0
0.00.050.567 I print_meta: ssm_d_state      = 0
0.00.050.567 I print_meta: ssm_dt_rank      = 0
0.00.050.568 I print_meta: ssm_dt_b_c_rms   = 0
0.00.050.568 I print_meta: model type       = 1.4B
0.00.050.568 I print_meta: model ftype      = Q5_0
0.00.050.568 I print_meta: model params     = 1.41 B
0.00.050.570 I print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.570 I print_meta: general.name     = 1.4B
0.00.050.570 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.570 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.570 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.570 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.571 I print_meta: LF token         = 128 'Ä'
0.00.050.571 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.571 I print_meta: max token length = 1024
0.00.052.602 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.602 I llm_load_tensors: offloading output layer to GPU
0.00.052.603 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.613 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.614 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.532 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.533 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.533 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.533 I llama_new_context_with_model: n_batch       = 2048
0.00.053.533 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.533 I llama_new_context_with_model: flash_attn    = 0
0.00.053.534 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.534 I llama_new_context_with_model: freq_scale    = 1
0.00.053.534 I ggml_metal_init: allocating
0.00.053.537 I ggml_metal_init: found device: Apple M4
0.00.053.539 I ggml_metal_init: picking default device: Apple M4
0.00.054.160 I ggml_metal_init: using embedded metal library
0.00.056.481 I ggml_metal_init: GPU name:   Apple M4
0.00.056.482 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.483 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.483 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.483 I ggml_metal_init: simdgroup reduction   = true
0.00.056.484 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.484 I ggml_metal_init: has bfloat            = true
0.00.056.484 I ggml_metal_init: use bfloat            = true
0.00.056.484 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.486 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.187 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.050 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.055 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.081 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.128 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.129 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.130 I llama_new_context_with_model: graph nodes  = 967
0.00.086.130 I llama_new_context_with_model: graph splits = 2
0.00.086.132 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.273 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.274 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.987 I main: llama threadpool init, n_threads = 4
0.00.763.021 I 
0.00.763.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.043 I 
0.00.763.291 I sampler seed: 1234
0.00.763.297 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.341 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.353 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.353 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.553.052 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.553.052 I llama_perf_context_print:        load time =     753.29 ms
0.01.553.053 I llama_perf_context_print: prompt eval time =      47.70 ms /     7 tokens (    6.81 ms per token,   146.75 tokens per second)
0.01.553.055 I llama_perf_context_print:        eval time =     739.00 ms /    63 runs   (   11.73 ms per token,    85.25 tokens per second)
0.01.553.055 I llama_perf_context_print:       total time =     790.07 ms /    70 tokens
0.01.553.299 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.110s
sys	0m0.162s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.733 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.545 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.551 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.555 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.556 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.405 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.453 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.237 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.238 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.238 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.238 I llama_model_loader: - type  f32:  194 tensors
0.00.025.239 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.239 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.402 I load_vocab: special tokens cache size = 25
0.00.051.237 I load_vocab: token to piece cache size = 0.2984 MB
0.00.051.240 I print_meta: format           = GGUF V3 (latest)
0.00.051.240 I print_meta: arch             = gptneox
0.00.051.241 I print_meta: vocab type       = BPE
0.00.051.241 I print_meta: n_vocab          = 50304
0.00.051.241 I print_meta: n_merges         = 50009
0.00.051.241 I print_meta: vocab_only       = 0
0.00.051.241 I print_meta: n_ctx_train      = 2048
0.00.051.241 I print_meta: n_embd           = 2048
0.00.051.242 I print_meta: n_layer          = 24
0.00.051.244 I print_meta: n_head           = 16
0.00.051.245 I print_meta: n_head_kv        = 16
0.00.051.245 I print_meta: n_rot            = 32
0.00.051.245 I print_meta: n_swa            = 0
0.00.051.245 I print_meta: n_embd_head_k    = 128
0.00.051.245 I print_meta: n_embd_head_v    = 128
0.00.051.246 I print_meta: n_gqa            = 1
0.00.051.247 I print_meta: n_embd_k_gqa     = 2048
0.00.051.248 I print_meta: n_embd_v_gqa     = 2048
0.00.051.248 I print_meta: f_norm_eps       = 1.0e-05
0.00.051.249 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.250 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.250 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.252 I print_meta: f_logit_scale    = 0.0e+00
0.00.051.253 I print_meta: n_ff             = 8192
0.00.051.253 I print_meta: n_expert         = 0
0.00.051.253 I print_meta: n_expert_used    = 0
0.00.051.255 I print_meta: causal attn      = 1
0.00.051.256 I print_meta: pooling type     = 0
0.00.051.256 I print_meta: rope type        = 2
0.00.051.256 I print_meta: rope scaling     = linear
0.00.051.257 I print_meta: freq_base_train  = 10000.0
0.00.051.257 I print_meta: freq_scale_train = 1
0.00.051.257 I print_meta: n_ctx_orig_yarn  = 2048
0.00.051.258 I print_meta: rope_finetuned   = unknown
0.00.051.258 I print_meta: ssm_d_conv       = 0
0.00.051.258 I print_meta: ssm_d_inner      = 0
0.00.051.258 I print_meta: ssm_d_state      = 0
0.00.051.258 I print_meta: ssm_dt_rank      = 0
0.00.051.258 I print_meta: ssm_dt_b_c_rms   = 0
0.00.051.258 I print_meta: model type       = 1.4B
0.00.051.259 I print_meta: model ftype      = Q5_1
0.00.051.263 I print_meta: model params     = 1.41 B
0.00.051.263 I print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.263 I print_meta: general.name     = 1.4B
0.00.051.264 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.264 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.264 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.264 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.264 I print_meta: LF token         = 128 'Ä'
0.00.051.264 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.265 I print_meta: max token length = 1024
0.00.053.316 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.316 I llm_load_tensors: offloading output layer to GPU
0.00.053.317 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.327 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.328 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.232 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.233 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.233 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.233 I llama_new_context_with_model: n_batch       = 2048
0.00.054.234 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.234 I llama_new_context_with_model: flash_attn    = 0
0.00.054.234 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.234 I llama_new_context_with_model: freq_scale    = 1
0.00.054.235 I ggml_metal_init: allocating
0.00.054.238 I ggml_metal_init: found device: Apple M4
0.00.054.240 I ggml_metal_init: picking default device: Apple M4
0.00.054.835 I ggml_metal_init: using embedded metal library
0.00.057.127 I ggml_metal_init: GPU name:   Apple M4
0.00.057.128 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.128 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.129 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.129 I ggml_metal_init: simdgroup reduction   = true
0.00.057.131 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.131 I ggml_metal_init: has bfloat            = true
0.00.057.131 I ggml_metal_init: use bfloat            = true
0.00.057.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.132 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.766 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.069 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.074 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.092 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.068 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.069 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.069 I llama_new_context_with_model: graph nodes  = 967
0.00.087.070 I llama_new_context_with_model: graph splits = 2
0.00.087.072 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.201 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.573 I main: llama threadpool init, n_threads = 4
0.00.694.616 I 
0.00.694.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.636 I 
0.00.694.878 I sampler seed: 1234
0.00.694.882 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.896 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.897 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.897 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.534.006 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.534.007 I llama_perf_context_print:        load time =     685.84 ms
0.01.534.008 I llama_perf_context_print: prompt eval time =      42.31 ms /     7 tokens (    6.04 ms per token,   165.46 tokens per second)
0.01.534.010 I llama_perf_context_print:        eval time =     793.82 ms /    63 runs   (   12.60 ms per token,    79.36 tokens per second)
0.01.534.010 I llama_perf_context_print:       total time =     839.44 ms /    70 tokens
0.01.534.236 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.338 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.864 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.869 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.871 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.871 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.872 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.872 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.873 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.873 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.874 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.874 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.875 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.875 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.877 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.879 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.879 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.880 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.830 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.647 I llama_model_loader: - type  f32:  194 tensors
0.00.023.647 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.647 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.559 I load_vocab: special tokens cache size = 25
0.00.050.656 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.659 I print_meta: format           = GGUF V3 (latest)
0.00.050.659 I print_meta: arch             = gptneox
0.00.050.660 I print_meta: vocab type       = BPE
0.00.050.660 I print_meta: n_vocab          = 50304
0.00.050.660 I print_meta: n_merges         = 50009
0.00.050.660 I print_meta: vocab_only       = 0
0.00.050.660 I print_meta: n_ctx_train      = 2048
0.00.050.661 I print_meta: n_embd           = 2048
0.00.050.661 I print_meta: n_layer          = 24
0.00.050.664 I print_meta: n_head           = 16
0.00.050.666 I print_meta: n_head_kv        = 16
0.00.050.667 I print_meta: n_rot            = 32
0.00.050.667 I print_meta: n_swa            = 0
0.00.050.667 I print_meta: n_embd_head_k    = 128
0.00.050.667 I print_meta: n_embd_head_v    = 128
0.00.050.668 I print_meta: n_gqa            = 1
0.00.050.669 I print_meta: n_embd_k_gqa     = 2048
0.00.050.669 I print_meta: n_embd_v_gqa     = 2048
0.00.050.670 I print_meta: f_norm_eps       = 1.0e-05
0.00.050.670 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.670 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.670 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.671 I print_meta: f_logit_scale    = 0.0e+00
0.00.050.671 I print_meta: n_ff             = 8192
0.00.050.671 I print_meta: n_expert         = 0
0.00.050.672 I print_meta: n_expert_used    = 0
0.00.050.672 I print_meta: causal attn      = 1
0.00.050.672 I print_meta: pooling type     = 0
0.00.050.673 I print_meta: rope type        = 2
0.00.050.673 I print_meta: rope scaling     = linear
0.00.050.673 I print_meta: freq_base_train  = 10000.0
0.00.050.674 I print_meta: freq_scale_train = 1
0.00.050.674 I print_meta: n_ctx_orig_yarn  = 2048
0.00.050.674 I print_meta: rope_finetuned   = unknown
0.00.050.674 I print_meta: ssm_d_conv       = 0
0.00.050.674 I print_meta: ssm_d_inner      = 0
0.00.050.674 I print_meta: ssm_d_state      = 0
0.00.050.676 I print_meta: ssm_dt_rank      = 0
0.00.050.676 I print_meta: ssm_dt_b_c_rms   = 0
0.00.050.676 I print_meta: model type       = 1.4B
0.00.050.677 I print_meta: model ftype      = Q2_K - Medium
0.00.050.677 I print_meta: model params     = 1.41 B
0.00.050.678 I print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.678 I print_meta: general.name     = 1.4B
0.00.050.678 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.678 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.679 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.679 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.679 I print_meta: LF token         = 128 'Ä'
0.00.050.679 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.680 I print_meta: max token length = 1024
0.00.052.613 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.614 I llm_load_tensors: offloading output layer to GPU
0.00.052.614 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.625 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.626 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.502 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.502 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.502 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.503 I llama_new_context_with_model: n_batch       = 2048
0.00.053.503 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.503 I llama_new_context_with_model: flash_attn    = 0
0.00.053.503 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.504 I llama_new_context_with_model: freq_scale    = 1
0.00.053.504 I ggml_metal_init: allocating
0.00.053.508 I ggml_metal_init: found device: Apple M4
0.00.053.510 I ggml_metal_init: picking default device: Apple M4
0.00.054.103 I ggml_metal_init: using embedded metal library
0.00.056.455 I ggml_metal_init: GPU name:   Apple M4
0.00.056.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.457 I ggml_metal_init: simdgroup reduction   = true
0.00.056.458 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.458 I ggml_metal_init: has bfloat            = true
0.00.056.458 I ggml_metal_init: use bfloat            = true
0.00.056.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.459 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.388 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.153 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.162 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.183 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.207 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.208 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.209 I llama_new_context_with_model: graph nodes  = 967
0.00.086.209 I llama_new_context_with_model: graph splits = 2
0.00.086.211 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.329 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.090 I main: llama threadpool init, n_threads = 4
0.00.439.134 I 
0.00.439.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.439.163 I 
0.00.439.409 I sampler seed: 1234
0.00.439.417 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.439.432 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.439.434 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.439.434 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.115.414 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.115.415 I llama_perf_context_print:        load time =     429.75 ms
0.01.115.416 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.49 tokens per second)
0.01.115.416 I llama_perf_context_print:        eval time =     637.53 ms /    63 runs   (   10.12 ms per token,    98.82 tokens per second)
0.01.115.416 I llama_perf_context_print:       total time =     676.33 ms /    70 tokens
0.01.115.664 I ggml_metal_free: deallocating

real	0m1.134s
user	0m0.111s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.008.639 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.184 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.186 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.186 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.192 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.194 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.197 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.198 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.203 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.203 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.757 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.759 I llama_model_loader: - type  f32:  194 tensors
0.00.023.760 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.760 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.760 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.000 I load_vocab: special tokens cache size = 25
0.00.051.177 I load_vocab: token to piece cache size = 0.2984 MB
0.00.051.182 I print_meta: format           = GGUF V3 (latest)
0.00.051.182 I print_meta: arch             = gptneox
0.00.051.183 I print_meta: vocab type       = BPE
0.00.051.183 I print_meta: n_vocab          = 50304
0.00.051.183 I print_meta: n_merges         = 50009
0.00.051.183 I print_meta: vocab_only       = 0
0.00.051.186 I print_meta: n_ctx_train      = 2048
0.00.051.186 I print_meta: n_embd           = 2048
0.00.051.187 I print_meta: n_layer          = 24
0.00.051.191 I print_meta: n_head           = 16
0.00.051.192 I print_meta: n_head_kv        = 16
0.00.051.192 I print_meta: n_rot            = 32
0.00.051.192 I print_meta: n_swa            = 0
0.00.051.192 I print_meta: n_embd_head_k    = 128
0.00.051.192 I print_meta: n_embd_head_v    = 128
0.00.051.194 I print_meta: n_gqa            = 1
0.00.051.194 I print_meta: n_embd_k_gqa     = 2048
0.00.051.196 I print_meta: n_embd_v_gqa     = 2048
0.00.051.198 I print_meta: f_norm_eps       = 1.0e-05
0.00.051.198 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.198 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.199 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.199 I print_meta: f_logit_scale    = 0.0e+00
0.00.051.199 I print_meta: n_ff             = 8192
0.00.051.200 I print_meta: n_expert         = 0
0.00.051.200 I print_meta: n_expert_used    = 0
0.00.051.200 I print_meta: causal attn      = 1
0.00.051.200 I print_meta: pooling type     = 0
0.00.051.200 I print_meta: rope type        = 2
0.00.051.200 I print_meta: rope scaling     = linear
0.00.051.201 I print_meta: freq_base_train  = 10000.0
0.00.051.201 I print_meta: freq_scale_train = 1
0.00.051.203 I print_meta: n_ctx_orig_yarn  = 2048
0.00.051.203 I print_meta: rope_finetuned   = unknown
0.00.051.203 I print_meta: ssm_d_conv       = 0
0.00.051.203 I print_meta: ssm_d_inner      = 0
0.00.051.203 I print_meta: ssm_d_state      = 0
0.00.051.203 I print_meta: ssm_dt_rank      = 0
0.00.051.204 I print_meta: ssm_dt_b_c_rms   = 0
0.00.051.204 I print_meta: model type       = 1.4B
0.00.051.205 I print_meta: model ftype      = Q3_K - Medium
0.00.051.205 I print_meta: model params     = 1.41 B
0.00.051.205 I print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.205 I print_meta: general.name     = 1.4B
0.00.051.206 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.206 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.206 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.208 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.208 I print_meta: LF token         = 128 'Ä'
0.00.051.208 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.208 I print_meta: max token length = 1024
0.00.053.191 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.191 I llm_load_tensors: offloading output layer to GPU
0.00.053.191 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.202 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.203 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.143 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.144 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.144 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.144 I llama_new_context_with_model: n_batch       = 2048
0.00.054.145 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.145 I llama_new_context_with_model: flash_attn    = 0
0.00.054.147 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.147 I llama_new_context_with_model: freq_scale    = 1
0.00.054.148 I ggml_metal_init: allocating
0.00.054.155 I ggml_metal_init: found device: Apple M4
0.00.054.157 I ggml_metal_init: picking default device: Apple M4
0.00.054.776 I ggml_metal_init: using embedded metal library
0.00.057.147 I ggml_metal_init: GPU name:   Apple M4
0.00.057.149 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.149 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.150 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.150 I ggml_metal_init: simdgroup reduction   = true
0.00.057.150 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.150 I ggml_metal_init: has bfloat            = true
0.00.057.150 I ggml_metal_init: use bfloat            = true
0.00.057.151 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.404 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.896 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.902 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.928 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.975 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.977 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.978 I llama_new_context_with_model: graph nodes  = 967
0.00.087.978 I llama_new_context_with_model: graph splits = 2
0.00.087.980 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.122 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.122 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.550.253 I main: llama threadpool init, n_threads = 4
0.00.550.287 I 
0.00.550.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.550.321 I 
0.00.550.530 I sampler seed: 1234
0.00.550.534 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.550.571 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.550.573 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.550.573 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.294.919 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.294.919 I llama_perf_context_print:        load time =     541.61 ms
0.01.294.920 I llama_perf_context_print: prompt eval time =      40.63 ms /     7 tokens (    5.80 ms per token,   172.28 tokens per second)
0.01.294.921 I llama_perf_context_print:        eval time =     700.64 ms /    63 runs   (   11.12 ms per token,    89.92 tokens per second)
0.01.294.921 I llama_perf_context_print:       total time =     744.67 ms /    70 tokens
0.01.295.109 I ggml_metal_free: deallocating

real	0m1.311s
user	0m0.111s
sys	0m0.135s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.734 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.071 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.076 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.077 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.080 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.082 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.083 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.951 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.858 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.860 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.860 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.861 I llama_model_loader: - type  f32:  194 tensors
0.00.023.862 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.862 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.862 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.078 I load_vocab: special tokens cache size = 25
0.00.049.944 I load_vocab: token to piece cache size = 0.2984 MB
0.00.049.947 I print_meta: format           = GGUF V3 (latest)
0.00.049.947 I print_meta: arch             = gptneox
0.00.049.948 I print_meta: vocab type       = BPE
0.00.049.948 I print_meta: n_vocab          = 50304
0.00.049.948 I print_meta: n_merges         = 50009
0.00.049.948 I print_meta: vocab_only       = 0
0.00.049.948 I print_meta: n_ctx_train      = 2048
0.00.049.949 I print_meta: n_embd           = 2048
0.00.049.949 I print_meta: n_layer          = 24
0.00.049.951 I print_meta: n_head           = 16
0.00.049.952 I print_meta: n_head_kv        = 16
0.00.049.952 I print_meta: n_rot            = 32
0.00.049.953 I print_meta: n_swa            = 0
0.00.049.953 I print_meta: n_embd_head_k    = 128
0.00.049.953 I print_meta: n_embd_head_v    = 128
0.00.049.954 I print_meta: n_gqa            = 1
0.00.049.954 I print_meta: n_embd_k_gqa     = 2048
0.00.049.955 I print_meta: n_embd_v_gqa     = 2048
0.00.049.956 I print_meta: f_norm_eps       = 1.0e-05
0.00.049.956 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.956 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.956 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.956 I print_meta: f_logit_scale    = 0.0e+00
0.00.049.958 I print_meta: n_ff             = 8192
0.00.049.958 I print_meta: n_expert         = 0
0.00.049.960 I print_meta: n_expert_used    = 0
0.00.049.961 I print_meta: causal attn      = 1
0.00.049.962 I print_meta: pooling type     = 0
0.00.049.962 I print_meta: rope type        = 2
0.00.049.962 I print_meta: rope scaling     = linear
0.00.049.962 I print_meta: freq_base_train  = 10000.0
0.00.049.963 I print_meta: freq_scale_train = 1
0.00.049.963 I print_meta: n_ctx_orig_yarn  = 2048
0.00.049.963 I print_meta: rope_finetuned   = unknown
0.00.049.963 I print_meta: ssm_d_conv       = 0
0.00.049.963 I print_meta: ssm_d_inner      = 0
0.00.049.963 I print_meta: ssm_d_state      = 0
0.00.049.964 I print_meta: ssm_dt_rank      = 0
0.00.049.964 I print_meta: ssm_dt_b_c_rms   = 0
0.00.049.964 I print_meta: model type       = 1.4B
0.00.049.965 I print_meta: model ftype      = Q4_K - Medium
0.00.049.965 I print_meta: model params     = 1.41 B
0.00.049.965 I print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.966 I print_meta: general.name     = 1.4B
0.00.049.966 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.967 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.967 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.971 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.971 I print_meta: LF token         = 128 'Ä'
0.00.049.972 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.972 I print_meta: max token length = 1024
0.00.051.996 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.996 I llm_load_tensors: offloading output layer to GPU
0.00.051.996 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.007 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.008 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.920 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.921 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.921 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.922 I llama_new_context_with_model: n_batch       = 2048
0.00.052.922 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.922 I llama_new_context_with_model: flash_attn    = 0
0.00.052.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.923 I llama_new_context_with_model: freq_scale    = 1
0.00.052.923 I ggml_metal_init: allocating
0.00.052.926 I ggml_metal_init: found device: Apple M4
0.00.052.929 I ggml_metal_init: picking default device: Apple M4
0.00.053.532 I ggml_metal_init: using embedded metal library
0.00.055.843 I ggml_metal_init: GPU name:   Apple M4
0.00.055.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.845 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.846 I ggml_metal_init: simdgroup reduction   = true
0.00.055.846 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.847 I ggml_metal_init: has bfloat            = true
0.00.055.847 I ggml_metal_init: use bfloat            = true
0.00.055.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.848 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.544 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.390 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.396 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.416 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.369 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.370 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.370 I llama_new_context_with_model: graph nodes  = 967
0.00.085.371 I llama_new_context_with_model: graph splits = 2
0.00.085.373 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.511 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.511 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.695 I main: llama threadpool init, n_threads = 4
0.00.623.737 I 
0.00.623.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.774 I 
0.00.624.011 I sampler seed: 1234
0.00.624.015 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.624.058 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.624.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.624.074 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.375.694 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.01.375.695 I llama_perf_context_print:        load time =     614.95 ms
0.01.375.696 I llama_perf_context_print: prompt eval time =      47.10 ms /     7 tokens (    6.73 ms per token,   148.61 tokens per second)
0.01.375.696 I llama_perf_context_print:        eval time =     701.57 ms /    63 runs   (   11.14 ms per token,    89.80 tokens per second)
0.01.375.697 I llama_perf_context_print:       total time =     752.01 ms /    70 tokens
0.01.375.957 I ggml_metal_free: deallocating

real	0m1.393s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.362 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.430 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.300 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.300 I llama_model_loader: - type  f32:  194 tensors
0.00.025.301 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.301 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.596 I load_vocab: special tokens cache size = 25
0.00.051.707 I load_vocab: token to piece cache size = 0.2984 MB
0.00.051.710 I print_meta: format           = GGUF V3 (latest)
0.00.051.710 I print_meta: arch             = gptneox
0.00.051.711 I print_meta: vocab type       = BPE
0.00.051.711 I print_meta: n_vocab          = 50304
0.00.051.711 I print_meta: n_merges         = 50009
0.00.051.712 I print_meta: vocab_only       = 0
0.00.051.712 I print_meta: n_ctx_train      = 2048
0.00.051.712 I print_meta: n_embd           = 2048
0.00.051.712 I print_meta: n_layer          = 24
0.00.051.715 I print_meta: n_head           = 16
0.00.051.716 I print_meta: n_head_kv        = 16
0.00.051.716 I print_meta: n_rot            = 32
0.00.051.716 I print_meta: n_swa            = 0
0.00.051.718 I print_meta: n_embd_head_k    = 128
0.00.051.718 I print_meta: n_embd_head_v    = 128
0.00.051.719 I print_meta: n_gqa            = 1
0.00.051.720 I print_meta: n_embd_k_gqa     = 2048
0.00.051.722 I print_meta: n_embd_v_gqa     = 2048
0.00.051.723 I print_meta: f_norm_eps       = 1.0e-05
0.00.051.723 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.723 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.723 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.723 I print_meta: f_logit_scale    = 0.0e+00
0.00.051.724 I print_meta: n_ff             = 8192
0.00.051.724 I print_meta: n_expert         = 0
0.00.051.724 I print_meta: n_expert_used    = 0
0.00.051.726 I print_meta: causal attn      = 1
0.00.051.727 I print_meta: pooling type     = 0
0.00.051.727 I print_meta: rope type        = 2
0.00.051.727 I print_meta: rope scaling     = linear
0.00.051.728 I print_meta: freq_base_train  = 10000.0
0.00.051.728 I print_meta: freq_scale_train = 1
0.00.051.728 I print_meta: n_ctx_orig_yarn  = 2048
0.00.051.728 I print_meta: rope_finetuned   = unknown
0.00.051.728 I print_meta: ssm_d_conv       = 0
0.00.051.729 I print_meta: ssm_d_inner      = 0
0.00.051.729 I print_meta: ssm_d_state      = 0
0.00.051.729 I print_meta: ssm_dt_rank      = 0
0.00.051.729 I print_meta: ssm_dt_b_c_rms   = 0
0.00.051.729 I print_meta: model type       = 1.4B
0.00.051.734 I print_meta: model ftype      = Q5_K - Medium
0.00.051.734 I print_meta: model params     = 1.41 B
0.00.051.734 I print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.735 I print_meta: general.name     = 1.4B
0.00.051.735 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.735 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.735 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.735 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.736 I print_meta: LF token         = 128 'Ä'
0.00.051.736 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.736 I print_meta: max token length = 1024
0.00.053.795 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.796 I llm_load_tensors: offloading output layer to GPU
0.00.053.796 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.806 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.807 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.823 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.824 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.824 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.824 I llama_new_context_with_model: n_batch       = 2048
0.00.054.824 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.824 I llama_new_context_with_model: flash_attn    = 0
0.00.054.825 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.825 I llama_new_context_with_model: freq_scale    = 1
0.00.054.825 I ggml_metal_init: allocating
0.00.054.832 I ggml_metal_init: found device: Apple M4
0.00.054.835 I ggml_metal_init: picking default device: Apple M4
0.00.055.438 I ggml_metal_init: using embedded metal library
0.00.057.778 I ggml_metal_init: GPU name:   Apple M4
0.00.057.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.782 I ggml_metal_init: simdgroup reduction   = true
0.00.057.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.783 I ggml_metal_init: has bfloat            = true
0.00.057.783 I ggml_metal_init: use bfloat            = true
0.00.057.783 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.784 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.278 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.430 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.437 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.453 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.396 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.397 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.398 I llama_new_context_with_model: graph nodes  = 967
0.00.087.398 I llama_new_context_with_model: graph splits = 2
0.00.087.401 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.889 I main: llama threadpool init, n_threads = 4
0.00.699.924 I 
0.00.699.945 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.945 I 
0.00.700.184 I sampler seed: 1234
0.00.700.192 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.207 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.207 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.207 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.549.291 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.549.292 I llama_perf_context_print:        load time =     689.52 ms
0.01.549.292 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.79 tokens per second)
0.01.549.293 I llama_perf_context_print:        eval time =     794.56 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.549.293 I llama_perf_context_print:       total time =     849.40 ms /    70 tokens
0.01.549.533 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.645 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.732 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.734 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.734 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.735 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.736 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.737 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.737 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.737 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.738 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.738 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.743 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.743 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.743 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.704 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.540 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.542 I llama_model_loader: - type  f32:  194 tensors
0.00.024.542 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.673 I load_vocab: special tokens cache size = 25
0.00.050.554 I load_vocab: token to piece cache size = 0.2984 MB
0.00.050.557 I print_meta: format           = GGUF V3 (latest)
0.00.050.558 I print_meta: arch             = gptneox
0.00.050.558 I print_meta: vocab type       = BPE
0.00.050.558 I print_meta: n_vocab          = 50304
0.00.050.558 I print_meta: n_merges         = 50009
0.00.050.559 I print_meta: vocab_only       = 0
0.00.050.559 I print_meta: n_ctx_train      = 2048
0.00.050.559 I print_meta: n_embd           = 2048
0.00.050.559 I print_meta: n_layer          = 24
0.00.050.562 I print_meta: n_head           = 16
0.00.050.564 I print_meta: n_head_kv        = 16
0.00.050.564 I print_meta: n_rot            = 32
0.00.050.564 I print_meta: n_swa            = 0
0.00.050.564 I print_meta: n_embd_head_k    = 128
0.00.050.564 I print_meta: n_embd_head_v    = 128
0.00.050.565 I print_meta: n_gqa            = 1
0.00.050.566 I print_meta: n_embd_k_gqa     = 2048
0.00.050.566 I print_meta: n_embd_v_gqa     = 2048
0.00.050.567 I print_meta: f_norm_eps       = 1.0e-05
0.00.050.568 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.568 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.568 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.568 I print_meta: f_logit_scale    = 0.0e+00
0.00.050.569 I print_meta: n_ff             = 8192
0.00.050.569 I print_meta: n_expert         = 0
0.00.050.569 I print_meta: n_expert_used    = 0
0.00.050.569 I print_meta: causal attn      = 1
0.00.050.571 I print_meta: pooling type     = 0
0.00.050.573 I print_meta: rope type        = 2
0.00.050.573 I print_meta: rope scaling     = linear
0.00.050.573 I print_meta: freq_base_train  = 10000.0
0.00.050.574 I print_meta: freq_scale_train = 1
0.00.050.574 I print_meta: n_ctx_orig_yarn  = 2048
0.00.050.574 I print_meta: rope_finetuned   = unknown
0.00.050.574 I print_meta: ssm_d_conv       = 0
0.00.050.574 I print_meta: ssm_d_inner      = 0
0.00.050.574 I print_meta: ssm_d_state      = 0
0.00.050.575 I print_meta: ssm_dt_rank      = 0
0.00.050.575 I print_meta: ssm_dt_b_c_rms   = 0
0.00.050.575 I print_meta: model type       = 1.4B
0.00.050.575 I print_meta: model ftype      = Q6_K
0.00.050.580 I print_meta: model params     = 1.41 B
0.00.050.580 I print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.580 I print_meta: general.name     = 1.4B
0.00.050.581 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.581 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.581 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.581 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.582 I print_meta: LF token         = 128 'Ä'
0.00.050.582 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.582 I print_meta: max token length = 1024
0.00.052.668 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.669 I llm_load_tensors: offloading output layer to GPU
0.00.052.669 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.679 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.680 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.563 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.563 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.564 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.564 I llama_new_context_with_model: n_batch       = 2048
0.00.053.564 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.564 I llama_new_context_with_model: flash_attn    = 0
0.00.053.565 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.565 I llama_new_context_with_model: freq_scale    = 1
0.00.053.565 I ggml_metal_init: allocating
0.00.053.568 I ggml_metal_init: found device: Apple M4
0.00.053.570 I ggml_metal_init: picking default device: Apple M4
0.00.054.165 I ggml_metal_init: using embedded metal library
0.00.056.473 I ggml_metal_init: GPU name:   Apple M4
0.00.056.474 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.475 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.475 I ggml_metal_init: simdgroup reduction   = true
0.00.056.477 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.477 I ggml_metal_init: has bfloat            = true
0.00.056.477 I ggml_metal_init: use bfloat            = true
0.00.056.477 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.478 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.185 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.257 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.267 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.298 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.315 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.317 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.317 I llama_new_context_with_model: graph nodes  = 967
0.00.086.317 I llama_new_context_with_model: graph splits = 2
0.00.086.320 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.462 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.462 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.985 I main: llama threadpool init, n_threads = 4
0.00.738.027 I 
0.00.738.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.069 I 
0.00.738.310 I sampler seed: 1234
0.00.738.314 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.330 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.330 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.330 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.620.531 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.620.532 I llama_perf_context_print:        load time =     729.33 ms
0.01.620.533 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.78 tokens per second)
0.01.620.534 I llama_perf_context_print:        eval time =     824.74 ms /    63 runs   (   13.09 ms per token,    76.39 tokens per second)
0.01.620.535 I llama_perf_context_print:       total time =     882.55 ms /    70 tokens
0.01.620.731 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.109s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.607 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.979 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.050 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.069 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.071 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.072 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.080 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.081 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.081 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.082 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.088 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.339 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.340 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.341 I llama_model_loader: - type  f32:  194 tensors
0.00.054.341 I llama_model_loader: - type  f16:   98 tensors
0.00.083.936 I load_vocab: special tokens cache size = 25
0.00.091.154 I load_vocab: token to piece cache size = 0.2984 MB
0.00.091.157 I print_meta: format           = GGUF V3 (latest)
0.00.091.157 I print_meta: arch             = gptneox
0.00.091.157 I print_meta: vocab type       = BPE
0.00.091.158 I print_meta: n_vocab          = 50304
0.00.091.158 I print_meta: n_merges         = 50009
0.00.091.158 I print_meta: vocab_only       = 0
0.00.091.158 I print_meta: n_ctx_train      = 2048
0.00.091.158 I print_meta: n_embd           = 2048
0.00.091.158 I print_meta: n_layer          = 24
0.00.091.161 I print_meta: n_head           = 16
0.00.091.162 I print_meta: n_head_kv        = 16
0.00.091.162 I print_meta: n_rot            = 32
0.00.091.162 I print_meta: n_swa            = 0
0.00.091.162 I print_meta: n_embd_head_k    = 128
0.00.091.163 I print_meta: n_embd_head_v    = 128
0.00.091.163 I print_meta: n_gqa            = 1
0.00.091.164 I print_meta: n_embd_k_gqa     = 2048
0.00.091.164 I print_meta: n_embd_v_gqa     = 2048
0.00.091.165 I print_meta: f_norm_eps       = 1.0e-05
0.00.091.165 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.165 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.165 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.165 I print_meta: f_logit_scale    = 0.0e+00
0.00.091.166 I print_meta: n_ff             = 8192
0.00.091.166 I print_meta: n_expert         = 0
0.00.091.166 I print_meta: n_expert_used    = 0
0.00.091.166 I print_meta: causal attn      = 1
0.00.091.166 I print_meta: pooling type     = 0
0.00.091.167 I print_meta: rope type        = 2
0.00.091.167 I print_meta: rope scaling     = linear
0.00.091.167 I print_meta: freq_base_train  = 10000.0
0.00.091.167 I print_meta: freq_scale_train = 1
0.00.091.168 I print_meta: n_ctx_orig_yarn  = 2048
0.00.091.168 I print_meta: rope_finetuned   = unknown
0.00.091.168 I print_meta: ssm_d_conv       = 0
0.00.091.170 I print_meta: ssm_d_inner      = 0
0.00.091.170 I print_meta: ssm_d_state      = 0
0.00.091.170 I print_meta: ssm_dt_rank      = 0
0.00.091.171 I print_meta: ssm_dt_b_c_rms   = 0
0.00.091.171 I print_meta: model type       = 1.4B
0.00.091.171 I print_meta: model ftype      = all F32 (guessed)
0.00.091.172 I print_meta: model params     = 1.41 B
0.00.091.172 I print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.172 I print_meta: general.name     = 1.4B
0.00.091.172 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.173 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.174 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.174 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.177 I print_meta: LF token         = 128 'Ä'
0.00.091.177 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.178 I print_meta: max token length = 1024
0.00.093.679 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.679 I llm_load_tensors: offloading output layer to GPU
0.00.093.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.689 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.691 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.581 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.582 I llama_new_context_with_model: n_ctx         = 128
0.00.094.582 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.582 I llama_new_context_with_model: n_batch       = 128
0.00.094.582 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.583 I llama_new_context_with_model: flash_attn    = 0
0.00.094.583 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.583 I llama_new_context_with_model: freq_scale    = 1
0.00.094.583 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.584 I ggml_metal_init: allocating
0.00.094.587 I ggml_metal_init: found device: Apple M4
0.00.094.588 I ggml_metal_init: picking default device: Apple M4
0.00.095.176 I ggml_metal_init: using embedded metal library
0.00.097.710 I ggml_metal_init: GPU name:   Apple M4
0.00.097.712 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.712 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.713 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.713 I ggml_metal_init: simdgroup reduction   = true
0.00.097.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.713 I ggml_metal_init: has bfloat            = true
0.00.097.713 I ggml_metal_init: use bfloat            = true
0.00.097.714 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.714 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.991 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.313 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.315 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.329 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.199 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.200 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.201 I llama_new_context_with_model: graph nodes  = 967
0.00.109.201 I llama_new_context_with_model: graph splits = 2
0.00.109.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.202 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.174.641 I 
0.01.174.699 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.174.748 I perplexity: tokenizing the input ..
0.01.189.191 I perplexity: tokenization took 14.44 ms
0.01.189.198 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.309.955 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.311.309 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.311.324 I llama_perf_context_print:        load time =    1150.63 ms
0.01.311.325 I llama_perf_context_print: prompt eval time =     119.77 ms /   128 tokens (    0.94 ms per token,  1068.72 tokens per second)
0.01.311.325 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.311.326 I llama_perf_context_print:       total time =     136.69 ms /   129 tokens
0.01.311.682 I ggml_metal_free: deallocating

real	0m1.499s
user	0m0.121s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.676 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.712 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.712 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.714 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.599 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.599 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.600 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.601 I llama_model_loader: - type  f32:  194 tensors
0.00.024.601 I llama_model_loader: - type q8_0:   98 tensors
0.00.045.249 I load_vocab: special tokens cache size = 25
0.00.051.488 I load_vocab: token to piece cache size = 0.2984 MB
0.00.051.491 I print_meta: format           = GGUF V3 (latest)
0.00.051.491 I print_meta: arch             = gptneox
0.00.051.492 I print_meta: vocab type       = BPE
0.00.051.492 I print_meta: n_vocab          = 50304
0.00.051.492 I print_meta: n_merges         = 50009
0.00.051.492 I print_meta: vocab_only       = 0
0.00.051.492 I print_meta: n_ctx_train      = 2048
0.00.051.492 I print_meta: n_embd           = 2048
0.00.051.493 I print_meta: n_layer          = 24
0.00.051.496 I print_meta: n_head           = 16
0.00.051.497 I print_meta: n_head_kv        = 16
0.00.051.497 I print_meta: n_rot            = 32
0.00.051.497 I print_meta: n_swa            = 0
0.00.051.498 I print_meta: n_embd_head_k    = 128
0.00.051.498 I print_meta: n_embd_head_v    = 128
0.00.051.498 I print_meta: n_gqa            = 1
0.00.051.499 I print_meta: n_embd_k_gqa     = 2048
0.00.051.501 I print_meta: n_embd_v_gqa     = 2048
0.00.051.502 I print_meta: f_norm_eps       = 1.0e-05
0.00.051.502 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.502 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.502 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.502 I print_meta: f_logit_scale    = 0.0e+00
0.00.051.503 I print_meta: n_ff             = 8192
0.00.051.503 I print_meta: n_expert         = 0
0.00.051.505 I print_meta: n_expert_used    = 0
0.00.051.505 I print_meta: causal attn      = 1
0.00.051.505 I print_meta: pooling type     = 0
0.00.051.505 I print_meta: rope type        = 2
0.00.051.505 I print_meta: rope scaling     = linear
0.00.051.506 I print_meta: freq_base_train  = 10000.0
0.00.051.506 I print_meta: freq_scale_train = 1
0.00.051.506 I print_meta: n_ctx_orig_yarn  = 2048
0.00.051.506 I print_meta: rope_finetuned   = unknown
0.00.051.506 I print_meta: ssm_d_conv       = 0
0.00.051.506 I print_meta: ssm_d_inner      = 0
0.00.051.507 I print_meta: ssm_d_state      = 0
0.00.051.507 I print_meta: ssm_dt_rank      = 0
0.00.051.507 I print_meta: ssm_dt_b_c_rms   = 0
0.00.051.507 I print_meta: model type       = 1.4B
0.00.051.508 I print_meta: model ftype      = Q8_0
0.00.051.508 I print_meta: model params     = 1.41 B
0.00.051.508 I print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.051.509 I print_meta: general.name     = 1.4B
0.00.051.509 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.509 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.509 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.513 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.513 I print_meta: LF token         = 128 'Ä'
0.00.051.513 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.515 I print_meta: max token length = 1024
0.00.053.470 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.470 I llm_load_tensors: offloading output layer to GPU
0.00.053.470 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.481 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.053.482 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.054.382 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.383 I llama_new_context_with_model: n_ctx         = 128
0.00.054.383 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.383 I llama_new_context_with_model: n_batch       = 128
0.00.054.383 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.384 I llama_new_context_with_model: flash_attn    = 0
0.00.054.384 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.384 I llama_new_context_with_model: freq_scale    = 1
0.00.054.385 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.385 I ggml_metal_init: allocating
0.00.054.392 I ggml_metal_init: found device: Apple M4
0.00.054.395 I ggml_metal_init: picking default device: Apple M4
0.00.055.001 I ggml_metal_init: using embedded metal library
0.00.057.475 I ggml_metal_init: GPU name:   Apple M4
0.00.057.476 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.477 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.477 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.478 I ggml_metal_init: simdgroup reduction   = true
0.00.057.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.478 I ggml_metal_init: has bfloat            = true
0.00.057.478 I ggml_metal_init: use bfloat            = true
0.00.057.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.457 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.737 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.750 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.767 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.667 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.668 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.669 I llama_new_context_with_model: graph nodes  = 967
0.00.069.669 I llama_new_context_with_model: graph splits = 2
0.00.069.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.424 I 
0.00.933.508 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.542 I perplexity: tokenizing the input ..
0.00.950.693 I perplexity: tokenization took 17.145 ms
0.00.950.700 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.091.183 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.092.524 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.092.541 I llama_perf_context_print:        load time =     923.73 ms
0.01.092.542 I llama_perf_context_print: prompt eval time =     139.59 ms /   128 tokens (    1.09 ms per token,   916.99 tokens per second)
0.01.092.543 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.092.543 I llama_perf_context_print:       total time =     159.13 ms /   129 tokens
0.01.092.955 I ggml_metal_free: deallocating

real	0m1.108s
user	0m0.091s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.452 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.123 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.131 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.134 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.136 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.137 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.139 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.947 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.770 I llama_model_loader: - type  f32:  194 tensors
0.00.025.770 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.771 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.107 I load_vocab: special tokens cache size = 25
0.00.054.132 I load_vocab: token to piece cache size = 0.2984 MB
0.00.054.135 I print_meta: format           = GGUF V3 (latest)
0.00.054.136 I print_meta: arch             = gptneox
0.00.054.136 I print_meta: vocab type       = BPE
0.00.054.136 I print_meta: n_vocab          = 50304
0.00.054.136 I print_meta: n_merges         = 50009
0.00.054.137 I print_meta: vocab_only       = 0
0.00.054.137 I print_meta: n_ctx_train      = 2048
0.00.054.137 I print_meta: n_embd           = 2048
0.00.054.137 I print_meta: n_layer          = 24
0.00.054.141 I print_meta: n_head           = 16
0.00.054.144 I print_meta: n_head_kv        = 16
0.00.054.144 I print_meta: n_rot            = 32
0.00.054.144 I print_meta: n_swa            = 0
0.00.054.145 I print_meta: n_embd_head_k    = 128
0.00.054.145 I print_meta: n_embd_head_v    = 128
0.00.054.145 I print_meta: n_gqa            = 1
0.00.054.146 I print_meta: n_embd_k_gqa     = 2048
0.00.054.147 I print_meta: n_embd_v_gqa     = 2048
0.00.054.147 I print_meta: f_norm_eps       = 1.0e-05
0.00.054.148 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.148 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.148 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.148 I print_meta: f_logit_scale    = 0.0e+00
0.00.054.149 I print_meta: n_ff             = 8192
0.00.054.149 I print_meta: n_expert         = 0
0.00.054.149 I print_meta: n_expert_used    = 0
0.00.054.149 I print_meta: causal attn      = 1
0.00.054.149 I print_meta: pooling type     = 0
0.00.054.150 I print_meta: rope type        = 2
0.00.054.150 I print_meta: rope scaling     = linear
0.00.054.150 I print_meta: freq_base_train  = 10000.0
0.00.054.151 I print_meta: freq_scale_train = 1
0.00.054.151 I print_meta: n_ctx_orig_yarn  = 2048
0.00.054.151 I print_meta: rope_finetuned   = unknown
0.00.054.151 I print_meta: ssm_d_conv       = 0
0.00.054.151 I print_meta: ssm_d_inner      = 0
0.00.054.151 I print_meta: ssm_d_state      = 0
0.00.054.151 I print_meta: ssm_dt_rank      = 0
0.00.054.152 I print_meta: ssm_dt_b_c_rms   = 0
0.00.054.152 I print_meta: model type       = 1.4B
0.00.054.152 I print_meta: model ftype      = Q4_0
0.00.054.153 I print_meta: model params     = 1.41 B
0.00.054.153 I print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.153 I print_meta: general.name     = 1.4B
0.00.054.154 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.154 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.154 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.154 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.154 I print_meta: LF token         = 128 'Ä'
0.00.054.155 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.155 I print_meta: max token length = 1024
0.00.056.319 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.319 I llm_load_tensors: offloading output layer to GPU
0.00.056.320 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.331 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.332 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.332 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.333 I llama_new_context_with_model: n_ctx         = 128
0.00.057.334 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.057.334 I llama_new_context_with_model: n_batch       = 128
0.00.057.334 I llama_new_context_with_model: n_ubatch      = 128
0.00.057.334 I llama_new_context_with_model: flash_attn    = 0
0.00.057.335 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.335 I llama_new_context_with_model: freq_scale    = 1
0.00.057.335 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.336 I ggml_metal_init: allocating
0.00.057.344 I ggml_metal_init: found device: Apple M4
0.00.057.347 I ggml_metal_init: picking default device: Apple M4
0.00.058.001 I ggml_metal_init: using embedded metal library
0.00.060.575 I ggml_metal_init: GPU name:   Apple M4
0.00.060.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.578 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.578 I ggml_metal_init: simdgroup reduction   = true
0.00.060.579 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.579 I ggml_metal_init: has bfloat            = true
0.00.060.579 I ggml_metal_init: use bfloat            = true
0.00.060.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.356 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.074.604 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.614 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.637 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.075.490 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.075.491 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.075.491 I llama_new_context_with_model: graph nodes  = 967
0.00.075.491 I llama_new_context_with_model: graph splits = 2
0.00.075.493 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.075.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.686 I 
0.00.617.727 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.617.740 I perplexity: tokenizing the input ..
0.00.625.856 I perplexity: tokenization took 8.114 ms
0.00.625.860 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.847 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.750.108 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.750.125 I llama_perf_context_print:        load time =     606.23 ms
0.00.750.126 I llama_perf_context_print: prompt eval time =     122.73 ms /   128 tokens (    0.96 ms per token,  1042.94 tokens per second)
0.00.750.127 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.129 I llama_perf_context_print:       total time =     132.44 ms /   129 tokens
0.00.750.636 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.081s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.384 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.021.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.582 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.582 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.582 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.583 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.584 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.584 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.584 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.584 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.587 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.225 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.130 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.131 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.131 I llama_model_loader: - type  f32:  194 tensors
0.00.030.132 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.132 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.467 I load_vocab: special tokens cache size = 25
0.00.056.290 I load_vocab: token to piece cache size = 0.2984 MB
0.00.056.293 I print_meta: format           = GGUF V3 (latest)
0.00.056.294 I print_meta: arch             = gptneox
0.00.056.294 I print_meta: vocab type       = BPE
0.00.056.294 I print_meta: n_vocab          = 50304
0.00.056.294 I print_meta: n_merges         = 50009
0.00.056.294 I print_meta: vocab_only       = 0
0.00.056.295 I print_meta: n_ctx_train      = 2048
0.00.056.295 I print_meta: n_embd           = 2048
0.00.056.295 I print_meta: n_layer          = 24
0.00.056.298 I print_meta: n_head           = 16
0.00.056.299 I print_meta: n_head_kv        = 16
0.00.056.299 I print_meta: n_rot            = 32
0.00.056.299 I print_meta: n_swa            = 0
0.00.056.299 I print_meta: n_embd_head_k    = 128
0.00.056.300 I print_meta: n_embd_head_v    = 128
0.00.056.300 I print_meta: n_gqa            = 1
0.00.056.301 I print_meta: n_embd_k_gqa     = 2048
0.00.056.302 I print_meta: n_embd_v_gqa     = 2048
0.00.056.302 I print_meta: f_norm_eps       = 1.0e-05
0.00.056.303 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.303 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.303 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.303 I print_meta: f_logit_scale    = 0.0e+00
0.00.056.304 I print_meta: n_ff             = 8192
0.00.056.304 I print_meta: n_expert         = 0
0.00.056.304 I print_meta: n_expert_used    = 0
0.00.056.305 I print_meta: causal attn      = 1
0.00.056.305 I print_meta: pooling type     = 0
0.00.056.305 I print_meta: rope type        = 2
0.00.056.305 I print_meta: rope scaling     = linear
0.00.056.307 I print_meta: freq_base_train  = 10000.0
0.00.056.309 I print_meta: freq_scale_train = 1
0.00.056.309 I print_meta: n_ctx_orig_yarn  = 2048
0.00.056.310 I print_meta: rope_finetuned   = unknown
0.00.056.310 I print_meta: ssm_d_conv       = 0
0.00.056.310 I print_meta: ssm_d_inner      = 0
0.00.056.310 I print_meta: ssm_d_state      = 0
0.00.056.310 I print_meta: ssm_dt_rank      = 0
0.00.056.310 I print_meta: ssm_dt_b_c_rms   = 0
0.00.056.311 I print_meta: model type       = 1.4B
0.00.056.311 I print_meta: model ftype      = Q4_1
0.00.056.311 I print_meta: model params     = 1.41 B
0.00.056.312 I print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.056.312 I print_meta: general.name     = 1.4B
0.00.056.313 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.313 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.313 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.313 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.313 I print_meta: LF token         = 128 'Ä'
0.00.056.314 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.314 I print_meta: max token length = 1024
0.00.058.145 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.145 I llm_load_tensors: offloading output layer to GPU
0.00.058.145 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.150 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.058.151 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.059.135 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.136 I llama_new_context_with_model: n_ctx         = 128
0.00.059.136 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.059.136 I llama_new_context_with_model: n_batch       = 128
0.00.059.136 I llama_new_context_with_model: n_ubatch      = 128
0.00.059.136 I llama_new_context_with_model: flash_attn    = 0
0.00.059.137 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.137 I llama_new_context_with_model: freq_scale    = 1
0.00.059.137 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.059.138 I ggml_metal_init: allocating
0.00.059.141 I ggml_metal_init: found device: Apple M4
0.00.059.143 I ggml_metal_init: picking default device: Apple M4
0.00.059.733 I ggml_metal_init: using embedded metal library
0.00.062.072 I ggml_metal_init: GPU name:   Apple M4
0.00.062.074 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.075 I ggml_metal_init: simdgroup reduction   = true
0.00.062.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.075 I ggml_metal_init: has bfloat            = true
0.00.062.075 I ggml_metal_init: use bfloat            = true
0.00.062.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.658 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.916 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.918 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.931 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.808 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.809 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.809 I llama_new_context_with_model: graph nodes  = 967
0.00.073.809 I llama_new_context_with_model: graph splits = 2
0.00.073.811 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.608 I 
0.00.765.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.663 I perplexity: tokenizing the input ..
0.00.773.922 I perplexity: tokenization took 8.258 ms
0.00.773.926 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.896.752 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.897.915 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.897.931 I llama_perf_context_print:        load time =     756.22 ms
0.00.897.932 I llama_perf_context_print: prompt eval time =     122.60 ms /   128 tokens (    0.96 ms per token,  1044.05 tokens per second)
0.00.897.933 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.934 I llama_perf_context_print:       total time =     132.32 ms /   129 tokens
0.00.898.478 I ggml_metal_free: deallocating

real	0m0.912s
user	0m0.078s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.020 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.765 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.769 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.770 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.771 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.773 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.773 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.773 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.774 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.774 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.774 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.775 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.775 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.775 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.777 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.777 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.778 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.668 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.497 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.498 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.498 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.498 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.499 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.499 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.499 I llama_model_loader: - type  f32:  194 tensors
0.00.026.500 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.500 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.283 I load_vocab: special tokens cache size = 25
0.00.053.401 I load_vocab: token to piece cache size = 0.2984 MB
0.00.053.404 I print_meta: format           = GGUF V3 (latest)
0.00.053.404 I print_meta: arch             = gptneox
0.00.053.405 I print_meta: vocab type       = BPE
0.00.053.405 I print_meta: n_vocab          = 50304
0.00.053.405 I print_meta: n_merges         = 50009
0.00.053.405 I print_meta: vocab_only       = 0
0.00.053.405 I print_meta: n_ctx_train      = 2048
0.00.053.405 I print_meta: n_embd           = 2048
0.00.053.406 I print_meta: n_layer          = 24
0.00.053.408 I print_meta: n_head           = 16
0.00.053.409 I print_meta: n_head_kv        = 16
0.00.053.409 I print_meta: n_rot            = 32
0.00.053.409 I print_meta: n_swa            = 0
0.00.053.409 I print_meta: n_embd_head_k    = 128
0.00.053.409 I print_meta: n_embd_head_v    = 128
0.00.053.410 I print_meta: n_gqa            = 1
0.00.053.411 I print_meta: n_embd_k_gqa     = 2048
0.00.053.411 I print_meta: n_embd_v_gqa     = 2048
0.00.053.412 I print_meta: f_norm_eps       = 1.0e-05
0.00.053.412 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.413 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.413 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.413 I print_meta: f_logit_scale    = 0.0e+00
0.00.053.413 I print_meta: n_ff             = 8192
0.00.053.414 I print_meta: n_expert         = 0
0.00.053.414 I print_meta: n_expert_used    = 0
0.00.053.414 I print_meta: causal attn      = 1
0.00.053.414 I print_meta: pooling type     = 0
0.00.053.414 I print_meta: rope type        = 2
0.00.053.414 I print_meta: rope scaling     = linear
0.00.053.416 I print_meta: freq_base_train  = 10000.0
0.00.053.417 I print_meta: freq_scale_train = 1
0.00.053.417 I print_meta: n_ctx_orig_yarn  = 2048
0.00.053.417 I print_meta: rope_finetuned   = unknown
0.00.053.417 I print_meta: ssm_d_conv       = 0
0.00.053.419 I print_meta: ssm_d_inner      = 0
0.00.053.419 I print_meta: ssm_d_state      = 0
0.00.053.419 I print_meta: ssm_dt_rank      = 0
0.00.053.420 I print_meta: ssm_dt_b_c_rms   = 0
0.00.053.420 I print_meta: model type       = 1.4B
0.00.053.420 I print_meta: model ftype      = Q5_0
0.00.053.421 I print_meta: model params     = 1.41 B
0.00.053.421 I print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.421 I print_meta: general.name     = 1.4B
0.00.053.422 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.423 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.423 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.423 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.424 I print_meta: LF token         = 128 'Ä'
0.00.053.424 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.424 I print_meta: max token length = 1024
0.00.055.436 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.436 I llm_load_tensors: offloading output layer to GPU
0.00.055.436 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.447 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.448 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.324 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.325 I llama_new_context_with_model: n_ctx         = 128
0.00.056.325 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.325 I llama_new_context_with_model: n_batch       = 128
0.00.056.325 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.325 I llama_new_context_with_model: flash_attn    = 0
0.00.056.326 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.326 I llama_new_context_with_model: freq_scale    = 1
0.00.056.327 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.327 I ggml_metal_init: allocating
0.00.056.334 I ggml_metal_init: found device: Apple M4
0.00.056.336 I ggml_metal_init: picking default device: Apple M4
0.00.056.917 I ggml_metal_init: using embedded metal library
0.00.059.225 I ggml_metal_init: GPU name:   Apple M4
0.00.059.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.228 I ggml_metal_init: simdgroup reduction   = true
0.00.059.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.228 I ggml_metal_init: has bfloat            = true
0.00.059.228 I ggml_metal_init: use bfloat            = true
0.00.059.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.601 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.816 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.819 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.834 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.723 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.724 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.725 I llama_new_context_with_model: graph nodes  = 967
0.00.070.725 I llama_new_context_with_model: graph splits = 2
0.00.070.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.110 I 
0.00.739.141 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.158 I perplexity: tokenizing the input ..
0.00.747.012 I perplexity: tokenization took 7.852 ms
0.00.747.015 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.882.127 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.883.288 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.883.307 I llama_perf_context_print:        load time =     729.08 ms
0.00.883.308 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   948.96 tokens per second)
0.00.883.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.883.309 I llama_perf_context_print:       total time =     144.20 ms /   129 tokens
0.00.883.838 I ggml_metal_free: deallocating

real	0m0.898s
user	0m0.078s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.856 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.889 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.895 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.895 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.896 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.896 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.896 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.899 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.899 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.899 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.903 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.908 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.909 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.909 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.585 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.607 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.414 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.415 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.415 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.416 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.416 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.416 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.417 I llama_model_loader: - type  f32:  194 tensors
0.00.024.417 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.418 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.361 I load_vocab: special tokens cache size = 25
0.00.051.188 I load_vocab: token to piece cache size = 0.2984 MB
0.00.051.191 I print_meta: format           = GGUF V3 (latest)
0.00.051.191 I print_meta: arch             = gptneox
0.00.051.192 I print_meta: vocab type       = BPE
0.00.051.192 I print_meta: n_vocab          = 50304
0.00.051.192 I print_meta: n_merges         = 50009
0.00.051.192 I print_meta: vocab_only       = 0
0.00.051.192 I print_meta: n_ctx_train      = 2048
0.00.051.193 I print_meta: n_embd           = 2048
0.00.051.193 I print_meta: n_layer          = 24
0.00.051.195 I print_meta: n_head           = 16
0.00.051.196 I print_meta: n_head_kv        = 16
0.00.051.196 I print_meta: n_rot            = 32
0.00.051.197 I print_meta: n_swa            = 0
0.00.051.197 I print_meta: n_embd_head_k    = 128
0.00.051.197 I print_meta: n_embd_head_v    = 128
0.00.051.198 I print_meta: n_gqa            = 1
0.00.051.198 I print_meta: n_embd_k_gqa     = 2048
0.00.051.199 I print_meta: n_embd_v_gqa     = 2048
0.00.051.200 I print_meta: f_norm_eps       = 1.0e-05
0.00.051.200 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.201 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.201 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.201 I print_meta: f_logit_scale    = 0.0e+00
0.00.051.201 I print_meta: n_ff             = 8192
0.00.051.202 I print_meta: n_expert         = 0
0.00.051.202 I print_meta: n_expert_used    = 0
0.00.051.202 I print_meta: causal attn      = 1
0.00.051.202 I print_meta: pooling type     = 0
0.00.051.202 I print_meta: rope type        = 2
0.00.051.203 I print_meta: rope scaling     = linear
0.00.051.204 I print_meta: freq_base_train  = 10000.0
0.00.051.206 I print_meta: freq_scale_train = 1
0.00.051.206 I print_meta: n_ctx_orig_yarn  = 2048
0.00.051.206 I print_meta: rope_finetuned   = unknown
0.00.051.207 I print_meta: ssm_d_conv       = 0
0.00.051.207 I print_meta: ssm_d_inner      = 0
0.00.051.207 I print_meta: ssm_d_state      = 0
0.00.051.207 I print_meta: ssm_dt_rank      = 0
0.00.051.207 I print_meta: ssm_dt_b_c_rms   = 0
0.00.051.207 I print_meta: model type       = 1.4B
0.00.051.208 I print_meta: model ftype      = Q5_1
0.00.051.208 I print_meta: model params     = 1.41 B
0.00.051.213 I print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.213 I print_meta: general.name     = 1.4B
0.00.051.213 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.213 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.213 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.213 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.214 I print_meta: LF token         = 128 'Ä'
0.00.051.214 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.215 I print_meta: max token length = 1024
0.00.053.214 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.215 I llm_load_tensors: offloading output layer to GPU
0.00.053.215 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.225 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.226 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.136 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.137 I llama_new_context_with_model: n_ctx         = 128
0.00.054.137 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.137 I llama_new_context_with_model: n_batch       = 128
0.00.054.138 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.138 I llama_new_context_with_model: flash_attn    = 0
0.00.054.138 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.138 I llama_new_context_with_model: freq_scale    = 1
0.00.054.139 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.139 I ggml_metal_init: allocating
0.00.054.147 I ggml_metal_init: found device: Apple M4
0.00.054.149 I ggml_metal_init: picking default device: Apple M4
0.00.054.713 I ggml_metal_init: using embedded metal library
0.00.057.002 I ggml_metal_init: GPU name:   Apple M4
0.00.057.003 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.004 I ggml_metal_init: simdgroup reduction   = true
0.00.057.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.004 I ggml_metal_init: has bfloat            = true
0.00.057.004 I ggml_metal_init: use bfloat            = true
0.00.057.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.316 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.574 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.588 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.436 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.437 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.438 I llama_new_context_with_model: graph nodes  = 967
0.00.068.438 I llama_new_context_with_model: graph splits = 2
0.00.068.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.090 I 
0.00.701.120 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.135 I perplexity: tokenizing the input ..
0.00.708.956 I perplexity: tokenization took 7.82 ms
0.00.708.960 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.963 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.845.215 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.845.233 I llama_perf_context_print:        load time =     692.23 ms
0.00.845.235 I llama_perf_context_print: prompt eval time =     134.78 ms /   128 tokens (    1.05 ms per token,   949.73 tokens per second)
0.00.845.240 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.240 I llama_perf_context_print:       total time =     144.14 ms /   129 tokens
0.00.845.785 I ggml_metal_free: deallocating

real	0m0.859s
user	0m0.078s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.713 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.138 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.138 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.138 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.139 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.140 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.140 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.140 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.141 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.141 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.142 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.145 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.145 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.063 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.914 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.914 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.915 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.916 I llama_model_loader: - type  f32:  194 tensors
0.00.025.916 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.916 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.916 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.949 I load_vocab: special tokens cache size = 25
0.00.053.224 I load_vocab: token to piece cache size = 0.2984 MB
0.00.053.230 I print_meta: format           = GGUF V3 (latest)
0.00.053.230 I print_meta: arch             = gptneox
0.00.053.230 I print_meta: vocab type       = BPE
0.00.053.230 I print_meta: n_vocab          = 50304
0.00.053.231 I print_meta: n_merges         = 50009
0.00.053.231 I print_meta: vocab_only       = 0
0.00.053.231 I print_meta: n_ctx_train      = 2048
0.00.053.231 I print_meta: n_embd           = 2048
0.00.053.232 I print_meta: n_layer          = 24
0.00.053.234 I print_meta: n_head           = 16
0.00.053.235 I print_meta: n_head_kv        = 16
0.00.053.235 I print_meta: n_rot            = 32
0.00.053.237 I print_meta: n_swa            = 0
0.00.053.237 I print_meta: n_embd_head_k    = 128
0.00.053.239 I print_meta: n_embd_head_v    = 128
0.00.053.239 I print_meta: n_gqa            = 1
0.00.053.240 I print_meta: n_embd_k_gqa     = 2048
0.00.053.241 I print_meta: n_embd_v_gqa     = 2048
0.00.053.241 I print_meta: f_norm_eps       = 1.0e-05
0.00.053.242 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.242 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.242 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.242 I print_meta: f_logit_scale    = 0.0e+00
0.00.053.243 I print_meta: n_ff             = 8192
0.00.053.243 I print_meta: n_expert         = 0
0.00.053.243 I print_meta: n_expert_used    = 0
0.00.053.248 I print_meta: causal attn      = 1
0.00.053.248 I print_meta: pooling type     = 0
0.00.053.248 I print_meta: rope type        = 2
0.00.053.250 I print_meta: rope scaling     = linear
0.00.053.251 I print_meta: freq_base_train  = 10000.0
0.00.053.251 I print_meta: freq_scale_train = 1
0.00.053.252 I print_meta: n_ctx_orig_yarn  = 2048
0.00.053.252 I print_meta: rope_finetuned   = unknown
0.00.053.252 I print_meta: ssm_d_conv       = 0
0.00.053.252 I print_meta: ssm_d_inner      = 0
0.00.053.252 I print_meta: ssm_d_state      = 0
0.00.053.252 I print_meta: ssm_dt_rank      = 0
0.00.053.252 I print_meta: ssm_dt_b_c_rms   = 0
0.00.053.253 I print_meta: model type       = 1.4B
0.00.053.253 I print_meta: model ftype      = Q2_K - Medium
0.00.053.253 I print_meta: model params     = 1.41 B
0.00.053.254 I print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.053.254 I print_meta: general.name     = 1.4B
0.00.053.254 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.254 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.259 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.263 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.264 I print_meta: LF token         = 128 'Ä'
0.00.053.265 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.265 I print_meta: max token length = 1024
0.00.055.052 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.052 I llm_load_tensors: offloading output layer to GPU
0.00.055.052 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.058 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.058 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.068 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.070 I llama_new_context_with_model: n_ctx         = 128
0.00.056.070 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.070 I llama_new_context_with_model: n_batch       = 128
0.00.056.070 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.070 I llama_new_context_with_model: flash_attn    = 0
0.00.056.071 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.071 I llama_new_context_with_model: freq_scale    = 1
0.00.056.071 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.072 I ggml_metal_init: allocating
0.00.056.077 I ggml_metal_init: found device: Apple M4
0.00.056.079 I ggml_metal_init: picking default device: Apple M4
0.00.056.668 I ggml_metal_init: using embedded metal library
0.00.059.002 I ggml_metal_init: GPU name:   Apple M4
0.00.059.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.005 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.005 I ggml_metal_init: simdgroup reduction   = true
0.00.059.005 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.005 I ggml_metal_init: has bfloat            = true
0.00.059.005 I ggml_metal_init: use bfloat            = true
0.00.059.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.008 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.725 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.985 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.987 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.001 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.828 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.829 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.830 I llama_new_context_with_model: graph nodes  = 967
0.00.070.830 I llama_new_context_with_model: graph splits = 2
0.00.070.831 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.831 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.467.132 I 
0.00.467.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.467.204 I perplexity: tokenizing the input ..
0.00.475.432 I perplexity: tokenization took 8.226 ms
0.00.475.435 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.608.359 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.609.534 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.609.555 I llama_perf_context_print:        load time =     457.41 ms
0.00.609.556 I llama_perf_context_print: prompt eval time =     132.70 ms /   128 tokens (    1.04 ms per token,   964.60 tokens per second)
0.00.609.557 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.609.557 I llama_perf_context_print:       total time =     142.42 ms /   129 tokens
0.00.610.076 I ggml_metal_free: deallocating

real	0m0.625s
user	0m0.080s
sys	0m0.074s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.827 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.020.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.359 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.360 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.364 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.207 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.014 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.014 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.014 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.015 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.015 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.029.015 I llama_model_loader: - type  f32:  194 tensors
0.00.029.015 I llama_model_loader: - type q3_K:   25 tensors
0.00.029.016 I llama_model_loader: - type q4_K:   71 tensors
0.00.029.016 I llama_model_loader: - type q5_K:    1 tensors
0.00.029.016 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.979 I load_vocab: special tokens cache size = 25
0.00.055.868 I load_vocab: token to piece cache size = 0.2984 MB
0.00.055.870 I print_meta: format           = GGUF V3 (latest)
0.00.055.871 I print_meta: arch             = gptneox
0.00.055.871 I print_meta: vocab type       = BPE
0.00.055.871 I print_meta: n_vocab          = 50304
0.00.055.871 I print_meta: n_merges         = 50009
0.00.055.872 I print_meta: vocab_only       = 0
0.00.055.872 I print_meta: n_ctx_train      = 2048
0.00.055.872 I print_meta: n_embd           = 2048
0.00.055.872 I print_meta: n_layer          = 24
0.00.055.875 I print_meta: n_head           = 16
0.00.055.876 I print_meta: n_head_kv        = 16
0.00.055.876 I print_meta: n_rot            = 32
0.00.055.876 I print_meta: n_swa            = 0
0.00.055.876 I print_meta: n_embd_head_k    = 128
0.00.055.876 I print_meta: n_embd_head_v    = 128
0.00.055.879 I print_meta: n_gqa            = 1
0.00.055.880 I print_meta: n_embd_k_gqa     = 2048
0.00.055.881 I print_meta: n_embd_v_gqa     = 2048
0.00.055.882 I print_meta: f_norm_eps       = 1.0e-05
0.00.055.882 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.882 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.882 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.882 I print_meta: f_logit_scale    = 0.0e+00
0.00.055.883 I print_meta: n_ff             = 8192
0.00.055.883 I print_meta: n_expert         = 0
0.00.055.884 I print_meta: n_expert_used    = 0
0.00.055.884 I print_meta: causal attn      = 1
0.00.055.884 I print_meta: pooling type     = 0
0.00.055.884 I print_meta: rope type        = 2
0.00.055.884 I print_meta: rope scaling     = linear
0.00.055.885 I print_meta: freq_base_train  = 10000.0
0.00.055.885 I print_meta: freq_scale_train = 1
0.00.055.885 I print_meta: n_ctx_orig_yarn  = 2048
0.00.055.885 I print_meta: rope_finetuned   = unknown
0.00.055.886 I print_meta: ssm_d_conv       = 0
0.00.055.886 I print_meta: ssm_d_inner      = 0
0.00.055.886 I print_meta: ssm_d_state      = 0
0.00.055.886 I print_meta: ssm_dt_rank      = 0
0.00.055.886 I print_meta: ssm_dt_b_c_rms   = 0
0.00.055.886 I print_meta: model type       = 1.4B
0.00.055.887 I print_meta: model ftype      = Q3_K - Medium
0.00.055.887 I print_meta: model params     = 1.41 B
0.00.055.888 I print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.055.888 I print_meta: general.name     = 1.4B
0.00.055.888 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.888 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.889 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.889 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.889 I print_meta: LF token         = 128 'Ä'
0.00.055.889 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.889 I print_meta: max token length = 1024
0.00.057.815 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.816 I llm_load_tensors: offloading output layer to GPU
0.00.057.816 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.826 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.057.827 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.058.730 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.730 I llama_new_context_with_model: n_ctx         = 128
0.00.058.730 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.058.731 I llama_new_context_with_model: n_batch       = 128
0.00.058.731 I llama_new_context_with_model: n_ubatch      = 128
0.00.058.731 I llama_new_context_with_model: flash_attn    = 0
0.00.058.731 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.731 I llama_new_context_with_model: freq_scale    = 1
0.00.058.732 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.732 I ggml_metal_init: allocating
0.00.058.736 I ggml_metal_init: found device: Apple M4
0.00.058.738 I ggml_metal_init: picking default device: Apple M4
0.00.059.313 I ggml_metal_init: using embedded metal library
0.00.061.635 I ggml_metal_init: GPU name:   Apple M4
0.00.061.636 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.636 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.637 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.637 I ggml_metal_init: simdgroup reduction   = true
0.00.061.637 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.637 I ggml_metal_init: has bfloat            = true
0.00.061.637 I ggml_metal_init: use bfloat            = true
0.00.061.638 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.638 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.430 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.921 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.923 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.937 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.920 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.921 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.921 I llama_new_context_with_model: graph nodes  = 967
0.00.073.921 I llama_new_context_with_model: graph splits = 2
0.00.073.922 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.702 I 
0.00.503.749 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.772 I perplexity: tokenizing the input ..
0.00.511.547 I perplexity: tokenization took 7.774 ms
0.00.511.551 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.643.841 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.645.074 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.645.090 I llama_perf_context_print:        load time =     494.87 ms
0.00.645.091 I llama_perf_context_print: prompt eval time =     132.06 ms /   128 tokens (    1.03 ms per token,   969.24 tokens per second)
0.00.645.092 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.645.092 I llama_perf_context_print:       total time =     141.39 ms /   129 tokens
0.00.645.455 I ggml_metal_free: deallocating

real	0m0.658s
user	0m0.078s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.717 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.714 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.715 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.716 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.716 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.717 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.717 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.717 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.722 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.722 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.589 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.473 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.473 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.474 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.474 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.474 I llama_model_loader: - type  f32:  194 tensors
0.00.023.475 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.475 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.475 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.593 I load_vocab: special tokens cache size = 25
0.00.049.720 I load_vocab: token to piece cache size = 0.2984 MB
0.00.049.723 I print_meta: format           = GGUF V3 (latest)
0.00.049.723 I print_meta: arch             = gptneox
0.00.049.723 I print_meta: vocab type       = BPE
0.00.049.724 I print_meta: n_vocab          = 50304
0.00.049.724 I print_meta: n_merges         = 50009
0.00.049.724 I print_meta: vocab_only       = 0
0.00.049.724 I print_meta: n_ctx_train      = 2048
0.00.049.724 I print_meta: n_embd           = 2048
0.00.049.725 I print_meta: n_layer          = 24
0.00.049.727 I print_meta: n_head           = 16
0.00.049.728 I print_meta: n_head_kv        = 16
0.00.049.728 I print_meta: n_rot            = 32
0.00.049.728 I print_meta: n_swa            = 0
0.00.049.728 I print_meta: n_embd_head_k    = 128
0.00.049.728 I print_meta: n_embd_head_v    = 128
0.00.049.729 I print_meta: n_gqa            = 1
0.00.049.730 I print_meta: n_embd_k_gqa     = 2048
0.00.049.730 I print_meta: n_embd_v_gqa     = 2048
0.00.049.731 I print_meta: f_norm_eps       = 1.0e-05
0.00.049.731 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.731 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.732 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.732 I print_meta: f_logit_scale    = 0.0e+00
0.00.049.732 I print_meta: n_ff             = 8192
0.00.049.732 I print_meta: n_expert         = 0
0.00.049.733 I print_meta: n_expert_used    = 0
0.00.049.733 I print_meta: causal attn      = 1
0.00.049.733 I print_meta: pooling type     = 0
0.00.049.733 I print_meta: rope type        = 2
0.00.049.735 I print_meta: rope scaling     = linear
0.00.049.737 I print_meta: freq_base_train  = 10000.0
0.00.049.737 I print_meta: freq_scale_train = 1
0.00.049.738 I print_meta: n_ctx_orig_yarn  = 2048
0.00.049.738 I print_meta: rope_finetuned   = unknown
0.00.049.738 I print_meta: ssm_d_conv       = 0
0.00.049.738 I print_meta: ssm_d_inner      = 0
0.00.049.738 I print_meta: ssm_d_state      = 0
0.00.049.738 I print_meta: ssm_dt_rank      = 0
0.00.049.739 I print_meta: ssm_dt_b_c_rms   = 0
0.00.049.739 I print_meta: model type       = 1.4B
0.00.049.739 I print_meta: model ftype      = Q4_K - Medium
0.00.049.740 I print_meta: model params     = 1.41 B
0.00.049.740 I print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.740 I print_meta: general.name     = 1.4B
0.00.049.745 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.745 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.745 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.745 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.745 I print_meta: LF token         = 128 'Ä'
0.00.049.746 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.746 I print_meta: max token length = 1024
0.00.051.786 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.786 I llm_load_tensors: offloading output layer to GPU
0.00.051.787 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.797 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.799 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.684 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.685 I llama_new_context_with_model: n_ctx         = 128
0.00.052.685 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.685 I llama_new_context_with_model: n_batch       = 128
0.00.052.686 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.686 I llama_new_context_with_model: flash_attn    = 0
0.00.052.686 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.686 I llama_new_context_with_model: freq_scale    = 1
0.00.052.687 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.687 I ggml_metal_init: allocating
0.00.052.693 I ggml_metal_init: found device: Apple M4
0.00.052.695 I ggml_metal_init: picking default device: Apple M4
0.00.053.254 I ggml_metal_init: using embedded metal library
0.00.055.588 I ggml_metal_init: GPU name:   Apple M4
0.00.055.589 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.589 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.590 I ggml_metal_init: simdgroup reduction   = true
0.00.055.590 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.590 I ggml_metal_init: has bfloat            = true
0.00.055.592 I ggml_metal_init: use bfloat            = true
0.00.055.592 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.168 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.525 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.528 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.544 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.457 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.458 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.458 I llama_new_context_with_model: graph nodes  = 967
0.00.067.458 I llama_new_context_with_model: graph splits = 2
0.00.067.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.000 I 
0.00.554.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.081 I perplexity: tokenizing the input ..
0.00.562.342 I perplexity: tokenization took 8.259 ms
0.00.562.345 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.696.824 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.697.985 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.698.002 I llama_perf_context_print:        load time =     545.28 ms
0.00.698.003 I llama_perf_context_print: prompt eval time =     134.25 ms /   128 tokens (    1.05 ms per token,   953.42 tokens per second)
0.00.698.004 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.698.004 I llama_perf_context_print:       total time =     144.00 ms /   129 tokens
0.00.698.453 I ggml_metal_free: deallocating

real	0m0.712s
user	0m0.078s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.232 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.926 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.935 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.936 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.937 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.937 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.938 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.940 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.943 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.943 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.801 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.683 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.684 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.684 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.685 I llama_model_loader: - type  f32:  194 tensors
0.00.024.685 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.685 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.482 I load_vocab: special tokens cache size = 25
0.00.051.518 I load_vocab: token to piece cache size = 0.2984 MB
0.00.051.521 I print_meta: format           = GGUF V3 (latest)
0.00.051.521 I print_meta: arch             = gptneox
0.00.051.521 I print_meta: vocab type       = BPE
0.00.051.522 I print_meta: n_vocab          = 50304
0.00.051.522 I print_meta: n_merges         = 50009
0.00.051.522 I print_meta: vocab_only       = 0
0.00.051.522 I print_meta: n_ctx_train      = 2048
0.00.051.522 I print_meta: n_embd           = 2048
0.00.051.522 I print_meta: n_layer          = 24
0.00.051.525 I print_meta: n_head           = 16
0.00.051.526 I print_meta: n_head_kv        = 16
0.00.051.526 I print_meta: n_rot            = 32
0.00.051.526 I print_meta: n_swa            = 0
0.00.051.526 I print_meta: n_embd_head_k    = 128
0.00.051.527 I print_meta: n_embd_head_v    = 128
0.00.051.527 I print_meta: n_gqa            = 1
0.00.051.528 I print_meta: n_embd_k_gqa     = 2048
0.00.051.529 I print_meta: n_embd_v_gqa     = 2048
0.00.051.529 I print_meta: f_norm_eps       = 1.0e-05
0.00.051.530 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.530 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.530 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.530 I print_meta: f_logit_scale    = 0.0e+00
0.00.051.531 I print_meta: n_ff             = 8192
0.00.051.531 I print_meta: n_expert         = 0
0.00.051.531 I print_meta: n_expert_used    = 0
0.00.051.531 I print_meta: causal attn      = 1
0.00.051.531 I print_meta: pooling type     = 0
0.00.051.532 I print_meta: rope type        = 2
0.00.051.532 I print_meta: rope scaling     = linear
0.00.051.532 I print_meta: freq_base_train  = 10000.0
0.00.051.533 I print_meta: freq_scale_train = 1
0.00.051.533 I print_meta: n_ctx_orig_yarn  = 2048
0.00.051.533 I print_meta: rope_finetuned   = unknown
0.00.051.533 I print_meta: ssm_d_conv       = 0
0.00.051.533 I print_meta: ssm_d_inner      = 0
0.00.051.535 I print_meta: ssm_d_state      = 0
0.00.051.535 I print_meta: ssm_dt_rank      = 0
0.00.051.535 I print_meta: ssm_dt_b_c_rms   = 0
0.00.051.535 I print_meta: model type       = 1.4B
0.00.051.536 I print_meta: model ftype      = Q5_K - Medium
0.00.051.536 I print_meta: model params     = 1.41 B
0.00.051.537 I print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.537 I print_meta: general.name     = 1.4B
0.00.051.538 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.538 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.538 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.538 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.538 I print_meta: LF token         = 128 'Ä'
0.00.051.539 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.539 I print_meta: max token length = 1024
0.00.053.598 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.598 I llm_load_tensors: offloading output layer to GPU
0.00.053.599 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.609 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.610 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.513 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.514 I llama_new_context_with_model: n_ctx         = 128
0.00.054.514 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.514 I llama_new_context_with_model: n_batch       = 128
0.00.054.514 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.514 I llama_new_context_with_model: flash_attn    = 0
0.00.054.515 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.515 I llama_new_context_with_model: freq_scale    = 1
0.00.054.515 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.516 I ggml_metal_init: allocating
0.00.054.519 I ggml_metal_init: found device: Apple M4
0.00.054.521 I ggml_metal_init: picking default device: Apple M4
0.00.055.086 I ggml_metal_init: using embedded metal library
0.00.057.420 I ggml_metal_init: GPU name:   Apple M4
0.00.057.421 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.422 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.422 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.422 I ggml_metal_init: simdgroup reduction   = true
0.00.057.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.423 I ggml_metal_init: has bfloat            = true
0.00.057.423 I ggml_metal_init: use bfloat            = true
0.00.057.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.424 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.213 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.458 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.460 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.474 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.424 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.425 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.425 I llama_new_context_with_model: graph nodes  = 967
0.00.069.425 I llama_new_context_with_model: graph splits = 2
0.00.069.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.886 I 
0.00.626.936 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.962 I perplexity: tokenizing the input ..
0.00.635.668 I perplexity: tokenization took 8.705 ms
0.00.635.675 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.777 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.777.960 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.777.976 I llama_perf_context_print:        load time =     616.65 ms
0.00.777.977 I llama_perf_context_print: prompt eval time =     140.87 ms /   128 tokens (    1.10 ms per token,   908.61 tokens per second)
0.00.777.977 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.978 I llama_perf_context_print:       total time =     151.10 ms /   129 tokens
0.00.778.349 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.079s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.906 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.533 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.539 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.543 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.544 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.544 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.544 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.545 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.546 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.548 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.548 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.549 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.550 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.551 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.551 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.296 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.331 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.194 I llama_model_loader: - type  f32:  194 tensors
0.00.023.194 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.352 I load_vocab: special tokens cache size = 25
0.00.049.176 I load_vocab: token to piece cache size = 0.2984 MB
0.00.049.180 I print_meta: format           = GGUF V3 (latest)
0.00.049.180 I print_meta: arch             = gptneox
0.00.049.180 I print_meta: vocab type       = BPE
0.00.049.180 I print_meta: n_vocab          = 50304
0.00.049.181 I print_meta: n_merges         = 50009
0.00.049.181 I print_meta: vocab_only       = 0
0.00.049.181 I print_meta: n_ctx_train      = 2048
0.00.049.181 I print_meta: n_embd           = 2048
0.00.049.181 I print_meta: n_layer          = 24
0.00.049.184 I print_meta: n_head           = 16
0.00.049.185 I print_meta: n_head_kv        = 16
0.00.049.186 I print_meta: n_rot            = 32
0.00.049.186 I print_meta: n_swa            = 0
0.00.049.186 I print_meta: n_embd_head_k    = 128
0.00.049.186 I print_meta: n_embd_head_v    = 128
0.00.049.187 I print_meta: n_gqa            = 1
0.00.049.187 I print_meta: n_embd_k_gqa     = 2048
0.00.049.188 I print_meta: n_embd_v_gqa     = 2048
0.00.049.189 I print_meta: f_norm_eps       = 1.0e-05
0.00.049.189 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.189 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.189 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.189 I print_meta: f_logit_scale    = 0.0e+00
0.00.049.190 I print_meta: n_ff             = 8192
0.00.049.190 I print_meta: n_expert         = 0
0.00.049.190 I print_meta: n_expert_used    = 0
0.00.049.190 I print_meta: causal attn      = 1
0.00.049.191 I print_meta: pooling type     = 0
0.00.049.191 I print_meta: rope type        = 2
0.00.049.191 I print_meta: rope scaling     = linear
0.00.049.191 I print_meta: freq_base_train  = 10000.0
0.00.049.192 I print_meta: freq_scale_train = 1
0.00.049.192 I print_meta: n_ctx_orig_yarn  = 2048
0.00.049.192 I print_meta: rope_finetuned   = unknown
0.00.049.192 I print_meta: ssm_d_conv       = 0
0.00.049.192 I print_meta: ssm_d_inner      = 0
0.00.049.193 I print_meta: ssm_d_state      = 0
0.00.049.193 I print_meta: ssm_dt_rank      = 0
0.00.049.193 I print_meta: ssm_dt_b_c_rms   = 0
0.00.049.193 I print_meta: model type       = 1.4B
0.00.049.194 I print_meta: model ftype      = Q6_K
0.00.049.194 I print_meta: model params     = 1.41 B
0.00.049.195 I print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.195 I print_meta: general.name     = 1.4B
0.00.049.195 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.195 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.195 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.196 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.196 I print_meta: LF token         = 128 'Ä'
0.00.049.196 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.197 I print_meta: max token length = 1024
0.00.051.294 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.294 I llm_load_tensors: offloading output layer to GPU
0.00.051.295 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.305 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.306 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.184 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.185 I llama_new_context_with_model: n_ctx         = 128
0.00.052.185 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.185 I llama_new_context_with_model: n_batch       = 128
0.00.052.186 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.186 I llama_new_context_with_model: flash_attn    = 0
0.00.052.186 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.186 I llama_new_context_with_model: freq_scale    = 1
0.00.052.187 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.187 I ggml_metal_init: allocating
0.00.052.190 I ggml_metal_init: found device: Apple M4
0.00.052.192 I ggml_metal_init: picking default device: Apple M4
0.00.052.756 I ggml_metal_init: using embedded metal library
0.00.055.036 I ggml_metal_init: GPU name:   Apple M4
0.00.055.037 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.037 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.038 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.038 I ggml_metal_init: simdgroup reduction   = true
0.00.055.038 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.038 I ggml_metal_init: has bfloat            = true
0.00.055.038 I ggml_metal_init: use bfloat            = true
0.00.055.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.039 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.597 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.807 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.810 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.824 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.703 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.704 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.704 I llama_new_context_with_model: graph nodes  = 967
0.00.066.704 I llama_new_context_with_model: graph splits = 2
0.00.066.706 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.414.739 I 
0.00.414.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.414.809 I perplexity: tokenizing the input ..
0.00.422.839 I perplexity: tokenization took 8.029 ms
0.00.422.846 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.738 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.563.917 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.563.935 I llama_perf_context_print:        load time =     405.83 ms
0.00.563.936 I llama_perf_context_print: prompt eval time =     139.66 ms /   128 tokens (    1.09 ms per token,   916.49 tokens per second)
0.00.563.937 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.937 I llama_perf_context_print:       total time =     149.20 ms /   129 tokens
0.00.564.374 I ggml_metal_free: deallocating

real	0m0.579s
user	0m0.077s
sys	0m0.084s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.260 I build: 4429 (53e61c66) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.046 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.628 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.634 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.641 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.641 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.642 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.642 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.645 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.645 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.645 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.646 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.646 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.649 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.406 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.224 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.873 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.874 I llama_model_loader: - type  f32:  194 tensors
0.00.048.874 I llama_model_loader: - type  f16:   98 tensors
0.00.076.916 I load_vocab: special tokens cache size = 25
0.00.083.065 I load_vocab: token to piece cache size = 0.2984 MB
0.00.083.068 I print_meta: format           = GGUF V3 (latest)
0.00.083.068 I print_meta: arch             = gptneox
0.00.083.069 I print_meta: vocab type       = BPE
0.00.083.069 I print_meta: n_vocab          = 50304
0.00.083.069 I print_meta: n_merges         = 50009
0.00.083.069 I print_meta: vocab_only       = 0
0.00.083.069 I print_meta: n_ctx_train      = 2048
0.00.083.069 I print_meta: n_embd           = 2048
0.00.083.070 I print_meta: n_layer          = 24
0.00.083.072 I print_meta: n_head           = 16
0.00.083.073 I print_meta: n_head_kv        = 16
0.00.083.073 I print_meta: n_rot            = 32
0.00.083.073 I print_meta: n_swa            = 0
0.00.083.073 I print_meta: n_embd_head_k    = 128
0.00.083.073 I print_meta: n_embd_head_v    = 128
0.00.083.076 I print_meta: n_gqa            = 1
0.00.083.077 I print_meta: n_embd_k_gqa     = 2048
0.00.083.077 I print_meta: n_embd_v_gqa     = 2048
0.00.083.078 I print_meta: f_norm_eps       = 1.0e-05
0.00.083.079 I print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.079 I print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.079 I print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.079 I print_meta: f_logit_scale    = 0.0e+00
0.00.083.080 I print_meta: n_ff             = 8192
0.00.083.080 I print_meta: n_expert         = 0
0.00.083.080 I print_meta: n_expert_used    = 0
0.00.083.080 I print_meta: causal attn      = 1
0.00.083.080 I print_meta: pooling type     = 0
0.00.083.081 I print_meta: rope type        = 2
0.00.083.081 I print_meta: rope scaling     = linear
0.00.083.081 I print_meta: freq_base_train  = 10000.0
0.00.083.081 I print_meta: freq_scale_train = 1
0.00.083.082 I print_meta: n_ctx_orig_yarn  = 2048
0.00.083.082 I print_meta: rope_finetuned   = unknown
0.00.083.082 I print_meta: ssm_d_conv       = 0
0.00.083.082 I print_meta: ssm_d_inner      = 0
0.00.083.082 I print_meta: ssm_d_state      = 0
0.00.083.082 I print_meta: ssm_dt_rank      = 0
0.00.083.083 I print_meta: ssm_dt_b_c_rms   = 0
0.00.083.083 I print_meta: model type       = 1.4B
0.00.083.084 I print_meta: model ftype      = all F32 (guessed)
0.00.083.084 I print_meta: model params     = 1.41 B
0.00.083.084 I print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.084 I print_meta: general.name     = 1.4B
0.00.083.085 I print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.085 I print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.085 I print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.085 I print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.085 I print_meta: LF token         = 128 'Ä'
0.00.083.086 I print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.086 I print_meta: max token length = 1024
0.00.085.777 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.777 I llm_load_tensors: offloading output layer to GPU
0.00.085.777 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.788 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.789 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.086.769 I llama_new_context_with_model: n_seq_max     = 1
0.00.086.770 I llama_new_context_with_model: n_ctx         = 128
0.00.086.770 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.086.770 I llama_new_context_with_model: n_batch       = 128
0.00.086.770 I llama_new_context_with_model: n_ubatch      = 128
0.00.086.770 I llama_new_context_with_model: flash_attn    = 0
0.00.086.771 I llama_new_context_with_model: freq_base     = 10000.0
0.00.086.771 I llama_new_context_with_model: freq_scale    = 1
0.00.086.771 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.086.772 I ggml_metal_init: allocating
0.00.086.778 I ggml_metal_init: found device: Apple M4
0.00.086.780 I ggml_metal_init: picking default device: Apple M4
0.00.087.380 I ggml_metal_init: using embedded metal library
0.00.090.040 I ggml_metal_init: GPU name:   Apple M4
0.00.090.041 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.042 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.042 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.043 I ggml_metal_init: simdgroup reduction   = true
0.00.090.043 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.043 I ggml_metal_init: has bfloat            = true
0.00.090.043 I ggml_metal_init: use bfloat            = true
0.00.090.044 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.044 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.660 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.930 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.933 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.950 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.848 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.849 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.849 I llama_new_context_with_model: graph nodes  = 967
0.00.101.849 I llama_new_context_with_model: graph splits = 2
0.00.101.850 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.101.851 I 
0.00.101.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.101.876 I compute_imatrix: tokenizing the input ..
0.00.108.538 I compute_imatrix: tokenization took 6.66 ms
0.00.108.539 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.638.345 I compute_imatrix: 1.53 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.640.673 I llama_perf_context_print:        load time =    1618.30 ms
0.01.640.674 I llama_perf_context_print: prompt eval time =    1529.16 ms /   128 tokens (   11.95 ms per token,    83.71 tokens per second)
0.01.640.675 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.640.675 I llama_perf_context_print:       total time =    1620.62 ms /   129 tokens
0.01.641.198 I ggml_metal_free: deallocating

real	0m1.826s
user	0m0.159s
sys	0m0.231s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4429 (53e61c66)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
load_vocab: control token:      1 '<|padding|>' is not marked as EOG
load_vocab: special tokens cache size = 25
load_vocab: token to piece cache size = 0.2984 MB
print_meta: format           = GGUF V3 (latest)
print_meta: arch             = gptneox
print_meta: vocab type       = BPE
print_meta: n_vocab          = 50304
print_meta: n_merges         = 50009
print_meta: vocab_only       = 0
print_meta: n_ctx_train      = 2048
print_meta: n_embd           = 2048
print_meta: n_layer          = 24
print_meta: n_head           = 16
print_meta: n_head_kv        = 16
print_meta: n_rot            = 32
print_meta: n_swa            = 0
print_meta: n_embd_head_k    = 128
print_meta: n_embd_head_v    = 128
print_meta: n_gqa            = 1
print_meta: n_embd_k_gqa     = 2048
print_meta: n_embd_v_gqa     = 2048
print_meta: f_norm_eps       = 1.0e-05
print_meta: f_norm_rms_eps   = 0.0e+00
print_meta: f_clamp_kqv      = 0.0e+00
print_meta: f_max_alibi_bias = 0.0e+00
print_meta: f_logit_scale    = 0.0e+00
print_meta: n_ff             = 8192
print_meta: n_expert         = 0
print_meta: n_expert_used    = 0
print_meta: causal attn      = 1
print_meta: pooling type     = 0
print_meta: rope type        = 2
print_meta: rope scaling     = linear
print_meta: freq_base_train  = 10000.0
print_meta: freq_scale_train = 1
print_meta: n_ctx_orig_yarn  = 2048
print_meta: rope_finetuned   = unknown
print_meta: ssm_d_conv       = 0
print_meta: ssm_d_inner      = 0
print_meta: ssm_d_state      = 0
print_meta: ssm_dt_rank      = 0
print_meta: ssm_dt_b_c_rms   = 0
print_meta: model type       = 1.4B
print_meta: model ftype      = Q4_0
print_meta: model params     = 1.41 B
print_meta: model size       = 786.31 MiB (4.66 BPW) 
print_meta: general.name     = 1.4B
print_meta: BOS token        = 0 '<|endoftext|>'
print_meta: EOS token        = 0 '<|endoftext|>'
print_meta: EOT token        = 0 '<|endoftext|>'
print_meta: UNK token        = 0 '<|endoftext|>'
print_meta: LF token         = 128 'Ä'
print_meta: EOG token        = 0 '<|endoftext|>'
print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d70a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d70a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d70aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d70b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d70ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d70bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d70c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d70cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d70d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d70d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d70daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d70dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d70eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d70f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d70fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d7101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d711030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d711f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d712d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d713480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d714440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d714700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d714d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d715980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d716180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d7168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d7176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d717970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d7182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d718750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d719090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d7199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d719e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d71a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d71a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d71abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d71b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d71bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d71c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d71c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d71cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d71d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d71d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d71df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d71ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d71f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d71f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d71f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d720160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d720420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d7208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d720d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d721200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d7216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d721b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d721fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d722920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d723260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d723700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d7240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d7250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d7260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d726620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d726b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d7270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d727610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d7280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d728b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d7290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d7295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d729b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d72a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d72a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d72ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d72b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d72b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d72bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d71b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d72bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d72c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d72cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d72d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d72d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d72dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d72e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d72e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d72ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d72f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d72f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d72fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d7301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d730700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d730c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d7310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d731590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d731a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d731ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d732370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d732810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d732cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d7335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d733a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d733f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d7343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d734870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d734d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d7351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d735650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d736430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d7368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d736d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d737210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d7376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d737b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d737ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d738490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d738dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d73a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d73a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d73a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d73ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d73b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d73b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d73bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d73c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d73c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d73c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d73ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d73d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d73d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d73dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d73e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d73e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d73ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d73eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d73f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d73f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d73fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d740170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d740610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d740ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d740f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d7413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d741890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d741d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d7421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d742b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d742fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d743450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d7438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d743d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d744230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d7446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d744b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d745010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d7454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d745950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d745df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d746290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d746730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d746bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d747070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d747510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d7479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d747e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d7483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d7488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d748e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d749650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d74a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d74a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d74b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d74b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d74b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d74bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d74c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d74cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d74d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d74d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d74d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d74e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d74e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d74ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d74f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d74f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d74fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d7506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d751690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d751be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d752bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d753120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d753670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d753bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d754110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d754660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d754bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d755100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d755ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d7560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d756640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d7570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d757b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d7580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d758620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d758b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d7590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d759610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d759b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d75a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d75a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d75ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d75b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d75b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d75bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d75c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d75c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d75cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d75d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d75d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d75db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d75e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d75e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d75eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d75f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d75f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d75fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d760050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d7605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d760af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d760f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d761430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d7618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d761d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d762210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d7626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d762b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d762ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d763490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d763930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d763dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d764270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d764710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d764bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d765050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d7655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d765cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d7663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d766b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d7674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d767cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d767f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d7685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.170.377 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.170.381 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d704d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d7051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d705630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d705aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d705f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d706380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d7067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d706c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d7070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d707540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d7079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d7080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d708bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d709370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d709b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d70a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d70a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d70b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d70b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d70bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d70c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d70cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d70d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d70dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d70e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d70e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d70e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d70ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d70f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d70f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d70fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d70ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d7103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d710670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d710ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d710f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d7113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d711830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d711ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d712110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d712580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d7129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d712e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d7132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d713740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d713bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d714020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d714490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d714900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d714d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d7151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d715650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d715ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d715f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d7163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d716810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d716d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d717280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d7176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d717b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d718440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d7188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d718d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d719190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d719600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d719a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d719ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d71a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d71a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d71ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d71b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d71b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d71b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d71bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d71c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d71c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d71cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d71cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d71d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d71d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d71dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d71e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d71e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d71ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d71eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d71f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d71f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d71fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d720080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d7204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d720960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d720dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d721240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d7216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d721b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d721f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d722400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d722870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d722ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d723150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d7235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d723a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d723ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d724310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d724780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d724bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d725060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d7254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d725940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d725db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d726220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d726690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d726b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d726f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d7273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d727850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d727cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d728130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d7285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d728a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d728e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d7292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d729760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d729bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d72a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d72a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d72a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d72ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d72b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d72b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d72bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d72bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d72c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d72c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d72cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d72d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d72d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d72d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d72de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d72e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d72e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d72ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d72f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d72f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d72f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d72fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d7301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d730650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d730ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d730f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d7313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d731810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d731c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d7320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d732560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d7329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d732e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d7332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d733720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d733b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d734000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d734470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d7348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d734d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d7351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d735df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d7360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d736370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d7367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d736c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d7370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d737530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d7379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d737e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d738280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d7386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d738b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d738fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d739440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d7398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d739d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d73a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d73a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d73aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d73aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d73b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d73b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d73bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d73c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d73c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d73c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d73cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d73d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d73d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d73db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d73dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d73e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d73e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d73ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d73f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d73f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d73fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d740050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d7404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d740930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d740da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d741210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d741730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d741c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d7427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d742a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d743030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d7435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d744170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d744730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d744cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d7452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d745870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d7463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d7469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d746f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d747530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d747af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d7480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d748670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d748c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d7491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d7497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d749d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d74a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d74a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d74aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d74b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d74ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d74bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d74c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d74cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d74d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d74d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d74dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d74e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d74e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d74edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d74f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d74f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d74ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d7504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d750ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d751070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d751630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d751bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d7521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d752770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d752d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d7532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d7538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d753e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d754430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d7549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d754fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d755570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d755b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d7560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d7566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d756c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d757170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d757670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d757b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d758070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d758570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d758a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d758f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d759470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d759970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d75a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d75a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d75ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d75b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d75b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d75c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d75c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d75cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d75d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d75d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d75e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d75e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d75ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d6044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d6056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d6063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d606cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d607140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d607870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d608390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d608b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d609350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d609a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d60a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d60a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d60afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d60b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d60be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d60c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d60cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d60d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d60daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d60dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d60e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d60e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d60e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d60ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d60f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d60f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d60fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d60fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d6102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d610720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d610b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d611000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d611470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d6118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d611d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d6121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d612630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d612aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d612f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d613380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d6137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d613c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d6140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d614540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d6149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d614e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d615290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d615700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d615b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d616550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d616a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d616ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d617330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d6177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d617c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d618080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d6184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d618960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d618dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d619240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d6196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d619b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d61a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d61a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d61ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d61b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d61b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d61ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d61bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d61c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d61c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d61cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d61d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d61d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d61d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d61ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d61e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d61e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d61eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d61ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d61f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d61f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d61fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d620130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d6205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d620a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d6212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d621760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d621bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d622040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d6224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d622920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d622d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d623200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d623a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d623d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d6241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d624630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d624aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d624f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d625380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d6257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d625c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d6260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d626540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d6269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d627290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d627700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d627b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d627fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d628450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d6288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d628d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d6291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d629610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d629a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d629ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d62a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d62a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d62ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d62b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d62b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d62b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d62be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d62c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d62c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d62cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d62cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d62d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d62d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d62dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d62e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d62e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d62ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d62eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d62f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d62f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d62fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d630090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d630500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d630970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d630de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d631250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d6316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d631b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d631fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d632410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d632880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d632cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d633160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d6335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d633a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d633eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d634320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d634790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d634c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d635070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d6354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d635950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d635dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d636230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d6366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d636b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d636f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d6373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d637860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d637cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d638140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d6385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d638a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d638e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d639300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d639770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d639be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d63a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d63a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d63a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d63ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d63b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d63b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d63baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d63bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d63c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d63c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d63ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d63d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d63d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d63da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d63de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d63e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d63e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d63ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d63f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d63f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d63f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d63fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d6401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d640660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d640f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d641ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d641d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d642040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d6424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d642920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d642d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d643200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d643ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d643f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d6443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d644830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d644ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d645110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d645580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d6459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d645e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d6462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d646740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d646bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d647020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d647490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d647900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d647d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d6481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d648650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d648ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d648f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d6493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d649810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d649c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d64a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d64a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d64a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d64ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d64b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d64b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d64bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d64c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d64c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d64c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d64cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d64d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d64d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d64daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d64df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d64e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d64e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d64ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d64f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d64f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d64f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d64fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d650290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d650700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d650b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d650fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d651450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d6518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d651d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d6521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d652610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d652a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d653360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d6537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d653c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d6540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d654520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d654990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d654e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d655270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d6556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d656150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d656870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d656f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d6576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d657970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d657de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d6583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d6589f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.848s
user	0m0.296s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4429 (53e61c66)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
load_vocab: control token:      1 '<|padding|>' is not marked as EOG
load_vocab: special tokens cache size = 25
load_vocab: token to piece cache size = 0.2984 MB
print_meta: format           = GGUF V3 (latest)
print_meta: arch             = gptneox
print_meta: vocab type       = BPE
print_meta: n_vocab          = 50304
print_meta: n_merges         = 50009
print_meta: vocab_only       = 0
print_meta: n_ctx_train      = 2048
print_meta: n_embd           = 2048
print_meta: n_layer          = 24
print_meta: n_head           = 16
print_meta: n_head_kv        = 16
print_meta: n_rot            = 32
print_meta: n_swa            = 0
print_meta: n_embd_head_k    = 128
print_meta: n_embd_head_v    = 128
print_meta: n_gqa            = 1
print_meta: n_embd_k_gqa     = 2048
print_meta: n_embd_v_gqa     = 2048
print_meta: f_norm_eps       = 1.0e-05
print_meta: f_norm_rms_eps   = 0.0e+00
print_meta: f_clamp_kqv      = 0.0e+00
print_meta: f_max_alibi_bias = 0.0e+00
print_meta: f_logit_scale    = 0.0e+00
print_meta: n_ff             = 8192
print_meta: n_expert         = 0
print_meta: n_expert_used    = 0
print_meta: causal attn      = 1
print_meta: pooling type     = 0
print_meta: rope type        = 2
print_meta: rope scaling     = linear
print_meta: freq_base_train  = 10000.0
print_meta: freq_scale_train = 1
print_meta: n_ctx_orig_yarn  = 2048
print_meta: rope_finetuned   = unknown
print_meta: ssm_d_conv       = 0
print_meta: ssm_d_inner      = 0
print_meta: ssm_d_state      = 0
print_meta: ssm_dt_rank      = 0
print_meta: ssm_dt_b_c_rms   = 0
print_meta: model type       = 1.4B
print_meta: model ftype      = Q4_0
print_meta: model params     = 1.41 B
print_meta: model size       = 786.31 MiB (4.66 BPW) 
print_meta: general.name     = 1.4B
print_meta: BOS token        = 0 '<|endoftext|>'
print_meta: EOS token        = 0 '<|endoftext|>'
print_meta: EOT token        = 0 '<|endoftext|>'
print_meta: UNK token        = 0 '<|endoftext|>'
print_meta: LF token         = 128 'Ä'
print_meta: EOG token        = 0 '<|endoftext|>'
print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127e05260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127e05720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127e05b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127e06000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127e06470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127e07710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127e07b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127e07ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127e08460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127e088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127e08d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127e093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127e09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127e0a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127e0aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127e0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127e0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127e0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127e0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127e0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127e0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127e0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127e0e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127e0f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127e0f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127e0faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127e0fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127e10220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127e108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127e10d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127e111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127e11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127e11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127e11e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127e122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127e12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127e12e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127e132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127e13730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127e13ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127e14010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127e14480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127e148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127e14d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127e151d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127e15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127e15ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127e164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127e167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127e16c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127e17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127e174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127e17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127e17dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127e18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127e188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127e18d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127e19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127e194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127e19b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127e19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127e1a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127e1a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127e1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127e1b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127e1b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127e1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127e1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127e1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127e1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127e1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127e1d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127e1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127e1de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127e1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127e1e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127e1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127e1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127e1fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127e20070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127e20620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127e20bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127e21180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127e21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127e21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127e22290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127e22840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127e22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127e233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127e23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127e23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127e244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127e24a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127e25010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127e255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127e25b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127e26120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127e160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127e26880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127e26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127e27160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127e27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127e27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127e28270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127e28820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127e28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127e29380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127e29930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127e29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127e2a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127e2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127e2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127e2b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127e2bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127e2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127e2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127e2ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127e2cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127e2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127e2d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127e2de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127e2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127e2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127e2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127e2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127e2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127e2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127e30150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127e30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127e30b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127e31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127e31550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127e31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127e31f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127e32450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127e32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127e32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127e33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127e33850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127e33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127e34250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127e34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127e34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127e35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127e35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127e35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127e36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127e36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127e36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127e36f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127e37450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127e37950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127e37e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127e38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127e38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127e38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127e39250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127e39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127e39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127e3a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127e3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127e3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127e3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127e3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127e3ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127e3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127e3c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127e3c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127e3ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127e3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127e3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127e3dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127e3e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127e3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127e3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127e3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127e3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127e3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127e40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127e40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127e40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127e40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127e41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127e41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127e42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127e42850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127e42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127e43250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127e43750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127e43c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127e44150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127e44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127e44b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127e45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127e456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127e45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127e46210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127e46820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127e46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127e47440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127e47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127e480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127e48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127e489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127e48fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127e497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127e49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127e4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127e4a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127e4ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127e4b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127e4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127e4bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127e4c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127e4c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127e4cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127e4d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127e4d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127e4dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127e4e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127e4e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127e4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127e4f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127e4f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127e4fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127e50230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127e50780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127e50cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127e51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127e51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127e51cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127e52210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127e52760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127e52cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127e53200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127e53750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127e53ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127e541f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127e54740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127e54c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127e551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127e55730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127e55c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127e561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127e56720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127e56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127e571c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127e57710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127e57c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127e581b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127e58700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127e58c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127e591a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127e596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127e59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127e5a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127e5a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127e5ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127e5b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127e5b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127e5bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127e5c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127e5c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127e5cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127e5d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127e5d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127e5db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127e5dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127e5e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127e5e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127e5edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127e5f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127e5f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127e5fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127e60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127e604f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127e60990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127e60e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127e612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127e61770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127e61c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127e62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127e62880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127e62fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127e636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127e63de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127e640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127e64890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127e64b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127e65160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127e1ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127e1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127e23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127e1e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127e25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127e23660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127e2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127e2a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127e2a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127e25880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127e20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127e28530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127e453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127e252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127e1fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127e230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127e219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127e27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127e44e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127e29bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127e24d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127e1f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127e22b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127e21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127e279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127e29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127e24770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127e1f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127e22550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127e27420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127e29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127e241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127e21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127e28ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127e64e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127e464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127e470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127e48c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127e0edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127e15d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127e125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127e18500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127e19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127e64360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127e263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127e49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127e47700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127e655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127e65880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127e65b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127e65e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127e660c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127e66380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127e66640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127e66900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127e66bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127e66e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127e67140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127e67400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127e676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127e67980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127e67c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127e67f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127e681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127e68480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127e68740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127e68a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127e68cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127e68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127e69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127e69500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127e697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127e69a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127e69d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127e6a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127e6a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127e6a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127e6a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127e6ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127e6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127e6b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127e6b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127e6b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127e6b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127e6bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127e6be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127e6c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127e6c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127e6c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127e6c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127e6cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127e6cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127e6d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127e6d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127e6d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127e6d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127e6dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127e6df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127e6e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127e6e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127e6e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127e6ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127e6ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127e6efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127e6f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127e6f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127e6f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127e6fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127e6fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127e70040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127e70300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127e705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127e70880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127e70b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127e70e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127e710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127e71380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127e71640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127e71900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127e71bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127e71e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127e72140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127e72400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127e726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127e72980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127e72c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127e72f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127e731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127e73480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127e73740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127e73a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127e73cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127e73f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127e74240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127e74500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127e747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x127e74a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127e74d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127e75000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127e752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127e75580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127e75840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127e75b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127e75dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127e76080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127e76340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127e76600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127e768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127e76b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127e76e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127e77100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127e773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127e77680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127e77940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127e77c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127e77ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127e78180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127e78440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127e78700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127e789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127e78c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127e78f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127e79200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127e794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127e79780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127e79a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127e79d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127e79fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127e7a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127e7a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127e7a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127e7aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127e7ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127e7b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127e7b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127e7b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127e7b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127e7bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127e7be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127e7c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127e7c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127e7c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127e7c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127e7cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127e7ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127e7d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127e7d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127e7da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127e7de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127e7e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127e7e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127e7ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127e7f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127e7f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127e7f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127e7fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127e80210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x127e80680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127e80af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127e80f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127e813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x127e81840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x127e81cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127e82220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127e82690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127e82b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127e82f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127e833e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127e83900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127e83e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127e84980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127e84c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127e85200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127e857c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127e85d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127e86340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127e86900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127e86ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127e87480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127e87a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127e88000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127e885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127e88b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127e89140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127e89700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127e89cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127e8a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127e8a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127e8ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127e8b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127e8b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127e8bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127e8c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127e8cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127e8d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127e8d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127e8dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127e8e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127e8e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127e8ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127e8f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127e8f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127e8fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127e90440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127e90a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127e90fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127e91580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127e91b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127e92100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127e926c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127e92c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127e93240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127e93800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127e93dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127e94380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127e94940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127e94f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127e954c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x127e95a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127e96040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127e96600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127e96bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127e97180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127e97740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127e97d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127e982c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127e98880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127e98e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127e99340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127e99840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127e99d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127e9a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127e9a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127e9ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127e9b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127e9b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127e9bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127e9c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127e9c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127e9ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127e9cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127e9d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127e9d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127e9e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127e9ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127e9f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127e9f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127e9fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127ea0360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127ea0620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127ea0c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107d044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107d04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107d04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107d05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107d056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107d05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107d05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107d063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107d06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107d06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107d07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107d07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107d08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107d08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107d09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107d09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107d0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107d0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107d0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107d0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107d0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107d0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107d0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107d0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107d0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107d0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107d0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107d0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107d0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107d0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107d0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107d0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107d0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107d0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107d102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107d10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107d10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107d10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107d11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107d118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107d11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107d121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107d12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107d12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107d12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107d13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107d137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107d13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107d140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107d14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107d149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107d14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107d15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107d156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107d15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107d15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107d16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107d16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107d16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107d17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107d17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107d17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107d18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107d184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107d18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107d18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107d19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107d196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107d19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107d19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107d1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107d1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107d1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107d1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107d1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107d1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107d1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107d1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107d1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107d1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107d1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107d1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107d1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107d1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107d1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107d1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107d1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107d1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107d1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107d1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107d1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107d20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107d20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107d20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107d20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107d212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107d21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107d21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107d22030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107d224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107d22910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107d22d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107d231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107d23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107d23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107d241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107d24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107d24a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107d24f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107d25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107d257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107d25c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107d260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107d26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107d269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107d26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107d27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107d276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107d27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107d27fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107d28440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107d288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107d28d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107d29190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107d29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107d29a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107d29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107d2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107d2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107d2ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107d2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107d2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107d2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107d2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107d2c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107d2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107d2cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107d2cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107d2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107d2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107d2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107d2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107d2e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107d2ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107d2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107d2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107d2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107d2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107d30080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107d304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107d30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107d30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107d31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107d316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107d31b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107d31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107d32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107d32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107d32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107d33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107d335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107d33a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107d33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107d34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107d34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107d34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107d35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107d354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107d35940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107d35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107d36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107d36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107d36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107d36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107d373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107d37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107d37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107d38130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107d385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107d38a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107d38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107d392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107d39760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107d39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107d3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107d3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107d3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107d3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107d3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107d3b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107d3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107d3bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107d3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107d3c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107d3cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107d3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107d3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107d3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107d3de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107d3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107d3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107d3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107d3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107d3f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107d3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107d3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107d401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107d40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107d40ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107d40f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107d41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107d41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107d42030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107d424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107d42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107d42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107d431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107d43660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107d43ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107d43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107d443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107d44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107d44c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107d45100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107d45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107d459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107d45e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107d462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107d46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107d46ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107d47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107d47480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107d478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107d47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107d481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107d48640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107d48ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107d48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107d49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107d49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107d49c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107d4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107d4a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107d4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107d4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107d4b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107d4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107d4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107d4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107d4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107d4c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107d4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107d4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107d4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107d4da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107d4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107d4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107d4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107d4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107d4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107d4f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107d4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107d4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107d50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107d506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107d50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107d50fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107d51440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107d518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107d51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107d52190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107d52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107d52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107d52ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107d53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107d537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107d53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107d540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107d54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107d54980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107d54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107d55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107d556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107d56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107d56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107d56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107d576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107d57960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107d57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107d583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107d589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.926s
user	0m0.246s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
