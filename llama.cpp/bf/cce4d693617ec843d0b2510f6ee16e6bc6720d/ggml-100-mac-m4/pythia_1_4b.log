Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.526s
user	0m0.967s
sys	0m1.195s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target sha256
[  4%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Built target llava
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target llama-simple
[ 34%] Built target test-c
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-chat
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-chat
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-quantize-perf
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-batched
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target test-rope
[ 71%] Built target llama-embedding
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-imatrix
[ 71%] Built target llama-gritlm
[ 71%] Built target llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 73%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-bench
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-parallel
[ 81%] Built target llama-cli
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-perplexity
[ 82%] Built target llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-tts
[ 91%] Built target llama-gen-docs
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.144s
user	0m6.766s
sys	0m11.082s

main: quantize time =  5486.25 ms
main:    total time =  5486.25 ms

main: quantize time =  3087.25 ms
main:    total time =  3087.25 ms

main: quantize time =  3061.54 ms
main:    total time =  3061.54 ms

main: quantize time =  2146.07 ms
main:    total time =  2146.07 ms

main: quantize time =  2493.45 ms
main:    total time =  2493.45 ms

main: quantize time =  5282.31 ms
main:    total time =  5282.31 ms

main: quantize time =  5397.68 ms
main:    total time =  5397.68 ms

main: quantize time =  6524.04 ms
main:    total time =  6524.04 ms

main: quantize time =  5664.55 ms
main:    total time =  5664.55 ms

main: quantize time =  4356.97 ms
main:    total time =  4356.97 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.197 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.360 I main: llama backend init
0.00.000.366 I main: load the model and apply lora adapter, if any
0.00.025.506 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.438 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.446 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.449 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.450 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.451 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.451 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.451 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.454 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.454 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.455 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.456 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.457 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.458 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.461 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.461 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.462 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.189 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.629 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.633 I llama_model_loader: - type  f32:  194 tensors
0.00.057.633 I llama_model_loader: - type  f16:   98 tensors
0.00.057.634 I print_info: file format = GGUF V3 (latest)
0.00.057.635 I print_info: file type   = all F32 (guessed)
0.00.057.640 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.403 I load: special tokens cache size = 25
0.00.077.107 I load: token to piece cache size = 0.2984 MB
0.00.077.110 I print_info: arch             = gptneox
0.00.077.111 I print_info: vocab_only       = 0
0.00.077.111 I print_info: n_ctx_train      = 2048
0.00.077.111 I print_info: n_embd           = 2048
0.00.077.111 I print_info: n_layer          = 24
0.00.077.115 I print_info: n_head           = 16
0.00.077.116 I print_info: n_head_kv        = 16
0.00.077.116 I print_info: n_rot            = 32
0.00.077.116 I print_info: n_swa            = 0
0.00.077.116 I print_info: n_embd_head_k    = 128
0.00.077.116 I print_info: n_embd_head_v    = 128
0.00.077.117 I print_info: n_gqa            = 1
0.00.077.118 I print_info: n_embd_k_gqa     = 2048
0.00.077.118 I print_info: n_embd_v_gqa     = 2048
0.00.077.119 I print_info: f_norm_eps       = 1.0e-05
0.00.077.119 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.119 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.120 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.120 I print_info: f_logit_scale    = 0.0e+00
0.00.077.120 I print_info: n_ff             = 8192
0.00.077.120 I print_info: n_expert         = 0
0.00.077.121 I print_info: n_expert_used    = 0
0.00.077.121 I print_info: causal attn      = 1
0.00.077.121 I print_info: pooling type     = 0
0.00.077.121 I print_info: rope type        = 2
0.00.077.121 I print_info: rope scaling     = linear
0.00.077.122 I print_info: freq_base_train  = 10000.0
0.00.077.122 I print_info: freq_scale_train = 1
0.00.077.122 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.122 I print_info: rope_finetuned   = unknown
0.00.077.122 I print_info: ssm_d_conv       = 0
0.00.077.123 I print_info: ssm_d_inner      = 0
0.00.077.123 I print_info: ssm_d_state      = 0
0.00.077.123 I print_info: ssm_dt_rank      = 0
0.00.077.124 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.124 I print_info: model type       = 1.4B
0.00.077.124 I print_info: model params     = 1.41 B
0.00.077.124 I print_info: general.name     = 1.4B
0.00.077.125 I print_info: vocab type       = BPE
0.00.077.125 I print_info: n_vocab          = 50304
0.00.077.125 I print_info: n_merges         = 50009
0.00.077.126 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.126 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.126 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.126 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.127 I print_info: LF token         = 187 'Ċ'
0.00.077.128 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.128 I print_info: max token length = 1024
0.00.108.934 I load_tensors: offloading 24 repeating layers to GPU
0.00.108.937 I load_tensors: offloading output layer to GPU
0.00.108.938 I load_tensors: offloaded 25/25 layers to GPU
0.00.108.959 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.108.961 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.109.245 I llama_init_from_model: n_seq_max     = 1
0.00.109.247 I llama_init_from_model: n_ctx         = 2048
0.00.109.247 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.109.247 I llama_init_from_model: n_batch       = 2048
0.00.109.247 I llama_init_from_model: n_ubatch      = 512
0.00.109.247 I llama_init_from_model: flash_attn    = 0
0.00.109.248 I llama_init_from_model: freq_base     = 10000.0
0.00.109.248 I llama_init_from_model: freq_scale    = 1
0.00.109.249 I ggml_metal_init: allocating
0.00.109.264 I ggml_metal_init: found device: Apple M4
0.00.109.269 I ggml_metal_init: picking default device: Apple M4
0.00.109.837 I ggml_metal_init: using embedded metal library
0.00.119.264 I ggml_metal_init: GPU name:   Apple M4
0.00.119.266 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.119.266 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.119.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.119.267 I ggml_metal_init: simdgroup reduction   = true
0.00.119.267 I ggml_metal_init: simdgroup matrix mul. = true
0.00.119.267 I ggml_metal_init: has residency sets    = true
0.00.119.267 I ggml_metal_init: has bfloat            = true
0.00.119.267 I ggml_metal_init: use bfloat            = true
0.00.119.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.119.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.142.918 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.169.599 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.169.606 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.169.651 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.174.826 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.174.829 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.174.829 I llama_init_from_model: graph nodes  = 967
0.00.174.829 I llama_init_from_model: graph splits = 2
0.00.174.833 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.174.963 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.174.963 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.233.179 I main: llama threadpool init, n_threads = 4
0.00.233.222 I 
0.00.233.257 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.233.259 I 
0.00.233.302 I sampler seed: 1234
0.00.233.307 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.233.331 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.233.332 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.233.332 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.038.683 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.02.038.684 I llama_perf_context_print:        load time =     206.58 ms
0.02.038.685 I llama_perf_context_print: prompt eval time =      43.56 ms /     7 tokens (    6.22 ms per token,   160.68 tokens per second)
0.02.038.686 I llama_perf_context_print:        eval time =    1758.94 ms /    63 runs   (   27.92 ms per token,    35.82 tokens per second)
0.02.038.686 I llama_perf_context_print:       total time =    1806.59 ms /    70 tokens
0.02.038.894 I ggml_metal_free: deallocating

real	0m2.358s
user	0m0.132s
sys	0m0.118s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.828 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.255 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.261 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.262 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.266 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.267 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.267 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.268 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.268 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.268 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.271 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.271 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.478 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.536 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.536 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.537 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.538 I llama_model_loader: - type  f32:  194 tensors
0.00.034.538 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.539 I print_info: file format = GGUF V3 (latest)
0.00.034.539 I print_info: file type   = Q8_0
0.00.034.540 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.662 I load: special tokens cache size = 25
0.00.050.279 I load: token to piece cache size = 0.2984 MB
0.00.050.285 I print_info: arch             = gptneox
0.00.050.286 I print_info: vocab_only       = 0
0.00.050.286 I print_info: n_ctx_train      = 2048
0.00.050.286 I print_info: n_embd           = 2048
0.00.050.287 I print_info: n_layer          = 24
0.00.050.293 I print_info: n_head           = 16
0.00.050.293 I print_info: n_head_kv        = 16
0.00.050.294 I print_info: n_rot            = 32
0.00.050.294 I print_info: n_swa            = 0
0.00.050.294 I print_info: n_embd_head_k    = 128
0.00.050.294 I print_info: n_embd_head_v    = 128
0.00.050.295 I print_info: n_gqa            = 1
0.00.050.295 I print_info: n_embd_k_gqa     = 2048
0.00.050.296 I print_info: n_embd_v_gqa     = 2048
0.00.050.297 I print_info: f_norm_eps       = 1.0e-05
0.00.050.297 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.297 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.298 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.298 I print_info: f_logit_scale    = 0.0e+00
0.00.050.298 I print_info: n_ff             = 8192
0.00.050.298 I print_info: n_expert         = 0
0.00.050.299 I print_info: n_expert_used    = 0
0.00.050.299 I print_info: causal attn      = 1
0.00.050.299 I print_info: pooling type     = 0
0.00.050.299 I print_info: rope type        = 2
0.00.050.299 I print_info: rope scaling     = linear
0.00.050.300 I print_info: freq_base_train  = 10000.0
0.00.050.300 I print_info: freq_scale_train = 1
0.00.050.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.300 I print_info: rope_finetuned   = unknown
0.00.050.300 I print_info: ssm_d_conv       = 0
0.00.050.300 I print_info: ssm_d_inner      = 0
0.00.050.300 I print_info: ssm_d_state      = 0
0.00.050.300 I print_info: ssm_dt_rank      = 0
0.00.050.301 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.301 I print_info: model type       = 1.4B
0.00.050.301 I print_info: model params     = 1.41 B
0.00.050.301 I print_info: general.name     = 1.4B
0.00.050.302 I print_info: vocab type       = BPE
0.00.050.302 I print_info: n_vocab          = 50304
0.00.050.302 I print_info: n_merges         = 50009
0.00.050.303 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.303 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.303 I print_info: LF token         = 187 'Ċ'
0.00.050.304 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.304 I print_info: max token length = 1024
0.01.119.726 I load_tensors: offloading 24 repeating layers to GPU
0.01.119.730 I load_tensors: offloading output layer to GPU
0.01.119.731 I load_tensors: offloaded 25/25 layers to GPU
0.01.119.754 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.119.755 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.120.360 I llama_init_from_model: n_seq_max     = 1
0.01.120.362 I llama_init_from_model: n_ctx         = 2048
0.01.120.362 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.120.363 I llama_init_from_model: n_batch       = 2048
0.01.120.363 I llama_init_from_model: n_ubatch      = 512
0.01.120.363 I llama_init_from_model: flash_attn    = 0
0.01.120.364 I llama_init_from_model: freq_base     = 10000.0
0.01.120.364 I llama_init_from_model: freq_scale    = 1
0.01.120.365 I ggml_metal_init: allocating
0.01.120.378 I ggml_metal_init: found device: Apple M4
0.01.120.384 I ggml_metal_init: picking default device: Apple M4
0.01.121.507 I ggml_metal_init: using embedded metal library
0.01.126.096 I ggml_metal_init: GPU name:   Apple M4
0.01.126.099 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.126.099 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.126.100 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.126.100 I ggml_metal_init: simdgroup reduction   = true
0.01.126.101 I ggml_metal_init: simdgroup matrix mul. = true
0.01.126.101 I ggml_metal_init: has residency sets    = true
0.01.126.101 I ggml_metal_init: has bfloat            = true
0.01.126.101 I ggml_metal_init: use bfloat            = true
0.01.126.102 I ggml_metal_init: hasUnifiedMemory      = true
0.01.126.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.139.265 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.172.080 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.172.087 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.172.120 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.178.401 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.178.403 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.178.404 I llama_init_from_model: graph nodes  = 967
0.01.178.404 I llama_init_from_model: graph splits = 2
0.01.178.409 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.178.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.178.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.224.961 I main: llama threadpool init, n_threads = 4
0.01.224.994 I 
0.01.225.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.225.015 I 
0.01.225.133 I sampler seed: 1234
0.01.225.138 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.225.155 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.225.155 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.225.155 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.316.903 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.02.316.903 I llama_perf_context_print:        load time =    1214.20 ms
0.02.316.904 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.57 tokens per second)
0.02.316.905 I llama_perf_context_print:        eval time =    1039.74 ms /    63 runs   (   16.50 ms per token,    60.59 tokens per second)
0.02.316.905 I llama_perf_context_print:       total time =    1092.87 ms /    70 tokens
0.02.317.172 I ggml_metal_free: deallocating

real	0m2.334s
user	0m0.106s
sys	0m0.315s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.548 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.943 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.955 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.955 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.956 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.956 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.956 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.960 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.960 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.960 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.961 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.961 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.962 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.962 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.964 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.965 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.983 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.156 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.158 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.159 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.159 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.160 I llama_model_loader: - type  f32:  194 tensors
0.00.028.160 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.160 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.161 I print_info: file format = GGUF V3 (latest)
0.00.028.162 I print_info: file type   = Q4_0
0.00.028.163 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.573 I load: special tokens cache size = 25
0.00.042.646 I load: token to piece cache size = 0.2984 MB
0.00.042.650 I print_info: arch             = gptneox
0.00.042.651 I print_info: vocab_only       = 0
0.00.042.651 I print_info: n_ctx_train      = 2048
0.00.042.651 I print_info: n_embd           = 2048
0.00.042.651 I print_info: n_layer          = 24
0.00.042.656 I print_info: n_head           = 16
0.00.042.657 I print_info: n_head_kv        = 16
0.00.042.657 I print_info: n_rot            = 32
0.00.042.657 I print_info: n_swa            = 0
0.00.042.657 I print_info: n_embd_head_k    = 128
0.00.042.660 I print_info: n_embd_head_v    = 128
0.00.042.661 I print_info: n_gqa            = 1
0.00.042.662 I print_info: n_embd_k_gqa     = 2048
0.00.042.662 I print_info: n_embd_v_gqa     = 2048
0.00.042.663 I print_info: f_norm_eps       = 1.0e-05
0.00.042.663 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.664 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.664 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.664 I print_info: f_logit_scale    = 0.0e+00
0.00.042.665 I print_info: n_ff             = 8192
0.00.042.665 I print_info: n_expert         = 0
0.00.042.665 I print_info: n_expert_used    = 0
0.00.042.674 I print_info: causal attn      = 1
0.00.042.676 I print_info: pooling type     = 0
0.00.042.676 I print_info: rope type        = 2
0.00.042.677 I print_info: rope scaling     = linear
0.00.042.678 I print_info: freq_base_train  = 10000.0
0.00.042.678 I print_info: freq_scale_train = 1
0.00.042.679 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.679 I print_info: rope_finetuned   = unknown
0.00.042.679 I print_info: ssm_d_conv       = 0
0.00.042.679 I print_info: ssm_d_inner      = 0
0.00.042.680 I print_info: ssm_d_state      = 0
0.00.042.680 I print_info: ssm_dt_rank      = 0
0.00.042.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.681 I print_info: model type       = 1.4B
0.00.042.681 I print_info: model params     = 1.41 B
0.00.042.681 I print_info: general.name     = 1.4B
0.00.042.682 I print_info: vocab type       = BPE
0.00.042.682 I print_info: n_vocab          = 50304
0.00.042.683 I print_info: n_merges         = 50009
0.00.042.684 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.684 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.684 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.684 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.684 I print_info: LF token         = 187 'Ċ'
0.00.042.685 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.685 I print_info: max token length = 1024
0.00.630.598 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.604 I load_tensors: offloading output layer to GPU
0.00.630.606 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.628 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.630.630 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.631.653 I llama_init_from_model: n_seq_max     = 1
0.00.631.655 I llama_init_from_model: n_ctx         = 2048
0.00.631.656 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.631.656 I llama_init_from_model: n_batch       = 2048
0.00.631.657 I llama_init_from_model: n_ubatch      = 512
0.00.631.657 I llama_init_from_model: flash_attn    = 0
0.00.631.658 I llama_init_from_model: freq_base     = 10000.0
0.00.631.659 I llama_init_from_model: freq_scale    = 1
0.00.631.660 I ggml_metal_init: allocating
0.00.631.677 I ggml_metal_init: found device: Apple M4
0.00.631.687 I ggml_metal_init: picking default device: Apple M4
0.00.633.097 I ggml_metal_init: using embedded metal library
0.00.639.259 I ggml_metal_init: GPU name:   Apple M4
0.00.639.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.266 I ggml_metal_init: simdgroup reduction   = true
0.00.639.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.266 I ggml_metal_init: has residency sets    = true
0.00.639.266 I ggml_metal_init: has bfloat            = true
0.00.639.267 I ggml_metal_init: use bfloat            = true
0.00.639.267 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.278 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.514 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.327 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.733.335 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.370 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.570 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.573 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.574 I llama_init_from_model: graph nodes  = 967
0.00.750.574 I llama_init_from_model: graph splits = 2
0.00.750.579 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.287 I main: llama threadpool init, n_threads = 4
0.00.799.335 I 
0.00.799.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.359 I 
0.00.799.478 I sampler seed: 1234
0.00.799.482 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.493 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.494 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.494 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.482.322 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48168.25 tokens per second)
0.01.482.322 I llama_perf_context_print:        load time =     786.82 ms
0.01.482.324 I llama_perf_context_print: prompt eval time =      49.86 ms /     7 tokens (    7.12 ms per token,   140.40 tokens per second)
0.01.482.325 I llama_perf_context_print:        eval time =     630.12 ms /    63 runs   (   10.00 ms per token,    99.98 tokens per second)
0.01.482.325 I llama_perf_context_print:       total time =     683.96 ms /    70 tokens
0.01.482.609 I ggml_metal_free: deallocating

real	0m1.501s
user	0m0.111s
sys	0m0.252s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.011.699 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.636 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.636 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.637 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.637 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.638 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.639 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.641 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.641 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.641 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.623 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.720 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.640 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.641 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.642 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.642 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.642 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.643 I llama_model_loader: - type  f32:  194 tensors
0.00.028.643 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.644 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.644 I print_info: file format = GGUF V3 (latest)
0.00.028.645 I print_info: file type   = Q4_1
0.00.028.646 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.036.495 I load: special tokens cache size = 25
0.00.042.485 I load: token to piece cache size = 0.2984 MB
0.00.042.488 I print_info: arch             = gptneox
0.00.042.489 I print_info: vocab_only       = 0
0.00.042.489 I print_info: n_ctx_train      = 2048
0.00.042.489 I print_info: n_embd           = 2048
0.00.042.489 I print_info: n_layer          = 24
0.00.042.492 I print_info: n_head           = 16
0.00.042.493 I print_info: n_head_kv        = 16
0.00.042.493 I print_info: n_rot            = 32
0.00.042.493 I print_info: n_swa            = 0
0.00.042.493 I print_info: n_embd_head_k    = 128
0.00.042.493 I print_info: n_embd_head_v    = 128
0.00.042.494 I print_info: n_gqa            = 1
0.00.042.495 I print_info: n_embd_k_gqa     = 2048
0.00.042.496 I print_info: n_embd_v_gqa     = 2048
0.00.042.496 I print_info: f_norm_eps       = 1.0e-05
0.00.042.497 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.497 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.497 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.497 I print_info: f_logit_scale    = 0.0e+00
0.00.042.498 I print_info: n_ff             = 8192
0.00.042.500 I print_info: n_expert         = 0
0.00.042.500 I print_info: n_expert_used    = 0
0.00.042.500 I print_info: causal attn      = 1
0.00.042.500 I print_info: pooling type     = 0
0.00.042.502 I print_info: rope type        = 2
0.00.042.502 I print_info: rope scaling     = linear
0.00.042.503 I print_info: freq_base_train  = 10000.0
0.00.042.503 I print_info: freq_scale_train = 1
0.00.042.503 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.503 I print_info: rope_finetuned   = unknown
0.00.042.503 I print_info: ssm_d_conv       = 0
0.00.042.504 I print_info: ssm_d_inner      = 0
0.00.042.504 I print_info: ssm_d_state      = 0
0.00.042.504 I print_info: ssm_dt_rank      = 0
0.00.042.504 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.504 I print_info: model type       = 1.4B
0.00.042.505 I print_info: model params     = 1.41 B
0.00.042.505 I print_info: general.name     = 1.4B
0.00.042.505 I print_info: vocab type       = BPE
0.00.042.506 I print_info: n_vocab          = 50304
0.00.042.506 I print_info: n_merges         = 50009
0.00.042.511 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.511 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.511 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.511 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.511 I print_info: LF token         = 187 'Ċ'
0.00.042.511 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.512 I print_info: max token length = 1024
0.00.657.535 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.539 I load_tensors: offloading output layer to GPU
0.00.657.539 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.560 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.657.562 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.659.005 I llama_init_from_model: n_seq_max     = 1
0.00.659.007 I llama_init_from_model: n_ctx         = 2048
0.00.659.008 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.659.008 I llama_init_from_model: n_batch       = 2048
0.00.659.008 I llama_init_from_model: n_ubatch      = 512
0.00.659.009 I llama_init_from_model: flash_attn    = 0
0.00.659.010 I llama_init_from_model: freq_base     = 10000.0
0.00.659.010 I llama_init_from_model: freq_scale    = 1
0.00.659.015 I ggml_metal_init: allocating
0.00.659.037 I ggml_metal_init: found device: Apple M4
0.00.659.046 I ggml_metal_init: picking default device: Apple M4
0.00.660.398 I ggml_metal_init: using embedded metal library
0.00.666.386 I ggml_metal_init: GPU name:   Apple M4
0.00.666.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.393 I ggml_metal_init: simdgroup reduction   = true
0.00.666.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.393 I ggml_metal_init: has residency sets    = true
0.00.666.393 I ggml_metal_init: has bfloat            = true
0.00.666.394 I ggml_metal_init: use bfloat            = true
0.00.666.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.404 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.526 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.385 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.737.391 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.737.427 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.661 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.742.663 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.742.664 I llama_init_from_model: graph nodes  = 967
0.00.742.664 I llama_init_from_model: graph splits = 2
0.00.742.671 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.742.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.517 I main: llama threadpool init, n_threads = 4
0.00.801.563 I 
0.00.801.599 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.601 I 
0.00.801.788 I sampler seed: 1234
0.00.801.793 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.829 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.830 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.830 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.516.270 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.516.271 I llama_perf_context_print:        load time =     788.63 ms
0.01.516.271 I llama_perf_context_print: prompt eval time =      39.25 ms /     7 tokens (    5.61 ms per token,   178.34 tokens per second)
0.01.516.272 I llama_perf_context_print:        eval time =     672.42 ms /    63 runs   (   10.67 ms per token,    93.69 tokens per second)
0.01.516.272 I llama_perf_context_print:       total time =     715.94 ms /    70 tokens
0.01.516.571 I ggml_metal_free: deallocating

real	0m1.536s
user	0m0.107s
sys	0m0.249s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.118 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.256 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.264 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.265 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.265 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.266 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.267 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.268 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.268 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.268 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.269 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.270 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.271 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.270 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.270 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.270 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.271 I llama_model_loader: - type  f32:  194 tensors
0.00.026.272 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.272 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.273 I print_info: file format = GGUF V3 (latest)
0.00.026.273 I print_info: file type   = Q5_0
0.00.026.274 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.116 I load: special tokens cache size = 25
0.00.040.124 I load: token to piece cache size = 0.2984 MB
0.00.040.127 I print_info: arch             = gptneox
0.00.040.128 I print_info: vocab_only       = 0
0.00.040.128 I print_info: n_ctx_train      = 2048
0.00.040.128 I print_info: n_embd           = 2048
0.00.040.128 I print_info: n_layer          = 24
0.00.040.131 I print_info: n_head           = 16
0.00.040.132 I print_info: n_head_kv        = 16
0.00.040.132 I print_info: n_rot            = 32
0.00.040.132 I print_info: n_swa            = 0
0.00.040.133 I print_info: n_embd_head_k    = 128
0.00.040.133 I print_info: n_embd_head_v    = 128
0.00.040.133 I print_info: n_gqa            = 1
0.00.040.134 I print_info: n_embd_k_gqa     = 2048
0.00.040.135 I print_info: n_embd_v_gqa     = 2048
0.00.040.135 I print_info: f_norm_eps       = 1.0e-05
0.00.040.136 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.136 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.136 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.136 I print_info: f_logit_scale    = 0.0e+00
0.00.040.137 I print_info: n_ff             = 8192
0.00.040.137 I print_info: n_expert         = 0
0.00.040.137 I print_info: n_expert_used    = 0
0.00.040.138 I print_info: causal attn      = 1
0.00.040.138 I print_info: pooling type     = 0
0.00.040.138 I print_info: rope type        = 2
0.00.040.138 I print_info: rope scaling     = linear
0.00.040.139 I print_info: freq_base_train  = 10000.0
0.00.040.139 I print_info: freq_scale_train = 1
0.00.040.139 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.139 I print_info: rope_finetuned   = unknown
0.00.040.140 I print_info: ssm_d_conv       = 0
0.00.040.140 I print_info: ssm_d_inner      = 0
0.00.040.140 I print_info: ssm_d_state      = 0
0.00.040.140 I print_info: ssm_dt_rank      = 0
0.00.040.140 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.140 I print_info: model type       = 1.4B
0.00.040.141 I print_info: model params     = 1.41 B
0.00.040.141 I print_info: general.name     = 1.4B
0.00.040.141 I print_info: vocab type       = BPE
0.00.040.142 I print_info: n_vocab          = 50304
0.00.040.142 I print_info: n_merges         = 50009
0.00.040.142 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.142 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.143 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.143 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.143 I print_info: LF token         = 187 'Ċ'
0.00.040.143 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.144 I print_info: max token length = 1024
0.00.718.886 I load_tensors: offloading 24 repeating layers to GPU
0.00.718.890 I load_tensors: offloading output layer to GPU
0.00.718.891 I load_tensors: offloaded 25/25 layers to GPU
0.00.718.910 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.718.911 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.720.029 I llama_init_from_model: n_seq_max     = 1
0.00.720.031 I llama_init_from_model: n_ctx         = 2048
0.00.720.032 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.720.033 I llama_init_from_model: n_batch       = 2048
0.00.720.033 I llama_init_from_model: n_ubatch      = 512
0.00.720.033 I llama_init_from_model: flash_attn    = 0
0.00.720.034 I llama_init_from_model: freq_base     = 10000.0
0.00.720.035 I llama_init_from_model: freq_scale    = 1
0.00.720.037 I ggml_metal_init: allocating
0.00.720.054 I ggml_metal_init: found device: Apple M4
0.00.720.067 I ggml_metal_init: picking default device: Apple M4
0.00.721.353 I ggml_metal_init: using embedded metal library
0.00.727.178 I ggml_metal_init: GPU name:   Apple M4
0.00.727.182 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.727.183 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.727.183 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.727.184 I ggml_metal_init: simdgroup reduction   = true
0.00.727.184 I ggml_metal_init: simdgroup matrix mul. = true
0.00.727.184 I ggml_metal_init: has residency sets    = true
0.00.727.185 I ggml_metal_init: has bfloat            = true
0.00.727.185 I ggml_metal_init: use bfloat            = true
0.00.727.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.727.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.200 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.798.095 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.798.102 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.798.145 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.803.581 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.803.584 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.803.584 I llama_init_from_model: graph nodes  = 967
0.00.803.585 I llama_init_from_model: graph splits = 2
0.00.803.591 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.803.715 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.803.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.852.837 I main: llama threadpool init, n_threads = 4
0.00.852.879 I 
0.00.852.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.852.908 I 
0.00.853.033 I sampler seed: 1234
0.00.853.038 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.853.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.853.077 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.853.077 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.636.948 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.636.949 I llama_perf_context_print:        load time =     842.77 ms
0.01.636.950 I llama_perf_context_print: prompt eval time =      43.37 ms /     7 tokens (    6.20 ms per token,   161.40 tokens per second)
0.01.636.950 I llama_perf_context_print:        eval time =     737.49 ms /    63 runs   (   11.71 ms per token,    85.42 tokens per second)
0.01.636.952 I llama_perf_context_print:       total time =     785.06 ms /    70 tokens
0.01.637.229 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.107s
sys	0m0.244s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.526 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.488 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.495 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.496 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.499 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.500 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.501 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.501 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.449 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.508 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.509 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.509 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.509 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.510 I llama_model_loader: - type  f32:  194 tensors
0.00.026.510 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.510 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.511 I print_info: file format = GGUF V3 (latest)
0.00.026.512 I print_info: file type   = Q5_1
0.00.026.512 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.388 I load: special tokens cache size = 25
0.00.040.212 I load: token to piece cache size = 0.2984 MB
0.00.040.215 I print_info: arch             = gptneox
0.00.040.215 I print_info: vocab_only       = 0
0.00.040.216 I print_info: n_ctx_train      = 2048
0.00.040.216 I print_info: n_embd           = 2048
0.00.040.216 I print_info: n_layer          = 24
0.00.040.219 I print_info: n_head           = 16
0.00.040.219 I print_info: n_head_kv        = 16
0.00.040.220 I print_info: n_rot            = 32
0.00.040.220 I print_info: n_swa            = 0
0.00.040.220 I print_info: n_embd_head_k    = 128
0.00.040.220 I print_info: n_embd_head_v    = 128
0.00.040.221 I print_info: n_gqa            = 1
0.00.040.221 I print_info: n_embd_k_gqa     = 2048
0.00.040.222 I print_info: n_embd_v_gqa     = 2048
0.00.040.223 I print_info: f_norm_eps       = 1.0e-05
0.00.040.223 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.223 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.223 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.224 I print_info: f_logit_scale    = 0.0e+00
0.00.040.226 I print_info: n_ff             = 8192
0.00.040.226 I print_info: n_expert         = 0
0.00.040.226 I print_info: n_expert_used    = 0
0.00.040.226 I print_info: causal attn      = 1
0.00.040.227 I print_info: pooling type     = 0
0.00.040.227 I print_info: rope type        = 2
0.00.040.227 I print_info: rope scaling     = linear
0.00.040.229 I print_info: freq_base_train  = 10000.0
0.00.040.229 I print_info: freq_scale_train = 1
0.00.040.229 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.229 I print_info: rope_finetuned   = unknown
0.00.040.230 I print_info: ssm_d_conv       = 0
0.00.040.230 I print_info: ssm_d_inner      = 0
0.00.040.231 I print_info: ssm_d_state      = 0
0.00.040.231 I print_info: ssm_dt_rank      = 0
0.00.040.231 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.231 I print_info: model type       = 1.4B
0.00.040.232 I print_info: model params     = 1.41 B
0.00.040.232 I print_info: general.name     = 1.4B
0.00.040.232 I print_info: vocab type       = BPE
0.00.040.233 I print_info: n_vocab          = 50304
0.00.040.233 I print_info: n_merges         = 50009
0.00.040.233 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.233 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.233 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.234 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.234 I print_info: LF token         = 187 'Ċ'
0.00.040.235 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.235 I print_info: max token length = 1024
0.00.719.695 I load_tensors: offloading 24 repeating layers to GPU
0.00.719.698 I load_tensors: offloading output layer to GPU
0.00.719.699 I load_tensors: offloaded 25/25 layers to GPU
0.00.719.720 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.719.722 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.720.800 I llama_init_from_model: n_seq_max     = 1
0.00.720.805 I llama_init_from_model: n_ctx         = 2048
0.00.720.805 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.720.805 I llama_init_from_model: n_batch       = 2048
0.00.720.806 I llama_init_from_model: n_ubatch      = 512
0.00.720.806 I llama_init_from_model: flash_attn    = 0
0.00.720.807 I llama_init_from_model: freq_base     = 10000.0
0.00.720.808 I llama_init_from_model: freq_scale    = 1
0.00.720.813 I ggml_metal_init: allocating
0.00.720.842 I ggml_metal_init: found device: Apple M4
0.00.720.853 I ggml_metal_init: picking default device: Apple M4
0.00.722.144 I ggml_metal_init: using embedded metal library
0.00.728.142 I ggml_metal_init: GPU name:   Apple M4
0.00.728.145 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.728.147 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.728.148 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.728.148 I ggml_metal_init: simdgroup reduction   = true
0.00.728.148 I ggml_metal_init: simdgroup matrix mul. = true
0.00.728.149 I ggml_metal_init: has residency sets    = true
0.00.728.149 I ggml_metal_init: has bfloat            = true
0.00.728.149 I ggml_metal_init: use bfloat            = true
0.00.728.150 I ggml_metal_init: hasUnifiedMemory      = true
0.00.728.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.744.686 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.797.141 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.797.147 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.797.181 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.801.862 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.801.865 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.801.865 I llama_init_from_model: graph nodes  = 967
0.00.801.865 I llama_init_from_model: graph splits = 2
0.00.801.870 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.801.992 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.801.992 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.849.624 I main: llama threadpool init, n_threads = 4
0.00.849.670 I 
0.00.849.695 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.849.697 I 
0.00.849.828 I sampler seed: 1234
0.00.849.833 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.849.842 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.849.843 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.849.846 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.682.799 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51673.94 tokens per second)
0.01.682.800 I llama_perf_context_print:        load time =     839.18 ms
0.01.682.801 I llama_perf_context_print: prompt eval time =      41.81 ms /     7 tokens (    5.97 ms per token,   167.42 tokens per second)
0.01.682.801 I llama_perf_context_print:        eval time =     788.10 ms /    63 runs   (   12.51 ms per token,    79.94 tokens per second)
0.01.682.802 I llama_perf_context_print:       total time =     834.10 ms /    70 tokens
0.01.683.073 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.107s
sys	0m0.258s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.397 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.912 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.919 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.920 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.920 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.920 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.921 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.922 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.922 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.922 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.923 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.926 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.926 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.985 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.998 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.999 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.999 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.000 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.000 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.000 I llama_model_loader: - type  f32:  194 tensors
0.00.025.001 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.001 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.001 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.002 I print_info: file format = GGUF V3 (latest)
0.00.025.002 I print_info: file type   = Q2_K - Medium
0.00.025.003 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.857 I load: special tokens cache size = 25
0.00.038.927 I load: token to piece cache size = 0.2984 MB
0.00.038.930 I print_info: arch             = gptneox
0.00.038.930 I print_info: vocab_only       = 0
0.00.038.931 I print_info: n_ctx_train      = 2048
0.00.038.931 I print_info: n_embd           = 2048
0.00.038.931 I print_info: n_layer          = 24
0.00.038.934 I print_info: n_head           = 16
0.00.038.934 I print_info: n_head_kv        = 16
0.00.038.934 I print_info: n_rot            = 32
0.00.038.935 I print_info: n_swa            = 0
0.00.038.935 I print_info: n_embd_head_k    = 128
0.00.038.935 I print_info: n_embd_head_v    = 128
0.00.038.936 I print_info: n_gqa            = 1
0.00.038.936 I print_info: n_embd_k_gqa     = 2048
0.00.038.937 I print_info: n_embd_v_gqa     = 2048
0.00.038.938 I print_info: f_norm_eps       = 1.0e-05
0.00.038.938 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.938 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.938 I print_info: f_logit_scale    = 0.0e+00
0.00.038.939 I print_info: n_ff             = 8192
0.00.038.939 I print_info: n_expert         = 0
0.00.038.939 I print_info: n_expert_used    = 0
0.00.038.940 I print_info: causal attn      = 1
0.00.038.940 I print_info: pooling type     = 0
0.00.038.940 I print_info: rope type        = 2
0.00.038.940 I print_info: rope scaling     = linear
0.00.038.941 I print_info: freq_base_train  = 10000.0
0.00.038.941 I print_info: freq_scale_train = 1
0.00.038.941 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.941 I print_info: rope_finetuned   = unknown
0.00.038.941 I print_info: ssm_d_conv       = 0
0.00.038.942 I print_info: ssm_d_inner      = 0
0.00.038.942 I print_info: ssm_d_state      = 0
0.00.038.942 I print_info: ssm_dt_rank      = 0
0.00.038.942 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.942 I print_info: model type       = 1.4B
0.00.038.943 I print_info: model params     = 1.41 B
0.00.038.943 I print_info: general.name     = 1.4B
0.00.038.944 I print_info: vocab type       = BPE
0.00.038.944 I print_info: n_vocab          = 50304
0.00.038.944 I print_info: n_merges         = 50009
0.00.038.944 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.947 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.947 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.947 I print_info: LF token         = 187 'Ċ'
0.00.038.948 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.948 I print_info: max token length = 1024
0.00.411.863 I load_tensors: offloading 24 repeating layers to GPU
0.00.411.876 I load_tensors: offloading output layer to GPU
0.00.411.876 I load_tensors: offloaded 25/25 layers to GPU
0.00.411.904 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.411.906 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.413.193 I llama_init_from_model: n_seq_max     = 1
0.00.413.198 I llama_init_from_model: n_ctx         = 2048
0.00.413.198 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.413.199 I llama_init_from_model: n_batch       = 2048
0.00.413.199 I llama_init_from_model: n_ubatch      = 512
0.00.413.200 I llama_init_from_model: flash_attn    = 0
0.00.413.202 I llama_init_from_model: freq_base     = 10000.0
0.00.413.203 I llama_init_from_model: freq_scale    = 1
0.00.413.205 I ggml_metal_init: allocating
0.00.413.266 I ggml_metal_init: found device: Apple M4
0.00.413.279 I ggml_metal_init: picking default device: Apple M4
0.00.415.097 I ggml_metal_init: using embedded metal library
0.00.421.939 I ggml_metal_init: GPU name:   Apple M4
0.00.421.943 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.421.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.421.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.421.945 I ggml_metal_init: simdgroup reduction   = true
0.00.421.946 I ggml_metal_init: simdgroup matrix mul. = true
0.00.421.946 I ggml_metal_init: has residency sets    = true
0.00.421.946 I ggml_metal_init: has bfloat            = true
0.00.421.947 I ggml_metal_init: use bfloat            = true
0.00.421.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.421.949 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.440.866 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.495.282 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.495.290 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.495.332 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.500.084 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.500.086 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.500.086 I llama_init_from_model: graph nodes  = 967
0.00.500.087 I llama_init_from_model: graph splits = 2
0.00.500.092 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.500.205 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.500.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.454 I main: llama threadpool init, n_threads = 4
0.00.549.499 I 
0.00.549.536 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.539 I 
0.00.549.680 I sampler seed: 1234
0.00.549.685 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.549.715 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.549.717 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.549.717 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.230.442 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.230.443 I llama_perf_context_print:        load time =     538.88 ms
0.01.230.443 I llama_perf_context_print: prompt eval time =      35.45 ms /     7 tokens (    5.06 ms per token,   197.48 tokens per second)
0.01.230.444 I llama_perf_context_print:        eval time =     642.32 ms /    63 runs   (   10.20 ms per token,    98.08 tokens per second)
0.01.230.444 I llama_perf_context_print:       total time =     682.16 ms /    70 tokens
0.01.230.736 I ggml_metal_free: deallocating

real	0m1.249s
user	0m0.111s
sys	0m0.186s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.149 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.088 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.096 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.096 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.096 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.098 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.099 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.099 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.099 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.100 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.103 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.103 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.103 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.250 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.262 I llama_model_loader: - type  f32:  194 tensors
0.00.026.262 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.263 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.263 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.263 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.263 I print_info: file format = GGUF V3 (latest)
0.00.026.264 I print_info: file type   = Q3_K - Medium
0.00.026.264 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.071 I load: special tokens cache size = 25
0.00.040.135 I load: token to piece cache size = 0.2984 MB
0.00.040.138 I print_info: arch             = gptneox
0.00.040.138 I print_info: vocab_only       = 0
0.00.040.139 I print_info: n_ctx_train      = 2048
0.00.040.139 I print_info: n_embd           = 2048
0.00.040.139 I print_info: n_layer          = 24
0.00.040.141 I print_info: n_head           = 16
0.00.040.142 I print_info: n_head_kv        = 16
0.00.040.142 I print_info: n_rot            = 32
0.00.040.142 I print_info: n_swa            = 0
0.00.040.143 I print_info: n_embd_head_k    = 128
0.00.040.143 I print_info: n_embd_head_v    = 128
0.00.040.144 I print_info: n_gqa            = 1
0.00.040.144 I print_info: n_embd_k_gqa     = 2048
0.00.040.145 I print_info: n_embd_v_gqa     = 2048
0.00.040.146 I print_info: f_norm_eps       = 1.0e-05
0.00.040.147 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.147 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.147 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.147 I print_info: f_logit_scale    = 0.0e+00
0.00.040.148 I print_info: n_ff             = 8192
0.00.040.148 I print_info: n_expert         = 0
0.00.040.148 I print_info: n_expert_used    = 0
0.00.040.150 I print_info: causal attn      = 1
0.00.040.150 I print_info: pooling type     = 0
0.00.040.150 I print_info: rope type        = 2
0.00.040.150 I print_info: rope scaling     = linear
0.00.040.151 I print_info: freq_base_train  = 10000.0
0.00.040.151 I print_info: freq_scale_train = 1
0.00.040.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.151 I print_info: rope_finetuned   = unknown
0.00.040.151 I print_info: ssm_d_conv       = 0
0.00.040.151 I print_info: ssm_d_inner      = 0
0.00.040.152 I print_info: ssm_d_state      = 0
0.00.040.152 I print_info: ssm_dt_rank      = 0
0.00.040.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.153 I print_info: model type       = 1.4B
0.00.040.153 I print_info: model params     = 1.41 B
0.00.040.153 I print_info: general.name     = 1.4B
0.00.040.154 I print_info: vocab type       = BPE
0.00.040.154 I print_info: n_vocab          = 50304
0.00.040.156 I print_info: n_merges         = 50009
0.00.040.157 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.157 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.157 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.157 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.157 I print_info: LF token         = 187 'Ċ'
0.00.040.158 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.158 I print_info: max token length = 1024
0.00.497.092 I load_tensors: offloading 24 repeating layers to GPU
0.00.497.097 I load_tensors: offloading output layer to GPU
0.00.497.098 I load_tensors: offloaded 25/25 layers to GPU
0.00.497.127 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.497.129 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.498.425 I llama_init_from_model: n_seq_max     = 1
0.00.498.429 I llama_init_from_model: n_ctx         = 2048
0.00.498.429 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.498.430 I llama_init_from_model: n_batch       = 2048
0.00.498.430 I llama_init_from_model: n_ubatch      = 512
0.00.498.431 I llama_init_from_model: flash_attn    = 0
0.00.498.437 I llama_init_from_model: freq_base     = 10000.0
0.00.498.437 I llama_init_from_model: freq_scale    = 1
0.00.498.440 I ggml_metal_init: allocating
0.00.498.493 I ggml_metal_init: found device: Apple M4
0.00.498.506 I ggml_metal_init: picking default device: Apple M4
0.00.500.281 I ggml_metal_init: using embedded metal library
0.00.507.038 I ggml_metal_init: GPU name:   Apple M4
0.00.507.044 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.507.045 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.507.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.507.046 I ggml_metal_init: simdgroup reduction   = true
0.00.507.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.507.047 I ggml_metal_init: has residency sets    = true
0.00.507.047 I ggml_metal_init: has bfloat            = true
0.00.507.048 I ggml_metal_init: use bfloat            = true
0.00.507.049 I ggml_metal_init: hasUnifiedMemory      = true
0.00.507.050 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.526.354 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.581.640 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.581.647 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.581.685 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.586.562 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.586.565 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.586.565 I llama_init_from_model: graph nodes  = 967
0.00.586.565 I llama_init_from_model: graph splits = 2
0.00.586.571 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.586.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.586.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.632.739 I main: llama threadpool init, n_threads = 4
0.00.632.783 I 
0.00.632.810 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.632.811 I 
0.00.632.941 I sampler seed: 1234
0.00.632.946 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.632.964 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.632.964 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.632.964 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.371.422 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.371.423 I llama_perf_context_print:        load time =     622.64 ms
0.01.371.424 I llama_perf_context_print: prompt eval time =      40.10 ms /     7 tokens (    5.73 ms per token,   174.56 tokens per second)
0.01.371.424 I llama_perf_context_print:        eval time =     695.29 ms /    63 runs   (   11.04 ms per token,    90.61 tokens per second)
0.01.371.426 I llama_perf_context_print:       total time =     739.63 ms /    70 tokens
0.01.371.702 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.111s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.310 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.496 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.502 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.504 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.504 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.505 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.549 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.430 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.432 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.433 I llama_model_loader: - type  f32:  194 tensors
0.00.027.433 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.433 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.433 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.434 I print_info: file format = GGUF V3 (latest)
0.00.027.434 I print_info: file type   = Q4_K - Medium
0.00.027.435 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.274 I load: special tokens cache size = 25
0.00.041.278 I load: token to piece cache size = 0.2984 MB
0.00.041.281 I print_info: arch             = gptneox
0.00.041.281 I print_info: vocab_only       = 0
0.00.041.282 I print_info: n_ctx_train      = 2048
0.00.041.282 I print_info: n_embd           = 2048
0.00.041.282 I print_info: n_layer          = 24
0.00.041.285 I print_info: n_head           = 16
0.00.041.286 I print_info: n_head_kv        = 16
0.00.041.286 I print_info: n_rot            = 32
0.00.041.286 I print_info: n_swa            = 0
0.00.041.286 I print_info: n_embd_head_k    = 128
0.00.041.286 I print_info: n_embd_head_v    = 128
0.00.041.287 I print_info: n_gqa            = 1
0.00.041.288 I print_info: n_embd_k_gqa     = 2048
0.00.041.288 I print_info: n_embd_v_gqa     = 2048
0.00.041.289 I print_info: f_norm_eps       = 1.0e-05
0.00.041.290 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.290 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.292 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.292 I print_info: f_logit_scale    = 0.0e+00
0.00.041.293 I print_info: n_ff             = 8192
0.00.041.293 I print_info: n_expert         = 0
0.00.041.293 I print_info: n_expert_used    = 0
0.00.041.294 I print_info: causal attn      = 1
0.00.041.295 I print_info: pooling type     = 0
0.00.041.296 I print_info: rope type        = 2
0.00.041.296 I print_info: rope scaling     = linear
0.00.041.296 I print_info: freq_base_train  = 10000.0
0.00.041.296 I print_info: freq_scale_train = 1
0.00.041.297 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.297 I print_info: rope_finetuned   = unknown
0.00.041.297 I print_info: ssm_d_conv       = 0
0.00.041.297 I print_info: ssm_d_inner      = 0
0.00.041.297 I print_info: ssm_d_state      = 0
0.00.041.297 I print_info: ssm_dt_rank      = 0
0.00.041.298 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.302 I print_info: model type       = 1.4B
0.00.041.302 I print_info: model params     = 1.41 B
0.00.041.303 I print_info: general.name     = 1.4B
0.00.041.303 I print_info: vocab type       = BPE
0.00.041.303 I print_info: n_vocab          = 50304
0.00.041.304 I print_info: n_merges         = 50009
0.00.041.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.305 I print_info: LF token         = 187 'Ċ'
0.00.041.306 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.306 I print_info: max token length = 1024
0.00.577.139 I load_tensors: offloading 24 repeating layers to GPU
0.00.577.154 I load_tensors: offloading output layer to GPU
0.00.577.155 I load_tensors: offloaded 25/25 layers to GPU
0.00.577.185 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.577.186 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.578.481 I llama_init_from_model: n_seq_max     = 1
0.00.578.485 I llama_init_from_model: n_ctx         = 2048
0.00.578.485 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.578.486 I llama_init_from_model: n_batch       = 2048
0.00.578.486 I llama_init_from_model: n_ubatch      = 512
0.00.578.487 I llama_init_from_model: flash_attn    = 0
0.00.578.487 I llama_init_from_model: freq_base     = 10000.0
0.00.578.490 I llama_init_from_model: freq_scale    = 1
0.00.578.491 I ggml_metal_init: allocating
0.00.578.504 I ggml_metal_init: found device: Apple M4
0.00.578.517 I ggml_metal_init: picking default device: Apple M4
0.00.579.938 I ggml_metal_init: using embedded metal library
0.00.586.164 I ggml_metal_init: GPU name:   Apple M4
0.00.586.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.586.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.586.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.586.170 I ggml_metal_init: simdgroup reduction   = true
0.00.586.170 I ggml_metal_init: simdgroup matrix mul. = true
0.00.586.170 I ggml_metal_init: has residency sets    = true
0.00.586.171 I ggml_metal_init: has bfloat            = true
0.00.586.171 I ggml_metal_init: use bfloat            = true
0.00.586.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.586.173 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.603.634 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.658.004 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.658.010 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.658.051 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.440 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.663.442 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.663.443 I llama_init_from_model: graph nodes  = 967
0.00.663.443 I llama_init_from_model: graph splits = 2
0.00.663.447 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.663.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.663.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.118 I main: llama threadpool init, n_threads = 4
0.00.711.162 I 
0.00.711.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.199 I 
0.00.711.329 I sampler seed: 1234
0.00.711.333 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.365 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.366 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.366 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.466.488 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.466.489 I llama_perf_context_print:        load time =     699.60 ms
0.01.466.489 I llama_perf_context_print: prompt eval time =      46.68 ms /     7 tokens (    6.67 ms per token,   149.95 tokens per second)
0.01.466.490 I llama_perf_context_print:        eval time =     705.41 ms /    63 runs   (   11.20 ms per token,    89.31 tokens per second)
0.01.466.491 I llama_perf_context_print:       total time =     756.58 ms /    70 tokens
0.01.466.741 I ggml_metal_free: deallocating

real	0m1.483s
user	0m0.109s
sys	0m0.230s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.635 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.228 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.236 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.237 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.238 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.239 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.239 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.240 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.242 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.549 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.551 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.551 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.552 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.552 I llama_model_loader: - type  f32:  194 tensors
0.00.025.552 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.553 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.553 I print_info: file format = GGUF V3 (latest)
0.00.025.554 I print_info: file type   = Q5_K - Medium
0.00.025.555 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.442 I load: special tokens cache size = 25
0.00.039.632 I load: token to piece cache size = 0.2984 MB
0.00.039.636 I print_info: arch             = gptneox
0.00.039.637 I print_info: vocab_only       = 0
0.00.039.637 I print_info: n_ctx_train      = 2048
0.00.039.637 I print_info: n_embd           = 2048
0.00.039.637 I print_info: n_layer          = 24
0.00.039.640 I print_info: n_head           = 16
0.00.039.640 I print_info: n_head_kv        = 16
0.00.039.640 I print_info: n_rot            = 32
0.00.039.641 I print_info: n_swa            = 0
0.00.039.641 I print_info: n_embd_head_k    = 128
0.00.039.641 I print_info: n_embd_head_v    = 128
0.00.039.642 I print_info: n_gqa            = 1
0.00.039.643 I print_info: n_embd_k_gqa     = 2048
0.00.039.643 I print_info: n_embd_v_gqa     = 2048
0.00.039.644 I print_info: f_norm_eps       = 1.0e-05
0.00.039.644 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.646 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.646 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.646 I print_info: f_logit_scale    = 0.0e+00
0.00.039.647 I print_info: n_ff             = 8192
0.00.039.647 I print_info: n_expert         = 0
0.00.039.647 I print_info: n_expert_used    = 0
0.00.039.647 I print_info: causal attn      = 1
0.00.039.647 I print_info: pooling type     = 0
0.00.039.647 I print_info: rope type        = 2
0.00.039.648 I print_info: rope scaling     = linear
0.00.039.648 I print_info: freq_base_train  = 10000.0
0.00.039.648 I print_info: freq_scale_train = 1
0.00.039.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.651 I print_info: rope_finetuned   = unknown
0.00.039.651 I print_info: ssm_d_conv       = 0
0.00.039.651 I print_info: ssm_d_inner      = 0
0.00.039.651 I print_info: ssm_d_state      = 0
0.00.039.651 I print_info: ssm_dt_rank      = 0
0.00.039.651 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.651 I print_info: model type       = 1.4B
0.00.039.652 I print_info: model params     = 1.41 B
0.00.039.652 I print_info: general.name     = 1.4B
0.00.039.652 I print_info: vocab type       = BPE
0.00.039.653 I print_info: n_vocab          = 50304
0.00.039.653 I print_info: n_merges         = 50009
0.00.039.653 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.654 I print_info: LF token         = 187 'Ċ'
0.00.039.654 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.658 I print_info: max token length = 1024
0.00.667.028 I load_tensors: offloading 24 repeating layers to GPU
0.00.667.033 I load_tensors: offloading output layer to GPU
0.00.667.034 I load_tensors: offloaded 25/25 layers to GPU
0.00.667.055 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.667.056 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.667.904 I llama_init_from_model: n_seq_max     = 1
0.00.667.906 I llama_init_from_model: n_ctx         = 2048
0.00.667.906 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.667.907 I llama_init_from_model: n_batch       = 2048
0.00.667.907 I llama_init_from_model: n_ubatch      = 512
0.00.667.907 I llama_init_from_model: flash_attn    = 0
0.00.667.908 I llama_init_from_model: freq_base     = 10000.0
0.00.667.908 I llama_init_from_model: freq_scale    = 1
0.00.667.909 I ggml_metal_init: allocating
0.00.667.918 I ggml_metal_init: found device: Apple M4
0.00.667.924 I ggml_metal_init: picking default device: Apple M4
0.00.669.220 I ggml_metal_init: using embedded metal library
0.00.674.803 I ggml_metal_init: GPU name:   Apple M4
0.00.674.807 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.808 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.809 I ggml_metal_init: simdgroup reduction   = true
0.00.674.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.809 I ggml_metal_init: has residency sets    = true
0.00.674.810 I ggml_metal_init: has bfloat            = true
0.00.674.810 I ggml_metal_init: use bfloat            = true
0.00.674.811 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.321 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.938 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.740.944 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.740.988 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.206 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.746.208 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.746.209 I llama_init_from_model: graph nodes  = 967
0.00.746.209 I llama_init_from_model: graph splits = 2
0.00.746.214 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.746.344 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.521 I main: llama threadpool init, n_threads = 4
0.00.799.570 I 
0.00.799.596 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.597 I 
0.00.799.712 I sampler seed: 1234
0.00.799.717 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.733 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.734 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.734 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.642.875 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.642.875 I llama_perf_context_print:        load time =     789.92 ms
0.01.642.876 I llama_perf_context_print: prompt eval time =      51.19 ms /     7 tokens (    7.31 ms per token,   136.76 tokens per second)
0.01.642.877 I llama_perf_context_print:        eval time =     789.04 ms /    63 runs   (   12.52 ms per token,    79.84 tokens per second)
0.01.642.878 I llama_perf_context_print:       total time =     844.32 ms /    70 tokens
0.01.643.123 I ggml_metal_free: deallocating

real	0m1.659s
user	0m0.106s
sys	0m0.253s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.647 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.582 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.582 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.722 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.813 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.814 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.814 I llama_model_loader: - type  f32:  194 tensors
0.00.026.815 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.815 I print_info: file format = GGUF V3 (latest)
0.00.026.816 I print_info: file type   = Q6_K
0.00.026.816 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.056 I load: special tokens cache size = 25
0.00.041.231 I load: token to piece cache size = 0.2984 MB
0.00.041.236 I print_info: arch             = gptneox
0.00.041.237 I print_info: vocab_only       = 0
0.00.041.237 I print_info: n_ctx_train      = 2048
0.00.041.237 I print_info: n_embd           = 2048
0.00.041.237 I print_info: n_layer          = 24
0.00.041.240 I print_info: n_head           = 16
0.00.041.241 I print_info: n_head_kv        = 16
0.00.041.241 I print_info: n_rot            = 32
0.00.041.242 I print_info: n_swa            = 0
0.00.041.243 I print_info: n_embd_head_k    = 128
0.00.041.243 I print_info: n_embd_head_v    = 128
0.00.041.243 I print_info: n_gqa            = 1
0.00.041.244 I print_info: n_embd_k_gqa     = 2048
0.00.041.245 I print_info: n_embd_v_gqa     = 2048
0.00.041.245 I print_info: f_norm_eps       = 1.0e-05
0.00.041.246 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.246 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.246 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.246 I print_info: f_logit_scale    = 0.0e+00
0.00.041.249 I print_info: n_ff             = 8192
0.00.041.249 I print_info: n_expert         = 0
0.00.041.249 I print_info: n_expert_used    = 0
0.00.041.249 I print_info: causal attn      = 1
0.00.041.249 I print_info: pooling type     = 0
0.00.041.250 I print_info: rope type        = 2
0.00.041.250 I print_info: rope scaling     = linear
0.00.041.250 I print_info: freq_base_train  = 10000.0
0.00.041.251 I print_info: freq_scale_train = 1
0.00.041.251 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.251 I print_info: rope_finetuned   = unknown
0.00.041.251 I print_info: ssm_d_conv       = 0
0.00.041.251 I print_info: ssm_d_inner      = 0
0.00.041.251 I print_info: ssm_d_state      = 0
0.00.041.252 I print_info: ssm_dt_rank      = 0
0.00.041.252 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.253 I print_info: model type       = 1.4B
0.00.041.254 I print_info: model params     = 1.41 B
0.00.041.254 I print_info: general.name     = 1.4B
0.00.041.254 I print_info: vocab type       = BPE
0.00.041.255 I print_info: n_vocab          = 50304
0.00.041.255 I print_info: n_merges         = 50009
0.00.041.255 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.255 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.256 I print_info: LF token         = 187 'Ċ'
0.00.041.256 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.256 I print_info: max token length = 1024
0.00.737.716 I load_tensors: offloading 24 repeating layers to GPU
0.00.737.719 I load_tensors: offloading output layer to GPU
0.00.737.720 I load_tensors: offloaded 25/25 layers to GPU
0.00.737.740 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.737.742 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.738.413 I llama_init_from_model: n_seq_max     = 1
0.00.738.415 I llama_init_from_model: n_ctx         = 2048
0.00.738.415 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.738.416 I llama_init_from_model: n_batch       = 2048
0.00.738.416 I llama_init_from_model: n_ubatch      = 512
0.00.738.416 I llama_init_from_model: flash_attn    = 0
0.00.738.417 I llama_init_from_model: freq_base     = 10000.0
0.00.738.417 I llama_init_from_model: freq_scale    = 1
0.00.738.418 I ggml_metal_init: allocating
0.00.738.440 I ggml_metal_init: found device: Apple M4
0.00.738.451 I ggml_metal_init: picking default device: Apple M4
0.00.739.675 I ggml_metal_init: using embedded metal library
0.00.744.811 I ggml_metal_init: GPU name:   Apple M4
0.00.744.814 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.744.815 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.744.816 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.744.816 I ggml_metal_init: simdgroup reduction   = true
0.00.744.817 I ggml_metal_init: simdgroup matrix mul. = true
0.00.744.817 I ggml_metal_init: has residency sets    = true
0.00.744.817 I ggml_metal_init: has bfloat            = true
0.00.744.817 I ggml_metal_init: use bfloat            = true
0.00.744.818 I ggml_metal_init: hasUnifiedMemory      = true
0.00.744.819 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.759.219 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.812.804 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.812.810 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.812.845 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.818.103 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.818.107 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.818.107 I llama_init_from_model: graph nodes  = 967
0.00.818.108 I llama_init_from_model: graph splits = 2
0.00.818.113 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.818.241 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.818.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.562 I main: llama threadpool init, n_threads = 4
0.00.874.606 I 
0.00.874.631 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.874.632 I 
0.00.874.748 I sampler seed: 1234
0.00.874.752 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.874.770 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.874.771 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.874.771 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.742.120 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.742.120 I llama_perf_context_print:        load time =     863.96 ms
0.01.742.121 I llama_perf_context_print: prompt eval time =      54.01 ms /     7 tokens (    7.72 ms per token,   129.60 tokens per second)
0.01.742.122 I llama_perf_context_print:        eval time =     810.37 ms /    63 runs   (   12.86 ms per token,    77.74 tokens per second)
0.01.742.122 I llama_perf_context_print:       total time =     868.51 ms /    70 tokens
0.01.742.371 I ggml_metal_free: deallocating

real	0m1.760s
user	0m0.106s
sys	0m0.277s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.510 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.965 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.558 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.565 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.572 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.573 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.574 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.577 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.578 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.581 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.581 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.532 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.533 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.534 I llama_model_loader: - type  f32:  194 tensors
0.00.056.534 I llama_model_loader: - type  f16:   98 tensors
0.00.056.535 I print_info: file format = GGUF V3 (latest)
0.00.056.536 I print_info: file type   = all F32 (guessed)
0.00.056.537 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.130 I load: special tokens cache size = 25
0.00.077.297 I load: token to piece cache size = 0.2984 MB
0.00.077.300 I print_info: arch             = gptneox
0.00.077.300 I print_info: vocab_only       = 0
0.00.077.300 I print_info: n_ctx_train      = 2048
0.00.077.301 I print_info: n_embd           = 2048
0.00.077.301 I print_info: n_layer          = 24
0.00.077.303 I print_info: n_head           = 16
0.00.077.304 I print_info: n_head_kv        = 16
0.00.077.304 I print_info: n_rot            = 32
0.00.077.305 I print_info: n_swa            = 0
0.00.077.305 I print_info: n_embd_head_k    = 128
0.00.077.305 I print_info: n_embd_head_v    = 128
0.00.077.306 I print_info: n_gqa            = 1
0.00.077.307 I print_info: n_embd_k_gqa     = 2048
0.00.077.307 I print_info: n_embd_v_gqa     = 2048
0.00.077.310 I print_info: f_norm_eps       = 1.0e-05
0.00.077.311 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.311 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.311 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.311 I print_info: f_logit_scale    = 0.0e+00
0.00.077.312 I print_info: n_ff             = 8192
0.00.077.312 I print_info: n_expert         = 0
0.00.077.312 I print_info: n_expert_used    = 0
0.00.077.312 I print_info: causal attn      = 1
0.00.077.312 I print_info: pooling type     = 0
0.00.077.313 I print_info: rope type        = 2
0.00.077.313 I print_info: rope scaling     = linear
0.00.077.314 I print_info: freq_base_train  = 10000.0
0.00.077.315 I print_info: freq_scale_train = 1
0.00.077.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.315 I print_info: rope_finetuned   = unknown
0.00.077.315 I print_info: ssm_d_conv       = 0
0.00.077.315 I print_info: ssm_d_inner      = 0
0.00.077.316 I print_info: ssm_d_state      = 0
0.00.077.316 I print_info: ssm_dt_rank      = 0
0.00.077.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.316 I print_info: model type       = 1.4B
0.00.077.316 I print_info: model params     = 1.41 B
0.00.077.317 I print_info: general.name     = 1.4B
0.00.077.317 I print_info: vocab type       = BPE
0.00.077.317 I print_info: n_vocab          = 50304
0.00.077.317 I print_info: n_merges         = 50009
0.00.077.317 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.318 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.318 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.322 I print_info: LF token         = 187 'Ċ'
0.00.077.322 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.323 I print_info: max token length = 1024
0.01.142.638 I load_tensors: offloading 24 repeating layers to GPU
0.01.142.643 I load_tensors: offloading output layer to GPU
0.01.142.644 I load_tensors: offloaded 25/25 layers to GPU
0.01.142.668 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.142.670 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.143.324 I llama_init_from_model: n_seq_max     = 1
0.01.143.324 I llama_init_from_model: n_ctx         = 128
0.01.143.325 I llama_init_from_model: n_ctx_per_seq = 128
0.01.143.325 I llama_init_from_model: n_batch       = 128
0.01.143.325 I llama_init_from_model: n_ubatch      = 128
0.01.143.325 I llama_init_from_model: flash_attn    = 0
0.01.143.325 I llama_init_from_model: freq_base     = 10000.0
0.01.143.326 I llama_init_from_model: freq_scale    = 1
0.01.143.326 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.143.327 I ggml_metal_init: allocating
0.01.143.377 I ggml_metal_init: found device: Apple M4
0.01.143.382 I ggml_metal_init: picking default device: Apple M4
0.01.144.296 I ggml_metal_init: using embedded metal library
0.01.147.608 I ggml_metal_init: GPU name:   Apple M4
0.01.147.610 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.147.610 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.147.611 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.147.611 I ggml_metal_init: simdgroup reduction   = true
0.01.147.611 I ggml_metal_init: simdgroup matrix mul. = true
0.01.147.611 I ggml_metal_init: has residency sets    = true
0.01.147.611 I ggml_metal_init: has bfloat            = true
0.01.147.612 I ggml_metal_init: use bfloat            = true
0.01.147.612 I ggml_metal_init: hasUnifiedMemory      = true
0.01.147.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.157.182 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.158.884 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.158.886 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.158.912 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.160.403 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.160.405 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.160.405 I llama_init_from_model: graph nodes  = 967
0.01.160.405 I llama_init_from_model: graph splits = 2
0.01.160.406 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.160.406 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.194.076 I 
0.01.194.110 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.194.113 I perplexity: tokenizing the input ..
0.01.198.581 I perplexity: tokenization took 4.466 ms
0.01.198.587 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.316.860 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.318.132 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.318.173 I llama_perf_context_print:        load time =    1169.10 ms
0.01.318.174 I llama_perf_context_print: prompt eval time =     118.05 ms /   128 tokens (    0.92 ms per token,  1084.31 tokens per second)
0.01.318.174 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.318.175 I llama_perf_context_print:       total time =     124.10 ms /   129 tokens
0.01.318.551 I ggml_metal_free: deallocating

real	0m1.515s
user	0m0.100s
sys	0m0.283s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.177 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.402 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.408 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.420 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.443 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.559 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.577 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.579 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.579 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.579 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.580 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.580 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.581 I llama_model_loader: - type  f32:  194 tensors
0.00.026.581 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.582 I print_info: file format = GGUF V3 (latest)
0.00.026.582 I print_info: file type   = Q8_0
0.00.026.583 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.871 I load: special tokens cache size = 25
0.00.041.225 I load: token to piece cache size = 0.2984 MB
0.00.041.232 I print_info: arch             = gptneox
0.00.041.232 I print_info: vocab_only       = 0
0.00.041.232 I print_info: n_ctx_train      = 2048
0.00.041.232 I print_info: n_embd           = 2048
0.00.041.233 I print_info: n_layer          = 24
0.00.041.237 I print_info: n_head           = 16
0.00.041.238 I print_info: n_head_kv        = 16
0.00.041.238 I print_info: n_rot            = 32
0.00.041.238 I print_info: n_swa            = 0
0.00.041.238 I print_info: n_embd_head_k    = 128
0.00.041.238 I print_info: n_embd_head_v    = 128
0.00.041.239 I print_info: n_gqa            = 1
0.00.041.240 I print_info: n_embd_k_gqa     = 2048
0.00.041.243 I print_info: n_embd_v_gqa     = 2048
0.00.041.244 I print_info: f_norm_eps       = 1.0e-05
0.00.041.244 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.244 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.244 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.244 I print_info: f_logit_scale    = 0.0e+00
0.00.041.245 I print_info: n_ff             = 8192
0.00.041.245 I print_info: n_expert         = 0
0.00.041.245 I print_info: n_expert_used    = 0
0.00.041.246 I print_info: causal attn      = 1
0.00.041.246 I print_info: pooling type     = 0
0.00.041.246 I print_info: rope type        = 2
0.00.041.246 I print_info: rope scaling     = linear
0.00.041.247 I print_info: freq_base_train  = 10000.0
0.00.041.247 I print_info: freq_scale_train = 1
0.00.041.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.247 I print_info: rope_finetuned   = unknown
0.00.041.247 I print_info: ssm_d_conv       = 0
0.00.041.248 I print_info: ssm_d_inner      = 0
0.00.041.248 I print_info: ssm_d_state      = 0
0.00.041.248 I print_info: ssm_dt_rank      = 0
0.00.041.248 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.248 I print_info: model type       = 1.4B
0.00.041.249 I print_info: model params     = 1.41 B
0.00.041.249 I print_info: general.name     = 1.4B
0.00.041.249 I print_info: vocab type       = BPE
0.00.041.250 I print_info: n_vocab          = 50304
0.00.041.250 I print_info: n_merges         = 50009
0.00.041.250 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.251 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.251 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.257 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.263 I print_info: LF token         = 187 'Ċ'
0.00.041.263 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.264 I print_info: max token length = 1024
0.00.935.871 I load_tensors: offloading 24 repeating layers to GPU
0.00.935.877 I load_tensors: offloading output layer to GPU
0.00.935.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.935.903 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.935.906 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.936.781 I llama_init_from_model: n_seq_max     = 1
0.00.936.783 I llama_init_from_model: n_ctx         = 128
0.00.936.783 I llama_init_from_model: n_ctx_per_seq = 128
0.00.936.783 I llama_init_from_model: n_batch       = 128
0.00.936.785 I llama_init_from_model: n_ubatch      = 128
0.00.936.785 I llama_init_from_model: flash_attn    = 0
0.00.936.786 I llama_init_from_model: freq_base     = 10000.0
0.00.936.786 I llama_init_from_model: freq_scale    = 1
0.00.936.787 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.936.788 I ggml_metal_init: allocating
0.00.936.861 I ggml_metal_init: found device: Apple M4
0.00.936.871 I ggml_metal_init: picking default device: Apple M4
0.00.938.080 I ggml_metal_init: using embedded metal library
0.00.942.520 I ggml_metal_init: GPU name:   Apple M4
0.00.942.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.942.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.942.524 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.942.524 I ggml_metal_init: simdgroup reduction   = true
0.00.942.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.942.525 I ggml_metal_init: has residency sets    = true
0.00.942.525 I ggml_metal_init: has bfloat            = true
0.00.942.525 I ggml_metal_init: use bfloat            = true
0.00.942.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.942.527 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.957.725 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.959.558 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.959.560 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.959.586 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.961.424 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.961.426 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.961.426 I llama_init_from_model: graph nodes  = 967
0.00.961.426 I llama_init_from_model: graph splits = 2
0.00.961.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.961.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.983.185 I 
0.00.983.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.983.218 I perplexity: tokenizing the input ..
0.00.988.433 I perplexity: tokenization took 5.218 ms
0.00.988.437 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.111.700 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.112.959 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.112.994 I llama_perf_context_print:        load time =     973.00 ms
0.01.112.994 I llama_perf_context_print: prompt eval time =     123.04 ms /   128 tokens (    0.96 ms per token,  1040.33 tokens per second)
0.01.112.995 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.112.995 I llama_perf_context_print:       total time =     129.81 ms /   129 tokens
0.01.113.395 I ggml_metal_free: deallocating

real	0m1.127s
user	0m0.072s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.811 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.053 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.054 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.055 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.056 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.056 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.057 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.061 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.065 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.134 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.101 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.102 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.102 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.102 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.103 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.103 I llama_model_loader: - type  f32:  194 tensors
0.00.027.104 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.104 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.104 I print_info: file format = GGUF V3 (latest)
0.00.027.105 I print_info: file type   = Q4_0
0.00.027.106 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.432 I load: special tokens cache size = 25
0.00.041.546 I load: token to piece cache size = 0.2984 MB
0.00.041.551 I print_info: arch             = gptneox
0.00.041.551 I print_info: vocab_only       = 0
0.00.041.551 I print_info: n_ctx_train      = 2048
0.00.041.551 I print_info: n_embd           = 2048
0.00.041.552 I print_info: n_layer          = 24
0.00.041.555 I print_info: n_head           = 16
0.00.041.556 I print_info: n_head_kv        = 16
0.00.041.556 I print_info: n_rot            = 32
0.00.041.556 I print_info: n_swa            = 0
0.00.041.557 I print_info: n_embd_head_k    = 128
0.00.041.557 I print_info: n_embd_head_v    = 128
0.00.041.557 I print_info: n_gqa            = 1
0.00.041.558 I print_info: n_embd_k_gqa     = 2048
0.00.041.559 I print_info: n_embd_v_gqa     = 2048
0.00.041.561 I print_info: f_norm_eps       = 1.0e-05
0.00.041.562 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.567 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.569 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.569 I print_info: f_logit_scale    = 0.0e+00
0.00.041.570 I print_info: n_ff             = 8192
0.00.041.570 I print_info: n_expert         = 0
0.00.041.570 I print_info: n_expert_used    = 0
0.00.041.570 I print_info: causal attn      = 1
0.00.041.570 I print_info: pooling type     = 0
0.00.041.570 I print_info: rope type        = 2
0.00.041.570 I print_info: rope scaling     = linear
0.00.041.571 I print_info: freq_base_train  = 10000.0
0.00.041.571 I print_info: freq_scale_train = 1
0.00.041.571 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.571 I print_info: rope_finetuned   = unknown
0.00.041.571 I print_info: ssm_d_conv       = 0
0.00.041.572 I print_info: ssm_d_inner      = 0
0.00.041.602 I print_info: ssm_d_state      = 0
0.00.041.604 I print_info: ssm_dt_rank      = 0
0.00.041.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.605 I print_info: model type       = 1.4B
0.00.041.605 I print_info: model params     = 1.41 B
0.00.041.605 I print_info: general.name     = 1.4B
0.00.041.606 I print_info: vocab type       = BPE
0.00.041.607 I print_info: n_vocab          = 50304
0.00.041.607 I print_info: n_merges         = 50009
0.00.041.607 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.607 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.607 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.608 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.608 I print_info: LF token         = 187 'Ċ'
0.00.041.611 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.611 I print_info: max token length = 1024
0.00.621.705 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.712 I load_tensors: offloading output layer to GPU
0.00.621.714 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.738 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.621.740 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.622.835 I llama_init_from_model: n_seq_max     = 1
0.00.622.837 I llama_init_from_model: n_ctx         = 128
0.00.622.838 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.838 I llama_init_from_model: n_batch       = 128
0.00.622.839 I llama_init_from_model: n_ubatch      = 128
0.00.622.839 I llama_init_from_model: flash_attn    = 0
0.00.622.840 I llama_init_from_model: freq_base     = 10000.0
0.00.622.841 I llama_init_from_model: freq_scale    = 1
0.00.622.842 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.843 I ggml_metal_init: allocating
0.00.622.861 I ggml_metal_init: found device: Apple M4
0.00.622.870 I ggml_metal_init: picking default device: Apple M4
0.00.624.074 I ggml_metal_init: using embedded metal library
0.00.629.864 I ggml_metal_init: GPU name:   Apple M4
0.00.629.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.870 I ggml_metal_init: simdgroup reduction   = true
0.00.629.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.870 I ggml_metal_init: has residency sets    = true
0.00.629.871 I ggml_metal_init: has bfloat            = true
0.00.629.871 I ggml_metal_init: use bfloat            = true
0.00.629.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.858 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.649.568 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.649.572 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.649.612 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.652.677 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.652.678 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.652.679 I llama_init_from_model: graph nodes  = 967
0.00.652.679 I llama_init_from_model: graph splits = 2
0.00.652.681 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.652.681 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.029 I 
0.00.679.092 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.099 I perplexity: tokenizing the input ..
0.00.685.869 I perplexity: tokenization took 6.766 ms
0.00.685.874 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.778 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.819.047 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.819.082 I llama_perf_context_print:        load time =     668.21 ms
0.00.819.082 I llama_perf_context_print: prompt eval time =     131.01 ms /   128 tokens (    1.02 ms per token,   977.01 tokens per second)
0.00.819.083 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.083 I llama_perf_context_print:       total time =     140.06 ms /   129 tokens
0.00.819.485 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.078s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.813 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.778 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.786 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.788 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.788 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.789 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.789 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.790 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.790 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.791 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.791 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.791 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.792 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.792 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.798 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.171 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.280 I llama_model_loader: - type  f32:  194 tensors
0.00.026.280 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.281 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.281 I print_info: file format = GGUF V3 (latest)
0.00.026.282 I print_info: file type   = Q4_1
0.00.026.285 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.879 I load: special tokens cache size = 25
0.00.041.041 I load: token to piece cache size = 0.2984 MB
0.00.041.045 I print_info: arch             = gptneox
0.00.041.045 I print_info: vocab_only       = 0
0.00.041.045 I print_info: n_ctx_train      = 2048
0.00.041.046 I print_info: n_embd           = 2048
0.00.041.046 I print_info: n_layer          = 24
0.00.041.050 I print_info: n_head           = 16
0.00.041.051 I print_info: n_head_kv        = 16
0.00.041.051 I print_info: n_rot            = 32
0.00.041.051 I print_info: n_swa            = 0
0.00.041.051 I print_info: n_embd_head_k    = 128
0.00.041.051 I print_info: n_embd_head_v    = 128
0.00.041.052 I print_info: n_gqa            = 1
0.00.041.053 I print_info: n_embd_k_gqa     = 2048
0.00.041.055 I print_info: n_embd_v_gqa     = 2048
0.00.041.055 I print_info: f_norm_eps       = 1.0e-05
0.00.041.055 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.056 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.056 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.056 I print_info: f_logit_scale    = 0.0e+00
0.00.041.057 I print_info: n_ff             = 8192
0.00.041.057 I print_info: n_expert         = 0
0.00.041.057 I print_info: n_expert_used    = 0
0.00.041.057 I print_info: causal attn      = 1
0.00.041.057 I print_info: pooling type     = 0
0.00.041.058 I print_info: rope type        = 2
0.00.041.058 I print_info: rope scaling     = linear
0.00.041.058 I print_info: freq_base_train  = 10000.0
0.00.041.059 I print_info: freq_scale_train = 1
0.00.041.059 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.059 I print_info: rope_finetuned   = unknown
0.00.041.059 I print_info: ssm_d_conv       = 0
0.00.041.059 I print_info: ssm_d_inner      = 0
0.00.041.059 I print_info: ssm_d_state      = 0
0.00.041.061 I print_info: ssm_dt_rank      = 0
0.00.041.061 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.062 I print_info: model type       = 1.4B
0.00.041.062 I print_info: model params     = 1.41 B
0.00.041.062 I print_info: general.name     = 1.4B
0.00.041.063 I print_info: vocab type       = BPE
0.00.041.063 I print_info: n_vocab          = 50304
0.00.041.063 I print_info: n_merges         = 50009
0.00.041.063 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.063 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.064 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.064 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.064 I print_info: LF token         = 187 'Ċ'
0.00.041.064 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.064 I print_info: max token length = 1024
0.00.716.837 I load_tensors: offloading 24 repeating layers to GPU
0.00.716.846 I load_tensors: offloading output layer to GPU
0.00.716.846 I load_tensors: offloaded 25/25 layers to GPU
0.00.716.867 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.716.868 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.717.947 I llama_init_from_model: n_seq_max     = 1
0.00.717.951 I llama_init_from_model: n_ctx         = 128
0.00.717.951 I llama_init_from_model: n_ctx_per_seq = 128
0.00.717.952 I llama_init_from_model: n_batch       = 128
0.00.717.952 I llama_init_from_model: n_ubatch      = 128
0.00.717.953 I llama_init_from_model: flash_attn    = 0
0.00.717.954 I llama_init_from_model: freq_base     = 10000.0
0.00.717.955 I llama_init_from_model: freq_scale    = 1
0.00.717.956 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.717.957 I ggml_metal_init: allocating
0.00.717.997 I ggml_metal_init: found device: Apple M4
0.00.718.013 I ggml_metal_init: picking default device: Apple M4
0.00.719.143 I ggml_metal_init: using embedded metal library
0.00.723.637 I ggml_metal_init: GPU name:   Apple M4
0.00.723.641 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.723.642 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.723.642 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.723.643 I ggml_metal_init: simdgroup reduction   = true
0.00.723.643 I ggml_metal_init: simdgroup matrix mul. = true
0.00.723.643 I ggml_metal_init: has residency sets    = true
0.00.723.644 I ggml_metal_init: has bfloat            = true
0.00.723.644 I ggml_metal_init: use bfloat            = true
0.00.723.645 I ggml_metal_init: hasUnifiedMemory      = true
0.00.723.647 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.736.178 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.737.880 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.737.883 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.737.918 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.739.532 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.739.534 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.739.534 I llama_init_from_model: graph nodes  = 967
0.00.739.534 I llama_init_from_model: graph splits = 2
0.00.739.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.739.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.066 I 
0.00.763.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.104 I perplexity: tokenizing the input ..
0.00.766.993 I perplexity: tokenization took 3.888 ms
0.00.766.997 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.688 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.912.508 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.912.556 I llama_perf_context_print:        load time =     753.25 ms
0.00.912.558 I llama_perf_context_print: prompt eval time =     139.46 ms /   128 tokens (    1.09 ms per token,   917.85 tokens per second)
0.00.912.560 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.912.562 I llama_perf_context_print:       total time =     149.49 ms /   129 tokens
0.00.913.924 I ggml_metal_free: deallocating

real	0m0.936s
user	0m0.099s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.195 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.526 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.680 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.029.688 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.690 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.691 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.692 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.692 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.693 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.703 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.703 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.704 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.707 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.708 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.449 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.451 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.452 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.044.453 I llama_model_loader: - type  f32:  194 tensors
0.00.044.453 I llama_model_loader: - type q5_0:   97 tensors
0.00.044.453 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.454 I print_info: file format = GGUF V3 (latest)
0.00.044.455 I print_info: file type   = Q5_0
0.00.044.456 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.055.777 I load: special tokens cache size = 25
0.00.063.447 I load: token to piece cache size = 0.2984 MB
0.00.063.450 I print_info: arch             = gptneox
0.00.063.450 I print_info: vocab_only       = 0
0.00.063.450 I print_info: n_ctx_train      = 2048
0.00.063.450 I print_info: n_embd           = 2048
0.00.063.451 I print_info: n_layer          = 24
0.00.063.453 I print_info: n_head           = 16
0.00.063.454 I print_info: n_head_kv        = 16
0.00.063.455 I print_info: n_rot            = 32
0.00.063.455 I print_info: n_swa            = 0
0.00.063.455 I print_info: n_embd_head_k    = 128
0.00.063.455 I print_info: n_embd_head_v    = 128
0.00.063.456 I print_info: n_gqa            = 1
0.00.063.456 I print_info: n_embd_k_gqa     = 2048
0.00.063.459 I print_info: n_embd_v_gqa     = 2048
0.00.063.459 I print_info: f_norm_eps       = 1.0e-05
0.00.063.459 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.460 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.460 I print_info: f_logit_scale    = 0.0e+00
0.00.063.462 I print_info: n_ff             = 8192
0.00.063.462 I print_info: n_expert         = 0
0.00.063.462 I print_info: n_expert_used    = 0
0.00.063.462 I print_info: causal attn      = 1
0.00.063.462 I print_info: pooling type     = 0
0.00.063.462 I print_info: rope type        = 2
0.00.063.471 I print_info: rope scaling     = linear
0.00.063.472 I print_info: freq_base_train  = 10000.0
0.00.063.473 I print_info: freq_scale_train = 1
0.00.063.473 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.473 I print_info: rope_finetuned   = unknown
0.00.063.473 I print_info: ssm_d_conv       = 0
0.00.063.473 I print_info: ssm_d_inner      = 0
0.00.063.474 I print_info: ssm_d_state      = 0
0.00.063.474 I print_info: ssm_dt_rank      = 0
0.00.063.474 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.474 I print_info: model type       = 1.4B
0.00.063.476 I print_info: model params     = 1.41 B
0.00.063.476 I print_info: general.name     = 1.4B
0.00.063.476 I print_info: vocab type       = BPE
0.00.063.477 I print_info: n_vocab          = 50304
0.00.063.477 I print_info: n_merges         = 50009
0.00.063.478 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.478 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.478 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.478 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.479 I print_info: LF token         = 187 'Ċ'
0.00.063.480 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.480 I print_info: max token length = 1024
0.00.746.145 I load_tensors: offloading 24 repeating layers to GPU
0.00.746.148 I load_tensors: offloading output layer to GPU
0.00.746.148 I load_tensors: offloaded 25/25 layers to GPU
0.00.746.169 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.746.171 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.747.159 I llama_init_from_model: n_seq_max     = 1
0.00.747.161 I llama_init_from_model: n_ctx         = 128
0.00.747.161 I llama_init_from_model: n_ctx_per_seq = 128
0.00.747.161 I llama_init_from_model: n_batch       = 128
0.00.747.162 I llama_init_from_model: n_ubatch      = 128
0.00.747.162 I llama_init_from_model: flash_attn    = 0
0.00.747.163 I llama_init_from_model: freq_base     = 10000.0
0.00.747.163 I llama_init_from_model: freq_scale    = 1
0.00.747.164 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.747.165 I ggml_metal_init: allocating
0.00.747.182 I ggml_metal_init: found device: Apple M4
0.00.747.189 I ggml_metal_init: picking default device: Apple M4
0.00.748.310 I ggml_metal_init: using embedded metal library
0.00.753.918 I ggml_metal_init: GPU name:   Apple M4
0.00.753.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.753.922 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.753.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.753.923 I ggml_metal_init: simdgroup reduction   = true
0.00.753.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.753.924 I ggml_metal_init: has residency sets    = true
0.00.753.924 I ggml_metal_init: has bfloat            = true
0.00.753.924 I ggml_metal_init: use bfloat            = true
0.00.753.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.753.929 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.769.513 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.772.761 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.772.765 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.772.812 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.775.784 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.775.786 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.775.786 I llama_init_from_model: graph nodes  = 967
0.00.775.786 I llama_init_from_model: graph splits = 2
0.00.775.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.775.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.804.374 I 
0.00.804.447 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.804.455 I perplexity: tokenizing the input ..
0.00.811.636 I perplexity: tokenization took 7.178 ms
0.00.811.643 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.957.242 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.958.527 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.958.557 I llama_perf_context_print:        load time =     788.84 ms
0.00.958.558 I llama_perf_context_print: prompt eval time =     144.66 ms /   128 tokens (    1.13 ms per token,   884.83 tokens per second)
0.00.958.559 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.958.559 I llama_perf_context_print:       total time =     154.19 ms /   129 tokens
0.00.958.957 I ggml_metal_free: deallocating

real	0m0.981s
user	0m0.099s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.663 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.006 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.012 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.014 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.014 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.014 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.015 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.015 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.020 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.020 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.022 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.023 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.023 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.335 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.335 I llama_model_loader: - type  f32:  194 tensors
0.00.025.336 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.336 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.337 I print_info: file format = GGUF V3 (latest)
0.00.025.337 I print_info: file type   = Q5_1
0.00.025.338 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.244 I load: special tokens cache size = 25
0.00.039.227 I load: token to piece cache size = 0.2984 MB
0.00.039.231 I print_info: arch             = gptneox
0.00.039.231 I print_info: vocab_only       = 0
0.00.039.231 I print_info: n_ctx_train      = 2048
0.00.039.232 I print_info: n_embd           = 2048
0.00.039.232 I print_info: n_layer          = 24
0.00.039.235 I print_info: n_head           = 16
0.00.039.236 I print_info: n_head_kv        = 16
0.00.039.236 I print_info: n_rot            = 32
0.00.039.237 I print_info: n_swa            = 0
0.00.039.237 I print_info: n_embd_head_k    = 128
0.00.039.237 I print_info: n_embd_head_v    = 128
0.00.039.238 I print_info: n_gqa            = 1
0.00.039.239 I print_info: n_embd_k_gqa     = 2048
0.00.039.239 I print_info: n_embd_v_gqa     = 2048
0.00.039.243 I print_info: f_norm_eps       = 1.0e-05
0.00.039.243 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.243 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.243 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.244 I print_info: f_logit_scale    = 0.0e+00
0.00.039.245 I print_info: n_ff             = 8192
0.00.039.245 I print_info: n_expert         = 0
0.00.039.245 I print_info: n_expert_used    = 0
0.00.039.245 I print_info: causal attn      = 1
0.00.039.245 I print_info: pooling type     = 0
0.00.039.245 I print_info: rope type        = 2
0.00.039.246 I print_info: rope scaling     = linear
0.00.039.247 I print_info: freq_base_train  = 10000.0
0.00.039.247 I print_info: freq_scale_train = 1
0.00.039.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.247 I print_info: rope_finetuned   = unknown
0.00.039.247 I print_info: ssm_d_conv       = 0
0.00.039.248 I print_info: ssm_d_inner      = 0
0.00.039.248 I print_info: ssm_d_state      = 0
0.00.039.249 I print_info: ssm_dt_rank      = 0
0.00.039.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.249 I print_info: model type       = 1.4B
0.00.039.250 I print_info: model params     = 1.41 B
0.00.039.250 I print_info: general.name     = 1.4B
0.00.039.250 I print_info: vocab type       = BPE
0.00.039.251 I print_info: n_vocab          = 50304
0.00.039.251 I print_info: n_merges         = 50009
0.00.039.251 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.251 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.251 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: LF token         = 187 'Ċ'
0.00.039.252 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: max token length = 1024
0.00.719.941 I load_tensors: offloading 24 repeating layers to GPU
0.00.719.947 I load_tensors: offloading output layer to GPU
0.00.719.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.719.971 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.719.973 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.720.948 I llama_init_from_model: n_seq_max     = 1
0.00.720.950 I llama_init_from_model: n_ctx         = 128
0.00.720.950 I llama_init_from_model: n_ctx_per_seq = 128
0.00.720.951 I llama_init_from_model: n_batch       = 128
0.00.720.951 I llama_init_from_model: n_ubatch      = 128
0.00.720.951 I llama_init_from_model: flash_attn    = 0
0.00.720.952 I llama_init_from_model: freq_base     = 10000.0
0.00.720.953 I llama_init_from_model: freq_scale    = 1
0.00.720.953 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.720.954 I ggml_metal_init: allocating
0.00.720.966 I ggml_metal_init: found device: Apple M4
0.00.720.975 I ggml_metal_init: picking default device: Apple M4
0.00.722.177 I ggml_metal_init: using embedded metal library
0.00.728.004 I ggml_metal_init: GPU name:   Apple M4
0.00.728.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.728.008 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.728.009 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.728.009 I ggml_metal_init: simdgroup reduction   = true
0.00.728.009 I ggml_metal_init: simdgroup matrix mul. = true
0.00.728.010 I ggml_metal_init: has residency sets    = true
0.00.728.010 I ggml_metal_init: has bfloat            = true
0.00.728.010 I ggml_metal_init: use bfloat            = true
0.00.728.011 I ggml_metal_init: hasUnifiedMemory      = true
0.00.728.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.744.239 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.510 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.747.515 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.747.573 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.591 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.750.592 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.750.593 I llama_init_from_model: graph nodes  = 967
0.00.750.593 I llama_init_from_model: graph splits = 2
0.00.750.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.750.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.549 I 
0.00.779.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.634 I perplexity: tokenizing the input ..
0.00.786.564 I perplexity: tokenization took 6.927 ms
0.00.786.576 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.932.073 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.933.350 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.933.381 I llama_perf_context_print:        load time =     770.88 ms
0.00.933.382 I llama_perf_context_print: prompt eval time =     144.57 ms /   128 tokens (    1.13 ms per token,   885.40 tokens per second)
0.00.933.382 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.933.383 I llama_perf_context_print:       total time =     153.84 ms /   129 tokens
0.00.933.764 I ggml_metal_free: deallocating

real	0m0.947s
user	0m0.078s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.719 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.695 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.706 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.707 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.707 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.710 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.712 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.539 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.513 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.515 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.515 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.516 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.516 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.517 I llama_model_loader: - type  f32:  194 tensors
0.00.026.517 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.517 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.517 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.518 I print_info: file format = GGUF V3 (latest)
0.00.026.519 I print_info: file type   = Q2_K - Medium
0.00.026.525 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.272 I load: special tokens cache size = 25
0.00.040.243 I load: token to piece cache size = 0.2984 MB
0.00.040.246 I print_info: arch             = gptneox
0.00.040.246 I print_info: vocab_only       = 0
0.00.040.246 I print_info: n_ctx_train      = 2048
0.00.040.247 I print_info: n_embd           = 2048
0.00.040.247 I print_info: n_layer          = 24
0.00.040.250 I print_info: n_head           = 16
0.00.040.251 I print_info: n_head_kv        = 16
0.00.040.251 I print_info: n_rot            = 32
0.00.040.251 I print_info: n_swa            = 0
0.00.040.252 I print_info: n_embd_head_k    = 128
0.00.040.252 I print_info: n_embd_head_v    = 128
0.00.040.252 I print_info: n_gqa            = 1
0.00.040.253 I print_info: n_embd_k_gqa     = 2048
0.00.040.254 I print_info: n_embd_v_gqa     = 2048
0.00.040.254 I print_info: f_norm_eps       = 1.0e-05
0.00.040.255 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.255 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.255 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.255 I print_info: f_logit_scale    = 0.0e+00
0.00.040.256 I print_info: n_ff             = 8192
0.00.040.256 I print_info: n_expert         = 0
0.00.040.256 I print_info: n_expert_used    = 0
0.00.040.256 I print_info: causal attn      = 1
0.00.040.257 I print_info: pooling type     = 0
0.00.040.259 I print_info: rope type        = 2
0.00.040.259 I print_info: rope scaling     = linear
0.00.040.260 I print_info: freq_base_train  = 10000.0
0.00.040.260 I print_info: freq_scale_train = 1
0.00.040.260 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.260 I print_info: rope_finetuned   = unknown
0.00.040.260 I print_info: ssm_d_conv       = 0
0.00.040.261 I print_info: ssm_d_inner      = 0
0.00.040.261 I print_info: ssm_d_state      = 0
0.00.040.261 I print_info: ssm_dt_rank      = 0
0.00.040.261 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.261 I print_info: model type       = 1.4B
0.00.040.263 I print_info: model params     = 1.41 B
0.00.040.263 I print_info: general.name     = 1.4B
0.00.040.264 I print_info: vocab type       = BPE
0.00.040.264 I print_info: n_vocab          = 50304
0.00.040.264 I print_info: n_merges         = 50009
0.00.040.264 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.264 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.265 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.265 I print_info: LF token         = 187 'Ċ'
0.00.040.265 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.265 I print_info: max token length = 1024
0.00.412.433 I load_tensors: offloading 24 repeating layers to GPU
0.00.412.446 I load_tensors: offloading output layer to GPU
0.00.412.447 I load_tensors: offloaded 25/25 layers to GPU
0.00.412.477 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.412.478 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.413.849 I llama_init_from_model: n_seq_max     = 1
0.00.413.854 I llama_init_from_model: n_ctx         = 128
0.00.413.855 I llama_init_from_model: n_ctx_per_seq = 128
0.00.413.856 I llama_init_from_model: n_batch       = 128
0.00.413.856 I llama_init_from_model: n_ubatch      = 128
0.00.413.856 I llama_init_from_model: flash_attn    = 0
0.00.413.858 I llama_init_from_model: freq_base     = 10000.0
0.00.413.859 I llama_init_from_model: freq_scale    = 1
0.00.413.860 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.413.868 I ggml_metal_init: allocating
0.00.413.956 I ggml_metal_init: found device: Apple M4
0.00.413.975 I ggml_metal_init: picking default device: Apple M4
0.00.416.153 I ggml_metal_init: using embedded metal library
0.00.422.292 I ggml_metal_init: GPU name:   Apple M4
0.00.422.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.422.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.422.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.422.303 I ggml_metal_init: simdgroup reduction   = true
0.00.422.304 I ggml_metal_init: simdgroup matrix mul. = true
0.00.422.304 I ggml_metal_init: has residency sets    = true
0.00.422.304 I ggml_metal_init: has bfloat            = true
0.00.422.305 I ggml_metal_init: use bfloat            = true
0.00.422.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.422.309 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.443.147 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.446.726 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.446.733 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.446.794 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.449.956 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.449.958 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.449.959 I llama_init_from_model: graph nodes  = 967
0.00.449.959 I llama_init_from_model: graph splits = 2
0.00.449.963 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.449.963 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.960 I 
0.00.480.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.052 I perplexity: tokenizing the input ..
0.00.486.034 I perplexity: tokenization took 5.981 ms
0.00.486.038 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.627.766 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.629.057 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.629.086 I llama_perf_context_print:        load time =     469.23 ms
0.00.629.087 I llama_perf_context_print: prompt eval time =     141.50 ms /   128 tokens (    1.11 ms per token,   904.57 tokens per second)
0.00.629.088 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.629.088 I llama_perf_context_print:       total time =     149.13 ms /   129 tokens
0.00.629.500 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.081s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.870 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.876 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.878 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.882 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.884 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.213 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.214 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.214 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.215 I llama_model_loader: - type  f32:  194 tensors
0.00.025.215 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.215 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.215 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.216 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.216 I print_info: file format = GGUF V3 (latest)
0.00.025.217 I print_info: file type   = Q3_K - Medium
0.00.025.218 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.008 I load: special tokens cache size = 25
0.00.039.164 I load: token to piece cache size = 0.2984 MB
0.00.039.169 I print_info: arch             = gptneox
0.00.039.169 I print_info: vocab_only       = 0
0.00.039.169 I print_info: n_ctx_train      = 2048
0.00.039.170 I print_info: n_embd           = 2048
0.00.039.170 I print_info: n_layer          = 24
0.00.039.173 I print_info: n_head           = 16
0.00.039.173 I print_info: n_head_kv        = 16
0.00.039.174 I print_info: n_rot            = 32
0.00.039.174 I print_info: n_swa            = 0
0.00.039.174 I print_info: n_embd_head_k    = 128
0.00.039.174 I print_info: n_embd_head_v    = 128
0.00.039.175 I print_info: n_gqa            = 1
0.00.039.176 I print_info: n_embd_k_gqa     = 2048
0.00.039.176 I print_info: n_embd_v_gqa     = 2048
0.00.039.177 I print_info: f_norm_eps       = 1.0e-05
0.00.039.177 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.177 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.177 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.178 I print_info: f_logit_scale    = 0.0e+00
0.00.039.178 I print_info: n_ff             = 8192
0.00.039.178 I print_info: n_expert         = 0
0.00.039.179 I print_info: n_expert_used    = 0
0.00.039.179 I print_info: causal attn      = 1
0.00.039.179 I print_info: pooling type     = 0
0.00.039.180 I print_info: rope type        = 2
0.00.039.180 I print_info: rope scaling     = linear
0.00.039.181 I print_info: freq_base_train  = 10000.0
0.00.039.181 I print_info: freq_scale_train = 1
0.00.039.181 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.182 I print_info: rope_finetuned   = unknown
0.00.039.183 I print_info: ssm_d_conv       = 0
0.00.039.184 I print_info: ssm_d_inner      = 0
0.00.039.184 I print_info: ssm_d_state      = 0
0.00.039.184 I print_info: ssm_dt_rank      = 0
0.00.039.184 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.184 I print_info: model type       = 1.4B
0.00.039.185 I print_info: model params     = 1.41 B
0.00.039.185 I print_info: general.name     = 1.4B
0.00.039.185 I print_info: vocab type       = BPE
0.00.039.186 I print_info: n_vocab          = 50304
0.00.039.186 I print_info: n_merges         = 50009
0.00.039.186 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.187 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.187 I print_info: LF token         = 187 'Ċ'
0.00.039.187 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.187 I print_info: max token length = 1024
0.00.491.103 I load_tensors: offloading 24 repeating layers to GPU
0.00.491.110 I load_tensors: offloading output layer to GPU
0.00.491.111 I load_tensors: offloaded 25/25 layers to GPU
0.00.491.142 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.491.143 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.492.328 I llama_init_from_model: n_seq_max     = 1
0.00.492.333 I llama_init_from_model: n_ctx         = 128
0.00.492.333 I llama_init_from_model: n_ctx_per_seq = 128
0.00.492.334 I llama_init_from_model: n_batch       = 128
0.00.492.334 I llama_init_from_model: n_ubatch      = 128
0.00.492.334 I llama_init_from_model: flash_attn    = 0
0.00.492.337 I llama_init_from_model: freq_base     = 10000.0
0.00.492.337 I llama_init_from_model: freq_scale    = 1
0.00.492.338 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.492.350 I ggml_metal_init: allocating
0.00.492.405 I ggml_metal_init: found device: Apple M4
0.00.492.418 I ggml_metal_init: picking default device: Apple M4
0.00.494.070 I ggml_metal_init: using embedded metal library
0.00.500.782 I ggml_metal_init: GPU name:   Apple M4
0.00.500.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.500.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.500.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.500.789 I ggml_metal_init: simdgroup reduction   = true
0.00.500.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.500.790 I ggml_metal_init: has residency sets    = true
0.00.500.790 I ggml_metal_init: has bfloat            = true
0.00.500.791 I ggml_metal_init: use bfloat            = true
0.00.500.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.500.801 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.519.172 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.522.675 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.522.678 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.522.722 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.525.855 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.525.857 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.525.857 I llama_init_from_model: graph nodes  = 967
0.00.525.857 I llama_init_from_model: graph splits = 2
0.00.525.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.525.861 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.552.805 I 
0.00.552.884 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.552.890 I perplexity: tokenizing the input ..
0.00.559.171 I perplexity: tokenization took 6.279 ms
0.00.559.179 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.701.862 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.703.113 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.703.144 I llama_perf_context_print:        load time =     543.06 ms
0.00.703.145 I llama_perf_context_print: prompt eval time =     142.30 ms /   128 tokens (    1.11 ms per token,   899.51 tokens per second)
0.00.703.146 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.703.146 I llama_perf_context_print:       total time =     150.34 ms /   129 tokens
0.00.703.519 I ggml_metal_free: deallocating

real	0m0.717s
user	0m0.078s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.548 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.283 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.286 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.286 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.287 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.287 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.287 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.288 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.291 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.291 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.292 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.294 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.203 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.114 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.114 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.115 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.115 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.116 I llama_model_loader: - type  f32:  194 tensors
0.00.026.116 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.116 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.117 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.117 I print_info: file format = GGUF V3 (latest)
0.00.026.118 I print_info: file type   = Q4_K - Medium
0.00.026.118 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.907 I load: special tokens cache size = 25
0.00.039.963 I load: token to piece cache size = 0.2984 MB
0.00.039.968 I print_info: arch             = gptneox
0.00.039.968 I print_info: vocab_only       = 0
0.00.039.968 I print_info: n_ctx_train      = 2048
0.00.039.968 I print_info: n_embd           = 2048
0.00.039.969 I print_info: n_layer          = 24
0.00.039.972 I print_info: n_head           = 16
0.00.039.972 I print_info: n_head_kv        = 16
0.00.039.977 I print_info: n_rot            = 32
0.00.039.977 I print_info: n_swa            = 0
0.00.039.978 I print_info: n_embd_head_k    = 128
0.00.039.979 I print_info: n_embd_head_v    = 128
0.00.039.979 I print_info: n_gqa            = 1
0.00.039.980 I print_info: n_embd_k_gqa     = 2048
0.00.039.981 I print_info: n_embd_v_gqa     = 2048
0.00.039.981 I print_info: f_norm_eps       = 1.0e-05
0.00.039.982 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.983 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.983 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.987 I print_info: f_logit_scale    = 0.0e+00
0.00.039.988 I print_info: n_ff             = 8192
0.00.039.988 I print_info: n_expert         = 0
0.00.039.988 I print_info: n_expert_used    = 0
0.00.039.989 I print_info: causal attn      = 1
0.00.039.989 I print_info: pooling type     = 0
0.00.039.989 I print_info: rope type        = 2
0.00.039.989 I print_info: rope scaling     = linear
0.00.039.990 I print_info: freq_base_train  = 10000.0
0.00.039.990 I print_info: freq_scale_train = 1
0.00.039.990 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.990 I print_info: rope_finetuned   = unknown
0.00.039.990 I print_info: ssm_d_conv       = 0
0.00.039.990 I print_info: ssm_d_inner      = 0
0.00.039.992 I print_info: ssm_d_state      = 0
0.00.039.992 I print_info: ssm_dt_rank      = 0
0.00.039.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.992 I print_info: model type       = 1.4B
0.00.039.992 I print_info: model params     = 1.41 B
0.00.039.993 I print_info: general.name     = 1.4B
0.00.039.993 I print_info: vocab type       = BPE
0.00.039.993 I print_info: n_vocab          = 50304
0.00.039.993 I print_info: n_merges         = 50009
0.00.039.998 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.998 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.998 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.999 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.999 I print_info: LF token         = 187 'Ċ'
0.00.039.999 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.999 I print_info: max token length = 1024
0.00.578.958 I load_tensors: offloading 24 repeating layers to GPU
0.00.578.963 I load_tensors: offloading output layer to GPU
0.00.578.963 I load_tensors: offloaded 25/25 layers to GPU
0.00.578.983 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.578.984 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.580.064 I llama_init_from_model: n_seq_max     = 1
0.00.580.066 I llama_init_from_model: n_ctx         = 128
0.00.580.067 I llama_init_from_model: n_ctx_per_seq = 128
0.00.580.067 I llama_init_from_model: n_batch       = 128
0.00.580.067 I llama_init_from_model: n_ubatch      = 128
0.00.580.068 I llama_init_from_model: flash_attn    = 0
0.00.580.069 I llama_init_from_model: freq_base     = 10000.0
0.00.580.069 I llama_init_from_model: freq_scale    = 1
0.00.580.070 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.580.071 I ggml_metal_init: allocating
0.00.580.092 I ggml_metal_init: found device: Apple M4
0.00.580.100 I ggml_metal_init: picking default device: Apple M4
0.00.581.369 I ggml_metal_init: using embedded metal library
0.00.587.270 I ggml_metal_init: GPU name:   Apple M4
0.00.587.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.587.275 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.587.275 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.587.276 I ggml_metal_init: simdgroup reduction   = true
0.00.587.276 I ggml_metal_init: simdgroup matrix mul. = true
0.00.587.276 I ggml_metal_init: has residency sets    = true
0.00.587.277 I ggml_metal_init: has bfloat            = true
0.00.587.277 I ggml_metal_init: use bfloat            = true
0.00.587.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.587.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.603.813 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.607.121 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.607.125 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.607.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.610.181 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.610.182 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.610.183 I llama_init_from_model: graph nodes  = 967
0.00.610.183 I llama_init_from_model: graph splits = 2
0.00.610.186 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.610.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.360 I 
0.00.635.444 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.451 I perplexity: tokenizing the input ..
0.00.642.434 I perplexity: tokenization took 6.98 ms
0.00.642.441 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.620 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.778.889 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.778.920 I llama_perf_context_print:        load time =     624.80 ms
0.00.778.921 I llama_perf_context_print: prompt eval time =     134.24 ms /   128 tokens (    1.05 ms per token,   953.49 tokens per second)
0.00.778.922 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.922 I llama_perf_context_print:       total time =     143.57 ms /   129 tokens
0.00.779.293 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.078s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.004 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.834 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.841 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.843 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.844 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.844 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.846 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.846 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.847 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.847 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.847 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.847 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.849 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.851 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.857 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.973 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.940 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.940 I llama_model_loader: - type  f32:  194 tensors
0.00.024.941 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.941 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.942 I print_info: file format = GGUF V3 (latest)
0.00.024.942 I print_info: file type   = Q5_K - Medium
0.00.024.943 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.847 I load: special tokens cache size = 25
0.00.038.945 I load: token to piece cache size = 0.2984 MB
0.00.038.949 I print_info: arch             = gptneox
0.00.038.949 I print_info: vocab_only       = 0
0.00.038.949 I print_info: n_ctx_train      = 2048
0.00.038.949 I print_info: n_embd           = 2048
0.00.038.950 I print_info: n_layer          = 24
0.00.038.954 I print_info: n_head           = 16
0.00.038.955 I print_info: n_head_kv        = 16
0.00.038.955 I print_info: n_rot            = 32
0.00.038.955 I print_info: n_swa            = 0
0.00.038.956 I print_info: n_embd_head_k    = 128
0.00.038.956 I print_info: n_embd_head_v    = 128
0.00.038.956 I print_info: n_gqa            = 1
0.00.038.957 I print_info: n_embd_k_gqa     = 2048
0.00.038.958 I print_info: n_embd_v_gqa     = 2048
0.00.038.958 I print_info: f_norm_eps       = 1.0e-05
0.00.038.959 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.959 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.959 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.959 I print_info: f_logit_scale    = 0.0e+00
0.00.038.960 I print_info: n_ff             = 8192
0.00.038.960 I print_info: n_expert         = 0
0.00.038.961 I print_info: n_expert_used    = 0
0.00.038.961 I print_info: causal attn      = 1
0.00.038.961 I print_info: pooling type     = 0
0.00.038.961 I print_info: rope type        = 2
0.00.038.961 I print_info: rope scaling     = linear
0.00.038.962 I print_info: freq_base_train  = 10000.0
0.00.038.962 I print_info: freq_scale_train = 1
0.00.038.964 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.964 I print_info: rope_finetuned   = unknown
0.00.038.964 I print_info: ssm_d_conv       = 0
0.00.038.965 I print_info: ssm_d_inner      = 0
0.00.038.965 I print_info: ssm_d_state      = 0
0.00.038.965 I print_info: ssm_dt_rank      = 0
0.00.038.965 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.965 I print_info: model type       = 1.4B
0.00.038.966 I print_info: model params     = 1.41 B
0.00.038.966 I print_info: general.name     = 1.4B
0.00.038.966 I print_info: vocab type       = BPE
0.00.038.966 I print_info: n_vocab          = 50304
0.00.038.966 I print_info: n_merges         = 50009
0.00.038.967 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.967 I print_info: LF token         = 187 'Ċ'
0.00.038.968 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.968 I print_info: max token length = 1024
0.00.666.312 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.316 I load_tensors: offloading output layer to GPU
0.00.666.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.334 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.666.335 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.667.246 I llama_init_from_model: n_seq_max     = 1
0.00.667.248 I llama_init_from_model: n_ctx         = 128
0.00.667.248 I llama_init_from_model: n_ctx_per_seq = 128
0.00.667.249 I llama_init_from_model: n_batch       = 128
0.00.667.249 I llama_init_from_model: n_ubatch      = 128
0.00.667.249 I llama_init_from_model: flash_attn    = 0
0.00.667.250 I llama_init_from_model: freq_base     = 10000.0
0.00.667.250 I llama_init_from_model: freq_scale    = 1
0.00.667.251 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.667.254 I ggml_metal_init: allocating
0.00.667.291 I ggml_metal_init: found device: Apple M4
0.00.667.301 I ggml_metal_init: picking default device: Apple M4
0.00.668.542 I ggml_metal_init: using embedded metal library
0.00.673.805 I ggml_metal_init: GPU name:   Apple M4
0.00.673.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.811 I ggml_metal_init: simdgroup reduction   = true
0.00.673.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.811 I ggml_metal_init: has residency sets    = true
0.00.673.812 I ggml_metal_init: has bfloat            = true
0.00.673.812 I ggml_metal_init: use bfloat            = true
0.00.673.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.688.668 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.909 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.691.912 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.691.955 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.694.890 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.694.891 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.694.892 I llama_init_from_model: graph nodes  = 967
0.00.694.892 I llama_init_from_model: graph splits = 2
0.00.694.895 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.694.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.255 I 
0.00.726.306 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.310 I perplexity: tokenizing the input ..
0.00.731.630 I perplexity: tokenization took 5.318 ms
0.00.731.634 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.499 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.872.774 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.872.807 I llama_perf_context_print:        load time =     717.24 ms
0.00.872.807 I llama_perf_context_print: prompt eval time =     139.64 ms /   128 tokens (    1.09 ms per token,   916.64 tokens per second)
0.00.872.808 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.872.808 I llama_perf_context_print:       total time =     146.55 ms /   129 tokens
0.00.873.243 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.074s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.436 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.371 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.373 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.373 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.374 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.375 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.375 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.379 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.379 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.379 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.326 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.436 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.394 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.396 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.396 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.397 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.397 I llama_model_loader: - type  f32:  194 tensors
0.00.026.397 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.398 I print_info: file format = GGUF V3 (latest)
0.00.026.399 I print_info: file type   = Q6_K
0.00.026.399 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.530 I load: special tokens cache size = 25
0.00.040.466 I load: token to piece cache size = 0.2984 MB
0.00.040.469 I print_info: arch             = gptneox
0.00.040.470 I print_info: vocab_only       = 0
0.00.040.470 I print_info: n_ctx_train      = 2048
0.00.040.470 I print_info: n_embd           = 2048
0.00.040.470 I print_info: n_layer          = 24
0.00.040.473 I print_info: n_head           = 16
0.00.040.474 I print_info: n_head_kv        = 16
0.00.040.474 I print_info: n_rot            = 32
0.00.040.474 I print_info: n_swa            = 0
0.00.040.474 I print_info: n_embd_head_k    = 128
0.00.040.474 I print_info: n_embd_head_v    = 128
0.00.040.475 I print_info: n_gqa            = 1
0.00.040.476 I print_info: n_embd_k_gqa     = 2048
0.00.040.476 I print_info: n_embd_v_gqa     = 2048
0.00.040.477 I print_info: f_norm_eps       = 1.0e-05
0.00.040.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.478 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.478 I print_info: f_logit_scale    = 0.0e+00
0.00.040.479 I print_info: n_ff             = 8192
0.00.040.479 I print_info: n_expert         = 0
0.00.040.479 I print_info: n_expert_used    = 0
0.00.040.481 I print_info: causal attn      = 1
0.00.040.481 I print_info: pooling type     = 0
0.00.040.481 I print_info: rope type        = 2
0.00.040.481 I print_info: rope scaling     = linear
0.00.040.482 I print_info: freq_base_train  = 10000.0
0.00.040.482 I print_info: freq_scale_train = 1
0.00.040.482 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.483 I print_info: rope_finetuned   = unknown
0.00.040.483 I print_info: ssm_d_conv       = 0
0.00.040.483 I print_info: ssm_d_inner      = 0
0.00.040.485 I print_info: ssm_d_state      = 0
0.00.040.485 I print_info: ssm_dt_rank      = 0
0.00.040.485 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.485 I print_info: model type       = 1.4B
0.00.040.486 I print_info: model params     = 1.41 B
0.00.040.486 I print_info: general.name     = 1.4B
0.00.040.487 I print_info: vocab type       = BPE
0.00.040.487 I print_info: n_vocab          = 50304
0.00.040.487 I print_info: n_merges         = 50009
0.00.040.487 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.487 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.487 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.488 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.488 I print_info: LF token         = 187 'Ċ'
0.00.040.492 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.492 I print_info: max token length = 1024
0.00.359.813 I load_tensors: offloading 24 repeating layers to GPU
0.00.359.820 I load_tensors: offloading output layer to GPU
0.00.359.821 I load_tensors: offloaded 25/25 layers to GPU
0.00.359.845 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.359.847 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.360.709 I llama_init_from_model: n_seq_max     = 1
0.00.360.711 I llama_init_from_model: n_ctx         = 128
0.00.360.711 I llama_init_from_model: n_ctx_per_seq = 128
0.00.360.712 I llama_init_from_model: n_batch       = 128
0.00.360.712 I llama_init_from_model: n_ubatch      = 128
0.00.360.712 I llama_init_from_model: flash_attn    = 0
0.00.360.713 I llama_init_from_model: freq_base     = 10000.0
0.00.360.714 I llama_init_from_model: freq_scale    = 1
0.00.360.714 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.360.715 I ggml_metal_init: allocating
0.00.360.744 I ggml_metal_init: found device: Apple M4
0.00.360.753 I ggml_metal_init: picking default device: Apple M4
0.00.361.895 I ggml_metal_init: using embedded metal library
0.00.367.194 I ggml_metal_init: GPU name:   Apple M4
0.00.367.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.367.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.367.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.367.200 I ggml_metal_init: simdgroup reduction   = true
0.00.367.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.367.200 I ggml_metal_init: has residency sets    = true
0.00.367.200 I ggml_metal_init: has bfloat            = true
0.00.367.201 I ggml_metal_init: use bfloat            = true
0.00.367.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.367.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.381.636 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.383.746 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.383.749 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.383.822 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.385.725 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.385.727 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.385.727 I llama_init_from_model: graph nodes  = 967
0.00.385.728 I llama_init_from_model: graph splits = 2
0.00.385.729 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.385.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.418.664 I 
0.00.418.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.418.707 I perplexity: tokenizing the input ..
0.00.424.409 I perplexity: tokenization took 5.699 ms
0.00.424.413 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.563.463 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.564.729 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.564.762 I llama_perf_context_print:        load time =     408.22 ms
0.00.564.763 I llama_perf_context_print: prompt eval time =     138.82 ms /   128 tokens (    1.08 ms per token,   922.04 tokens per second)
0.00.564.764 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.564.764 I llama_perf_context_print:       total time =     146.10 ms /   129 tokens
0.00.565.203 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.073s
sys	0m0.124s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.253 I build: 4615 (bfcce4d6) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.830 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.269 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.273 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.275 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.275 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.276 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.276 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.276 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.280 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.280 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.280 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.281 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.281 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.281 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.282 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.284 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.287 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.287 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.220 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.342 I llama_model_loader: - type  f32:  194 tensors
0.00.052.342 I llama_model_loader: - type  f16:   98 tensors
0.00.052.343 I print_info: file format = GGUF V3 (latest)
0.00.052.344 I print_info: file type   = all F32 (guessed)
0.00.052.350 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.867 I load: special tokens cache size = 25
0.00.071.894 I load: token to piece cache size = 0.2984 MB
0.00.071.898 I print_info: arch             = gptneox
0.00.071.898 I print_info: vocab_only       = 0
0.00.071.898 I print_info: n_ctx_train      = 2048
0.00.071.898 I print_info: n_embd           = 2048
0.00.071.899 I print_info: n_layer          = 24
0.00.071.901 I print_info: n_head           = 16
0.00.071.902 I print_info: n_head_kv        = 16
0.00.071.902 I print_info: n_rot            = 32
0.00.071.902 I print_info: n_swa            = 0
0.00.071.903 I print_info: n_embd_head_k    = 128
0.00.071.903 I print_info: n_embd_head_v    = 128
0.00.071.904 I print_info: n_gqa            = 1
0.00.071.904 I print_info: n_embd_k_gqa     = 2048
0.00.071.905 I print_info: n_embd_v_gqa     = 2048
0.00.071.906 I print_info: f_norm_eps       = 1.0e-05
0.00.071.906 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.906 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.906 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.907 I print_info: f_logit_scale    = 0.0e+00
0.00.071.907 I print_info: n_ff             = 8192
0.00.071.907 I print_info: n_expert         = 0
0.00.071.907 I print_info: n_expert_used    = 0
0.00.071.909 I print_info: causal attn      = 1
0.00.071.909 I print_info: pooling type     = 0
0.00.071.910 I print_info: rope type        = 2
0.00.071.910 I print_info: rope scaling     = linear
0.00.071.911 I print_info: freq_base_train  = 10000.0
0.00.071.911 I print_info: freq_scale_train = 1
0.00.071.911 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.911 I print_info: rope_finetuned   = unknown
0.00.071.912 I print_info: ssm_d_conv       = 0
0.00.071.912 I print_info: ssm_d_inner      = 0
0.00.071.927 I print_info: ssm_d_state      = 0
0.00.071.927 I print_info: ssm_dt_rank      = 0
0.00.071.928 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.928 I print_info: model type       = 1.4B
0.00.071.929 I print_info: model params     = 1.41 B
0.00.071.929 I print_info: general.name     = 1.4B
0.00.071.929 I print_info: vocab type       = BPE
0.00.071.930 I print_info: n_vocab          = 50304
0.00.071.930 I print_info: n_merges         = 50009
0.00.071.930 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.930 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.930 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.931 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.931 I print_info: LF token         = 187 'Ċ'
0.00.071.932 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.932 I print_info: max token length = 1024
0.01.443.281 I load_tensors: offloading 24 repeating layers to GPU
0.01.443.285 I load_tensors: offloading output layer to GPU
0.01.443.285 I load_tensors: offloaded 25/25 layers to GPU
0.01.443.309 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.443.310 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.443.946 I llama_init_from_model: n_seq_max     = 1
0.01.443.947 I llama_init_from_model: n_ctx         = 128
0.01.443.947 I llama_init_from_model: n_ctx_per_seq = 128
0.01.443.948 I llama_init_from_model: n_batch       = 128
0.01.443.948 I llama_init_from_model: n_ubatch      = 128
0.01.443.948 I llama_init_from_model: flash_attn    = 0
0.01.443.948 I llama_init_from_model: freq_base     = 10000.0
0.01.443.949 I llama_init_from_model: freq_scale    = 1
0.01.443.949 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.443.951 I ggml_metal_init: allocating
0.01.443.997 I ggml_metal_init: found device: Apple M4
0.01.444.002 I ggml_metal_init: picking default device: Apple M4
0.01.444.954 I ggml_metal_init: using embedded metal library
0.01.448.489 I ggml_metal_init: GPU name:   Apple M4
0.01.448.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.448.492 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.448.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.448.492 I ggml_metal_init: simdgroup reduction   = true
0.01.448.493 I ggml_metal_init: simdgroup matrix mul. = true
0.01.448.493 I ggml_metal_init: has residency sets    = true
0.01.448.493 I ggml_metal_init: has bfloat            = true
0.01.448.493 I ggml_metal_init: use bfloat            = true
0.01.448.493 I ggml_metal_init: hasUnifiedMemory      = true
0.01.448.494 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.458.275 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.459.942 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.459.944 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.459.969 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.461.401 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.461.402 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.461.403 I llama_init_from_model: graph nodes  = 967
0.01.461.403 I llama_init_from_model: graph splits = 2
0.01.461.404 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.461.405 I 
0.01.461.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.461.443 I compute_imatrix: tokenizing the input ..
0.01.464.963 I compute_imatrix: tokenization took 3.52 ms
0.01.464.965 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.724.649 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.726.958 I llama_perf_context_print:        load time =    1703.82 ms
0.01.726.959 I llama_perf_context_print: prompt eval time =     258.55 ms /   128 tokens (    2.02 ms per token,   495.08 tokens per second)
0.01.726.960 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.726.961 I llama_perf_context_print:       total time =    1706.12 ms /   129 tokens
0.01.727.431 I ggml_metal_free: deallocating

real	0m1.917s
user	0m0.125s
sys	0m0.337s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4615 (bfcce4d6)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13410b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13410bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13410c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13410c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13410cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13410d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13410d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13410dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13410e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13410e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13410ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13410f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13410fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1341104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134110cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1341113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134111b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134112220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134112940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134113110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134113830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134113f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134114670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134114f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134115630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1341158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134115f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134116b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1341170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134117370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134117810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134117ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134118360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1341188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134118b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134119000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1341194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134119940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134119de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13411a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13411a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13411abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13411b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13411b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13411b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13411bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13411c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13411cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13411d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13411d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13411df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13411e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13411eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13411f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13411f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13411fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134120290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134120550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134120b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134121350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134121610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134121ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134121f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1341223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134122890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134122d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1341231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134123670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134123b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134123fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134124450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1341248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134124d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1341252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134125830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134125d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1341262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134126820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134126d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1341272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134127810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134127d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1341282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134128800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134128d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1341292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1341297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134129d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13412a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13412a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13412ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13412b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13412b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13412bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13412c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13412c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13412cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13411c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13412d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13412d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13412de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13412e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13412e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13412ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13412f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13412f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13412fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1341303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134130900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134130e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1341313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1341318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134131e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1341322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134132780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134132c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1341330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134133560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134133a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134133ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134134340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1341347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134134c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134135120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1341355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134135a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134135f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1341363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134136840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134136ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134137180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134137620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134137ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134137f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134138400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1341388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134138d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1341391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134139680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134139b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134139fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13413a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13413a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13413ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13413b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13413b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13413bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13413c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13413c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13413c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13413ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13413d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13413d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13413dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13413e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13413e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13413e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13413ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13413f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13413f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13413fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1341400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134140580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134140a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134140ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134141360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134141800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134141ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134142140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1341425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134142a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134142f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1341433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134143860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134143d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1341441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134144640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134144ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134144f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134145420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1341458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134145d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134146200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1341466a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134146b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134146fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134147480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134147920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134147dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134148260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134148700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134148ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134149040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134149590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134149ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13414a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13414a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13414a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13414ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13414b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13414ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13414c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13414c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13414c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13414cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13414d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13414ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13414e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13414e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13414ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13414f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13414f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13414fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134150350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1341508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134150df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134151340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134151890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134151de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134152330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134152880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134152dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134153320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134153870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134153dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134154310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134154860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134154db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134155300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134155850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134155da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1341562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134156840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134156d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1341572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134157830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134157d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1341582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134158820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134158d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1341592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134159810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134159d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13415a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13415a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13415ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13415b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13415b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13415bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13415c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13415c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13415cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13415d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13415d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13415dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13415e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13415e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13415ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13415f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13415f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13415fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134160250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1341607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134160cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134161240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134161790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134161ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134162180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134162620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134162ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134162f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134163400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1341638a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134163d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1341641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134164680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134164b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134164fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134165460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134165900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134165da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134166240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134166790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134166eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1341675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134167cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134168410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1341686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134168ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134169180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134169790 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.742.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.742.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134169440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13414b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13414ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13414b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13411e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13411e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134120810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13414d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134115bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13411c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13411cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13411d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13411ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13411dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134114bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134120e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13412d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134168990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134117d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134118050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13414d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13414bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1341161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134116480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134116740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134169bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134169eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13416a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13416a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13416a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13416a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13416ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13416af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13416b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13416b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13416b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13416ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13416bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13416bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13416c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13416c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13416c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13416cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13416cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13416d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13416d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13416d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13416d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13416db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13416ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13416e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13416e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13416e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13416e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13416ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13416ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13416f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13416f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13416f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13416f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13416fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13416fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1341701b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134170470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134170730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1341709f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134170cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134170f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134171230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1341714f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1341717b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134171a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134171d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134171ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1341722b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134172570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134172830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134172af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134172db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134173070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134173330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1341735f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1341738b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134173b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134173e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1341740f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1341743b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134174670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134174930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134174bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134174eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134175170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134175430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1341756f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1341759b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134175c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134175f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1341761f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1341764b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134176770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134176a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134176cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134176fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134177270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134177530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1341777f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134177ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134177d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134178030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1341782f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1341785b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134178870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134178b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134178df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1341790b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134179370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134179630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1341798f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134179bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134179e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13417a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13417a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13417a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13417a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13417ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13417aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13417b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13417b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13417b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13417b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13417bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13417bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13417c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13417c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13417c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13417ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13417cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13417cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13417d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13417d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13417d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13417daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13417ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13417e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13417e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13417e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13417e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13417eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13417ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13417f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13417f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13417f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13417f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13417fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13417feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134180170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134180430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1341806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1341809b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134180c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134180f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1341811f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1341814b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134181770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134181a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134181cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134181fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134182270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134182530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1341827f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134182ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134182d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134183030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1341832f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1341835b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134183870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134183b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134183df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1341840b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134184370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134184630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1341848f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134184bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134184e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134185130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1341853f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1341856b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134185970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134185c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134185ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1341861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134186470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134186730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1341869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134186cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134186f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134187230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1341874f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1341877b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134187a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134187d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134187ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1341882b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134188570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134188ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134188ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1341892b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134189750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134189bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13418a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13418a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13418ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13418adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13418b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13418b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13418bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13418bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13418c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13418c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13418ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13418d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13418d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13418da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13418de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13418e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13418e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13418ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13418f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13418f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13418f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13418fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134190210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134190680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134190af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134190f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1341913d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134191840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134191cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134192120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134192590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134192a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134192e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1341932e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134193750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134193bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134194030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1341944a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134194910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134194d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1341951f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134195660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134195ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134195f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1341963b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134196820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134196c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134197100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134197570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1341979e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134197e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1341982c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134198730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134198ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134199010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134199480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1341998f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134199d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13419a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13419a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13419aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13419af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13419b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13419b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13419bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13419c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13419c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13419c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13419ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13419d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13419d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13419db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13419dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13419e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13419eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13419f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13419fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1341a0430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1341a06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1341a0ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1341a11a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1341a17b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126c044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126c04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126c04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126c05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126c056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126c05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126c05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126c063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126c06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126c06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126c07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126c078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126c083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126c08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126c09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126c09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126c0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126c0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126c0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126c0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126c0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126c0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126c0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126c0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126c0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126c0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126c0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126c0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126c0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126c0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126c0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126c0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126c0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126c0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126c10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126c107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126c10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126c110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126c11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126c119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126c11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126c12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126c12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126c12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126c12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126c13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126c138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126c13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126c141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126c14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126c14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126c14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126c15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126c157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126c15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126c160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126c16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126c16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126c16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126c17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126c17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126c17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126c18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126c185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126c18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126c18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126c19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126c19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126c19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126c1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126c1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126c1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126c1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126c1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126c1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126c1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126c1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126c1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126c1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126c1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126c1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126c1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126c1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126c1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126c1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126c1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126c1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126c1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126c1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126c1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126c1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126c20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126c20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126c20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126c20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126c213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126c21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126c21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126c22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126c22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126c229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126c22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126c232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126c23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126c23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126c24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126c24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126c24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126c24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126c25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126c258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126c25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126c261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126c26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126c26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126c26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126c27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126c277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126c27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126c280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126c28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126c28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126c28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126c29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126c296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126c29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126c29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126c2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126c2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126c2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126c2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126c2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126c2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126c2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126c2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126c2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126c2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126c2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126c2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126c2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126c2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126c2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126c2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126c2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126c2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126c2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126c2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126c2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126c30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126c305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126c30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126c30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126c31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126c31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126c31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126c32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126c324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126c32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126c32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126c33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126c336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126c33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126c33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126c343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126c34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126c34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126c35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126c355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126c35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126c35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126c36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126c36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126c36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126c37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126c374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126c37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126c37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126c38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126c38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126c38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126c38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126c393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126c39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126c39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126c3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126c3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126c3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126c3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126c3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126c3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126c3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126c3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126c3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126c3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126c3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126c3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126c3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126c3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126c3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126c3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126c3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126c3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126c3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126c3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126c3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126c3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126c402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126c40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126c40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126c41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126c41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126c41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126c42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126c42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126c429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126c42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126c432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126c43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126c43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126c44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126c44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126c44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126c44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126c451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126c45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126c45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126c45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126c463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126c46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126c46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126c470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126c47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126c479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126c47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126c482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126c48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126c48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126c49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126c49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126c498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126c49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126c4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126c4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126c4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126c4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126c4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126c4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126c4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126c4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126c4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126c4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126c4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126c4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126c4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126c4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126c4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126c4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126c4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126c4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126c4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126c4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126c4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126c4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126c50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126c507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126c50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126c510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126c51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126c51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126c51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126c52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126c526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126c52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126c52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126c53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126c538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126c53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126c54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126c545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126c54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126c54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126c55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126c557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126c56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126c56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126c57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126c57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126c57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126c57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126c584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126c58ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.797s
user	0m0.281s
sys	0m0.329s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4615 (bfcce4d6)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12980a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12980ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12980b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12980b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12980bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12980c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12980c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12980cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12980d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12980d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12980dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12980e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12980ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12980f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12980fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1298103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129810b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129811220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129811940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129812110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129812830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129813670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129813f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129814630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1298148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129814f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129815b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1298160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129816370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129816810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129816ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129817360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1298178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129817b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129818000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1298184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129818940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129818de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129819720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129819bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12981a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12981a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12981a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12981add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12981b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12981bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12981c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12981c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12981cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12981d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12981db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12981e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12981e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12981edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12981f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12981f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12981fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129820350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129820610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129820ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129820f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1298213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129821890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129821d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1298221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129822670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129822b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129822fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129823450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1298238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1298242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129824830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129824d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1298252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129825820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129825d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1298262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129826810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129826d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1298272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129827800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129827d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1298282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1298287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129828d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129829290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1298297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129829d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12982a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12982a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12982ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12982b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12982b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12982bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12981b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12982c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12982c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12982ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12982d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12982d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12982de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12982e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12982e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12982ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12982f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12982f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12982fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1298303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1298308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129830e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1298312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129831780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129831c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1298320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129832560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129832a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129832ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129833340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1298337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129833c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129834120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1298345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129834a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129834f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1298353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129835840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129835ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129836180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129836620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129836ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129836f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129837400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1298378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129837d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1298381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129838680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129838b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129838fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129839460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129839900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129839da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12983a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12983a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12983ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12983b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12983b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12983b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12983be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12983c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12983c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12983cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12983d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12983d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12983d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12983de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12983e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12983e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12983ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12983f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12983f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12983fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12983fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129840360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129840800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129840ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129841140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1298415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129841a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129841f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1298423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129842860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129842d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1298431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129843640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129843ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129843f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129844420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1298448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129844d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129845200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1298456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129845b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129845fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129846480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129846920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129846dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129847260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129847700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129847ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129848040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129848590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129848ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129849030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129849580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129849840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129849e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12984a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12984aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12984b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12984b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12984b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12984bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12984c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12984cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12984d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12984d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12984dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12984e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12984e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12984ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12984f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12984f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12984fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129850340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129850890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129850de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129851330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129851880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129851dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129852320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129852870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129852dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129853310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129853860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129853db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129854300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129854850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129854da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1298552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129855840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129855d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1298562e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129856830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129856d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1298572d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129857820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129857d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1298582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129858810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129858d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1298592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129859800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129859d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12985a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12985a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12985ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12985b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12985b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12985bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12985c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12985c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12985cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12985d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12985d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12985dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12985e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12985e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12985ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12985f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12985f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12985fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129860240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129860790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129860ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129861180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129861620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129861ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129861f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129862400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1298628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129862d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1298631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129863680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129863b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129863fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129864460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129864900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129864da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129865240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129865790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129865eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1298665d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129866cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129867410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1298676d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129867ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129868180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129868790 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.522 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129868440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12984a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129849b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12984a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12981d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12981d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12981f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12984c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129814bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12981b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12981bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12981c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12981aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12981cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129813bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129809a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12981e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12981fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12982c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129867990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129816d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129817050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12984c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12984ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1298151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129815480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129868bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129868eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129869170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129869430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1298696f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1298699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129869c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129869f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12986a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12986a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12986a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12986aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12986acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12986afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12986b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12986b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12986b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12986bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12986bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12986c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12986c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12986c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12986c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12986cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12986cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12986d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12986d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12986d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12986d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12986dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12986de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12986e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12986e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12986e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12986e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12986ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12986eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12986f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12986f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12986f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12986f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12986fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12986ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129870230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1298704f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1298707b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129870a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129870d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129870ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1298712b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129871570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129871830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129871af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129871db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129872070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129872330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1298725f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1298728b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129872b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129872e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1298730f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1298733b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129873670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129873930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129873bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129873eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129874170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129874430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1298746f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1298749b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129874c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129874f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1298751f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1298754b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129875770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129875a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129875cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129875fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129876270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129876530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1298767f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129876ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129876d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129877030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1298772f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1298775b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129877870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129877b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129877df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1298780b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129878370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129878630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1298788f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129878bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129878e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129879130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1298793f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1298796b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129879970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129879c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129879ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12987a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12987a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12987a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12987a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12987acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12987af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12987b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12987b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12987b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12987ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12987bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12987bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12987c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12987c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12987c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12987caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12987cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12987d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12987d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12987d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12987d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12987db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12987de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12987e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12987e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12987e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12987e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12987ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12987eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12987f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12987f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12987f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12987f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12987fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12987ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1298801f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1298804b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129880770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129880a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129880cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129880fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129881270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129881530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1298817f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129881ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129881d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129882030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1298822f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1298825b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129882870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129882b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129882df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1298830b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129883370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129883630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1298838f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129883bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129883e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129884130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1298843f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1298846b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129884970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129884c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129884ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1298851b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129885470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129885730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1298859f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129885cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129885f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129886230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1298864f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1298867b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129886a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129886d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129886ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1298872b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129887570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129887830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129887af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129887db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129888070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129888640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129888900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129888bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129888e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129889140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129889400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1298896c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129889980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129889c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129889f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12988a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12988a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12988a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12988aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12988acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12988af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12988b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12988b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12988b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12988ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12988bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12988c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12988c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12988c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12988c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12988cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12988cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12988d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12988d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12988d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12988d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12988db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12988de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12988e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12988e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12988ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12988f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12988f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12988fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129890370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1298908c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129890e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129891360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1298918b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129891e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129892350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1298928a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129892df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129893340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129893890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129893de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129894330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129894880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129894dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129895320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129895870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129895dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129896080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129896340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129896840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129896d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129897240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129897740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129897c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129898140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129898640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129898b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129899040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129899540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129899a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129899f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12989a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12989a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12989b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12989ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12989c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12989c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12989cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12989d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12989d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12989dc30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x118608d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x118606e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x118609370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x118609920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x118609ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11860a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11860aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11860afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11860b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11860ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11860bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11860c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11860cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11860d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11860df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11860e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11860edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11860f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11860fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1186103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x118610ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x118611200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x118611920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x118612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x118612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x118612a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118613030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x118613640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118613c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x118614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1186148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x118614ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x118615430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x118615970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118615c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1186160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118616570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x118616a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x118616eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x118617350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1186177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118617c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x118618130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1186185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118618890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x118618ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1186194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x118619ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11861a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11861a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11861acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11861b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11861b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11861bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11861c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11861cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11861d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11861d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11861d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11861e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11861e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11861ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11861eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11861f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11861f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11861fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118620170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118620610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118620ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118620f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1186213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118621890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118621d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x118622280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1186227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x118622d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x118623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1186237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118623d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118624260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1186247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x118624d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x118625250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1186257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118625cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x118626240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x118626790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x118626ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118627230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118627780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118627cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118628220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118628770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118628cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118629210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118629760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118629cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11862a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11862a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11862aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11862b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11862b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11862bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11862c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11862c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11862cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11862d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11862d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11862dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11862e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11862e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11862ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11862f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11862f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11862faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11862ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x118630430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1186308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118630d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x118631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1186316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118631ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x118632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x118632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118632dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x118634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1186344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x118634e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1186352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1186360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x118636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1186369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x118636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x118637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1186377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118637c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1186385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x118638a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x118638ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x118639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x118639830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x118639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11863a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11863a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11863aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11863af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11863b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11863b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11863bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11863c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11863c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11863cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11863cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11863d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11863d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11863dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11863e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11863e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11863eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11863f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11863f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11863f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11863fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x118640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x118640730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x118640bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x118641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x118641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1186419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x118641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1186422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x118642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x118642c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1186430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x118643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x118643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x118644350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1186447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x118644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x118645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1186455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x118645a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x118645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1186463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x118646900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x118646e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1186473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1186478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x118647bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1186481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1186487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x118648de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1186495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x118649a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x118649d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11864a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11864a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11864b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11864b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11864ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11864bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11864c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11864cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11864d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11864d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11864dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11864e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11864e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11864ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11864f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11864f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11864fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x118650140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x118650690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118650be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118651130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x118651680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x118651bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118652120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x118652670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118652bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118653110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x118653660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118653bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118654100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118654650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118654ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1186550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118655640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x118655b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1186560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x118656630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118656b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1186570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118657620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x118657b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1186580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x118658610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x118658b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1186590b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118659600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x118659b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11865a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11865a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11865ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11865b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11865b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11865bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11865c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11865c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11865cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11865d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11865d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11865db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11865e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11865e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11865eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11865f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11865f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11865f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11865fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1186602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x118660770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1186610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x118661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1186619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1186627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x118662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x118663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1186635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x118663b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x118664220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x118664940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x118665060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x118665780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x118665a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x118666230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1186664f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x118666b00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.914s
user	0m0.235s
sys	0m0.146s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
