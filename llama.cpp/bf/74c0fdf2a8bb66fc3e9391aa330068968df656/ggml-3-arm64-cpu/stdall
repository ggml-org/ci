The virtual environment was not created successfully because ensurepip is not
available.  On Debian/Ubuntu systems, you need to install the python3-venv
package using the following command.

    apt install python3.10-venv

You may need to use sudo with that command.  After installing the python3-venv
package, recreate your virtual environment.

Failing command: /mnt/llama.cpp/venv/bin/python3

ci/run.sh: line 630: /mnt/llama.cpp/venv/bin/activate: No such file or directory
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: numpy~=1.24.4 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.98)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.37.1)
Requirement already satisfied: gguf>=0.1.0 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.7.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.23.4)
Requirement already satisfied: torch~=2.1.1 in /home/ggml/.local/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.19.4)
Requirement already satisfied: packaging>=20.0 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.1)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.1)
Requirement already satisfied: safetensors>=0.3.1 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.1)
Requirement already satisfied: regex!=2019.12.17 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.6.3)
Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.25.1)
Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (5.4.1)
Requirement already satisfied: tqdm>=4.27 in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.65.0)
Requirement already satisfied: filelock in /home/ggml/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.12.2)
Requirement already satisfied: fsspec in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)
Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.0.3)
Requirement already satisfied: networkx in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1)
Requirement already satisfied: typing-extensions in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.7.1)
Requirement already satisfied: sympy in /home/ggml/.local/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: mpmath>=0.19 in /home/ggml/.local/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Defaulting to user installation because normal site-packages is not writeable
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /home/ggml/.local/lib/python3.10/site-packages (from gguf==0.7.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.7.0-py3-none-any.whl size=3229 sha256=e77d3489e0beea138863799a05abb421092af60b67ffc67b376508ee3f0720bf
  Stored in directory: /tmp/pip-ephem-wheel-cache-v1_p38l8/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.7.0
    Uninstalling gguf-0.7.0:
      Successfully uninstalled gguf-0.7.0
Successfully installed gguf-0.7.0
+ gg_run_ctest_debug
+ tee /home/ggml/results/llama.cpp/bf/74c0fdf2a8bb66fc3e9391aa330068968df656/ggml-3-arm64-cpu/ctest_debug.log
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/bf/74c0fdf2a8bb66fc3e9391aa330068968df656/ggml-3-arm64-cpu/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.752s
user	0m0.444s
sys	0m0.312s
+ tee -a /home/ggml/results/llama.cpp/bf/74c0fdf2a8bb66fc3e9391aa330068968df656/ggml-3-arm64-cpu/ctest_debug-make.log
+ make -j
[  0%] Generating build details from Git
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Built target build_info
/home/ggml/work/llama.cpp/ggml.c: In function ‘ggml_vec_dot_f16’:
/home/ggml/work/llama.cpp/ggml.c:1336:45: error: passing argument 1 of ‘vld1_f16’ from incompatible pointer type [-Werror=incompatible-pointer-types]
 1336 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);
      |                                             ^
      |                                             |
      |                                             ggml_fp16_t * {aka short unsigned int *}
/home/ggml/work/llama.cpp/ggml.c:844:60: note: in definition of macro ‘GGML_F32Cx4_LOAD’
  844 |     #define GGML_F32Cx4_LOAD(x)      vcvt_f32_f16(vld1_f16(x))
      |                                                            ^
/home/ggml/work/llama.cpp/ggml.c:1336:21: note: in expansion of macro ‘GGML_F16_VEC_LOAD’
 1336 |             ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);
      |                     ^~~~~~~~~~~~~~~~~
In file included from /home/ggml/work/llama.cpp/ggml-impl.h:54,
                 from /home/ggml/work/llama.cpp/ggml.c:4:
/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h:15583:28: note: expected ‘const float16_t *’ {aka ‘const __fp16 *’} but argument is of type ‘ggml_fp16_t *’ {aka ‘short unsigned int *’}
15583 | vld1_f16 (const float16_t *__a)
      |           ~~~~~~~~~~~~~~~~~^~~
/home/ggml/work/llama.cpp/ggml.c:1337:45: error: passing argument 1 of ‘vld1_f16’ from incompatible pointer type [-Werror=incompatible-pointer-types]
 1337 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);
      |                                             ^
      |                                             |
      |                                             ggml_fp16_t * {aka short unsigned int *}
/home/ggml/work/llama.cpp/ggml.c:844:60: note: in definition of macro ‘GGML_F32Cx4_LOAD’
  844 |     #define GGML_F32Cx4_LOAD(x)      vcvt_f32_f16(vld1_f16(x))
      |                                                            ^
/home/ggml/work/llama.cpp/ggml.c:1337:21: note: in expansion of macro ‘GGML_F16_VEC_LOAD’
 1337 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);
      |                     ^~~~~~~~~~~~~~~~~
In file included from /home/ggml/work/llama.cpp/ggml-impl.h:54,
                 from /home/ggml/work/llama.cpp/ggml.c:4:
/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h:15583:28: note: expected ‘const float16_t *’ {aka ‘const __fp16 *’} but argument is of type ‘ggml_fp16_t *’ {aka ‘short unsigned int *’}
15583 | vld1_f16 (const float16_t *__a)
      |           ~~~~~~~~~~~~~~~~~^~~
/home/ggml/work/llama.cpp/ggml.c: In function ‘ggml_vec_dot_f16_unroll’:
/home/ggml/work/llama.cpp/ggml.c:1380:45: error: passing argument 1 of ‘vld1_f16’ from incompatible pointer type [-Werror=incompatible-pointer-types]
 1380 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);
      |                                             ^
      |                                             |
      |                                             ggml_fp16_t * {aka short unsigned int *}
/home/ggml/work/llama.cpp/ggml.c:844:60: note: in definition of macro ‘GGML_F32Cx4_LOAD’
  844 |     #define GGML_F32Cx4_LOAD(x)      vcvt_f32_f16(vld1_f16(x))
      |                                                            ^
/home/ggml/work/llama.cpp/ggml.c:1380:21: note: in expansion of macro ‘GGML_F16_VEC_LOAD’
 1380 |             ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);
      |                     ^~~~~~~~~~~~~~~~~
In file included from /home/ggml/work/llama.cpp/ggml-impl.h:54,
                 from /home/ggml/work/llama.cpp/ggml.c:4:
/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h:15583:28: note: expected ‘const float16_t *’ {aka ‘const __fp16 *’} but argument is of type ‘ggml_fp16_t *’ {aka ‘short unsigned int *’}
15583 | vld1_f16 (const float16_t *__a)
      |           ~~~~~~~~~~~~~~~~~^~~
/home/ggml/work/llama.cpp/ggml.c:1383:52: error: passing argument 1 of ‘vld1_f16’ from incompatible pointer type [-Werror=incompatible-pointer-types]
 1383 |                 ax[j] = GGML_F16_VEC_LOAD(x[k] + i + j*GGML_F16_EPR, j);
      |                                                    ^
      |                                                    |
      |                                                    ggml_fp16_t * {aka short unsigned int *}
/home/ggml/work/llama.cpp/ggml.c:844:60: note: in definition of macro ‘GGML_F32Cx4_LOAD’
  844 |     #define GGML_F32Cx4_LOAD(x)      vcvt_f32_f16(vld1_f16(x))
      |                                                            ^
/home/ggml/work/llama.cpp/ggml.c:1383:25: note: in expansion of macro ‘GGML_F16_VEC_LOAD’
 1383 |                 ax[j] = GGML_F16_VEC_LOAD(x[k] + i + j*GGML_F16_EPR, j);
      |                         ^~~~~~~~~~~~~~~~~
In file included from /home/ggml/work/llama.cpp/ggml-impl.h:54,
                 from /home/ggml/work/llama.cpp/ggml.c:4:
/usr/lib/gcc/aarch64-linux-gnu/11/include/arm_neon.h:15583:28: note: expected ‘const float16_t *’ {aka ‘const __fp16 *’} but argument is of type ‘ggml_fp16_t *’ {aka ‘short unsigned int *’}
15583 | vld1_f16 (const float16_t *__a)
      |           ~~~~~~~~~~~~~~~~~^~~
cc1: all warnings being treated as errors
make[2]: *** [CMakeFiles/ggml.dir/build.make:76: CMakeFiles/ggml.dir/ggml.c.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:740: CMakeFiles/ggml.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m1.524s
user	0m2.853s
sys	0m0.476s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/bf/74c0fdf2a8bb66fc3e9391aa330068968df656/ggml-3-arm64-cpu/ctest_debug-ctest.log: No such file or directory
