### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.36 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.85 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.69 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.44 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.50 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.02 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.34 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.21 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.24 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   25.55 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.32 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    2.23 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.18 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.18 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.23 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.18 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.85 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed  174.42 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    1.30 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.66 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 218.21 sec*proc (27 tests)

Total Test time (real) = 218.22 sec

real	3m38.325s
user	7m24.588s
sys	0m5.247s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    3.72 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.74 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.55 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.54 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   14.53 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.65 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    1.28 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.62 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.61 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.62 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.61 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.65 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed   28.66 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.61 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.51 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  58.08 sec*proc (27 tests)

Total Test time (real) =  58.10 sec

real	0m58.106s
user	1m10.467s
sys	0m4.707s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.134 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.171 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.239 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.249 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.253 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.028.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.259 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.028.260 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.028.261 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.028.262 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.028.262 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.028.263 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.028.263 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.028.264 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.028.269 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.269 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.270 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.028.275 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.028.276 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.276 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.028.277 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.200 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.034.948 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.952 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.952 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.953 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.954 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.034.954 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.955 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.034.956 I llama_model_loader: - type  f32:  124 tensors
0.00.034.960 I llama_model_loader: - type  f16:   73 tensors
0.00.040.365 I llm_load_vocab: special tokens cache size = 5
0.00.043.198 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.043.206 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.043.206 I llm_load_print_meta: arch             = bert
0.00.043.208 I llm_load_print_meta: vocab type       = WPM
0.00.043.209 I llm_load_print_meta: n_vocab          = 30522
0.00.043.209 I llm_load_print_meta: n_merges         = 0
0.00.043.209 I llm_load_print_meta: vocab_only       = 0
0.00.043.210 I llm_load_print_meta: n_ctx_train      = 512
0.00.043.210 I llm_load_print_meta: n_embd           = 384
0.00.043.210 I llm_load_print_meta: n_layer          = 12
0.00.043.214 I llm_load_print_meta: n_head           = 12
0.00.043.215 I llm_load_print_meta: n_head_kv        = 12
0.00.043.219 I llm_load_print_meta: n_rot            = 32
0.00.043.219 I llm_load_print_meta: n_swa            = 0
0.00.043.219 I llm_load_print_meta: n_embd_head_k    = 32
0.00.043.220 I llm_load_print_meta: n_embd_head_v    = 32
0.00.043.221 I llm_load_print_meta: n_gqa            = 1
0.00.043.222 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.043.223 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.043.225 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.043.226 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.043.226 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.043.226 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.043.229 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.043.230 I llm_load_print_meta: n_ff             = 1536
0.00.043.230 I llm_load_print_meta: n_expert         = 0
0.00.043.230 I llm_load_print_meta: n_expert_used    = 0
0.00.043.231 I llm_load_print_meta: causal attn      = 0
0.00.043.231 I llm_load_print_meta: pooling type     = 2
0.00.043.231 I llm_load_print_meta: rope type        = 2
0.00.043.232 I llm_load_print_meta: rope scaling     = linear
0.00.043.232 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.043.233 I llm_load_print_meta: freq_scale_train = 1
0.00.043.233 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.043.234 I llm_load_print_meta: rope_finetuned   = unknown
0.00.043.234 I llm_load_print_meta: ssm_d_conv       = 0
0.00.043.234 I llm_load_print_meta: ssm_d_inner      = 0
0.00.043.234 I llm_load_print_meta: ssm_d_state      = 0
0.00.043.235 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.043.241 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.043.257 I llm_load_print_meta: model type       = 33M
0.00.043.257 I llm_load_print_meta: model ftype      = F16
0.00.043.258 I llm_load_print_meta: model params     = 33.21 M
0.00.043.259 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.043.259 I llm_load_print_meta: general.name     = Bge Small
0.00.043.260 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.043.261 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.043.261 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.043.261 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.043.262 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.043.262 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.043.262 I llm_load_print_meta: max token length = 21
0.00.046.175 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.046.177 I llm_load_tensors: offloading output layer to GPU
0.00.046.177 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.046.208 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.046.210 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.046.949 I llama_new_context_with_model: n_seq_max     = 1
0.00.046.950 I llama_new_context_with_model: n_ctx         = 512
0.00.046.951 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.046.951 I llama_new_context_with_model: n_batch       = 2048
0.00.046.951 I llama_new_context_with_model: n_ubatch      = 2048
0.00.046.952 I llama_new_context_with_model: flash_attn    = 0
0.00.046.952 I llama_new_context_with_model: freq_base     = 10000.0
0.00.046.953 I llama_new_context_with_model: freq_scale    = 1
0.00.046.953 I ggml_metal_init: allocating
0.00.046.960 I ggml_metal_init: found device: Apple M4
0.00.046.967 I ggml_metal_init: picking default device: Apple M4
0.00.048.141 I ggml_metal_init: using embedded metal library
0.00.052.384 I ggml_metal_init: GPU name:   Apple M4
0.00.052.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.052.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.052.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.052.389 I ggml_metal_init: simdgroup reduction   = true
0.00.052.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.052.389 I ggml_metal_init: has bfloat            = true
0.00.052.390 I ggml_metal_init: use bfloat            = true
0.00.052.390 I ggml_metal_init: hasUnifiedMemory      = true
0.00.052.391 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.388 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.065.391 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.065.394 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.066.517 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.066.518 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.066.518 I llama_new_context_with_model: graph nodes  = 429
0.00.066.519 I llama_new_context_with_model: graph splits = 2
0.00.066.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.075.682 I 
0.00.075.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.076.584 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.081.932 I llama_perf_context_print:        load time =      51.50 ms
0.00.081.933 I llama_perf_context_print: prompt eval time =       5.18 ms /     9 tokens (    0.58 ms per token,  1736.11 tokens per second)
0.00.081.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.081.935 I llama_perf_context_print:       total time =       6.25 ms /    10 tokens
0.00.082.107 I ggml_metal_free: deallocating

real	0m0.627s
user	0m0.052s
sys	0m0.036s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.041 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.670 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.769 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.773 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.774 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.775 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.778 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.779 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.780 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.780 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.781 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.781 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.781 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.783 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.783 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.783 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.784 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.784 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.784 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.785 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.967 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.969 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.969 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.969 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.970 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.970 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.970 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.971 I llama_model_loader: - type  f32:  124 tensors
0.00.014.971 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.580 I llm_load_vocab: special tokens cache size = 5
0.00.018.868 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.871 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.871 I llm_load_print_meta: arch             = bert
0.00.018.871 I llm_load_print_meta: vocab type       = WPM
0.00.018.871 I llm_load_print_meta: n_vocab          = 30522
0.00.018.872 I llm_load_print_meta: n_merges         = 0
0.00.018.872 I llm_load_print_meta: vocab_only       = 0
0.00.018.872 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.872 I llm_load_print_meta: n_embd           = 384
0.00.018.872 I llm_load_print_meta: n_layer          = 12
0.00.018.874 I llm_load_print_meta: n_head           = 12
0.00.018.874 I llm_load_print_meta: n_head_kv        = 12
0.00.018.875 I llm_load_print_meta: n_rot            = 32
0.00.018.875 I llm_load_print_meta: n_swa            = 0
0.00.018.875 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.875 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.876 I llm_load_print_meta: n_gqa            = 1
0.00.018.876 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.877 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.877 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.878 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.878 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.878 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.879 I llm_load_print_meta: n_ff             = 1536
0.00.018.879 I llm_load_print_meta: n_expert         = 0
0.00.018.879 I llm_load_print_meta: n_expert_used    = 0
0.00.018.879 I llm_load_print_meta: causal attn      = 0
0.00.018.879 I llm_load_print_meta: pooling type     = 2
0.00.018.879 I llm_load_print_meta: rope type        = 2
0.00.018.880 I llm_load_print_meta: rope scaling     = linear
0.00.018.880 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.880 I llm_load_print_meta: freq_scale_train = 1
0.00.018.880 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.880 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.881 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.881 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.881 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.881 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.881 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.887 I llm_load_print_meta: model type       = 33M
0.00.018.887 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.888 I llm_load_print_meta: model params     = 33.21 M
0.00.018.888 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.892 I llm_load_print_meta: general.name     = Bge Small
0.00.018.892 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.892 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.892 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.892 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.893 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.893 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.893 I llm_load_print_meta: max token length = 21
0.00.020.220 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.222 I llm_load_tensors: offloading output layer to GPU
0.00.020.222 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.229 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.230 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.606 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.607 I llama_new_context_with_model: n_ctx         = 512
0.00.020.607 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.607 I llama_new_context_with_model: n_batch       = 2048
0.00.020.608 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.608 I llama_new_context_with_model: flash_attn    = 0
0.00.020.608 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.609 I llama_new_context_with_model: freq_scale    = 1
0.00.020.609 I ggml_metal_init: allocating
0.00.020.615 I ggml_metal_init: found device: Apple M4
0.00.020.617 I ggml_metal_init: picking default device: Apple M4
0.00.021.128 I ggml_metal_init: using embedded metal library
0.00.023.260 I ggml_metal_init: GPU name:   Apple M4
0.00.023.262 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.263 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.263 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.263 I ggml_metal_init: simdgroup reduction   = true
0.00.023.263 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.263 I ggml_metal_init: has bfloat            = true
0.00.023.264 I ggml_metal_init: use bfloat            = true
0.00.023.264 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.488 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.490 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.491 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.080 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.081 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.081 I llama_new_context_with_model: graph nodes  = 429
0.00.032.081 I llama_new_context_with_model: graph splits = 2
0.00.032.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.036.799 I 
0.00.036.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.037.381 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.960 I llama_perf_context_print:        load time =      27.13 ms
0.00.041.961 I llama_perf_context_print: prompt eval time =       4.46 ms /     9 tokens (    0.50 ms per token,  2019.75 tokens per second)
0.00.041.962 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.963 I llama_perf_context_print:       total time =       5.16 ms /    10 tokens
0.00.042.131 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.027s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.155 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.482 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.635 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.644 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.648 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.031.649 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.650 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.031.660 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.031.661 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.031.663 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.031.665 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.031.665 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.031.666 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.031.667 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.031.673 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.031.673 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.031.674 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.031.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.810 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.271 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.300 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.301 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.302 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.302 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.302 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.303 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.303 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.304 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.304 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.304 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.305 I llama_model_loader: - type  f32:   41 tensors
0.00.048.305 I llama_model_loader: - type  f16:   29 tensors
0.00.068.362 W llm_load_vocab: empty token at index 5
0.00.072.976 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.317 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.361 I llm_load_vocab: special tokens cache size = 5
0.00.313.317 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.313.322 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.313.322 I llm_load_print_meta: arch             = jina-bert-v2
0.00.313.323 I llm_load_print_meta: vocab type       = BPE
0.00.313.323 I llm_load_print_meta: n_vocab          = 61056
0.00.313.323 I llm_load_print_meta: n_merges         = 39382
0.00.313.323 I llm_load_print_meta: vocab_only       = 0
0.00.313.323 I llm_load_print_meta: n_ctx_train      = 8192
0.00.313.324 I llm_load_print_meta: n_embd           = 384
0.00.313.324 I llm_load_print_meta: n_layer          = 4
0.00.313.328 I llm_load_print_meta: n_head           = 12
0.00.313.329 I llm_load_print_meta: n_head_kv        = 12
0.00.313.329 I llm_load_print_meta: n_rot            = 32
0.00.313.329 I llm_load_print_meta: n_swa            = 0
0.00.313.329 I llm_load_print_meta: n_embd_head_k    = 32
0.00.313.331 I llm_load_print_meta: n_embd_head_v    = 32
0.00.313.331 I llm_load_print_meta: n_gqa            = 1
0.00.313.332 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.313.333 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.313.334 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.313.334 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.313.334 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.313.334 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.313.335 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.313.335 I llm_load_print_meta: n_ff             = 1536
0.00.313.335 I llm_load_print_meta: n_expert         = 0
0.00.313.336 I llm_load_print_meta: n_expert_used    = 0
0.00.313.336 I llm_load_print_meta: causal attn      = 0
0.00.313.336 I llm_load_print_meta: pooling type     = -1
0.00.313.336 I llm_load_print_meta: rope type        = -1
0.00.313.336 I llm_load_print_meta: rope scaling     = linear
0.00.313.339 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.313.339 I llm_load_print_meta: freq_scale_train = 1
0.00.313.339 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.313.339 I llm_load_print_meta: rope_finetuned   = unknown
0.00.313.340 I llm_load_print_meta: ssm_d_conv       = 0
0.00.313.340 I llm_load_print_meta: ssm_d_inner      = 0
0.00.313.340 I llm_load_print_meta: ssm_d_state      = 0
0.00.313.341 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.313.341 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.313.362 I llm_load_print_meta: model type       = 33M
0.00.313.363 I llm_load_print_meta: model ftype      = F16
0.00.313.363 I llm_load_print_meta: model params     = 32.90 M
0.00.313.364 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.313.364 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.313.364 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.313.364 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.313.364 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.313.365 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.313.365 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.313.365 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.313.365 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.313.365 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.313.365 I llm_load_print_meta: max token length = 45
0.00.314.466 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.314.466 I llm_load_tensors: offloading output layer to GPU
0.00.314.467 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.314.488 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.314.489 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.315.385 I llama_new_context_with_model: n_seq_max     = 1
0.00.315.386 I llama_new_context_with_model: n_ctx         = 8192
0.00.315.386 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.315.386 I llama_new_context_with_model: n_batch       = 2048
0.00.315.386 I llama_new_context_with_model: n_ubatch      = 2048
0.00.315.386 I llama_new_context_with_model: flash_attn    = 0
0.00.315.387 I llama_new_context_with_model: freq_base     = 10000.0
0.00.315.387 I llama_new_context_with_model: freq_scale    = 1
0.00.315.387 I ggml_metal_init: allocating
0.00.315.390 I ggml_metal_init: found device: Apple M4
0.00.315.393 I ggml_metal_init: picking default device: Apple M4
0.00.316.159 I ggml_metal_init: using embedded metal library
0.00.318.518 I ggml_metal_init: GPU name:   Apple M4
0.00.318.520 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.318.520 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.318.521 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.318.521 I ggml_metal_init: simdgroup reduction   = true
0.00.318.521 I ggml_metal_init: simdgroup matrix mul. = true
0.00.318.521 I ggml_metal_init: has bfloat            = true
0.00.318.521 I ggml_metal_init: use bfloat            = true
0.00.318.522 I ggml_metal_init: hasUnifiedMemory      = true
0.00.318.523 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.328.837 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.328.840 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.328.841 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.329.393 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.329.394 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.329.394 I llama_new_context_with_model: graph nodes  = 154
0.00.329.395 I llama_new_context_with_model: graph splits = 2
0.00.329.412 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.341.788 I 
0.00.341.806 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.342.146 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.342.147 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.342.150 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.342.150 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.342.154 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.342.154 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.342.652 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.346.351 I llama_perf_context_print:        load time =     321.30 ms
0.00.346.352 I llama_perf_context_print: prompt eval time =       3.69 ms /    62 tokens (    0.06 ms per token, 16797.62 tokens per second)
0.00.346.353 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.346.354 I llama_perf_context_print:       total time =       4.56 ms /    63 tokens
0.00.346.564 I ggml_metal_free: deallocating

real	0m3.286s
user	0m0.319s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.114 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.263 I main: llama backend init
0.00.000.269 I main: load the model and apply lora adapter, if any
0.00.032.570 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.796 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.815 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.815 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.818 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.075 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.062.077 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.078 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.080 I llama_model_loader: - type  f32:  194 tensors
0.00.062.080 I llama_model_loader: - type  f16:   98 tensors
0.00.092.808 I llm_load_vocab: special tokens cache size = 25
0.00.099.817 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.820 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.820 I llm_load_print_meta: arch             = gptneox
0.00.099.820 I llm_load_print_meta: vocab type       = BPE
0.00.099.821 I llm_load_print_meta: n_vocab          = 50304
0.00.099.821 I llm_load_print_meta: n_merges         = 50009
0.00.099.821 I llm_load_print_meta: vocab_only       = 0
0.00.099.821 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.821 I llm_load_print_meta: n_embd           = 2048
0.00.099.821 I llm_load_print_meta: n_layer          = 24
0.00.099.824 I llm_load_print_meta: n_head           = 16
0.00.099.825 I llm_load_print_meta: n_head_kv        = 16
0.00.099.825 I llm_load_print_meta: n_rot            = 32
0.00.099.825 I llm_load_print_meta: n_swa            = 0
0.00.099.826 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.826 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.826 I llm_load_print_meta: n_gqa            = 1
0.00.099.827 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.828 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.828 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.829 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.829 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.829 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.829 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.830 I llm_load_print_meta: n_ff             = 8192
0.00.099.830 I llm_load_print_meta: n_expert         = 0
0.00.099.830 I llm_load_print_meta: n_expert_used    = 0
0.00.099.830 I llm_load_print_meta: causal attn      = 1
0.00.099.830 I llm_load_print_meta: pooling type     = 0
0.00.099.830 I llm_load_print_meta: rope type        = 2
0.00.099.831 I llm_load_print_meta: rope scaling     = linear
0.00.099.831 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.831 I llm_load_print_meta: freq_scale_train = 1
0.00.099.831 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.833 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.833 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.833 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.833 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.833 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.833 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.840 I llm_load_print_meta: model type       = 1.4B
0.00.099.841 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.841 I llm_load_print_meta: model params     = 1.41 B
0.00.099.841 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.842 I llm_load_print_meta: general.name     = 1.4B
0.00.099.842 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.842 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.843 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.843 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.843 I llm_load_print_meta: LF token         = 128 ''
0.00.099.843 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.843 I llm_load_print_meta: max token length = 1024
0.00.101.937 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.938 I llm_load_tensors: offloading output layer to GPU
0.00.101.938 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.950 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.951 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.924 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.925 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.925 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.925 I llama_new_context_with_model: n_batch       = 2048
0.00.102.925 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.925 I llama_new_context_with_model: flash_attn    = 0
0.00.102.926 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.926 I llama_new_context_with_model: freq_scale    = 1
0.00.102.927 I ggml_metal_init: allocating
0.00.102.935 I ggml_metal_init: found device: Apple M4
0.00.102.938 I ggml_metal_init: picking default device: Apple M4
0.00.103.604 I ggml_metal_init: using embedded metal library
0.00.136.931 I ggml_metal_init: GPU name:   Apple M4
0.00.136.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.136.933 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.136.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.136.934 I ggml_metal_init: simdgroup reduction   = true
0.00.136.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.136.934 I ggml_metal_init: has bfloat            = true
0.00.136.935 I ggml_metal_init: use bfloat            = true
0.00.136.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.136.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.175.620 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.175.626 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.175.646 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.176.610 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.176.611 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.176.612 I llama_new_context_with_model: graph nodes  = 967
0.00.176.612 I llama_new_context_with_model: graph splits = 2
0.00.176.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.270.298 I main: llama threadpool init, n_threads = 4
0.00.270.327 I 
0.00.270.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.270.344 I 
0.00.270.422 I sampler seed: 1234
0.00.270.426 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.270.452 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.270.454 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.270.454 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.130.190 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.02.130.191 I llama_perf_context_print:        load time =     237.71 ms
0.02.130.191 I llama_perf_context_print: prompt eval time =      38.33 ms /     7 tokens (    5.48 ms per token,   182.63 tokens per second)
0.02.130.192 I llama_perf_context_print:        eval time =    1818.55 ms /    63 runs   (   28.87 ms per token,    34.64 tokens per second)
0.02.130.194 I llama_perf_context_print:       total time =    1859.89 ms /    70 tokens
0.02.130.355 I ggml_metal_free: deallocating

real	0m2.441s
user	0m0.145s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.932 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.029.438 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.698 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.714 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.715 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.716 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.716 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.717 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.727 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.729 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.035 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.769 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.818 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.824 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.825 I llama_model_loader: - type  f32:  194 tensors
0.00.061.826 I llama_model_loader: - type  f16:   98 tensors
0.00.097.172 I llm_load_vocab: special tokens cache size = 25
0.00.104.703 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.104.706 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.104.707 I llm_load_print_meta: arch             = gptneox
0.00.104.707 I llm_load_print_meta: vocab type       = BPE
0.00.104.707 I llm_load_print_meta: n_vocab          = 50304
0.00.104.708 I llm_load_print_meta: n_merges         = 50009
0.00.104.708 I llm_load_print_meta: vocab_only       = 0
0.00.104.708 I llm_load_print_meta: n_ctx_train      = 2048
0.00.104.708 I llm_load_print_meta: n_embd           = 2048
0.00.104.708 I llm_load_print_meta: n_layer          = 24
0.00.104.711 I llm_load_print_meta: n_head           = 16
0.00.104.712 I llm_load_print_meta: n_head_kv        = 16
0.00.104.712 I llm_load_print_meta: n_rot            = 32
0.00.104.712 I llm_load_print_meta: n_swa            = 0
0.00.104.712 I llm_load_print_meta: n_embd_head_k    = 128
0.00.104.712 I llm_load_print_meta: n_embd_head_v    = 128
0.00.104.713 I llm_load_print_meta: n_gqa            = 1
0.00.104.714 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.104.714 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.104.716 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.104.717 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.104.717 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.104.717 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.104.717 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.104.718 I llm_load_print_meta: n_ff             = 8192
0.00.104.718 I llm_load_print_meta: n_expert         = 0
0.00.104.718 I llm_load_print_meta: n_expert_used    = 0
0.00.104.722 I llm_load_print_meta: causal attn      = 1
0.00.104.722 I llm_load_print_meta: pooling type     = 0
0.00.104.722 I llm_load_print_meta: rope type        = 2
0.00.104.722 I llm_load_print_meta: rope scaling     = linear
0.00.104.722 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.104.723 I llm_load_print_meta: freq_scale_train = 1
0.00.104.723 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.104.723 I llm_load_print_meta: rope_finetuned   = unknown
0.00.104.723 I llm_load_print_meta: ssm_d_conv       = 0
0.00.104.723 I llm_load_print_meta: ssm_d_inner      = 0
0.00.104.723 I llm_load_print_meta: ssm_d_state      = 0
0.00.104.724 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.104.727 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.104.739 I llm_load_print_meta: model type       = 1.4B
0.00.104.740 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.104.740 I llm_load_print_meta: model params     = 1.41 B
0.00.104.741 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.104.741 I llm_load_print_meta: general.name     = 1.4B
0.00.104.741 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.104.741 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.104.741 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.104.741 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.104.742 I llm_load_print_meta: LF token         = 128 ''
0.00.104.742 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.104.742 I llm_load_print_meta: max token length = 1024
0.00.107.494 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.107.494 I llm_load_tensors: offloading output layer to GPU
0.00.107.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.107.504 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.107.505 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.108.584 I llama_new_context_with_model: n_seq_max     = 1
0.00.108.585 I llama_new_context_with_model: n_ctx         = 128
0.00.108.585 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.108.586 I llama_new_context_with_model: n_batch       = 128
0.00.108.586 I llama_new_context_with_model: n_ubatch      = 128
0.00.108.586 I llama_new_context_with_model: flash_attn    = 0
0.00.108.587 I llama_new_context_with_model: freq_base     = 10000.0
0.00.108.587 I llama_new_context_with_model: freq_scale    = 1
0.00.108.587 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.108.588 I ggml_metal_init: allocating
0.00.108.596 I ggml_metal_init: found device: Apple M4
0.00.108.598 I ggml_metal_init: picking default device: Apple M4
0.00.109.291 I ggml_metal_init: using embedded metal library
0.00.111.613 I ggml_metal_init: GPU name:   Apple M4
0.00.111.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.111.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.111.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.111.618 I ggml_metal_init: simdgroup reduction   = true
0.00.111.618 I ggml_metal_init: simdgroup matrix mul. = true
0.00.111.618 I ggml_metal_init: has bfloat            = true
0.00.111.618 I ggml_metal_init: use bfloat            = true
0.00.111.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.111.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.123.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.123.036 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.123.050 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.028 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.124.029 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.124.030 I llama_new_context_with_model: graph nodes  = 967
0.00.124.030 I llama_new_context_with_model: graph splits = 2
0.00.124.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.527.367 I 
0.01.527.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.527.482 I perplexity: tokenizing the input ..
0.01.539.440 I perplexity: tokenization took 11.955 ms
0.01.539.444 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.659.644 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.661.278 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.661.298 I llama_perf_context_print:        load time =    1497.92 ms
0.01.661.299 I llama_perf_context_print: prompt eval time =     119.94 ms /   128 tokens (    0.94 ms per token,  1067.20 tokens per second)
0.01.661.301 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.661.301 I llama_perf_context_print:       total time =     133.94 ms /   129 tokens
0.01.661.741 I ggml_metal_free: deallocating

real	0m1.856s
user	0m0.125s
sys	0m0.229s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.443 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.107 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.111 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.115 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.115 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.116 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.117 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.118 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.120 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.123 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.123 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.771 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.603 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.604 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.605 I llama_model_loader: - type  f32:  194 tensors
0.00.026.605 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.770 I llm_load_vocab: special tokens cache size = 25
0.00.053.815 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.819 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.820 I llm_load_print_meta: arch             = gptneox
0.00.053.820 I llm_load_print_meta: vocab type       = BPE
0.00.053.820 I llm_load_print_meta: n_vocab          = 50304
0.00.053.821 I llm_load_print_meta: n_merges         = 50009
0.00.053.823 I llm_load_print_meta: vocab_only       = 0
0.00.053.823 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.823 I llm_load_print_meta: n_embd           = 2048
0.00.053.824 I llm_load_print_meta: n_layer          = 24
0.00.053.827 I llm_load_print_meta: n_head           = 16
0.00.053.828 I llm_load_print_meta: n_head_kv        = 16
0.00.053.828 I llm_load_print_meta: n_rot            = 32
0.00.053.828 I llm_load_print_meta: n_swa            = 0
0.00.053.829 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.829 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.829 I llm_load_print_meta: n_gqa            = 1
0.00.053.830 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.833 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.833 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.834 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.834 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.834 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.835 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.835 I llm_load_print_meta: n_ff             = 8192
0.00.053.836 I llm_load_print_meta: n_expert         = 0
0.00.053.836 I llm_load_print_meta: n_expert_used    = 0
0.00.053.836 I llm_load_print_meta: causal attn      = 1
0.00.053.836 I llm_load_print_meta: pooling type     = 0
0.00.053.836 I llm_load_print_meta: rope type        = 2
0.00.053.837 I llm_load_print_meta: rope scaling     = linear
0.00.053.837 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.837 I llm_load_print_meta: freq_scale_train = 1
0.00.053.837 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.838 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.838 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.838 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.838 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.838 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.838 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.854 I llm_load_print_meta: model type       = 1.4B
0.00.053.855 I llm_load_print_meta: model ftype      = Q8_0
0.00.053.855 I llm_load_print_meta: model params     = 1.41 B
0.00.053.855 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.053.855 I llm_load_print_meta: general.name     = 1.4B
0.00.053.856 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.856 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.856 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.856 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.857 I llm_load_print_meta: LF token         = 128 ''
0.00.053.857 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.857 I llm_load_print_meta: max token length = 1024
0.00.055.593 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.593 I llm_load_tensors: offloading output layer to GPU
0.00.055.593 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.603 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.604 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.056.503 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.504 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.504 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.505 I llama_new_context_with_model: n_batch       = 2048
0.00.056.505 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.505 I llama_new_context_with_model: flash_attn    = 0
0.00.056.506 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.506 I llama_new_context_with_model: freq_scale    = 1
0.00.056.506 I ggml_metal_init: allocating
0.00.056.510 I ggml_metal_init: found device: Apple M4
0.00.056.513 I ggml_metal_init: picking default device: Apple M4
0.00.057.205 I ggml_metal_init: using embedded metal library
0.00.059.270 I ggml_metal_init: GPU name:   Apple M4
0.00.059.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.273 I ggml_metal_init: simdgroup reduction   = true
0.00.059.273 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.273 I ggml_metal_init: has bfloat            = true
0.00.059.274 I ggml_metal_init: use bfloat            = true
0.00.059.274 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.599 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.612 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.638 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.735 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.736 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.737 I llama_new_context_with_model: graph nodes  = 967
0.00.092.737 I llama_new_context_with_model: graph splits = 2
0.00.092.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.474.548 I main: llama threadpool init, n_threads = 4
0.01.474.580 I 
0.01.474.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.474.602 I 
0.01.474.753 I sampler seed: 1234
0.01.474.757 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.474.776 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.474.776 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.474.776 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.559.806 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63111.11 tokens per second)
0.02.559.807 I llama_perf_context_print:        load time =    1465.10 ms
0.02.559.808 I llama_perf_context_print: prompt eval time =      33.44 ms /     7 tokens (    4.78 ms per token,   209.32 tokens per second)
0.02.559.808 I llama_perf_context_print:        eval time =    1048.71 ms /    63 runs   (   16.65 ms per token,    60.07 tokens per second)
0.02.559.809 I llama_perf_context_print:       total time =    1085.26 ms /    70 tokens
0.02.559.987 I ggml_metal_free: deallocating

real	0m2.575s
user	0m0.110s
sys	0m0.247s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.886 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.068 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.074 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.075 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.075 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.075 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.076 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.077 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.077 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.078 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.079 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.081 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.762 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.946 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.949 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.949 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.949 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.950 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.950 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.951 I llama_model_loader: - type  f32:  194 tensors
0.00.028.951 I llama_model_loader: - type q8_0:   98 tensors
0.00.052.006 I llm_load_vocab: special tokens cache size = 25
0.00.057.865 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.867 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.868 I llm_load_print_meta: arch             = gptneox
0.00.057.868 I llm_load_print_meta: vocab type       = BPE
0.00.057.868 I llm_load_print_meta: n_vocab          = 50304
0.00.057.868 I llm_load_print_meta: n_merges         = 50009
0.00.057.869 I llm_load_print_meta: vocab_only       = 0
0.00.057.869 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.869 I llm_load_print_meta: n_embd           = 2048
0.00.057.869 I llm_load_print_meta: n_layer          = 24
0.00.057.873 I llm_load_print_meta: n_head           = 16
0.00.057.874 I llm_load_print_meta: n_head_kv        = 16
0.00.057.874 I llm_load_print_meta: n_rot            = 32
0.00.057.874 I llm_load_print_meta: n_swa            = 0
0.00.057.874 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.874 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.878 I llm_load_print_meta: n_gqa            = 1
0.00.057.878 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.879 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.879 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.880 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.880 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.882 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.882 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.882 I llm_load_print_meta: n_ff             = 8192
0.00.057.883 I llm_load_print_meta: n_expert         = 0
0.00.057.883 I llm_load_print_meta: n_expert_used    = 0
0.00.057.883 I llm_load_print_meta: causal attn      = 1
0.00.057.883 I llm_load_print_meta: pooling type     = 0
0.00.057.883 I llm_load_print_meta: rope type        = 2
0.00.057.883 I llm_load_print_meta: rope scaling     = linear
0.00.057.884 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.884 I llm_load_print_meta: freq_scale_train = 1
0.00.057.884 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.885 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.885 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.885 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.885 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.885 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.885 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.897 I llm_load_print_meta: model type       = 1.4B
0.00.057.897 I llm_load_print_meta: model ftype      = Q8_0
0.00.057.897 I llm_load_print_meta: model params     = 1.41 B
0.00.057.898 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.057.898 I llm_load_print_meta: general.name     = 1.4B
0.00.057.898 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.898 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.899 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.899 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.899 I llm_load_print_meta: LF token         = 128 ''
0.00.057.899 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.899 I llm_load_print_meta: max token length = 1024
0.00.059.578 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.578 I llm_load_tensors: offloading output layer to GPU
0.00.059.578 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.588 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.589 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.060.496 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.496 I llama_new_context_with_model: n_ctx         = 128
0.00.060.496 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.060.497 I llama_new_context_with_model: n_batch       = 128
0.00.060.497 I llama_new_context_with_model: n_ubatch      = 128
0.00.060.497 I llama_new_context_with_model: flash_attn    = 0
0.00.060.497 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.498 I llama_new_context_with_model: freq_scale    = 1
0.00.060.498 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.060.498 I ggml_metal_init: allocating
0.00.060.502 I ggml_metal_init: found device: Apple M4
0.00.060.504 I ggml_metal_init: picking default device: Apple M4
0.00.061.070 I ggml_metal_init: using embedded metal library
0.00.063.052 I ggml_metal_init: GPU name:   Apple M4
0.00.063.053 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.054 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.054 I ggml_metal_init: simdgroup reduction   = true
0.00.063.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.055 I ggml_metal_init: has bfloat            = true
0.00.063.055 I ggml_metal_init: use bfloat            = true
0.00.063.055 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.056 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.642 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.644 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.662 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.568 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.569 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.570 I llama_new_context_with_model: graph nodes  = 967
0.00.073.570 I llama_new_context_with_model: graph splits = 2
0.00.073.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.015.244 I 
0.01.015.292 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.015.319 I perplexity: tokenizing the input ..
0.01.023.050 I perplexity: tokenization took 7.73 ms
0.01.023.056 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.144.947 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.146.252 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.146.269 I llama_perf_context_print:        load time =    1004.35 ms
0.01.146.270 I llama_perf_context_print: prompt eval time =     121.65 ms /   128 tokens (    0.95 ms per token,  1052.16 tokens per second)
0.01.146.271 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.146.271 I llama_perf_context_print:       total time =     131.03 ms /   129 tokens
0.01.146.646 I ggml_metal_free: deallocating

real	0m1.160s
user	0m0.083s
sys	0m0.170s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.603 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.178 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.179 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.179 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.179 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.180 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.181 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.181 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.182 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.182 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.184 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.185 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.865 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.865 I llama_model_loader: - type  f32:  194 tensors
0.00.032.865 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.961 I llm_load_vocab: special tokens cache size = 25
0.00.059.891 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.894 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.894 I llm_load_print_meta: arch             = gptneox
0.00.059.895 I llm_load_print_meta: vocab type       = BPE
0.00.059.895 I llm_load_print_meta: n_vocab          = 50304
0.00.059.895 I llm_load_print_meta: n_merges         = 50009
0.00.059.896 I llm_load_print_meta: vocab_only       = 0
0.00.059.896 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.896 I llm_load_print_meta: n_embd           = 2048
0.00.059.896 I llm_load_print_meta: n_layer          = 24
0.00.059.900 I llm_load_print_meta: n_head           = 16
0.00.059.901 I llm_load_print_meta: n_head_kv        = 16
0.00.059.902 I llm_load_print_meta: n_rot            = 32
0.00.059.902 I llm_load_print_meta: n_swa            = 0
0.00.059.902 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.902 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.903 I llm_load_print_meta: n_gqa            = 1
0.00.059.903 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.904 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.905 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.905 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.906 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.906 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.906 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.907 I llm_load_print_meta: n_ff             = 8192
0.00.059.907 I llm_load_print_meta: n_expert         = 0
0.00.059.907 I llm_load_print_meta: n_expert_used    = 0
0.00.059.907 I llm_load_print_meta: causal attn      = 1
0.00.059.907 I llm_load_print_meta: pooling type     = 0
0.00.059.907 I llm_load_print_meta: rope type        = 2
0.00.059.908 I llm_load_print_meta: rope scaling     = linear
0.00.059.908 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.908 I llm_load_print_meta: freq_scale_train = 1
0.00.059.909 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.909 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.909 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.910 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.910 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.910 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.910 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.925 I llm_load_print_meta: model type       = 1.4B
0.00.059.925 I llm_load_print_meta: model ftype      = Q4_0
0.00.059.925 I llm_load_print_meta: model params     = 1.41 B
0.00.059.926 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.059.926 I llm_load_print_meta: general.name     = 1.4B
0.00.059.926 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.926 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.927 I llm_load_print_meta: LF token         = 128 ''
0.00.059.927 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.927 I llm_load_print_meta: max token length = 1024
0.00.061.673 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.673 I llm_load_tensors: offloading output layer to GPU
0.00.061.673 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.683 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.061.684 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.062.599 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.601 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.601 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.601 I llama_new_context_with_model: n_batch       = 2048
0.00.062.601 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.601 I llama_new_context_with_model: flash_attn    = 0
0.00.062.602 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.602 I llama_new_context_with_model: freq_scale    = 1
0.00.062.603 I ggml_metal_init: allocating
0.00.062.609 I ggml_metal_init: found device: Apple M4
0.00.062.612 I ggml_metal_init: picking default device: Apple M4
0.00.063.286 I ggml_metal_init: using embedded metal library
0.00.065.438 I ggml_metal_init: GPU name:   Apple M4
0.00.065.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.440 I ggml_metal_init: simdgroup reduction   = true
0.00.065.440 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.440 I ggml_metal_init: has bfloat            = true
0.00.065.440 I ggml_metal_init: use bfloat            = true
0.00.065.441 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.505 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.511 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.534 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.686 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.688 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.688 I llama_new_context_with_model: graph nodes  = 967
0.00.098.688 I llama_new_context_with_model: graph splits = 2
0.00.098.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.624 I main: llama threadpool init, n_threads = 4
0.00.774.653 I 
0.00.774.674 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.774.677 I 
0.00.774.833 I sampler seed: 1234
0.00.774.837 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.846 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.848 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.848 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.447.798 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62171.63 tokens per second)
0.01.447.798 I llama_perf_context_print:        load time =     764.02 ms
0.01.447.799 I llama_perf_context_print: prompt eval time =      32.60 ms /     7 tokens (    4.66 ms per token,   214.74 tokens per second)
0.01.447.800 I llama_perf_context_print:        eval time =     637.48 ms /    63 runs   (   10.12 ms per token,    98.83 tokens per second)
0.01.447.800 I llama_perf_context_print:       total time =     673.18 ms /    70 tokens
0.01.447.968 I ggml_metal_free: deallocating

real	0m1.465s
user	0m0.109s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.424 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.206 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.212 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.213 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.213 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.214 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.214 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.215 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.215 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.215 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.216 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.827 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.828 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.829 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.829 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.829 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.830 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.831 I llama_model_loader: - type  f32:  194 tensors
0.00.024.831 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.831 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.988 I llm_load_vocab: special tokens cache size = 25
0.00.050.997 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.000 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.000 I llm_load_print_meta: arch             = gptneox
0.00.051.001 I llm_load_print_meta: vocab type       = BPE
0.00.051.001 I llm_load_print_meta: n_vocab          = 50304
0.00.051.001 I llm_load_print_meta: n_merges         = 50009
0.00.051.001 I llm_load_print_meta: vocab_only       = 0
0.00.051.002 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.002 I llm_load_print_meta: n_embd           = 2048
0.00.051.002 I llm_load_print_meta: n_layer          = 24
0.00.051.005 I llm_load_print_meta: n_head           = 16
0.00.051.005 I llm_load_print_meta: n_head_kv        = 16
0.00.051.006 I llm_load_print_meta: n_rot            = 32
0.00.051.006 I llm_load_print_meta: n_swa            = 0
0.00.051.006 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.006 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.007 I llm_load_print_meta: n_gqa            = 1
0.00.051.008 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.010 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.011 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.011 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.011 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.012 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.012 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.012 I llm_load_print_meta: n_ff             = 8192
0.00.051.012 I llm_load_print_meta: n_expert         = 0
0.00.051.013 I llm_load_print_meta: n_expert_used    = 0
0.00.051.014 I llm_load_print_meta: causal attn      = 1
0.00.051.015 I llm_load_print_meta: pooling type     = 0
0.00.051.015 I llm_load_print_meta: rope type        = 2
0.00.051.015 I llm_load_print_meta: rope scaling     = linear
0.00.051.016 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.016 I llm_load_print_meta: freq_scale_train = 1
0.00.051.016 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.016 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.017 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.017 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.017 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.017 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.029 I llm_load_print_meta: model type       = 1.4B
0.00.051.029 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.029 I llm_load_print_meta: model params     = 1.41 B
0.00.051.030 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.030 I llm_load_print_meta: general.name     = 1.4B
0.00.051.030 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.030 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.031 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.031 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.031 I llm_load_print_meta: LF token         = 128 ''
0.00.051.031 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.031 I llm_load_print_meta: max token length = 1024
0.00.052.963 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.963 I llm_load_tensors: offloading output layer to GPU
0.00.052.963 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.973 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.974 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.924 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.925 I llama_new_context_with_model: n_ctx         = 128
0.00.053.925 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.925 I llama_new_context_with_model: n_batch       = 128
0.00.053.925 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.925 I llama_new_context_with_model: flash_attn    = 0
0.00.053.926 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.926 I llama_new_context_with_model: freq_scale    = 1
0.00.053.926 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.927 I ggml_metal_init: allocating
0.00.053.933 I ggml_metal_init: found device: Apple M4
0.00.053.935 I ggml_metal_init: picking default device: Apple M4
0.00.054.472 I ggml_metal_init: using embedded metal library
0.00.056.443 I ggml_metal_init: GPU name:   Apple M4
0.00.056.444 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.445 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.445 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.445 I ggml_metal_init: simdgroup reduction   = true
0.00.056.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.446 I ggml_metal_init: has bfloat            = true
0.00.056.446 I ggml_metal_init: use bfloat            = true
0.00.056.446 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.447 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.002 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.005 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.019 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.954 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.955 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.956 I llama_new_context_with_model: graph nodes  = 967
0.00.066.956 I llama_new_context_with_model: graph splits = 2
0.00.066.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.736 I 
0.00.670.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.670.798 I perplexity: tokenizing the input ..
0.00.678.640 I perplexity: tokenization took 7.839 ms
0.00.678.646 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.381 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.802.639 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.802.659 I llama_perf_context_print:        load time =     660.30 ms
0.00.802.660 I llama_perf_context_print: prompt eval time =     122.51 ms /   128 tokens (    0.96 ms per token,  1044.82 tokens per second)
0.00.802.661 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.661 I llama_perf_context_print:       total time =     131.93 ms /   129 tokens
0.00.803.078 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.075s
sys	0m0.119s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.366 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.328 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.334 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.335 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.335 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.338 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.338 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.343 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.343 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.297 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.118 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.121 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.121 I llama_model_loader: - type  f32:  194 tensors
0.00.025.122 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.122 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.244 I llm_load_vocab: special tokens cache size = 25
0.00.052.206 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.209 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.209 I llm_load_print_meta: arch             = gptneox
0.00.052.209 I llm_load_print_meta: vocab type       = BPE
0.00.052.210 I llm_load_print_meta: n_vocab          = 50304
0.00.052.210 I llm_load_print_meta: n_merges         = 50009
0.00.052.210 I llm_load_print_meta: vocab_only       = 0
0.00.052.210 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.210 I llm_load_print_meta: n_embd           = 2048
0.00.052.210 I llm_load_print_meta: n_layer          = 24
0.00.052.214 I llm_load_print_meta: n_head           = 16
0.00.052.215 I llm_load_print_meta: n_head_kv        = 16
0.00.052.215 I llm_load_print_meta: n_rot            = 32
0.00.052.215 I llm_load_print_meta: n_swa            = 0
0.00.052.215 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.215 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.216 I llm_load_print_meta: n_gqa            = 1
0.00.052.217 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.218 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.218 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.220 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.220 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.220 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.220 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.221 I llm_load_print_meta: n_ff             = 8192
0.00.052.221 I llm_load_print_meta: n_expert         = 0
0.00.052.221 I llm_load_print_meta: n_expert_used    = 0
0.00.052.224 I llm_load_print_meta: causal attn      = 1
0.00.052.224 I llm_load_print_meta: pooling type     = 0
0.00.052.224 I llm_load_print_meta: rope type        = 2
0.00.052.225 I llm_load_print_meta: rope scaling     = linear
0.00.052.225 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.225 I llm_load_print_meta: freq_scale_train = 1
0.00.052.225 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.226 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.226 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.226 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.226 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.226 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.226 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.238 I llm_load_print_meta: model type       = 1.4B
0.00.052.241 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.241 I llm_load_print_meta: model params     = 1.41 B
0.00.052.242 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.242 I llm_load_print_meta: general.name     = 1.4B
0.00.052.242 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.242 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.242 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.242 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.243 I llm_load_print_meta: LF token         = 128 ''
0.00.052.243 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.244 I llm_load_print_meta: max token length = 1024
0.00.054.321 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.321 I llm_load_tensors: offloading output layer to GPU
0.00.054.321 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.331 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.332 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.292 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.293 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.293 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.293 I llama_new_context_with_model: n_batch       = 2048
0.00.055.293 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.293 I llama_new_context_with_model: flash_attn    = 0
0.00.055.294 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.294 I llama_new_context_with_model: freq_scale    = 1
0.00.055.294 I ggml_metal_init: allocating
0.00.055.301 I ggml_metal_init: found device: Apple M4
0.00.055.303 I ggml_metal_init: picking default device: Apple M4
0.00.055.857 I ggml_metal_init: using embedded metal library
0.00.057.807 I ggml_metal_init: GPU name:   Apple M4
0.00.057.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.809 I ggml_metal_init: simdgroup reduction   = true
0.00.057.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.810 I ggml_metal_init: has bfloat            = true
0.00.057.810 I ggml_metal_init: use bfloat            = true
0.00.057.810 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.811 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.208 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.213 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.231 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.151 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.152 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.153 I llama_new_context_with_model: graph nodes  = 967
0.00.086.153 I llama_new_context_with_model: graph splits = 2
0.00.086.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.664 I main: llama threadpool init, n_threads = 4
0.00.820.703 I 
0.00.820.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.820.725 I 
0.00.820.954 I sampler seed: 1234
0.00.820.960 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.820.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.820.996 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.820.997 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.539.219 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63563.12 tokens per second)
0.01.539.220 I llama_perf_context_print:        load time =     811.29 ms
0.01.539.221 I llama_perf_context_print: prompt eval time =      32.90 ms /     7 tokens (    4.70 ms per token,   212.77 tokens per second)
0.01.539.223 I llama_perf_context_print:        eval time =     682.47 ms /    63 runs   (   10.83 ms per token,    92.31 tokens per second)
0.01.539.223 I llama_perf_context_print:       total time =     718.56 ms /    70 tokens
0.01.539.404 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.108s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.980 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.842 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.846 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.849 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.849 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.849 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.850 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.850 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.851 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.853 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.854 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.854 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.854 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.855 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.856 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.856 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.857 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.613 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.701 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.661 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.662 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.663 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.663 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.663 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.664 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.664 I llama_model_loader: - type  f32:  194 tensors
0.00.023.664 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.665 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.909 I llm_load_vocab: special tokens cache size = 25
0.00.049.862 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.865 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.865 I llm_load_print_meta: arch             = gptneox
0.00.049.866 I llm_load_print_meta: vocab type       = BPE
0.00.049.866 I llm_load_print_meta: n_vocab          = 50304
0.00.049.866 I llm_load_print_meta: n_merges         = 50009
0.00.049.866 I llm_load_print_meta: vocab_only       = 0
0.00.049.867 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.867 I llm_load_print_meta: n_embd           = 2048
0.00.049.867 I llm_load_print_meta: n_layer          = 24
0.00.049.870 I llm_load_print_meta: n_head           = 16
0.00.049.871 I llm_load_print_meta: n_head_kv        = 16
0.00.049.872 I llm_load_print_meta: n_rot            = 32
0.00.049.872 I llm_load_print_meta: n_swa            = 0
0.00.049.872 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.872 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.873 I llm_load_print_meta: n_gqa            = 1
0.00.049.874 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.874 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.875 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.875 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.877 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.878 I llm_load_print_meta: n_ff             = 8192
0.00.049.878 I llm_load_print_meta: n_expert         = 0
0.00.049.878 I llm_load_print_meta: n_expert_used    = 0
0.00.049.878 I llm_load_print_meta: causal attn      = 1
0.00.049.878 I llm_load_print_meta: pooling type     = 0
0.00.049.878 I llm_load_print_meta: rope type        = 2
0.00.049.879 I llm_load_print_meta: rope scaling     = linear
0.00.049.879 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.879 I llm_load_print_meta: freq_scale_train = 1
0.00.049.879 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.880 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.880 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.880 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.880 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.882 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.882 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.894 I llm_load_print_meta: model type       = 1.4B
0.00.049.894 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.894 I llm_load_print_meta: model params     = 1.41 B
0.00.049.895 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.895 I llm_load_print_meta: general.name     = 1.4B
0.00.049.895 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.896 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.896 I llm_load_print_meta: LF token         = 128 ''
0.00.049.896 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.896 I llm_load_print_meta: max token length = 1024
0.00.051.894 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.894 I llm_load_tensors: offloading output layer to GPU
0.00.051.895 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.904 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.906 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.817 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.818 I llama_new_context_with_model: n_ctx         = 128
0.00.052.818 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.818 I llama_new_context_with_model: n_batch       = 128
0.00.052.818 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.818 I llama_new_context_with_model: flash_attn    = 0
0.00.052.819 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.819 I llama_new_context_with_model: freq_scale    = 1
0.00.052.819 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.820 I ggml_metal_init: allocating
0.00.052.826 I ggml_metal_init: found device: Apple M4
0.00.052.828 I ggml_metal_init: picking default device: Apple M4
0.00.053.375 I ggml_metal_init: using embedded metal library
0.00.055.290 I ggml_metal_init: GPU name:   Apple M4
0.00.055.292 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.292 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.293 I ggml_metal_init: simdgroup reduction   = true
0.00.055.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.293 I ggml_metal_init: has bfloat            = true
0.00.055.293 I ggml_metal_init: use bfloat            = true
0.00.055.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.294 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.369 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.374 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.393 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.302 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.303 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.303 I llama_new_context_with_model: graph nodes  = 967
0.00.065.303 I llama_new_context_with_model: graph splits = 2
0.00.065.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.074 I 
0.00.730.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.730.171 I perplexity: tokenizing the input ..
0.00.737.619 I perplexity: tokenization took 7.447 ms
0.00.737.622 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.860.711 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.861.909 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.861.921 I llama_perf_context_print:        load time =     721.09 ms
0.00.861.923 I llama_perf_context_print: prompt eval time =     122.85 ms /   128 tokens (    0.96 ms per token,  1041.92 tokens per second)
0.00.861.923 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.924 I llama_perf_context_print:       total time =     131.85 ms /   129 tokens
0.00.862.237 I ggml_metal_free: deallocating

real	0m0.875s
user	0m0.075s
sys	0m0.134s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.501 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.148 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.153 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.155 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.038 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.890 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.891 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.891 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.892 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.892 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.893 I llama_model_loader: - type  f32:  194 tensors
0.00.023.893 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.893 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.941 I llm_load_vocab: special tokens cache size = 25
0.00.050.980 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.983 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.984 I llm_load_print_meta: arch             = gptneox
0.00.050.984 I llm_load_print_meta: vocab type       = BPE
0.00.050.984 I llm_load_print_meta: n_vocab          = 50304
0.00.050.984 I llm_load_print_meta: n_merges         = 50009
0.00.050.985 I llm_load_print_meta: vocab_only       = 0
0.00.050.985 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.985 I llm_load_print_meta: n_embd           = 2048
0.00.050.985 I llm_load_print_meta: n_layer          = 24
0.00.050.987 I llm_load_print_meta: n_head           = 16
0.00.050.988 I llm_load_print_meta: n_head_kv        = 16
0.00.050.988 I llm_load_print_meta: n_rot            = 32
0.00.050.989 I llm_load_print_meta: n_swa            = 0
0.00.050.989 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.989 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.990 I llm_load_print_meta: n_gqa            = 1
0.00.050.990 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.991 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.992 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.992 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.992 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.992 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.992 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.993 I llm_load_print_meta: n_ff             = 8192
0.00.050.993 I llm_load_print_meta: n_expert         = 0
0.00.050.993 I llm_load_print_meta: n_expert_used    = 0
0.00.050.995 I llm_load_print_meta: causal attn      = 1
0.00.050.996 I llm_load_print_meta: pooling type     = 0
0.00.050.996 I llm_load_print_meta: rope type        = 2
0.00.050.997 I llm_load_print_meta: rope scaling     = linear
0.00.050.997 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.997 I llm_load_print_meta: freq_scale_train = 1
0.00.050.998 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.998 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.998 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.998 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.998 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.998 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.999 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.010 I llm_load_print_meta: model type       = 1.4B
0.00.051.011 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.011 I llm_load_print_meta: model params     = 1.41 B
0.00.051.012 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.012 I llm_load_print_meta: general.name     = 1.4B
0.00.051.012 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.012 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.012 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.013 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.013 I llm_load_print_meta: LF token         = 128 ''
0.00.051.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.013 I llm_load_print_meta: max token length = 1024
0.00.053.071 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.071 I llm_load_tensors: offloading output layer to GPU
0.00.053.071 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.081 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.082 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.998 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.999 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.999 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.999 I llama_new_context_with_model: n_batch       = 2048
0.00.053.999 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.999 I llama_new_context_with_model: flash_attn    = 0
0.00.054.000 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.000 I llama_new_context_with_model: freq_scale    = 1
0.00.054.000 I ggml_metal_init: allocating
0.00.054.006 I ggml_metal_init: found device: Apple M4
0.00.054.009 I ggml_metal_init: picking default device: Apple M4
0.00.054.550 I ggml_metal_init: using embedded metal library
0.00.056.501 I ggml_metal_init: GPU name:   Apple M4
0.00.056.502 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.503 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.503 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.503 I ggml_metal_init: simdgroup reduction   = true
0.00.056.504 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.504 I ggml_metal_init: has bfloat            = true
0.00.056.504 I ggml_metal_init: use bfloat            = true
0.00.056.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.440 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.447 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.467 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.404 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.405 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.405 I llama_new_context_with_model: graph nodes  = 967
0.00.084.405 I llama_new_context_with_model: graph splits = 2
0.00.084.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.601 I main: llama threadpool init, n_threads = 4
0.00.812.644 I 
0.00.812.670 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.812.670 I 
0.00.812.909 I sampler seed: 1234
0.00.812.912 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.812.923 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.812.924 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.812.924 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.595.930 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.595.931 I llama_perf_context_print:        load time =     804.10 ms
0.01.595.931 I llama_perf_context_print: prompt eval time =      36.64 ms /     7 tokens (    5.23 ms per token,   191.06 tokens per second)
0.01.595.932 I llama_perf_context_print:        eval time =     743.49 ms /    63 runs   (   11.80 ms per token,    84.74 tokens per second)
0.01.595.935 I llama_perf_context_print:       total time =     783.33 ms /    70 tokens
0.01.596.105 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.108s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.051 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.597 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.036 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.038 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.038 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.038 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.039 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.039 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.039 I llama_model_loader: - type  f32:  194 tensors
0.00.024.040 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.040 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.139 I llm_load_vocab: special tokens cache size = 25
0.00.051.313 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.316 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.317 I llm_load_print_meta: arch             = gptneox
0.00.051.317 I llm_load_print_meta: vocab type       = BPE
0.00.051.317 I llm_load_print_meta: n_vocab          = 50304
0.00.051.317 I llm_load_print_meta: n_merges         = 50009
0.00.051.317 I llm_load_print_meta: vocab_only       = 0
0.00.051.318 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.318 I llm_load_print_meta: n_embd           = 2048
0.00.051.318 I llm_load_print_meta: n_layer          = 24
0.00.051.321 I llm_load_print_meta: n_head           = 16
0.00.051.322 I llm_load_print_meta: n_head_kv        = 16
0.00.051.323 I llm_load_print_meta: n_rot            = 32
0.00.051.323 I llm_load_print_meta: n_swa            = 0
0.00.051.323 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.323 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.324 I llm_load_print_meta: n_gqa            = 1
0.00.051.325 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.325 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.326 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.332 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.333 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.333 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.333 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.340 I llm_load_print_meta: n_ff             = 8192
0.00.051.340 I llm_load_print_meta: n_expert         = 0
0.00.051.340 I llm_load_print_meta: n_expert_used    = 0
0.00.051.340 I llm_load_print_meta: causal attn      = 1
0.00.051.341 I llm_load_print_meta: pooling type     = 0
0.00.051.341 I llm_load_print_meta: rope type        = 2
0.00.051.342 I llm_load_print_meta: rope scaling     = linear
0.00.051.342 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.342 I llm_load_print_meta: freq_scale_train = 1
0.00.051.343 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.343 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.343 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.343 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.343 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.343 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.343 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.355 I llm_load_print_meta: model type       = 1.4B
0.00.051.355 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.356 I llm_load_print_meta: model params     = 1.41 B
0.00.051.356 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.356 I llm_load_print_meta: general.name     = 1.4B
0.00.051.357 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.358 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.358 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.358 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.358 I llm_load_print_meta: LF token         = 128 ''
0.00.051.359 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.359 I llm_load_print_meta: max token length = 1024
0.00.053.316 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.316 I llm_load_tensors: offloading output layer to GPU
0.00.053.316 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.326 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.327 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.253 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.254 I llama_new_context_with_model: n_ctx         = 128
0.00.054.254 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.255 I llama_new_context_with_model: n_batch       = 128
0.00.054.255 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.255 I llama_new_context_with_model: flash_attn    = 0
0.00.054.255 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.255 I llama_new_context_with_model: freq_scale    = 1
0.00.054.256 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.256 I ggml_metal_init: allocating
0.00.054.259 I ggml_metal_init: found device: Apple M4
0.00.054.261 I ggml_metal_init: picking default device: Apple M4
0.00.054.768 I ggml_metal_init: using embedded metal library
0.00.056.671 I ggml_metal_init: GPU name:   Apple M4
0.00.056.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.673 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.673 I ggml_metal_init: simdgroup reduction   = true
0.00.056.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.673 I ggml_metal_init: has bfloat            = true
0.00.056.674 I ggml_metal_init: use bfloat            = true
0.00.056.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.676 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.782 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.786 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.802 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.694 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.695 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.696 I llama_new_context_with_model: graph nodes  = 967
0.00.066.696 I llama_new_context_with_model: graph splits = 2
0.00.066.708 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.122 I 
0.00.767.173 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.767.203 I perplexity: tokenizing the input ..
0.00.774.281 I perplexity: tokenization took 7.077 ms
0.00.774.284 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.909.540 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.910.961 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.910.984 I llama_perf_context_print:        load time =     757.06 ms
0.00.910.986 I llama_perf_context_print: prompt eval time =     135.02 ms /   128 tokens (    1.05 ms per token,   948.04 tokens per second)
0.00.910.987 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.910.987 I llama_perf_context_print:       total time =     143.87 ms /   129 tokens
0.00.911.516 I ggml_metal_free: deallocating

real	0m0.925s
user	0m0.075s
sys	0m0.127s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.857 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.410 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.414 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.416 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.421 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.187 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.236 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.952 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.953 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.953 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.954 I llama_model_loader: - type  f32:  194 tensors
0.00.024.954 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.955 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.004 I llm_load_vocab: special tokens cache size = 25
0.00.051.004 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.007 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.007 I llm_load_print_meta: arch             = gptneox
0.00.051.008 I llm_load_print_meta: vocab type       = BPE
0.00.051.008 I llm_load_print_meta: n_vocab          = 50304
0.00.051.008 I llm_load_print_meta: n_merges         = 50009
0.00.051.008 I llm_load_print_meta: vocab_only       = 0
0.00.051.009 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.009 I llm_load_print_meta: n_embd           = 2048
0.00.051.009 I llm_load_print_meta: n_layer          = 24
0.00.051.012 I llm_load_print_meta: n_head           = 16
0.00.051.013 I llm_load_print_meta: n_head_kv        = 16
0.00.051.013 I llm_load_print_meta: n_rot            = 32
0.00.051.013 I llm_load_print_meta: n_swa            = 0
0.00.051.013 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.013 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.014 I llm_load_print_meta: n_gqa            = 1
0.00.051.015 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.016 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.016 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.016 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.017 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.017 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.017 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.018 I llm_load_print_meta: n_ff             = 8192
0.00.051.018 I llm_load_print_meta: n_expert         = 0
0.00.051.018 I llm_load_print_meta: n_expert_used    = 0
0.00.051.018 I llm_load_print_meta: causal attn      = 1
0.00.051.018 I llm_load_print_meta: pooling type     = 0
0.00.051.018 I llm_load_print_meta: rope type        = 2
0.00.051.019 I llm_load_print_meta: rope scaling     = linear
0.00.051.019 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.019 I llm_load_print_meta: freq_scale_train = 1
0.00.051.019 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.020 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.020 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.020 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.020 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.020 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.022 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.033 I llm_load_print_meta: model type       = 1.4B
0.00.051.034 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.034 I llm_load_print_meta: model params     = 1.41 B
0.00.051.035 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.035 I llm_load_print_meta: general.name     = 1.4B
0.00.051.035 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.036 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.037 I llm_load_print_meta: LF token         = 128 ''
0.00.051.037 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.038 I llm_load_print_meta: max token length = 1024
0.00.052.599 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.599 I llm_load_tensors: offloading output layer to GPU
0.00.052.599 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.608 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.610 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.499 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.499 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.500 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.500 I llama_new_context_with_model: n_batch       = 2048
0.00.053.500 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.500 I llama_new_context_with_model: flash_attn    = 0
0.00.053.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.501 I llama_new_context_with_model: freq_scale    = 1
0.00.053.501 I ggml_metal_init: allocating
0.00.053.504 I ggml_metal_init: found device: Apple M4
0.00.053.506 I ggml_metal_init: picking default device: Apple M4
0.00.054.048 I ggml_metal_init: using embedded metal library
0.00.055.940 I ggml_metal_init: GPU name:   Apple M4
0.00.055.942 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.942 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.942 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.943 I ggml_metal_init: simdgroup reduction   = true
0.00.055.943 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.943 I ggml_metal_init: has bfloat            = true
0.00.055.943 I ggml_metal_init: use bfloat            = true
0.00.055.944 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.944 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.180 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.186 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.213 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.157 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.158 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.159 I llama_new_context_with_model: graph nodes  = 967
0.00.084.159 I llama_new_context_with_model: graph splits = 2
0.00.084.171 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.187 I main: llama threadpool init, n_threads = 4
0.00.754.228 I 
0.00.754.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.754.264 I 
0.00.754.416 I sampler seed: 1234
0.00.754.421 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.438 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.438 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.592.554 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.592.554 I llama_perf_context_print:        load time =     744.33 ms
0.01.592.555 I llama_perf_context_print: prompt eval time =      36.55 ms /     7 tokens (    5.22 ms per token,   191.51 tokens per second)
0.01.592.556 I llama_perf_context_print:        eval time =     798.63 ms /    63 runs   (   12.68 ms per token,    78.89 tokens per second)
0.01.592.556 I llama_perf_context_print:       total time =     838.37 ms /    70 tokens
0.01.592.714 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.108s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.501 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.171 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.172 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.173 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.979 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.084 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.842 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.843 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.844 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.844 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.844 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.844 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.845 I llama_model_loader: - type  f32:  194 tensors
0.00.022.845 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.845 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.135 I llm_load_vocab: special tokens cache size = 25
0.00.049.193 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.196 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.196 I llm_load_print_meta: arch             = gptneox
0.00.049.197 I llm_load_print_meta: vocab type       = BPE
0.00.049.197 I llm_load_print_meta: n_vocab          = 50304
0.00.049.197 I llm_load_print_meta: n_merges         = 50009
0.00.049.197 I llm_load_print_meta: vocab_only       = 0
0.00.049.197 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.198 I llm_load_print_meta: n_embd           = 2048
0.00.049.198 I llm_load_print_meta: n_layer          = 24
0.00.049.201 I llm_load_print_meta: n_head           = 16
0.00.049.204 I llm_load_print_meta: n_head_kv        = 16
0.00.049.204 I llm_load_print_meta: n_rot            = 32
0.00.049.204 I llm_load_print_meta: n_swa            = 0
0.00.049.204 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.204 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.205 I llm_load_print_meta: n_gqa            = 1
0.00.049.206 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.207 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.207 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.207 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.208 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.208 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.208 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.213 I llm_load_print_meta: n_ff             = 8192
0.00.049.213 I llm_load_print_meta: n_expert         = 0
0.00.049.213 I llm_load_print_meta: n_expert_used    = 0
0.00.049.213 I llm_load_print_meta: causal attn      = 1
0.00.049.215 I llm_load_print_meta: pooling type     = 0
0.00.049.215 I llm_load_print_meta: rope type        = 2
0.00.049.215 I llm_load_print_meta: rope scaling     = linear
0.00.049.216 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.216 I llm_load_print_meta: freq_scale_train = 1
0.00.049.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.217 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.217 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.217 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.217 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.217 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.229 I llm_load_print_meta: model type       = 1.4B
0.00.049.229 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.229 I llm_load_print_meta: model params     = 1.41 B
0.00.049.230 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.230 I llm_load_print_meta: general.name     = 1.4B
0.00.049.230 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.230 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.230 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.231 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.231 I llm_load_print_meta: LF token         = 128 ''
0.00.049.231 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.232 I llm_load_print_meta: max token length = 1024
0.00.051.191 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.191 I llm_load_tensors: offloading output layer to GPU
0.00.051.191 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.201 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.203 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.199 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.200 I llama_new_context_with_model: n_ctx         = 128
0.00.052.200 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.200 I llama_new_context_with_model: n_batch       = 128
0.00.052.200 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.201 I llama_new_context_with_model: flash_attn    = 0
0.00.052.201 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.201 I llama_new_context_with_model: freq_scale    = 1
0.00.052.202 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.202 I ggml_metal_init: allocating
0.00.052.207 I ggml_metal_init: found device: Apple M4
0.00.052.211 I ggml_metal_init: picking default device: Apple M4
0.00.052.720 I ggml_metal_init: using embedded metal library
0.00.054.669 I ggml_metal_init: GPU name:   Apple M4
0.00.054.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.671 I ggml_metal_init: simdgroup reduction   = true
0.00.054.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.671 I ggml_metal_init: has bfloat            = true
0.00.054.671 I ggml_metal_init: use bfloat            = true
0.00.054.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.661 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.665 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.631 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.632 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.632 I llama_new_context_with_model: graph nodes  = 967
0.00.064.633 I llama_new_context_with_model: graph splits = 2
0.00.064.645 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.683 I 
0.00.706.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.706.712 I perplexity: tokenizing the input ..
0.00.713.792 I perplexity: tokenization took 7.078 ms
0.00.713.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.165 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.850.451 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.850.466 I llama_perf_context_print:        load time =     698.18 ms
0.00.850.467 I llama_perf_context_print: prompt eval time =     135.12 ms /   128 tokens (    1.06 ms per token,   947.31 tokens per second)
0.00.850.468 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.469 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.850.903 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.074s
sys	0m0.140s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.914 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.309 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.316 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.316 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.316 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.317 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.317 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.318 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.318 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.319 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.320 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.322 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.836 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.837 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.838 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.838 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.022.839 I llama_model_loader: - type  f32:  194 tensors
0.00.022.839 I llama_model_loader: - type q2_K:   49 tensors
0.00.022.840 I llama_model_loader: - type q3_K:   48 tensors
0.00.022.840 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.013 I llm_load_vocab: special tokens cache size = 25
0.00.048.950 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.953 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.953 I llm_load_print_meta: arch             = gptneox
0.00.048.953 I llm_load_print_meta: vocab type       = BPE
0.00.048.954 I llm_load_print_meta: n_vocab          = 50304
0.00.048.954 I llm_load_print_meta: n_merges         = 50009
0.00.048.954 I llm_load_print_meta: vocab_only       = 0
0.00.048.954 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.954 I llm_load_print_meta: n_embd           = 2048
0.00.048.955 I llm_load_print_meta: n_layer          = 24
0.00.048.957 I llm_load_print_meta: n_head           = 16
0.00.048.957 I llm_load_print_meta: n_head_kv        = 16
0.00.048.957 I llm_load_print_meta: n_rot            = 32
0.00.048.958 I llm_load_print_meta: n_swa            = 0
0.00.048.958 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.958 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.959 I llm_load_print_meta: n_gqa            = 1
0.00.048.960 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.960 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.962 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.962 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.962 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.962 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.962 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.963 I llm_load_print_meta: n_ff             = 8192
0.00.048.963 I llm_load_print_meta: n_expert         = 0
0.00.048.963 I llm_load_print_meta: n_expert_used    = 0
0.00.048.964 I llm_load_print_meta: causal attn      = 1
0.00.048.964 I llm_load_print_meta: pooling type     = 0
0.00.048.964 I llm_load_print_meta: rope type        = 2
0.00.048.964 I llm_load_print_meta: rope scaling     = linear
0.00.048.965 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.965 I llm_load_print_meta: freq_scale_train = 1
0.00.048.965 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.965 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.965 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.966 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.967 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.968 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.968 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.979 I llm_load_print_meta: model type       = 1.4B
0.00.048.980 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.048.980 I llm_load_print_meta: model params     = 1.41 B
0.00.048.981 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.048.981 I llm_load_print_meta: general.name     = 1.4B
0.00.048.981 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.982 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.982 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.982 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.982 I llm_load_print_meta: LF token         = 128 ''
0.00.048.982 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.983 I llm_load_print_meta: max token length = 1024
0.00.050.559 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.560 I llm_load_tensors: offloading output layer to GPU
0.00.050.560 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.565 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.565 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.443 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.443 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.444 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.444 I llama_new_context_with_model: n_batch       = 2048
0.00.051.444 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.444 I llama_new_context_with_model: flash_attn    = 0
0.00.051.444 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.445 I llama_new_context_with_model: freq_scale    = 1
0.00.051.445 I ggml_metal_init: allocating
0.00.051.448 I ggml_metal_init: found device: Apple M4
0.00.051.450 I ggml_metal_init: picking default device: Apple M4
0.00.051.976 I ggml_metal_init: using embedded metal library
0.00.053.871 I ggml_metal_init: GPU name:   Apple M4
0.00.053.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.873 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.873 I ggml_metal_init: simdgroup reduction   = true
0.00.053.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.874 I ggml_metal_init: has bfloat            = true
0.00.053.874 I ggml_metal_init: use bfloat            = true
0.00.053.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.081 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.089 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.108 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.987 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.081.988 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.081.989 I llama_new_context_with_model: graph nodes  = 967
0.00.081.989 I llama_new_context_with_model: graph splits = 2
0.00.081.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.138 I main: llama threadpool init, n_threads = 4
0.00.487.166 I 
0.00.487.189 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.487.189 I 
0.00.487.433 I sampler seed: 1234
0.00.487.440 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.487.451 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.487.451 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.487.451 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.167.050 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61954.62 tokens per second)
0.01.167.051 I llama_perf_context_print:        load time =     478.22 ms
0.01.167.051 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.54 tokens per second)
0.01.167.052 I llama_perf_context_print:        eval time =     640.77 ms /    63 runs   (   10.17 ms per token,    98.32 tokens per second)
0.01.167.053 I llama_perf_context_print:       total time =     679.92 ms /    70 tokens
0.01.167.236 I ggml_metal_free: deallocating

real	0m1.182s
user	0m0.108s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.103 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.482 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.484 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.485 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.485 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.486 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.486 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.487 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.488 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.491 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.247 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.271 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.032 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.033 I llama_model_loader: - type  f32:  194 tensors
0.00.024.033 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.033 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.033 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.952 I llm_load_vocab: special tokens cache size = 25
0.00.050.954 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.957 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.957 I llm_load_print_meta: arch             = gptneox
0.00.050.958 I llm_load_print_meta: vocab type       = BPE
0.00.050.958 I llm_load_print_meta: n_vocab          = 50304
0.00.050.958 I llm_load_print_meta: n_merges         = 50009
0.00.050.958 I llm_load_print_meta: vocab_only       = 0
0.00.050.958 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.958 I llm_load_print_meta: n_embd           = 2048
0.00.050.959 I llm_load_print_meta: n_layer          = 24
0.00.050.962 I llm_load_print_meta: n_head           = 16
0.00.050.962 I llm_load_print_meta: n_head_kv        = 16
0.00.050.963 I llm_load_print_meta: n_rot            = 32
0.00.050.963 I llm_load_print_meta: n_swa            = 0
0.00.050.963 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.963 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.966 I llm_load_print_meta: n_gqa            = 1
0.00.050.967 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.968 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.969 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.969 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.969 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.969 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.970 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.970 I llm_load_print_meta: n_ff             = 8192
0.00.050.971 I llm_load_print_meta: n_expert         = 0
0.00.050.971 I llm_load_print_meta: n_expert_used    = 0
0.00.050.971 I llm_load_print_meta: causal attn      = 1
0.00.050.971 I llm_load_print_meta: pooling type     = 0
0.00.050.971 I llm_load_print_meta: rope type        = 2
0.00.050.971 I llm_load_print_meta: rope scaling     = linear
0.00.050.972 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.972 I llm_load_print_meta: freq_scale_train = 1
0.00.050.972 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.973 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.973 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.973 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.973 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.973 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.973 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.985 I llm_load_print_meta: model type       = 1.4B
0.00.050.986 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.986 I llm_load_print_meta: model params     = 1.41 B
0.00.050.986 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.986 I llm_load_print_meta: general.name     = 1.4B
0.00.050.988 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: LF token         = 128 ''
0.00.050.989 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: max token length = 1024
0.00.052.907 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.907 I llm_load_tensors: offloading output layer to GPU
0.00.052.907 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.917 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.918 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.896 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.897 I llama_new_context_with_model: n_ctx         = 128
0.00.053.897 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.897 I llama_new_context_with_model: n_batch       = 128
0.00.053.897 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.897 I llama_new_context_with_model: flash_attn    = 0
0.00.053.898 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.898 I llama_new_context_with_model: freq_scale    = 1
0.00.053.898 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.899 I ggml_metal_init: allocating
0.00.053.902 I ggml_metal_init: found device: Apple M4
0.00.053.904 I ggml_metal_init: picking default device: Apple M4
0.00.054.492 I ggml_metal_init: using embedded metal library
0.00.056.391 I ggml_metal_init: GPU name:   Apple M4
0.00.056.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.393 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.393 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.393 I ggml_metal_init: simdgroup reduction   = true
0.00.056.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.394 I ggml_metal_init: has bfloat            = true
0.00.056.394 I ggml_metal_init: use bfloat            = true
0.00.056.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.395 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.768 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.774 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.786 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.736 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.737 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.737 I llama_new_context_with_model: graph nodes  = 967
0.00.066.738 I llama_new_context_with_model: graph splits = 2
0.00.066.750 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.425.589 I 
0.00.425.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.425.656 I perplexity: tokenizing the input ..
0.00.433.008 I perplexity: tokenization took 7.353 ms
0.00.433.011 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.564.938 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.566.193 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.566.221 I llama_perf_context_print:        load time =     415.48 ms
0.00.566.222 I llama_perf_context_print: prompt eval time =     131.70 ms /   128 tokens (    1.03 ms per token,   971.94 tokens per second)
0.00.566.222 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.566.223 I llama_perf_context_print:       total time =     140.64 ms /   129 tokens
0.00.566.688 I ggml_metal_free: deallocating

real	0m0.580s
user	0m0.076s
sys	0m0.080s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.018 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.507 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.511 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.513 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.514 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.514 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.514 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.514 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.515 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.516 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.516 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.519 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.519 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.519 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.363 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.231 I llama_model_loader: - type  f32:  194 tensors
0.00.023.232 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.232 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.232 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.232 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.180 I llm_load_vocab: special tokens cache size = 25
0.00.050.132 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.135 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.135 I llm_load_print_meta: arch             = gptneox
0.00.050.135 I llm_load_print_meta: vocab type       = BPE
0.00.050.136 I llm_load_print_meta: n_vocab          = 50304
0.00.050.136 I llm_load_print_meta: n_merges         = 50009
0.00.050.136 I llm_load_print_meta: vocab_only       = 0
0.00.050.136 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.136 I llm_load_print_meta: n_embd           = 2048
0.00.050.136 I llm_load_print_meta: n_layer          = 24
0.00.050.139 I llm_load_print_meta: n_head           = 16
0.00.050.140 I llm_load_print_meta: n_head_kv        = 16
0.00.050.140 I llm_load_print_meta: n_rot            = 32
0.00.050.140 I llm_load_print_meta: n_swa            = 0
0.00.050.140 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.140 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.141 I llm_load_print_meta: n_gqa            = 1
0.00.050.142 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.142 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.143 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.143 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.143 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.144 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.146 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.147 I llm_load_print_meta: n_ff             = 8192
0.00.050.147 I llm_load_print_meta: n_expert         = 0
0.00.050.147 I llm_load_print_meta: n_expert_used    = 0
0.00.050.148 I llm_load_print_meta: causal attn      = 1
0.00.050.148 I llm_load_print_meta: pooling type     = 0
0.00.050.150 I llm_load_print_meta: rope type        = 2
0.00.050.150 I llm_load_print_meta: rope scaling     = linear
0.00.050.150 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.150 I llm_load_print_meta: freq_scale_train = 1
0.00.050.151 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.151 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.151 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.151 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.151 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.151 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.151 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.163 I llm_load_print_meta: model type       = 1.4B
0.00.050.163 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.164 I llm_load_print_meta: model params     = 1.41 B
0.00.050.164 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.164 I llm_load_print_meta: general.name     = 1.4B
0.00.050.165 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.165 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.165 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.166 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.166 I llm_load_print_meta: LF token         = 128 ''
0.00.050.166 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.166 I llm_load_print_meta: max token length = 1024
0.00.051.729 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.729 I llm_load_tensors: offloading output layer to GPU
0.00.051.729 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.738 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.740 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.545 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.546 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.546 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.546 I llama_new_context_with_model: n_batch       = 2048
0.00.052.546 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.546 I llama_new_context_with_model: flash_attn    = 0
0.00.052.547 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.547 I llama_new_context_with_model: freq_scale    = 1
0.00.052.548 I ggml_metal_init: allocating
0.00.052.553 I ggml_metal_init: found device: Apple M4
0.00.052.555 I ggml_metal_init: picking default device: Apple M4
0.00.053.095 I ggml_metal_init: using embedded metal library
0.00.055.021 I ggml_metal_init: GPU name:   Apple M4
0.00.055.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.023 I ggml_metal_init: simdgroup reduction   = true
0.00.055.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.023 I ggml_metal_init: has bfloat            = true
0.00.055.024 I ggml_metal_init: use bfloat            = true
0.00.055.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.025 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.469 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.081.474 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.081.492 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.382 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.082.383 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.082.384 I llama_new_context_with_model: graph nodes  = 967
0.00.082.384 I llama_new_context_with_model: graph splits = 2
0.00.082.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.935 I main: llama threadpool init, n_threads = 4
0.00.594.967 I 
0.00.594.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.594.989 I 
0.00.595.182 I sampler seed: 1234
0.00.595.188 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.595.230 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.595.246 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.595.246 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.333.593 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60169.49 tokens per second)
0.01.333.593 I llama_perf_context_print:        load time =     585.91 ms
0.01.333.594 I llama_perf_context_print: prompt eval time =      35.72 ms /     7 tokens (    5.10 ms per token,   195.96 tokens per second)
0.01.333.595 I llama_perf_context_print:        eval time =     699.63 ms /    63 runs   (   11.11 ms per token,    90.05 tokens per second)
0.01.333.595 I llama_perf_context_print:       total time =     738.66 ms /    70 tokens
0.01.333.773 I ggml_metal_free: deallocating

real	0m1.348s
user	0m0.107s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.509 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.017 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.026 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.027 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.027 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.028 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.029 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.029 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.030 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.030 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.030 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.032 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.033 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.819 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.653 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.654 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.655 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.655 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.655 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.656 I llama_model_loader: - type  f32:  194 tensors
0.00.022.656 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.656 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.656 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.657 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.872 I llm_load_vocab: special tokens cache size = 25
0.00.048.658 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.660 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.661 I llm_load_print_meta: arch             = gptneox
0.00.048.661 I llm_load_print_meta: vocab type       = BPE
0.00.048.661 I llm_load_print_meta: n_vocab          = 50304
0.00.048.661 I llm_load_print_meta: n_merges         = 50009
0.00.048.661 I llm_load_print_meta: vocab_only       = 0
0.00.048.662 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.662 I llm_load_print_meta: n_embd           = 2048
0.00.048.662 I llm_load_print_meta: n_layer          = 24
0.00.048.664 I llm_load_print_meta: n_head           = 16
0.00.048.665 I llm_load_print_meta: n_head_kv        = 16
0.00.048.665 I llm_load_print_meta: n_rot            = 32
0.00.048.665 I llm_load_print_meta: n_swa            = 0
0.00.048.667 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.667 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.668 I llm_load_print_meta: n_gqa            = 1
0.00.048.669 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.674 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.674 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.675 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.676 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.676 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.677 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.679 I llm_load_print_meta: n_ff             = 8192
0.00.048.679 I llm_load_print_meta: n_expert         = 0
0.00.048.680 I llm_load_print_meta: n_expert_used    = 0
0.00.048.680 I llm_load_print_meta: causal attn      = 1
0.00.048.680 I llm_load_print_meta: pooling type     = 0
0.00.048.680 I llm_load_print_meta: rope type        = 2
0.00.048.680 I llm_load_print_meta: rope scaling     = linear
0.00.048.681 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.682 I llm_load_print_meta: freq_scale_train = 1
0.00.048.682 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.682 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.682 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.682 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.682 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.683 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.683 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.694 I llm_load_print_meta: model type       = 1.4B
0.00.048.695 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.695 I llm_load_print_meta: model params     = 1.41 B
0.00.048.695 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.696 I llm_load_print_meta: general.name     = 1.4B
0.00.048.696 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.697 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.697 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.697 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.697 I llm_load_print_meta: LF token         = 128 ''
0.00.048.698 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.698 I llm_load_print_meta: max token length = 1024
0.00.050.621 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.621 I llm_load_tensors: offloading output layer to GPU
0.00.050.621 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.631 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.632 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.595 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.595 I llama_new_context_with_model: n_ctx         = 128
0.00.051.595 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.596 I llama_new_context_with_model: n_batch       = 128
0.00.051.596 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.596 I llama_new_context_with_model: flash_attn    = 0
0.00.051.596 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.597 I llama_new_context_with_model: freq_scale    = 1
0.00.051.597 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.597 I ggml_metal_init: allocating
0.00.051.603 I ggml_metal_init: found device: Apple M4
0.00.051.605 I ggml_metal_init: picking default device: Apple M4
0.00.052.143 I ggml_metal_init: using embedded metal library
0.00.054.076 I ggml_metal_init: GPU name:   Apple M4
0.00.054.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.078 I ggml_metal_init: simdgroup reduction   = true
0.00.054.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.079 I ggml_metal_init: has bfloat            = true
0.00.054.079 I ggml_metal_init: use bfloat            = true
0.00.054.079 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.080 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.921 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.924 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.939 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.792 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.793 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.793 I llama_new_context_with_model: graph nodes  = 967
0.00.063.794 I llama_new_context_with_model: graph splits = 2
0.00.063.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.514.003 I 
0.00.514.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.514.075 I perplexity: tokenizing the input ..
0.00.522.175 I perplexity: tokenization took 8.099 ms
0.00.522.179 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.654.463 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.655.761 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.655.791 I llama_perf_context_print:        load time =     505.49 ms
0.00.655.792 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.50 tokens per second)
0.00.655.793 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.655.794 I llama_perf_context_print:       total time =     141.79 ms /   129 tokens
0.00.656.305 I ggml_metal_free: deallocating

real	0m0.669s
user	0m0.075s
sys	0m0.095s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.011.075 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.676 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.681 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.682 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.682 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.682 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.683 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.686 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.471 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.281 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.282 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.282 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.283 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.283 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.283 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.284 I llama_model_loader: - type  f32:  194 tensors
0.00.026.284 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.284 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.285 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.749 I llm_load_vocab: special tokens cache size = 25
0.00.052.754 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.756 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.757 I llm_load_print_meta: arch             = gptneox
0.00.052.757 I llm_load_print_meta: vocab type       = BPE
0.00.052.757 I llm_load_print_meta: n_vocab          = 50304
0.00.052.757 I llm_load_print_meta: n_merges         = 50009
0.00.052.758 I llm_load_print_meta: vocab_only       = 0
0.00.052.758 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.758 I llm_load_print_meta: n_embd           = 2048
0.00.052.758 I llm_load_print_meta: n_layer          = 24
0.00.052.761 I llm_load_print_meta: n_head           = 16
0.00.052.762 I llm_load_print_meta: n_head_kv        = 16
0.00.052.762 I llm_load_print_meta: n_rot            = 32
0.00.052.762 I llm_load_print_meta: n_swa            = 0
0.00.052.762 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.762 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.763 I llm_load_print_meta: n_gqa            = 1
0.00.052.764 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.764 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.765 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.765 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.765 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.768 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.768 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.768 I llm_load_print_meta: n_ff             = 8192
0.00.052.769 I llm_load_print_meta: n_expert         = 0
0.00.052.770 I llm_load_print_meta: n_expert_used    = 0
0.00.052.772 I llm_load_print_meta: causal attn      = 1
0.00.052.772 I llm_load_print_meta: pooling type     = 0
0.00.052.772 I llm_load_print_meta: rope type        = 2
0.00.052.772 I llm_load_print_meta: rope scaling     = linear
0.00.052.773 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.773 I llm_load_print_meta: freq_scale_train = 1
0.00.052.773 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.773 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.773 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.773 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.773 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.774 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.774 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.780 I llm_load_print_meta: model type       = 1.4B
0.00.052.781 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.781 I llm_load_print_meta: model params     = 1.41 B
0.00.052.781 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.782 I llm_load_print_meta: general.name     = 1.4B
0.00.052.782 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.783 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.783 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.783 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.784 I llm_load_print_meta: LF token         = 128 ''
0.00.052.784 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.784 I llm_load_print_meta: max token length = 1024
0.00.054.587 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.588 I llm_load_tensors: offloading output layer to GPU
0.00.054.588 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.593 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.593 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.625 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.626 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.626 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.626 I llama_new_context_with_model: n_batch       = 2048
0.00.055.626 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.627 I llama_new_context_with_model: flash_attn    = 0
0.00.055.627 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.627 I llama_new_context_with_model: freq_scale    = 1
0.00.055.627 I ggml_metal_init: allocating
0.00.055.630 I ggml_metal_init: found device: Apple M4
0.00.055.632 I ggml_metal_init: picking default device: Apple M4
0.00.056.177 I ggml_metal_init: using embedded metal library
0.00.058.094 I ggml_metal_init: GPU name:   Apple M4
0.00.058.096 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.096 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.096 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.098 I ggml_metal_init: simdgroup reduction   = true
0.00.058.098 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.098 I ggml_metal_init: has bfloat            = true
0.00.058.100 I ggml_metal_init: use bfloat            = true
0.00.058.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.557 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.572 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.592 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.689 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.690 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.691 I llama_new_context_with_model: graph nodes  = 967
0.00.086.691 I llama_new_context_with_model: graph splits = 2
0.00.086.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.665 I main: llama threadpool init, n_threads = 4
0.00.666.704 I 
0.00.666.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.666.732 I 
0.00.666.885 I sampler seed: 1234
0.00.666.889 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.666.898 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.666.898 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.666.898 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.427.340 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.427.342 I llama_perf_context_print:        load time =     655.59 ms
0.01.427.342 I llama_perf_context_print: prompt eval time =      36.45 ms /     7 tokens (    5.21 ms per token,   192.05 tokens per second)
0.01.427.343 I llama_perf_context_print:        eval time =     720.94 ms /    63 runs   (   11.44 ms per token,    87.39 tokens per second)
0.01.427.343 I llama_perf_context_print:       total time =     760.68 ms /    70 tokens
0.01.427.523 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.107s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.330 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.106 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.111 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.116 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.117 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.117 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.118 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.118 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.119 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.119 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.120 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.120 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.121 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.122 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.123 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.123 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.813 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.815 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.815 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.815 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.816 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.816 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.817 I llama_model_loader: - type  f32:  194 tensors
0.00.022.817 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.817 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.817 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.932 I llm_load_vocab: special tokens cache size = 25
0.00.049.956 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.959 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.959 I llm_load_print_meta: arch             = gptneox
0.00.049.959 I llm_load_print_meta: vocab type       = BPE
0.00.049.960 I llm_load_print_meta: n_vocab          = 50304
0.00.049.960 I llm_load_print_meta: n_merges         = 50009
0.00.049.960 I llm_load_print_meta: vocab_only       = 0
0.00.049.960 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.960 I llm_load_print_meta: n_embd           = 2048
0.00.049.960 I llm_load_print_meta: n_layer          = 24
0.00.049.963 I llm_load_print_meta: n_head           = 16
0.00.049.964 I llm_load_print_meta: n_head_kv        = 16
0.00.049.964 I llm_load_print_meta: n_rot            = 32
0.00.049.964 I llm_load_print_meta: n_swa            = 0
0.00.049.965 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.966 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.966 I llm_load_print_meta: n_gqa            = 1
0.00.049.967 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.968 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.969 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.969 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.970 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.970 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.970 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.971 I llm_load_print_meta: n_ff             = 8192
0.00.049.971 I llm_load_print_meta: n_expert         = 0
0.00.049.971 I llm_load_print_meta: n_expert_used    = 0
0.00.049.971 I llm_load_print_meta: causal attn      = 1
0.00.049.972 I llm_load_print_meta: pooling type     = 0
0.00.049.972 I llm_load_print_meta: rope type        = 2
0.00.049.972 I llm_load_print_meta: rope scaling     = linear
0.00.049.972 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.973 I llm_load_print_meta: freq_scale_train = 1
0.00.049.973 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.973 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.973 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.975 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.975 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.975 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.975 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.987 I llm_load_print_meta: model type       = 1.4B
0.00.049.987 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.989 I llm_load_print_meta: model params     = 1.41 B
0.00.049.990 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.990 I llm_load_print_meta: general.name     = 1.4B
0.00.049.990 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.990 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.990 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.990 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.991 I llm_load_print_meta: LF token         = 128 ''
0.00.049.991 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.991 I llm_load_print_meta: max token length = 1024
0.00.051.638 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.639 I llm_load_tensors: offloading output layer to GPU
0.00.051.639 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.648 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.650 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.517 I llama_new_context_with_model: n_ctx         = 128
0.00.052.518 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.518 I llama_new_context_with_model: n_batch       = 128
0.00.052.518 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.518 I llama_new_context_with_model: flash_attn    = 0
0.00.052.519 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.519 I llama_new_context_with_model: freq_scale    = 1
0.00.052.519 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.520 I ggml_metal_init: allocating
0.00.052.526 I ggml_metal_init: found device: Apple M4
0.00.052.528 I ggml_metal_init: picking default device: Apple M4
0.00.053.062 I ggml_metal_init: using embedded metal library
0.00.055.031 I ggml_metal_init: GPU name:   Apple M4
0.00.055.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.034 I ggml_metal_init: simdgroup reduction   = true
0.00.055.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.034 I ggml_metal_init: has bfloat            = true
0.00.055.034 I ggml_metal_init: use bfloat            = true
0.00.055.035 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.479 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.482 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.498 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.402 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.403 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.404 I llama_new_context_with_model: graph nodes  = 967
0.00.065.404 I llama_new_context_with_model: graph splits = 2
0.00.065.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.780 I 
0.00.592.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.592.812 I perplexity: tokenizing the input ..
0.00.600.328 I perplexity: tokenization took 7.515 ms
0.00.600.331 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.493 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.735.729 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.735.753 I llama_perf_context_print:        load time =     584.45 ms
0.00.735.754 I llama_perf_context_print: prompt eval time =     133.94 ms /   128 tokens (    1.05 ms per token,   955.68 tokens per second)
0.00.735.755 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.735.755 I llama_perf_context_print:       total time =     142.97 ms /   129 tokens
0.00.736.113 I ggml_metal_free: deallocating

real	0m0.748s
user	0m0.077s
sys	0m0.110s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.735 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.001 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.017 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.017 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.017 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.829 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.886 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.676 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.677 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.678 I llama_model_loader: - type  f32:  194 tensors
0.00.024.678 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.678 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.889 I llm_load_vocab: special tokens cache size = 25
0.00.050.941 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.944 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.945 I llm_load_print_meta: arch             = gptneox
0.00.050.945 I llm_load_print_meta: vocab type       = BPE
0.00.050.945 I llm_load_print_meta: n_vocab          = 50304
0.00.050.945 I llm_load_print_meta: n_merges         = 50009
0.00.050.946 I llm_load_print_meta: vocab_only       = 0
0.00.050.946 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.946 I llm_load_print_meta: n_embd           = 2048
0.00.050.946 I llm_load_print_meta: n_layer          = 24
0.00.050.949 I llm_load_print_meta: n_head           = 16
0.00.050.950 I llm_load_print_meta: n_head_kv        = 16
0.00.050.950 I llm_load_print_meta: n_rot            = 32
0.00.050.950 I llm_load_print_meta: n_swa            = 0
0.00.050.950 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.950 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.951 I llm_load_print_meta: n_gqa            = 1
0.00.050.952 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.952 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.953 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.954 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.954 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.954 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.954 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.955 I llm_load_print_meta: n_ff             = 8192
0.00.050.955 I llm_load_print_meta: n_expert         = 0
0.00.050.955 I llm_load_print_meta: n_expert_used    = 0
0.00.050.955 I llm_load_print_meta: causal attn      = 1
0.00.050.955 I llm_load_print_meta: pooling type     = 0
0.00.050.956 I llm_load_print_meta: rope type        = 2
0.00.050.956 I llm_load_print_meta: rope scaling     = linear
0.00.050.956 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.957 I llm_load_print_meta: freq_scale_train = 1
0.00.050.957 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.957 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.957 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.957 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.959 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.959 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.959 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.971 I llm_load_print_meta: model type       = 1.4B
0.00.050.972 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.972 I llm_load_print_meta: model params     = 1.41 B
0.00.050.973 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.973 I llm_load_print_meta: general.name     = 1.4B
0.00.050.973 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.973 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.973 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.973 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.974 I llm_load_print_meta: LF token         = 128 ''
0.00.050.974 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.974 I llm_load_print_meta: max token length = 1024
0.00.053.029 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.029 I llm_load_tensors: offloading output layer to GPU
0.00.053.029 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.039 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.040 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.026 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.026 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.027 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.027 I llama_new_context_with_model: n_batch       = 2048
0.00.054.027 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.027 I llama_new_context_with_model: flash_attn    = 0
0.00.054.028 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.028 I llama_new_context_with_model: freq_scale    = 1
0.00.054.028 I ggml_metal_init: allocating
0.00.054.032 I ggml_metal_init: found device: Apple M4
0.00.054.034 I ggml_metal_init: picking default device: Apple M4
0.00.054.624 I ggml_metal_init: using embedded metal library
0.00.056.538 I ggml_metal_init: GPU name:   Apple M4
0.00.056.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.540 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.541 I ggml_metal_init: simdgroup reduction   = true
0.00.056.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.541 I ggml_metal_init: has bfloat            = true
0.00.056.541 I ggml_metal_init: use bfloat            = true
0.00.056.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.814 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.820 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.841 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.857 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.858 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.859 I llama_new_context_with_model: graph nodes  = 967
0.00.086.859 I llama_new_context_with_model: graph splits = 2
0.00.086.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.608 I main: llama threadpool init, n_threads = 4
0.00.755.639 I 
0.00.755.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.755.657 I 
0.00.755.873 I sampler seed: 1234
0.00.755.877 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.891 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.891 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.603.635 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.603.636 I llama_perf_context_print:        load time =     746.87 ms
0.01.603.637 I llama_perf_context_print: prompt eval time =      38.72 ms /     7 tokens (    5.53 ms per token,   180.79 tokens per second)
0.01.603.638 I llama_perf_context_print:        eval time =     806.12 ms /    63 runs   (   12.80 ms per token,    78.15 tokens per second)
0.01.603.638 I llama_perf_context_print:       total time =     848.03 ms /    70 tokens
0.01.603.839 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.107s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.905 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.812 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.813 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.813 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.813 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.815 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.815 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.815 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.818 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.818 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.820 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.820 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.529 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.587 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.347 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.349 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.349 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.350 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.351 I llama_model_loader: - type  f32:  194 tensors
0.00.024.351 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.351 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.473 I llm_load_vocab: special tokens cache size = 25
0.00.050.195 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.198 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.199 I llm_load_print_meta: arch             = gptneox
0.00.050.199 I llm_load_print_meta: vocab type       = BPE
0.00.050.199 I llm_load_print_meta: n_vocab          = 50304
0.00.050.199 I llm_load_print_meta: n_merges         = 50009
0.00.050.200 I llm_load_print_meta: vocab_only       = 0
0.00.050.200 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.200 I llm_load_print_meta: n_embd           = 2048
0.00.050.200 I llm_load_print_meta: n_layer          = 24
0.00.050.203 I llm_load_print_meta: n_head           = 16
0.00.050.204 I llm_load_print_meta: n_head_kv        = 16
0.00.050.204 I llm_load_print_meta: n_rot            = 32
0.00.050.204 I llm_load_print_meta: n_swa            = 0
0.00.050.204 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.206 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.207 I llm_load_print_meta: n_gqa            = 1
0.00.050.208 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.210 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.211 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.213 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.213 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.213 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.213 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.214 I llm_load_print_meta: n_ff             = 8192
0.00.050.214 I llm_load_print_meta: n_expert         = 0
0.00.050.214 I llm_load_print_meta: n_expert_used    = 0
0.00.050.214 I llm_load_print_meta: causal attn      = 1
0.00.050.214 I llm_load_print_meta: pooling type     = 0
0.00.050.214 I llm_load_print_meta: rope type        = 2
0.00.050.215 I llm_load_print_meta: rope scaling     = linear
0.00.050.215 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.215 I llm_load_print_meta: freq_scale_train = 1
0.00.050.215 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.216 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.220 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.220 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.220 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.232 I llm_load_print_meta: model type       = 1.4B
0.00.050.232 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.233 I llm_load_print_meta: model params     = 1.41 B
0.00.050.233 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.234 I llm_load_print_meta: general.name     = 1.4B
0.00.050.234 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.234 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.234 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.234 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.234 I llm_load_print_meta: LF token         = 128 ''
0.00.050.235 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.235 I llm_load_print_meta: max token length = 1024
0.00.052.262 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.263 I llm_load_tensors: offloading output layer to GPU
0.00.052.263 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.273 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.274 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.203 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.204 I llama_new_context_with_model: n_ctx         = 128
0.00.053.204 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.205 I llama_new_context_with_model: n_batch       = 128
0.00.053.205 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.205 I llama_new_context_with_model: flash_attn    = 0
0.00.053.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.206 I llama_new_context_with_model: freq_scale    = 1
0.00.053.206 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.206 I ggml_metal_init: allocating
0.00.053.209 I ggml_metal_init: found device: Apple M4
0.00.053.211 I ggml_metal_init: picking default device: Apple M4
0.00.053.751 I ggml_metal_init: using embedded metal library
0.00.055.674 I ggml_metal_init: GPU name:   Apple M4
0.00.055.675 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.676 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.676 I ggml_metal_init: simdgroup reduction   = true
0.00.055.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.677 I ggml_metal_init: has bfloat            = true
0.00.055.677 I ggml_metal_init: use bfloat            = true
0.00.055.677 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.646 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.650 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.665 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.529 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.530 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.531 I llama_new_context_with_model: graph nodes  = 967
0.00.065.531 I llama_new_context_with_model: graph splits = 2
0.00.065.543 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.071 I 
0.00.679.110 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.679.134 I perplexity: tokenizing the input ..
0.00.686.458 I perplexity: tokenization took 7.323 ms
0.00.686.461 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.358 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.828.671 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.828.688 I llama_perf_context_print:        load time =     669.16 ms
0.00.828.690 I llama_perf_context_print: prompt eval time =     140.68 ms /   128 tokens (    1.10 ms per token,   909.87 tokens per second)
0.00.828.691 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.691 I llama_perf_context_print:       total time =     149.62 ms /   129 tokens
0.00.829.092 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.074s
sys	0m0.130s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.138 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.472 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.478 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.478 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.479 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.170 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.005 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.005 I llama_model_loader: - type  f32:  194 tensors
0.00.025.006 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.402 I llm_load_vocab: special tokens cache size = 25
0.00.051.206 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.209 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.209 I llm_load_print_meta: arch             = gptneox
0.00.051.209 I llm_load_print_meta: vocab type       = BPE
0.00.051.210 I llm_load_print_meta: n_vocab          = 50304
0.00.051.210 I llm_load_print_meta: n_merges         = 50009
0.00.051.210 I llm_load_print_meta: vocab_only       = 0
0.00.051.210 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.211 I llm_load_print_meta: n_embd           = 2048
0.00.051.211 I llm_load_print_meta: n_layer          = 24
0.00.051.214 I llm_load_print_meta: n_head           = 16
0.00.051.215 I llm_load_print_meta: n_head_kv        = 16
0.00.051.215 I llm_load_print_meta: n_rot            = 32
0.00.051.215 I llm_load_print_meta: n_swa            = 0
0.00.051.215 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.216 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.216 I llm_load_print_meta: n_gqa            = 1
0.00.051.217 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.218 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.218 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.219 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.219 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.219 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.219 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.220 I llm_load_print_meta: n_ff             = 8192
0.00.051.220 I llm_load_print_meta: n_expert         = 0
0.00.051.220 I llm_load_print_meta: n_expert_used    = 0
0.00.051.220 I llm_load_print_meta: causal attn      = 1
0.00.051.222 I llm_load_print_meta: pooling type     = 0
0.00.051.224 I llm_load_print_meta: rope type        = 2
0.00.051.224 I llm_load_print_meta: rope scaling     = linear
0.00.051.224 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.225 I llm_load_print_meta: freq_scale_train = 1
0.00.051.225 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.225 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.225 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.225 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.225 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.226 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.226 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.233 I llm_load_print_meta: model type       = 1.4B
0.00.051.233 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.233 I llm_load_print_meta: model params     = 1.41 B
0.00.051.235 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.236 I llm_load_print_meta: general.name     = 1.4B
0.00.051.236 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.236 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.236 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.236 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.237 I llm_load_print_meta: LF token         = 128 ''
0.00.051.237 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.237 I llm_load_print_meta: max token length = 1024
0.00.053.118 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.118 I llm_load_tensors: offloading output layer to GPU
0.00.053.118 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.123 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.124 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.069 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.069 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.069 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.070 I llama_new_context_with_model: n_batch       = 2048
0.00.054.070 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.070 I llama_new_context_with_model: flash_attn    = 0
0.00.054.070 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.071 I llama_new_context_with_model: freq_scale    = 1
0.00.054.071 I ggml_metal_init: allocating
0.00.054.078 I ggml_metal_init: found device: Apple M4
0.00.054.080 I ggml_metal_init: picking default device: Apple M4
0.00.054.620 I ggml_metal_init: using embedded metal library
0.00.056.574 I ggml_metal_init: GPU name:   Apple M4
0.00.056.575 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.576 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.576 I ggml_metal_init: simdgroup reduction   = true
0.00.056.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.578 I ggml_metal_init: has bfloat            = true
0.00.056.578 I ggml_metal_init: use bfloat            = true
0.00.056.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.380 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.392 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.413 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.339 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.340 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.340 I llama_new_context_with_model: graph nodes  = 967
0.00.085.340 I llama_new_context_with_model: graph splits = 2
0.00.085.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.838.806 I main: llama threadpool init, n_threads = 4
0.00.838.850 I 
0.00.838.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.838.869 I 
0.00.839.018 I sampler seed: 1234
0.00.839.022 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.839.032 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.839.032 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.839.033 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.736.594 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.01.736.595 I llama_perf_context_print:        load time =     828.66 ms
0.01.736.595 I llama_perf_context_print: prompt eval time =      38.31 ms /     7 tokens (    5.47 ms per token,   182.72 tokens per second)
0.01.736.596 I llama_perf_context_print:        eval time =     856.23 ms /    63 runs   (   13.59 ms per token,    73.58 tokens per second)
0.01.736.596 I llama_perf_context_print:       total time =     897.79 ms /    70 tokens
0.01.736.791 I ggml_metal_free: deallocating

real	0m1.756s
user	0m0.107s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4157 (f4f2a889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.850 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.579 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.583 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.585 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.586 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.586 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.588 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.588 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.379 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.454 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.331 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.331 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.331 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.332 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.332 I llama_model_loader: - type  f32:  194 tensors
0.00.023.333 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.278 I llm_load_vocab: special tokens cache size = 25
0.00.050.337 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.340 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.340 I llm_load_print_meta: arch             = gptneox
0.00.050.341 I llm_load_print_meta: vocab type       = BPE
0.00.050.341 I llm_load_print_meta: n_vocab          = 50304
0.00.050.341 I llm_load_print_meta: n_merges         = 50009
0.00.050.341 I llm_load_print_meta: vocab_only       = 0
0.00.050.342 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.342 I llm_load_print_meta: n_embd           = 2048
0.00.050.342 I llm_load_print_meta: n_layer          = 24
0.00.050.345 I llm_load_print_meta: n_head           = 16
0.00.050.346 I llm_load_print_meta: n_head_kv        = 16
0.00.050.346 I llm_load_print_meta: n_rot            = 32
0.00.050.346 I llm_load_print_meta: n_swa            = 0
0.00.050.346 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.347 I llm_load_print_meta: n_gqa            = 1
0.00.050.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.349 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.349 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.349 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.350 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.350 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.350 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.351 I llm_load_print_meta: n_ff             = 8192
0.00.050.351 I llm_load_print_meta: n_expert         = 0
0.00.050.351 I llm_load_print_meta: n_expert_used    = 0
0.00.050.354 I llm_load_print_meta: causal attn      = 1
0.00.050.354 I llm_load_print_meta: pooling type     = 0
0.00.050.354 I llm_load_print_meta: rope type        = 2
0.00.050.354 I llm_load_print_meta: rope scaling     = linear
0.00.050.354 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.355 I llm_load_print_meta: freq_scale_train = 1
0.00.050.355 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.355 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.355 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.356 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.356 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.356 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.356 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.368 I llm_load_print_meta: model type       = 1.4B
0.00.050.368 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.368 I llm_load_print_meta: model params     = 1.41 B
0.00.050.369 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.369 I llm_load_print_meta: general.name     = 1.4B
0.00.050.369 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.369 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.369 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.369 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.370 I llm_load_print_meta: LF token         = 128 ''
0.00.050.370 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.370 I llm_load_print_meta: max token length = 1024
0.00.052.454 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.455 I llm_load_tensors: offloading output layer to GPU
0.00.052.455 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.465 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.467 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.419 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.420 I llama_new_context_with_model: n_ctx         = 128
0.00.053.420 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.420 I llama_new_context_with_model: n_batch       = 128
0.00.053.420 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.421 I llama_new_context_with_model: flash_attn    = 0
0.00.053.421 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.421 I llama_new_context_with_model: freq_scale    = 1
0.00.053.422 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.422 I ggml_metal_init: allocating
0.00.053.425 I ggml_metal_init: found device: Apple M4
0.00.053.427 I ggml_metal_init: picking default device: Apple M4
0.00.053.977 I ggml_metal_init: using embedded metal library
0.00.055.894 I ggml_metal_init: GPU name:   Apple M4
0.00.055.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.897 I ggml_metal_init: simdgroup reduction   = true
0.00.055.897 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.897 I ggml_metal_init: has bfloat            = true
0.00.055.897 I ggml_metal_init: use bfloat            = true
0.00.055.897 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.898 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.233 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.237 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.250 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.163 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.164 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.165 I llama_new_context_with_model: graph nodes  = 967
0.00.066.165 I llama_new_context_with_model: graph splits = 2
0.00.066.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.369.593 I 
0.00.369.650 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.369.680 I perplexity: tokenizing the input ..
0.00.377.691 I perplexity: tokenization took 8.01 ms
0.00.377.697 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.517.481 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.518.703 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.518.718 I llama_perf_context_print:        load time =     360.74 ms
0.00.518.719 I llama_perf_context_print: prompt eval time =     139.54 ms /   128 tokens (    1.09 ms per token,   917.31 tokens per second)
0.00.518.720 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.518.720 I llama_perf_context_print:       total time =     149.13 ms /   129 tokens
0.00.519.093 I ggml_metal_free: deallocating

real	0m0.531s
user	0m0.077s
sys	0m0.078s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4157 (f4f2a889)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e0a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e0a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e0acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e0b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e0b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e0bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e0c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e0c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e0cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e0d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e0d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e0ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e0e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e0f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e0fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e10e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e11550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e12b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e13280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e13b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e14240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e14b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e15780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e15cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e1a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e1a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e1bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e1c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e1dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e1e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e1ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e1eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e20220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e20b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e21940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e22720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e22bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e23500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e24c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e27120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e28ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e29180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e2a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e2b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e2b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e2bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e2c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e2c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e2cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e2cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e2d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e2e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e2f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e2f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e2fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e30230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e31950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e31df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e34790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e35a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e36350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e36c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e39190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e39630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e3a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e3b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e3bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e3c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e3df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e3eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e3f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e41710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e41c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e42c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e44190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e45c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e47160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e48150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e48bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e49140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e49be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e4a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e4abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e4b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e4c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e4d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e4d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e4dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e4eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e4f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e4fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e50620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e50b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e51610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e52aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e52f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e53880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e53d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e54b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e54fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e55d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e56770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e56e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e57cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e58cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e592d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.172.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115f04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115f05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115f056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115f05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115f05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115f06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x115f06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x115f06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x115f07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115f075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x115f07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115f08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x115f08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115f093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x115f09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x115f0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x115f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x115f0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x115f0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x115f0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x115f0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x115f0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x115f0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x115f0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x115f0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x115f0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115f0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x115f0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115f0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115f0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115f0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115f0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115f10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115f106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115f10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115f10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115f11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115f118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115f11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115f12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115f12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115f12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115f12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115f13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115f137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115f13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115f140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115f14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115f14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115f14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115f15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x115f156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x115f15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115f15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x115f16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115f16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115f16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115f17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115f17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115f17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x115f18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115f184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115f18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115f18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115f19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115f19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x115f19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x115f19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x115f1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x115f1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x115f1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x115f1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115f1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115f1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115f1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115f1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115f1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115f1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115f1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115f1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115f1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115f1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115f1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115f1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x115f1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115f1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115f1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115f1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115f1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115f20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115f20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115f209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115f20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115f212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115f21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115f21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115f22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115f22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115f228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115f22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115f231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115f23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115f23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115f23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115f24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115f24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115f24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115f250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115f25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115f259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115f25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115f262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115f26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115f26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115f26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115f27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115f278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115f27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115f281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115f28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115f28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115f28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115f29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115f297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x115f29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115f2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x115f2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x115f2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115f2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x115f2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115f2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115f2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115f2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x115f2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115f2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115f2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115f2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x115f2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115f2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115f2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x115f2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115f2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x115f2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115f2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115f2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115f2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115f2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115f30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115f306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115f30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115f30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115f31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115f31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115f31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115f32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115f325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115f32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115f32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115f33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115f337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x115f33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115f34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115f344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115f34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115f34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115f35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115f356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115f36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115f36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115f367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115f36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115f370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115f37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115f37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115f37df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115f38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115f386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115f38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115f38fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115f39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115f39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115f39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115f3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115f3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115f3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115f3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115f3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115f3b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115f3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115f3c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115f3c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115f3c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115f3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115f3d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115f3d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115f3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115f3df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115f3e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115f3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115f3ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115f3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115f3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115f3fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x115f3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115f40310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115f40780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x115f40bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115f41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115f414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115f41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115f41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115f42220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115f42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115f42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115f42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115f433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115f43850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115f43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115f44130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115f445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x115f44a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115f44e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115f452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115f45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115f45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115f46040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115f464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115f46920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115f46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115f47200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115f47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115f47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115f47f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115f483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115f48830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115f48ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115f49110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115f4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115f4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115f4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115f4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115f4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115f4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115f4c010 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x115f04ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x115f05150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x115f055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x115f05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x115f05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x115f06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x115f06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x115f06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x115f07060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x115f074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x115f07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x115f07f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x115f08810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x115f08f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x115f09770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x115f09e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x115f0a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x115f0ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x115f0b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x115f0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x115f0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x115f0ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x115f0d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x115f0d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x115f0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x115f0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x115f0e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x115f0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x115f0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x115f0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x115f0fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x115f0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x115f102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x115f105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x115f10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x115f10e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x115f112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x115f11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x115f11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x115f12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x115f124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x115f12920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x115f12d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x115f13200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x115f13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x115f13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x115f13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x115f143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x115f14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x115f14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x115f15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x115f15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x115f159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x115f15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x115f162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x115f16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x115f16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x115f17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x115f17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x115f17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x115f17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x115f181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x115f18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x115f18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x115f18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x115f193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x115f19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x115f19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x115f1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x115f1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x115f1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x115f1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x115f1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x115f1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x115f1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x115f1c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x115f1c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x115f1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x115f1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x115f1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x115f1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x115f1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x115f1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x115f1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x115f1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x115f1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x115f1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x115f1f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x115f1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x115f1fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x115f20290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x115f20700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x115f20b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x115f20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x115f21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x115f218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x115f21d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x115f221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x115f22610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x115f22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x115f22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x115f23360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x115f237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x115f23c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x115f240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x115f24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x115f24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x115f24e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x115f25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x115f256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x115f25b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x115f25fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x115f26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x115f268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x115f26d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x115f27180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x115f275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x115f27a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x115f27ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x115f28340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x115f287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x115f28c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x115f29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x115f29500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x115f29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x115f29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x115f2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x115f2a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x115f2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x115f2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x115f2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x115f2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x115f2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x115f2c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x115f2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x115f2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x115f2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x115f2d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x115f2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x115f2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x115f2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x115f2e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x115f2e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x115f2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x115f2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x115f2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x115f2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x115f2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x115f303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x115f30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x115f30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x115f31140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x115f315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x115f31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x115f31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x115f32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x115f32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x115f32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x115f33050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x115f334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x115f33930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x115f33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x115f34210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x115f34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x115f34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x115f34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x115f353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x115f35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x115f35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x115f36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x115f368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x115f36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x115f37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x115f375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x115f37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x115f37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x115f38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x115f387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x115f38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x115f39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x115f39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x115f39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x115f39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x115f3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x115f3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x115f3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x115f3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x115f3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x115f3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x115f3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x115f3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x115f3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x115f3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x115f3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x115f3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x115f3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x115f3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x115f3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x115f3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x115f3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x115f3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x115f3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x115f3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x115f3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x115f3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x115f403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x115f40860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x115f40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x115f41140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x115f415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x115f41a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x115f41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x115f42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x115f42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x115f42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x115f43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x115f434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x115f43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x115f43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x115f44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x115f44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x115f44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x115f44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x115f453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x115f45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x115f45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x115f46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x115f46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x115f46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x115f46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x115f472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x115f47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x115f47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x115f48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x115f484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x115f48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x115f48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x115f491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x115f498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x115f49fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x115f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x115f4adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x115f4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x115f4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x115f4bb00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.127s
user	0m0.297s
sys	0m0.327s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4157 (f4f2a889)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124f0f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124f0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124f10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124f10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124f110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124f11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124f11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x124f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124f12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124f12c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124f13160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124f13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124f14180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124f14930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124f15140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124f15860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124f15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124f166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124f16dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124f17590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124f17cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124f183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124f19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124f19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124f19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124f1a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124f1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124f1b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124f1b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124f1bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124f1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124f1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124f1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124f1cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124f1d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124f1d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124f1ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124f1e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124f1e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124f1eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124f1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124f1f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124f1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124f1fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124f20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124f20860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124f21180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124f21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124f21da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124f223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124f229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124f22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124f235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124f23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124f24270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124f24710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124f249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124f24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124f257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124f25a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124f25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124f263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124f26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124f26d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124f27650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124f27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124f27f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124f28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124f288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124f28d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124f29210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124f296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124f29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124f29ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124f2a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124f2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124f2add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124f2b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124f2b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124f2bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124f2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124f2c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124f2c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124f2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124f2d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124f2d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124f2dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124f2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124f2e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124f2e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124f2ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124f2f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124f2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124f2fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124f30110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124f20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124f30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124f30c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124f310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124f31540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124f319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124f31e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124f32320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124f327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124f32c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124f33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124f335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124f33a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124f33ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124f34380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124f34820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124f34cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124f35160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124f35600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124f35aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124f35f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124f363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124f36880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124f36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124f371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124f37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124f37b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124f37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124f38440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124f388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124f38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124f39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124f396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124f39b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124f3a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124f3a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124f3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124f3ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124f3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124f3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124f3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124f3c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124f3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124f3ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124f3d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124f3dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124f3e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124f3e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124f3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124f3eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124f3f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124f3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124f3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124f40120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124f40670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124f40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124f41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124f41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124f41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124f41f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124f42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124f42b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124f43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124f43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124f43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124f44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124f448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124f44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124f454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124f45a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124f45f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124f464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124f46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124f46f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124f474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124f47a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124f47f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124f484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124f48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124f48f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124f494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124f49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124f49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124f4a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124f4a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124f4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124f4b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124f4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124f4bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124f4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124f4c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124f4cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124f4d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124f4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124f4df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124f4e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124f4e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124f4ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124f4f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124f4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124f4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124f50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124f50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124f50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124f51430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124f51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124f51ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124f52420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124f52970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124f52ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124f53410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124f53960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124f53eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124f54400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124f54950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124f54ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124f553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124f55940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124f55e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124f563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124f56930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124f56e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124f573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124f57920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124f57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124f58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124f587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124f58c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124f590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124f59590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124f59a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124f59ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124f5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124f5a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124f5acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124f5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124f5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124f5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124f5bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124f5c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124f5ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124f5d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124f5dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124f5df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124f5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124f5eb40 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.789 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124204ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124204f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1242053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124205830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124205ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124206110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124206580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1242069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124206e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1242073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124207850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124207ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1242089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1242091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1242099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12420a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12420a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12420af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12420b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12420be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12420c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12420cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12420d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12420da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12420e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12420e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12420e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12420eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12420f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12420f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12420f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12420fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124210280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124210540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1242109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124210e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124211290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124211700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124211b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124211fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124212450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1242128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124212d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1242131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124213610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124213a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124213ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124214360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1242147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124214c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1242150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124215520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124215990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124215e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124216270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1242166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124216c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124217150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1242175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124217a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124217ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124218310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124218780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124218bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124219060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1242194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124219940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124219db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12421a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12421a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12421ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12421af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12421b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12421b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12421bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12421c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12421c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12421ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12421ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12421d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12421d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12421dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12421e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12421e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12421e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12421ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12421f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12421f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12421fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12421ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1242203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124220830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124220ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124221110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124221580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1242219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124221e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1242222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124222740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124222bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124223020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124223490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124223900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124223d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1242241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124224650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124224ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124224f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1242253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124225810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124225c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1242260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124226560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1242269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124226e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1242272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124227720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124227b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124228000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124228470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1242288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124228d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1242291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124229630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124229aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124229f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12422a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12422a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12422ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12422b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12422b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12422b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12422be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12422c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12422c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12422cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12422cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12422d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12422d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12422dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12422e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12422e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12422ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12422eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12422f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12422f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12422fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1242300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124230520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124230990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124230e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124231270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1242316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124231b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124231fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124232430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1242328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124232d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124233180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1242335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124233a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124233ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124234340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1242347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124234c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124235090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124235500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124236090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124236350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124236610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124236a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124236ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124237360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1242377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124237c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1242380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124238520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124238990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124238e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124239270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1242396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124239b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124239fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12423a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12423a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12423ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12423b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12423b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12423ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12423bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12423c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12423c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12423cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12423d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12423d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12423d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12423dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12423e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12423e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12423eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12423efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12423f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12423f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12423fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124240160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1242405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124240a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124240eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124241320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124241790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124241c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124242070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1242424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124242950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124242dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124243230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1242436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124243b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124243f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1242443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124244860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124244cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124245140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1242455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124245a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124245e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124246300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124246770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124246be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124247050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1242474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124247930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124247da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124248210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124248680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124248af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124248f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1242493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124249f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12424a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12424ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12424b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12424b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12424b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12424be60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124f10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124f11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124f11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124f10530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124f12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124f11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x124f0f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124f4e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124f4ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124f4f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124f4f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124f4fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124f50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124f50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124f51af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124f521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124f528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124f53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124f53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124f54030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124f54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124f54e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124f55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124f55970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124f55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124f56250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124f566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124f56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124f56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124f57410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124f57880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124f57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124f57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124f58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124f58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124f58d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124f59170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124f595e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124f59a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124f59ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124f5a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124f5a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124f5ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124f5b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124f5b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124f5b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124f5bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124f5c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124f5c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124f5cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124f5cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124f5d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124f5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124f5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124f5e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124f5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124f5ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124f1cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124f1d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124f1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124f1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124f1ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124f1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124f1e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124f1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124f1f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124f1f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124f1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124f1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124f20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124f206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124f20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124f20fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124f21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124f21880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124f21cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124f22160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124f225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124f22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124f22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124f23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124f23790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124f23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124f24070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124f244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124f24950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124f24dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124f25230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124f256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124f25b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124f25f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124f263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124f26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124f26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124f27140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124f275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124f27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124f27e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124f28300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124f28770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124f28be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124f29050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124f294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124f29930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124f29da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124f2a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124f2a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124f2aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124f2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124f2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124f2b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124f2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124f2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124f2c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124f2ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124f2ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124f2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124f2d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124f2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124f2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124f2e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124f2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124f2ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124f2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124f2f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124f2fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124f2ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124f303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124f30820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124f30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124f31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124f31570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124f319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124f31e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124f322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124f32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124f32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124f33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124f33480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124f338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124f33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124f341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124f34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124f34ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124f34f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124f35390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124f35800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124f35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124f360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124f36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124f369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124f36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124f372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124f37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124f37b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124f37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124f38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124f388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124f38d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124f391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124f39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124f39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124f39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124f3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124f3a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124f3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124f3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124f3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124f3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124f3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124f3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124f3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124f3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124f3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124f3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124f3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124f3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124f3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124f3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124f3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124f3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124f3fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124f3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124f403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124f40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124f40c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124f41100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124f41570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124f419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124f41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124f422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124f42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124f42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124f43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124f43480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124f438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124f43d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124f441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124f44640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124f44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124f44f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124f45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124f45800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124f45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124f460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124f46550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124f469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124f46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124f472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124f47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124f47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124f47ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124f48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124f488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124f48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124f491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124f49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124f49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124f49f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124f4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124f4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124f4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124f4b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124f4b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124f4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124f4be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124f4c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124f4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124f4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124f4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124f4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124f4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124f4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124f4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124f1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124f1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124f1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124f12c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124f13310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124f13780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124f13bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124f14060 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.959s
user	0m0.240s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.54 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.12 sec
        1.14 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.26 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.33 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.59 sec*proc (2 tests)

Total Test time (real) =   0.60 sec
        0.60 real         0.15 user         0.04 sys
```
