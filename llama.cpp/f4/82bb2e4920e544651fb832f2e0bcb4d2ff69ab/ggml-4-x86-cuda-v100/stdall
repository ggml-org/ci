Requirement already satisfied: numpy~=1.24.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.8.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.2.0)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: typing-extensions in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.8.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.8.0-py3-none-any.whl size=3229 sha256=e3aafedf1f270ac30d9f1b3b317301ce657a1cae1c7f5a947ba0148604ce0003
  Stored in directory: /tmp/pip-ephem-wheel-cache-gti6nz7c/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.8.0
    Uninstalling gguf-0.8.0:
      Successfully uninstalled gguf-0.8.0
Successfully installed gguf-0.8.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.2s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m3.477s
user	0m2.520s
sys	0m0.940s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_debug-make.log
+ make -j
[  0%] Generating build details from Git
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Built target json-schema-to-grammar
[  5%] Built target ggml
[  5%] Built target build_info
[  5%] Linking CUDA static library libggml_static.a
[  6%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  8%] Built target ggml_static
[  8%] Built target gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target llama
[ 10%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 12%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 13%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 15%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 17%] Linking CXX executable ../bin/test-c
[ 17%] Built target test-c
[ 17%] Built target llava
[ 18%] Linking CXX executable ../../bin/benchmark
[ 19%] Linking CXX static library libllava_static.a
[ 19%] Built target llava_static
[ 19%] Built target benchmark
[ 19%] Linking CXX executable ../../bin/quantize
[ 19%] Built target quantize
[ 19%] Linking CXX executable ../../bin/quantize-stats
[ 19%] Built target quantize-stats
[ 20%] Linking CXX static library libcommon.a
[ 20%] Built target common
[ 20%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 30%] Linking CXX executable ../bin/test-quantize-perf
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Linking CXX executable ../bin/test-quantize-fns
[ 33%] Linking CXX executable ../bin/test-chat-template
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Linking CXX executable ../bin/test-sampling
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-grad0
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-grammar-parser
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Linking CXX executable ../bin/test-rope
[ 55%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 55%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../../bin/baby-llama
[ 58%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 61%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 61%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 61%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 61%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 61%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 62%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 63%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 63%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 63%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 64%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 65%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 65%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 66%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 67%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 68%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 69%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 74%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 74%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 75%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 76%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 76%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 77%] Linking CXX executable ../../bin/vdot
[ 78%] Linking CXX executable ../../bin/q8dot
[ 78%] Built target test-quantize-perf
[ 78%] Built target test-quantize-fns
[ 78%] Built target vdot
[ 78%] Built target q8dot
[ 78%] Built target test-rope
[ 78%] Built target test-grammar-parser
[ 78%] Built target test-grad0
[ 78%] Built target test-backend-ops
[ 78%] Built target test-json-schema-to-grammar
[ 78%] Built target baby-llama
[ 78%] Built target test-chat-template
[ 78%] Built target test-sampling
[ 78%] Built target test-autorelease
[ 78%] Built target test-model-load-cancel
[ 79%] Linking CXX executable ../../bin/tokenize
[ 79%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 80%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 81%] Linking CXX executable ../../bin/gguf-split
[ 81%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 82%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 83%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 83%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 84%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 84%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 85%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 86%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 87%] Linking CXX executable ../../bin/lookup-merge
[ 88%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 88%] Linking CXX executable ../../bin/beam-search
[ 88%] Linking CXX executable ../../bin/train-text-from-scratch
[ 89%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 90%] Linking CXX executable ../../bin/lookup-create
[ 90%] Linking CXX executable ../../bin/infill
[ 91%] Linking CXX executable ../../bin/save-load-state
[ 92%] Linking CXX executable ../../bin/batched
[ 92%] Built target lookup-merge
[ 92%] Linking CXX executable ../../bin/simple
[ 92%] Linking CXX executable ../../bin/llava-cli
[ 92%] Linking CXX executable ../../bin/lookahead
[ 92%] Linking CXX executable ../../bin/batched-bench
[ 93%] Linking CXX executable ../../bin/gritlm
[ 93%] Linking CXX executable ../../bin/embedding
[ 94%] Linking CXX executable ../../bin/passkey
[ 95%] Linking CXX executable ../../bin/finetune
[ 96%] Linking CXX executable ../../bin/lookup-stats
[ 96%] Built target gguf-split
[ 96%] Linking CXX executable ../../bin/parallel
[ 97%] Linking CXX executable ../../bin/export-lora
[ 97%] Linking CXX executable ../../bin/lookup
[ 97%] Built target test-tokenizer-1-gpt-neox
[ 97%] Built target tokenize
[ 97%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 97%] Built target export-lora
[ 97%] Built target lookup-create
[ 97%] Built target convert-llama2c-to-ggml
[ 97%] Built target simple
[ 97%] Built target test-tokenizer-1-gpt2
[ 97%] Built target test-tokenizer-1-refact
[ 97%] Built target train-text-from-scratch
[ 97%] Built target test-tokenizer-1-llama
[ 97%] Built target test-tokenizer-1-baichuan
[ 97%] Linking CXX executable ../../bin/main
[ 97%] Built target test-tokenizer-1-falcon
[ 97%] Built target test-tokenizer-1-aquila
[ 97%] Built target test-tokenizer-0-llama
[ 97%] Built target beam-search
[ 97%] Built target gritlm
[ 97%] Built target save-load-state
[ 97%] Built target test-tokenizer-1-stablelm-3b-4e1t
[ 97%] Built target test-tokenizer-1-starcoder
[ 97%] Built target infill
[ 97%] Built target lookup-stats
[ 97%] Built target lookahead
[ 97%] Built target test-tokenizer-0-falcon
[ 97%] Built target batched
[ 97%] Built target batched-bench
[ 97%] Linking CXX executable ../../bin/imatrix
[ 97%] Built target test-tokenizer-1-mpt
[ 97%] Built target llava-cli
[ 97%] Linking CXX executable ../../bin/speculative
[ 97%] Built target passkey
[ 97%] Built target finetune
[ 97%] Built target lookup
[ 97%] Built target embedding
[ 97%] Built target parallel
[ 97%] Built target main
[ 97%] Built target speculative
[ 98%] Linking CXX executable ../../bin/perplexity
[ 98%] Built target imatrix
[ 98%] Built target perplexity
[ 99%] Linking CXX executable ../../bin/llama-bench
[ 99%] Built target llama-bench
[ 99%] Linking CXX executable ../bin/test-llama-grammar
[ 99%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	0m51.020s
user	2m19.668s
sys	0m16.848s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-quantize-fns
 1/22 Test  #1: test-quantize-fns ...................   Passed   30.15 sec
      Start  2: test-quantize-perf
 2/22 Test  #2: test-quantize-perf ..................   Passed   10.61 sec
      Start  3: test-sampling
 3/22 Test  #3: test-sampling .......................   Passed    0.07 sec
      Start  4: test-chat-template
 4/22 Test  #4: test-chat-template ..................   Passed    0.04 sec
      Start  5: test-tokenizer-0-llama
 5/22 Test  #5: test-tokenizer-0-llama ..............   Passed    0.22 sec
      Start  6: test-tokenizer-0-falcon
 6/22 Test  #6: test-tokenizer-0-falcon .............   Passed    1.37 sec
      Start  7: test-tokenizer-1-llama
 7/22 Test  #7: test-tokenizer-1-llama ..............   Passed    4.22 sec
      Start  8: test-tokenizer-1-baichuan
 8/22 Test  #8: test-tokenizer-1-baichuan ...........   Passed    4.12 sec
      Start  9: test-tokenizer-1-falcon
 9/22 Test  #9: test-tokenizer-1-falcon .............   Passed    8.46 sec
      Start 10: test-tokenizer-1-aquila
10/22 Test #10: test-tokenizer-1-aquila .............   Passed   12.16 sec
      Start 11: test-tokenizer-1-mpt
11/22 Test #11: test-tokenizer-1-mpt ................   Passed    6.66 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/22 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    6.64 sec
      Start 13: test-tokenizer-1-gpt-neox
13/22 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    6.74 sec
      Start 14: test-tokenizer-1-refact
14/22 Test #14: test-tokenizer-1-refact .............   Passed    6.75 sec
      Start 15: test-tokenizer-1-starcoder
15/22 Test #15: test-tokenizer-1-starcoder ..........   Passed    6.55 sec
      Start 16: test-tokenizer-1-gpt2
16/22 Test #16: test-tokenizer-1-gpt2 ...............   Passed    6.54 sec
      Start 17: test-grammar-parser
17/22 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/22 Test #18: test-llama-grammar ..................   Passed    0.04 sec
      Start 19: test-grad0
19/22 Test #19: test-grad0 ..........................   Passed    4.26 sec
      Start 20: test-backend-ops
20/22 Test #20: test-backend-ops ....................   Passed   96.46 sec
      Start 21: test-rope
21/22 Test #21: test-rope ...........................   Passed    0.08 sec
      Start 24: test-json-schema-to-grammar
22/22 Test #24: test-json-schema-to-grammar .........   Passed    1.42 sec

100% tests passed, 0 tests failed out of 22

Label Time Summary:
main    = 213.54 sec*proc (22 tests)

Total Test time (real) = 213.56 sec

real	3m33.589s
user	5m26.308s
sys	0m10.549s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.3s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.588s
user	0m2.649s
sys	0m0.935s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_release-make.log
+ make -j
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Built target json-schema-to-grammar
[  5%] Built target build_info
[  5%] Built target ggml
[  5%] Linking CUDA static library libggml_static.a
[  5%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  8%] Built target ggml_static
[  8%] Built target gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target llama
[ 10%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 11%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 14%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 16%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 17%] Linking CXX executable ../bin/test-c
[ 17%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 17%] Built target test-c
[ 18%] Linking CXX executable ../../bin/benchmark
[ 18%] Built target llava
[ 18%] Built target benchmark
[ 19%] Linking CXX static library libllava_static.a
[ 19%] Built target llava_static
[ 19%] Linking CXX executable ../../bin/quantize
[ 19%] Built target quantize
[ 19%] Linking CXX executable ../../bin/quantize-stats
[ 19%] Built target quantize-stats
[ 20%] Linking CXX static library libcommon.a
[ 20%] Built target common
[ 20%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 31%] Linking CXX executable ../bin/test-sampling
[ 32%] Linking CXX executable ../bin/test-quantize-fns
[ 33%] Linking CXX executable ../bin/test-chat-template
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Linking CXX executable ../bin/test-quantize-perf
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-grad0
[ 50%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 51%] Linking CXX executable ../bin/test-grammar-parser
[ 52%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-rope
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 54%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 58%] Linking CXX executable ../../bin/baby-llama
[ 58%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 59%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 60%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 61%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 61%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 62%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 62%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 63%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 63%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 64%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 64%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 64%] Built target test-grammar-parser
[ 65%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 65%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 66%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 67%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 68%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 68%] Built target test-quantize-perf
[ 69%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target test-quantize-fns
[ 72%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 74%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 74%] Built target test-rope
[ 74%] Built target test-grad0
[ 74%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 75%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 76%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 76%] Built target test-json-schema-to-grammar
[ 76%] Built target test-sampling
[ 77%] Linking CXX executable ../../bin/vdot
[ 77%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 78%] Linking CXX executable ../../bin/q8dot
[ 78%] Built target test-chat-template
[ 78%] Built target test-model-load-cancel
[ 78%] Built target baby-llama
[ 78%] Built target test-backend-ops
[ 78%] Built target test-autorelease
[ 78%] Built target vdot
[ 78%] Built target q8dot
[ 79%] Linking CXX executable ../../bin/tokenize
[ 79%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 79%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 80%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 80%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 81%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 82%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 83%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 84%] Linking CXX executable ../../bin/lookup-merge
[ 84%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 85%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 86%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 86%] Built target tokenize
[ 86%] Built target lookup-merge
[ 86%] Built target test-tokenizer-1-falcon
[ 86%] Built target test-tokenizer-1-baichuan
[ 86%] Built target test-tokenizer-1-llama
[ 86%] Built target test-tokenizer-1-mpt
[ 87%] Linking CXX executable ../../bin/save-load-state
[ 87%] Built target test-tokenizer-1-aquila
[ 87%] Built target test-tokenizer-1-refact
[ 87%] Built target test-tokenizer-1-stablelm-3b-4e1t
[ 87%] Built target test-tokenizer-1-gpt-neox
[ 87%] Linking CXX executable ../../bin/llava-cli
[ 88%] Linking CXX executable ../../bin/gritlm
[ 88%] Built target test-tokenizer-1-gpt2
[ 89%] Linking CXX executable ../../bin/lookup-create
[ 89%] Built target test-tokenizer-1-starcoder
[ 89%] Linking CXX executable ../../bin/beam-search
[ 90%] Linking CXX executable ../../bin/gguf-split
[ 91%] Linking CXX executable ../../bin/lookup-stats
[ 91%] Linking CXX executable ../../bin/batched-bench
[ 91%] Built target save-load-state
[ 91%] Linking CXX executable ../../bin/simple
[ 92%] Linking CXX executable ../../bin/batched
[ 92%] Built target gritlm
[ 92%] Built target llava-cli
[ 92%] Built target beam-search
[ 92%] Linking CXX executable ../../bin/embedding
[ 92%] Built target lookup-create
[ 92%] Built target batched
[ 93%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 94%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 94%] Built target lookup-stats
[ 94%] Built target batched-bench
[ 94%] Built target gguf-split
[ 95%] Linking CXX executable ../../bin/export-lora
[ 95%] Built target simple
[ 95%] Built target embedding
[ 96%] Linking CXX executable ../../bin/passkey
[ 96%] Built target export-lora
[ 96%] Built target test-tokenizer-0-falcon
[ 96%] Built target test-tokenizer-0-llama
[ 96%] Linking CXX executable ../../bin/lookup
[ 96%] Linking CXX executable ../../bin/lookahead
[ 96%] Linking CXX executable ../../bin/train-text-from-scratch
[ 96%] Built target passkey
[ 96%] Linking CXX executable ../../bin/parallel
[ 96%] Built target parallel
[ 96%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 96%] Built target lookup
[ 96%] Built target lookahead
[ 96%] Built target train-text-from-scratch
[ 96%] Built target convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/imatrix
[ 96%] Built target imatrix
[ 96%] Linking CXX executable ../../bin/infill
[ 97%] Linking CXX executable ../../bin/finetune
[ 97%] Linking CXX executable ../../bin/speculative
[ 97%] Built target infill
[ 97%] Built target finetune
[ 97%] Built target speculative
[ 97%] Linking CXX executable ../../bin/main
[ 97%] Built target main
[ 98%] Linking CXX executable ../../bin/perplexity
[ 98%] Built target perplexity
[ 99%] Linking CXX executable ../../bin/llama-bench
[ 99%] Built target llama-bench
[ 99%] Linking CXX executable ../bin/test-llama-grammar
[ 99%] Built target test-llama-grammar
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	1m19.957s
user	2m59.130s
sys	0m12.132s
+ '[' -z ']'
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /home/ggml/work/llama.cpp/build-ci-release
      Start  1: test-quantize-fns
 1/22 Test  #1: test-quantize-fns ...................   Passed   16.91 sec
      Start  2: test-quantize-perf
 2/22 Test  #2: test-quantize-perf ..................   Passed    6.13 sec
      Start  3: test-sampling
 3/22 Test  #3: test-sampling .......................   Passed    0.05 sec
      Start  4: test-chat-template
 4/22 Test  #4: test-chat-template ..................   Passed    0.04 sec
      Start  5: test-tokenizer-0-llama
 5/22 Test  #5: test-tokenizer-0-llama ..............   Passed    0.10 sec
      Start  6: test-tokenizer-0-falcon
 6/22 Test  #6: test-tokenizer-0-falcon .............   Passed    0.33 sec
      Start  7: test-tokenizer-1-llama
 7/22 Test  #7: test-tokenizer-1-llama ..............   Passed    0.59 sec
      Start  8: test-tokenizer-1-baichuan
 8/22 Test  #8: test-tokenizer-1-baichuan ...........   Passed    0.60 sec
      Start  9: test-tokenizer-1-falcon
 9/22 Test  #9: test-tokenizer-1-falcon .............   Passed    1.15 sec
      Start 10: test-tokenizer-1-aquila
10/22 Test #10: test-tokenizer-1-aquila .............   Passed    1.68 sec
      Start 11: test-tokenizer-1-mpt
11/22 Test #11: test-tokenizer-1-mpt ................   Passed    0.91 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/22 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    1.05 sec
      Start 13: test-tokenizer-1-gpt-neox
13/22 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    0.98 sec
      Start 14: test-tokenizer-1-refact
14/22 Test #14: test-tokenizer-1-refact .............   Passed    0.92 sec
      Start 15: test-tokenizer-1-starcoder
15/22 Test #15: test-tokenizer-1-starcoder ..........   Passed    0.90 sec
      Start 16: test-tokenizer-1-gpt2
16/22 Test #16: test-tokenizer-1-gpt2 ...............   Passed    0.94 sec
      Start 17: test-grammar-parser
17/22 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/22 Test #18: test-llama-grammar ..................   Passed    0.04 sec
      Start 19: test-grad0
19/22 Test #19: test-grad0 ..........................   Passed    4.19 sec
      Start 20: test-backend-ops
20/22 Test #20: test-backend-ops ....................   Passed   36.99 sec
      Start 21: test-rope
21/22 Test #21: test-rope ...........................   Passed    0.06 sec
      Start 24: test-json-schema-to-grammar
22/22 Test #24: test-json-schema-to-grammar .........   Passed    1.38 sec

100% tests passed, 0 tests failed out of 22

Label Time Summary:
main    =  75.96 sec*proc (22 tests)

Total Test time (real) =  75.98 sec

real	1m16.012s
user	1m17.784s
sys	0m7.901s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/embd_bge_small.log
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:32:47 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model:
2024-03-23 17:32:48 ERROR 404: Not Found.
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:32:48 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:32:48 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:32:49 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:32:49 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:32:49 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:32:50 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:32:50 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.3s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.559s
user	0m2.696s
sys	0m0.859s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/embd_bge_small-make.log
+ make -j
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Built target build_info
[  5%] Built target json-schema-to-grammar
[  5%] Built target ggml
[  5%] Linking CUDA static library libggml_static.a
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  6%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[ 10%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 10%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 11%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 15%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 16%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 17%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 17%] Linking CXX executable ../bin/test-c
[ 18%] Linking CXX executable ../../bin/benchmark
[ 18%] Linking CXX executable ../../bin/quantize
[ 18%] Built target gguf
[ 18%] Linking CXX executable ../../bin/quantize-stats
[ 18%] Built target llava
[ 19%] Linking CXX static library libcommon.a
[ 20%] Linking CXX static library libllava_static.a
[ 20%] Built target test-c
[ 20%] Built target benchmark
[ 20%] Built target llava_static
[ 20%] Built target common
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 26%] Linking CXX executable ../bin/test-quantize-fns
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 28%] Built target quantize
[ 29%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 30%] Linking CXX executable ../bin/test-quantize-perf
[ 31%] Linking CXX executable ../bin/test-chat-template
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 34%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Linking CXX executable ../bin/test-sampling
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Built target quantize-stats
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 46%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 48%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 48%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 49%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 50%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 52%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 53%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-grammar-parser
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-grad0
[ 59%] Linking CXX executable ../bin/test-llama-grammar
[ 60%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 60%] Linking CXX executable ../bin/test-rope
[ 61%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 61%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 63%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 64%] Linking CXX executable ../../bin/baby-llama
[ 65%] Linking CXX executable ../bin/test-model-load-cancel
[ 66%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 67%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 68%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 69%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 69%] Linking CXX executable ../bin/test-autorelease
[ 69%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 69%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 70%] Linking CXX executable ../../bin/batched
[ 70%] Linking CXX executable ../../bin/batched-bench
[ 70%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 70%] Built target test-quantize-perf
[ 70%] Linking CXX executable ../../bin/beam-search
[ 70%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 70%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 70%] Linking CXX executable ../../bin/embedding
[ 71%] Linking CXX executable ../../bin/finetune
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 73%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 73%] Built target test-quantize-fns
[ 74%] Linking CXX executable ../../bin/gritlm
[ 75%] Linking CXX executable ../../bin/gguf-split
[ 75%] Linking CXX executable ../../bin/infill
[ 76%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-bench
[ 77%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llava-cli
[ 78%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 79%] Built target test-tokenizer-0-falcon
[ 79%] Linking CXX executable ../../bin/main
[ 80%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 81%] Linking CXX executable ../../bin/tokenize
[ 81%] Linking CXX executable ../../bin/parallel
[ 82%] Linking CXX executable ../../bin/perplexity
[ 83%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 85%] Linking CXX executable ../../bin/save-load-state
[ 86%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 87%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 88%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 88%] Linking CXX executable ../../bin/simple
[ 88%] Built target test-chat-template
[ 89%] Linking CXX executable ../../bin/passkey
[ 89%] Built target test-grammar-parser
[ 89%] Linking CXX executable ../../bin/speculative
[ 89%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 89%] Built target test-sampling
[ 89%] Linking CXX executable ../../bin/lookahead
[ 89%] Built target test-rope
[ 89%] Linking CXX executable ../../bin/lookup
[ 89%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 90%] Linking CXX executable ../../bin/lookup-create
[ 91%] Linking CXX executable ../../bin/lookup-merge
[ 92%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 93%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 94%] Linking CXX executable ../../bin/lookup-stats
[ 94%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 95%] Built target test-tokenizer-1-baichuan
[ 96%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/train-text-from-scratch
[ 96%] Built target test-grad0
[ 97%] Linking CXX executable ../../bin/vdot
[ 97%] Linking CXX executable ../../bin/imatrix
[ 97%] Built target test-tokenizer-1-stablelm-3b-4e1t
[ 97%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/export-lora
[ 98%] Built target test-tokenizer-1-llama
[ 98%] Built target test-json-schema-to-grammar
[ 99%] Linking CXX executable ../../bin/server
[ 99%] Built target test-tokenizer-0-llama
[ 99%] Built target test-tokenizer-1-falcon
[100%] Linking CXX executable ../../bin/q8dot
[100%] Built target convert-llama2c-to-ggml
[100%] Built target test-tokenizer-1-aquila
[100%] Built target test-tokenizer-1-gpt-neox
[100%] Built target test-tokenizer-1-mpt
[100%] Built target test-tokenizer-1-starcoder
[100%] Built target batched
[100%] Built target test-tokenizer-1-gpt2
[100%] Built target test-llama-grammar
[100%] Built target test-autorelease
[100%] Built target beam-search
[100%] Built target test-tokenizer-1-refact
[100%] Built target vdot
[100%] Built target test-model-load-cancel
[100%] Built target lookup-merge
[100%] Built target test-backend-ops
[100%] Built target llama-bench
[100%] Built target parallel
[100%] Built target embedding
[100%] Built target baby-llama
[100%] Built target tokenize
[100%] Built target gguf-split
[100%] Built target batched-bench
[100%] Built target gritlm
[100%] Built target finetune
[100%] Built target llava-cli
[100%] Built target infill
[100%] Built target main
[100%] Built target simple
[100%] Built target save-load-state
[100%] Built target speculative
[100%] Built target q8dot
[100%] Built target perplexity
[100%] Built target lookup-create
[100%] Built target lookup
[100%] Built target lookup-stats
[100%] Built target imatrix
[100%] Built target server
[100%] Built target train-text-from-scratch
[100%] Built target passkey
[100%] Built target lookahead
[100%] Built target export-lora

real	0m2.561s
user	0m10.394s
sys	0m3.606s
+ python3 ../convert-hf-to-gguf.py ../models-mnt/bge-small
/mnt/llama.cpp/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading model: bge-small
gguf: This GGUF file is for Little Endian only
Set model parameters
gguf: context length = 512
gguf: embedding length = 384
gguf: feed forward length = 1536
gguf: head count = 12
gguf: layer norm epsilon = 1e-12
gguf: file type = 1
Set model tokenizer
fname_tokenizer: ../models-mnt/bge-small
gguf: Setting special token type pad to 0
Exporting model to '../models-mnt/bge-small/ggml-model-f16.gguf'
gguf: loading model part 'pytorch_model.bin'
token_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
position_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
token_types.weight, n_dims = 2, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
Model successfully exported to '../models-mnt/bge-small/ggml-model-f16.gguf'
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llama_model_quantize_internal ============ Strange model: n_attention_wv = 12, n_ffn_down = 24, hparams.n_layer = 12
llama_model_quantize_internal: meta size = 760800 bytes
[   1/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   2/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f16, size =    0.375 MB
[   3/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   6/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   7/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   8/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   9/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  10/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  11/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  13/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  14/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  16/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  17/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  18/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  19/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  20/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  23/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  24/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  25/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  26/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  27/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  29/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  30/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  32/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  33/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  34/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  35/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  36/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  39/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  40/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  41/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  42/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  43/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  45/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  46/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  48/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  49/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  50/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  51/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  52/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  55/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  56/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  57/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  58/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  59/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  61/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  62/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  64/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  65/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  66/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  67/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  68/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  71/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  72/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  73/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  74/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  75/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  77/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  78/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  80/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  81/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  82/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  83/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  84/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  87/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  88/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  89/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  90/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  91/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  93/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  94/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  96/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  97/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  98/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  99/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 100/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 103/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 104/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 105/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 106/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 107/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 109/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 110/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 112/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 113/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 114/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 115/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 116/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 119/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 120/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 121/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 122/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 123/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 125/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 126/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 128/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 129/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 130/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 131/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 132/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 135/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 136/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 137/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 138/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 139/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 141/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 142/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 144/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 145/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 146/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 147/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 148/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 151/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 152/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 153/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 154/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 155/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 157/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 158/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 160/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 161/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 162/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 163/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 164/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 167/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 168/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 169/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 170/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 171/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 173/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 174/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 176/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 177/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 178/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 179/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 180/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 183/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 184/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 185/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 186/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 187/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 189/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 190/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 192/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 193/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 194/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 195/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 196/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_internal: model size  =    63.46 MB
llama_model_quantize_internal: quant size  =    34.00 MB

main: quantize time =   156.13 ms
main:    total time =   156.13 ms
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/embd_bge_small-tg-f16.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is'
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215179
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 63.46 MiB (16.03 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/13 layers to GPU
llm_load_tensors:        CPU buffer size =    63.46 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =   241.45 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    16.28 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     2.51 MiB
llama_new_context_with_model: graph nodes  = 429
llama_new_context_with_model: graph splits = 196

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1

llama_print_timings:        load time =     637.73 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =      10.76 ms /     9 tokens (    1.20 ms per token,   836.74 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =     116.32 ms /    10 tokens

embedding 0: -0.043980 -0.019912  0.007672 -0.000847  0.001357 -0.037027  0.109449  0.042568  0.092071 -0.015919  0.006763 -0.035700 -0.017880  0.015042  0.018139  0.015873 

cosine similarity matrix:

  1.00 

real	0m1.013s
user	0m0.289s
sys	0m0.675s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/embd_bge_small-tg-q8_0.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is'
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215180
llama_model_loader: loaded meta data with 20 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 7
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q8_0:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 34.00 MiB (8.59 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/13 layers to GPU
llm_load_tensors:        CPU buffer size =    34.00 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =   241.45 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    16.90 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     2.51 MiB
llama_new_context_with_model: graph nodes  = 429
llama_new_context_with_model: graph splits = 196

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1

llama_print_timings:        load time =     566.15 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       8.91 ms /     9 tokens (    0.99 ms per token,  1010.33 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =     109.93 ms /    10 tokens

embedding 0: -0.044454 -0.019806  0.008425 -0.001658  0.001779 -0.036759  0.109019  0.043373  0.091165 -0.015181  0.006789 -0.035570 -0.019173  0.013641  0.017719  0.015106 

cosine similarity matrix:

  1.00 

real	0m0.944s
user	0m0.267s
sys	0m0.693s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_open_llama_7b_v2
+ tee /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:33:01 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json [502/502] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:33:02 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json [593/593] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:33:02 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json [330/330] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:33:02 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json [26788/26788] -> "pytorch_model.bin.index.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:33:03 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json [132/132] -> "generation_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /home/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ path_models=../models-mnt/open-llama/7B-v2
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.5s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.796s
user	0m2.875s
sys	0m0.913s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-make.log
+ make -j
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Building CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o
[  5%] Built target build_info
[  5%] Built target json-schema-to-grammar
[  5%] Built target ggml
[  5%] Linking CUDA static library libggml_static.a
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  6%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[  9%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 10%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 13%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 14%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 14%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 15%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 16%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 16%] Built target gguf
[ 16%] Linking CXX executable ../bin/test-c
[ 17%] Linking CXX executable ../../bin/benchmark
[ 18%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 18%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 18%] Linking CXX executable ../../bin/quantize-stats
[ 18%] Built target llava
[ 18%] Linking CXX executable ../../bin/quantize
[ 19%] Linking CXX static library libcommon.a
[ 20%] Linking CXX static library libllava_static.a
[ 20%] Built target test-c
[ 20%] Built target benchmark
[ 20%] Built target llava_static
[ 20%] Built target common
[ 20%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 29%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 30%] Linking CXX executable ../bin/test-quantize-fns
[ 31%] Linking CXX executable ../bin/test-chat-template
[ 31%] Linking CXX executable ../bin/test-sampling
[ 32%] Linking CXX executable ../bin/test-quantize-perf
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o
[ 42%] Built target quantize
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-mpt
[ 43%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-falcon
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-gpt-neox
[ 45%] Building CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-refact
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-baichuan
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-aquila
[ 47%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 48%] Built target quantize-stats
[ 48%] Building CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t
[ 52%] Linking CXX executable ../bin/test-grammar-parser
[ 53%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-tokenizer-1-starcoder
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Linking CXX executable ../bin/test-tokenizer-1-gpt2
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-llama-grammar
[ 59%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 60%] Linking CXX executable ../bin/test-grad0
[ 61%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-rope
[ 61%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 62%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 63%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 64%] Linking CXX executable ../bin/test-model-load-cancel
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 65%] Linking CXX executable ../bin/test-autorelease
[ 65%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 66%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 67%] Linking CXX executable ../../bin/baby-llama
[ 67%] Linking CXX executable ../../bin/batched-bench
[ 67%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 68%] Linking CXX executable ../../bin/batched
[ 68%] Built target test-quantize-fns
[ 69%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 70%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 70%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 70%] Linking CXX executable ../../bin/beam-search
[ 70%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 70%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 71%] Linking CXX executable ../../bin/finetune
[ 71%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 72%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 72%] Linking CXX executable ../../bin/embedding
[ 73%] Linking CXX executable ../../bin/gguf-split
[ 74%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 74%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 75%] Linking CXX executable ../../bin/gritlm
[ 76%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 76%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 76%] Linking CXX executable ../../bin/llava-cli
[ 77%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/main
[ 77%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-bench
[ 80%] Linking CXX executable ../../bin/tokenize
[ 80%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 80%] Linking CXX executable ../../bin/infill
[ 81%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 81%] Linking CXX executable ../../bin/parallel
[ 81%] Built target test-grad0
[ 82%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 84%] Linking CXX executable ../../bin/perplexity
[ 85%] Linking CXX executable ../../bin/save-load-state
[ 85%] Built target test-chat-template
[ 85%] Linking CXX executable ../../bin/simple
[ 86%] Linking CXX executable ../../bin/passkey
[ 87%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 87%] Built target test-grammar-parser
[ 88%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 89%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 89%] Built target test-llama-grammar
[ 89%] Linking CXX executable ../../bin/speculative
[ 89%] Linking CXX executable ../../bin/lookahead
[ 90%] Linking CXX executable ../../bin/lookup-create
[ 90%] Linking CXX executable ../../bin/lookup
[ 90%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 90%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 91%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 91%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 92%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 93%] Linking CXX executable ../../bin/lookup-merge
[ 94%] Linking CXX executable ../../bin/lookup-stats
[ 95%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 95%] Built target test-tokenizer-1-mpt
[ 95%] Built target test-rope
[ 95%] Linking CXX executable ../../bin/train-text-from-scratch
[ 95%] Built target test-tokenizer-1-gpt-neox
[ 96%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 96%] Built target convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 96%] Built target test-tokenizer-0-llama
[ 96%] Linking CXX executable ../../bin/imatrix
[ 97%] Linking CXX executable ../../bin/export-lora
[ 98%] Linking CXX executable ../../bin/server
[ 99%] Linking CXX executable ../../bin/vdot
[100%] Linking CXX executable ../../bin/q8dot
[100%] Built target test-model-load-cancel
[100%] Built target test-json-schema-to-grammar
[100%] Built target test-tokenizer-1-llama
[100%] Built target test-sampling
[100%] Built target test-tokenizer-0-falcon
[100%] Built target test-backend-ops
[100%] Built target test-tokenizer-1-starcoder
[100%] Built target test-tokenizer-1-falcon
[100%] Built target test-tokenizer-1-baichuan
[100%] Built target test-tokenizer-1-aquila
[100%] Built target test-tokenizer-1-refact
[100%] Built target test-tokenizer-1-stablelm-3b-4e1t
[100%] Built target test-tokenizer-1-gpt2
[100%] Built target baby-llama
[100%] Built target batched-bench
[100%] Built target batched
[100%] Built target beam-search
[100%] Built target test-autorelease
[100%] Built target main
[100%] Built target parallel
[100%] Built target gguf-split
[100%] Built target lookup-merge
[100%] Built target finetune
[100%] Built target tokenize
[100%] Built target embedding
[100%] Built target gritlm
[100%] Built target simple
[100%] Built target passkey
[100%] Built target infill
[100%] Built target llama-bench
[100%] Built target save-load-state
[100%] Built target llava-cli
[100%] Built target perplexity
[100%] Built target lookup-create
[100%] Built target q8dot
[100%] Built target vdot
[100%] Built target imatrix
[100%] Built target export-lora
[100%] Built target lookup
[100%] Built target lookahead
[100%] Built target speculative
[100%] Built target train-text-from-scratch
[100%] Built target lookup-stats
[100%] Built target server

real	0m2.752s
user	0m10.860s
sys	0m3.868s
+ python3 ../convert.py ../models-mnt/open-llama/7B-v2
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00002-of-00002.bin
params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/7B-v2'))
Found vocab files: {'spm': PosixPath('../models-mnt/open-llama/7B-v2/tokenizer.model'), 'bpe': None, 'hfft': None}
Loading vocab file PosixPath('../models-mnt/open-llama/7B-v2/tokenizer.model'), type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
Permuting layer 26
Permuting layer 27
Permuting layer 28
Permuting layer 29
Permuting layer 30
Permuting layer 31
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.0.attn_rot_embd
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.1.attn_rot_embd
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.2.attn_rot_embd
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.3.attn_rot_embd
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.4.attn_rot_embd
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.5.attn_rot_embd
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.6.attn_rot_embd
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.7.attn_rot_embd
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.8.attn_rot_embd
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.9.attn_rot_embd
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.10.attn_rot_embd
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.11.attn_rot_embd
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.12.attn_rot_embd
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.13.attn_rot_embd
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.14.attn_rot_embd
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.15.attn_rot_embd
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.16.attn_rot_embd
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.17.attn_rot_embd
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.18.attn_rot_embd
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.19.attn_rot_embd
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.20.attn_rot_embd
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.21.attn_rot_embd
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.22.attn_rot_embd
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.23.attn_rot_embd
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.24.attn_rot_embd
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.25.attn_rot_embd
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]
model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.26.attn_rot_embd
model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]
model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]
model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.27.attn_rot_embd
model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]
model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]
model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.28.attn_rot_embd
model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]
model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]
model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.29.attn_rot_embd
model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]
model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]
model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.30.attn_rot_embd
model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]
model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]
model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.31.attn_rot_embd
model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]
model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]
model.norm.weight                                -> output_norm.weight                       | F16    | [4096]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]
Writing ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf, format 1
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0
[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0
[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0
[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0
[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0
[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0
[  7/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0
[  8/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0
[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0
[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0
[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0
[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 16/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 17/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 25/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 26/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 34/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 35/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 43/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 44/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 52/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 53/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 61/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 62/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[ 70/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 71/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[ 79/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 80/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[ 88/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 89/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3
[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[ 97/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[ 98/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   4
[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[106/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[107/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   4
[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[115/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[116/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   4
[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[124/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[125/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   5
[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5
[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[133/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[134/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   5
[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5
[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[142/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[143/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   5
[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5
[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[151/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[152/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   6
[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6
[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[160/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[161/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   6
[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6
[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[169/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[170/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   6
[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6
[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[178/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[179/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7
[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   7
[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[187/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[188/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7
[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   7
[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[196/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[197/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7
[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   7
[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[205/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[206/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   8
[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[214/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[215/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   8
[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[223/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[224/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   8
[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[232/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[233/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   9
[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9
[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9
[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9
[241/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9
[242/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9
[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+   9
[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9
[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9
[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9
[250/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9
[251/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9
[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+   9
[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9
[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9
[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9
[259/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9
[260/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9
[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  10
[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10
[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10
[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10
[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10
[268/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10
[269/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10
[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  10
[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10
[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10
[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10
[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10
[277/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10
[278/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10
[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  10
[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10
[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10
[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10
[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10
[286/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10
[287/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10
[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  10
[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  10
[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+  11
Wrote ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_f16=../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf
+ wiki_test=../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf q8_0
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   250.00 MiB ->   132.81 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q8_0 .. size =    86.00 MiB ->    45.69 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   250.00 MiB ->   132.81 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  6828.64 MB

main: quantize time = 47847.42 ms
main:    total time = 47847.42 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf q4_0
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_0 .. size =   250.00 MiB ->    70.31 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_0 .. size =    86.00 MiB ->    24.19 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3647.87 MB

main: quantize time = 25422.65 ms
main:    total time = 25422.65 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf q4_1
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_1 .. size =   250.00 MiB ->    78.12 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_1 .. size =    86.00 MiB ->    26.88 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4041.68 MB

main: quantize time = 27660.72 ms
main:    total time = 27660.72 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf q5_0
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q5_0 .. size =   250.00 MiB ->    85.94 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_0 .. size =    86.00 MiB ->    29.56 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4435.49 MB

main: quantize time = 30757.39 ms
main:    total time = 30757.39 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf q5_1
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q5_1 .. size =   250.00 MiB ->    93.75 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_1 .. size =    86.00 MiB ->    32.25 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4829.30 MB

main: quantize time = 32872.45 ms
main:    total time = 32872.45 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf q2_k
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q2_K .. size =   250.00 MiB ->    41.02 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  2414.82 MB

main: quantize time = 73889.44 ms
main:    total time = 73889.44 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf q3_k
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q3_K .. size =   250.00 MiB ->    53.71 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3144.52 MB

main: quantize time = 72651.65 ms
main:    total time = 72651.65 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf q4_k
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q4_K .. size =   250.00 MiB ->    70.31 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3891.24 MB

main: quantize time = 128373.34 ms
main:    total time = 128373.34 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf q5_k
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q5_K .. size =   250.00 MiB ->    85.94 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4560.87 MB

main: quantize time = 112139.69 ms
main:    total time = 112139.69 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf q6_k
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753056 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, converting to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, converting to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  5272.34 MB

main: quantize time = 61087.22 ms
main:    total time = 61087.22 ms
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to give meaning to your life. The meaning of life is to do what you love and love what you do. To have your life be meaningful you need to find what you’re passionate about and pursue it. The world is too big, so what’s the point in trying to do everything? You need to focus on what you are best at and what you are most passionate about. If you focus on what you are best at and what you’re most passionate about then you can give meaning to your life.
Your life is meant to be lived, not to be lived for you. That’s the meaning of life.
The meaning of life is to live a life filled with love and joy. The meaning of life is to be happy and free. The meaning of life is to live a life filled with meaning. The meaning of life is to be fulfilled and happy. The meaning of life is to be happy and content. The meaning of life is to be happy and content with the life you have. The meaning of life is to be happy and content with the life you have.
Related Questions
What is the meaning of life?
The meaning of life is to live a life filled with love and joy. The meaning of life is to
llama_print_timings:        load time =    3326.70 ms
llama_print_timings:      sample time =      10.58 ms /   256 runs   (    0.04 ms per token, 24201.17 tokens per second)
llama_print_timings: prompt eval time =      25.72 ms /     8 tokens (    3.22 ms per token,   310.99 tokens per second)
llama_print_timings:        eval time =    4936.17 ms /   255 runs   (   19.36 ms per token,    51.66 tokens per second)
llama_print_timings:       total time =    5057.31 ms /   263 tokens
Log end

real	0m8.603s
user	0m6.865s
sys	0m1.716s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   132.81 MiB
llm_load_tensors:      CUDA0 buffer size =  6695.84 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to give it meaning.
I have been a student of the Alexander Technique for 10 years. It is one of the most powerful tools I have ever used for self-discovery and personal development. I have personally benefitted greatly from working with a teacher, but I also feel that the technique is a self-teaching one, and that if I have the desire and persistence to learn, I can benefit just as much, if not more.
I have been involved in the creative arts for most of my life. I studied violin and piano as a child. I love to draw, paint, sing and write. I believe that these activities have helped me to become a better teacher.
I am a registered teacher of the Alexander Technique in Victoria. I am also a member of the Australian Society of Teachers of the Alexander Technique and the Victorian branch of the Society.
I am a registered teacher of the Alexander Technique. I am also a member of the Australian Society of Teachers of the Alexander Technique and the Victorian branch of the Society.
I am a registered teacher of the Alexander Technique in Victoria. I am also a member of the Australian Society of Teachers of the Alexander Technique and the Victorian branch of the Society.

llama_print_timings:        load time =    1919.45 ms
llama_print_timings:      sample time =      10.10 ms /   256 runs   (    0.04 ms per token, 25349.04 tokens per second)
llama_print_timings: prompt eval time =      21.80 ms /     8 tokens (    2.73 ms per token,   366.89 tokens per second)
llama_print_timings:        eval time =    3207.41 ms /   255 runs   (   12.58 ms per token,    79.50 tokens per second)
llama_print_timings:       total time =    3326.38 ms /   263 tokens
Log end

real	0m5.466s
user	0m4.421s
sys	0m1.045s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to make the most of the time you have.
I believe in the power of positive thinking.
I believe in the importance of sharing.
I believe in the importance of community.
I believe in the importance of sharing.
I believe in the importance of community.
I believe in the importance of sharing.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of sharing.
I believe in the power of positive thinking.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.
I believe in the importance of community.

llama_print_timings:        load time =    1305.83 ms
llama_print_timings:      sample time =      10.67 ms /   256 runs   (    0.04 ms per token, 23992.50 tokens per second)
llama_print_timings: prompt eval time =      19.55 ms /     8 tokens (    2.44 ms per token,   409.25 tokens per second)
llama_print_timings:        eval time =    2259.05 ms /   255 runs   (    8.86 ms per token,   112.88 tokens per second)
llama_print_timings:       total time =    2370.65 ms /   263 tokens
Log end

real	0m3.890s
user	0m3.063s
sys	0m0.818s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 3
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.95 GiB (5.03 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    78.12 MiB
llm_load_tensors:      CUDA0 buffer size =  3963.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to enjoy the ride and find the joy in everything.
I believe the meaning of life is to enjoy the ride and find the joy in everything.
I believe the meaning of life is to enjoy the ride and find the joy in everything.
I believe the meaning of life is to enjoy the ride and find the joy in everything.
I believe that the meaning of life is to enjoy the ride and find the joy in everything.
I believe that the meaning of life is to enjoy the ride and find the joy in everything.
I believe that the meaning of life is to enjoy the ride and find the joy in everything.
I believe the meaning of life is to enjoy the ride and find the joy in everything.
I believe the meaning of life is to enjoy the ride and find the joy in everything.
I believe the meaning of life is to enjoy the ride and find the joy in everything.
I believe the meaning of life is to enjoy the ride and find the joy in everything.
I believe that the meaning of life is to enjoy the ride and find the joy in everything.
I believe that the meaning of life is to enjoy the ride and find the joy in everything.
I believe that the meaning of life is to enjoy the
llama_print_timings:        load time =    1314.10 ms
llama_print_timings:      sample time =       9.34 ms /   256 runs   (    0.04 ms per token, 27417.80 tokens per second)
llama_print_timings: prompt eval time =      18.95 ms /     8 tokens (    2.37 ms per token,   422.21 tokens per second)
llama_print_timings:        eval time =    2351.40 ms /   255 runs   (    9.22 ms per token,   108.45 tokens per second)
llama_print_timings:       total time =    2461.55 ms /   263 tokens
Log end

real	0m3.984s
user	0m3.159s
sys	0m0.821s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 8
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.33 GiB (5.52 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4349.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to find what you are best at and do that.
I am a graphic designer and illustrator. I love what I do, so I am always trying to push myself to learn something new.
I have a wide range of interests but my two passions in life are my family and my artwork.
I love to travel and experience new cultures.
I am a musician, a writer, and an artist.
I am a Christian. My faith has had a huge impact on my life and it is something I am very passionate about.
I am a big nerd.
I believe that the best thing to do is to simply be happy.
I love my wife and my 2 children.
I am a fan of all the 80s and 90s movies and music.
I like to think that my artwork is a reflection of my personality.
I love to explore and try new things.
I am a big fan of the 80s and 90s.
I am a big fan of music, movies, and sports.
I am an artist, musician, and writer.
I love to read.
I love to learn new things.
I am an avid fan of science
llama_print_timings:        load time =    1384.03 ms
llama_print_timings:      sample time =      10.59 ms /   256 runs   (    0.04 ms per token, 24178.32 tokens per second)
llama_print_timings: prompt eval time =      20.48 ms /     8 tokens (    2.56 ms per token,   390.57 tokens per second)
llama_print_timings:        eval time =    2540.77 ms /   255 runs   (    9.96 ms per token,   100.36 tokens per second)
llama_print_timings:       total time =    2657.60 ms /   263 tokens
Log end

real	0m4.268s
user	0m3.434s
sys	0m0.833s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 9
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.72 GiB (6.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    93.75 MiB
llm_load_tensors:      CUDA0 buffer size =  4735.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to find something that makes you happy and follow it.
I believe in the power of positive thinking.
I believe in being strong even when you don’t feel like it.
I believe in being a positive role model.
I believe in being kind.
I believe in being courageous.
I believe in having courage.
I believe in being yourself.
I believe in having faith.
I believe in having faith in yourself.
I believe in having faith in your loved ones.
I believe in having faith in your friends.
I believe in having faith in the world.
I believe in having faith in your soul.
I believe in believing in others.
I believe in believing in yourself.
I believe in believing in others.
I believe in believing in yourself.
I believe in believing in others.
I believe in believing in yourself.
I believe in believing in others.
I believe in believing in yourself.
I believe in believing in others.
I believe in believing in yourself.
I believe in believing in others.
I believe in believing in yourself.
I believe in believing in yourself.
I believe in believing in yourself.
I believe in believing in others.
I believe
llama_print_timings:        load time =    1458.45 ms
llama_print_timings:      sample time =      10.46 ms /   256 runs   (    0.04 ms per token, 24474.19 tokens per second)
llama_print_timings: prompt eval time =      20.07 ms /     8 tokens (    2.51 ms per token,   398.68 tokens per second)
llama_print_timings:        eval time =    2637.99 ms /   255 runs   (   10.35 ms per token,    96.66 tokens per second)
llama_print_timings:       total time =    2758.91 ms /   263 tokens
Log end

real	0m4.445s
user	0m3.577s
sys	0m0.863s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 10
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q2_K:  129 tensors
llama_model_loader: - type q3_K:   96 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 2.36 GiB (3.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    41.02 MiB
llm_load_tensors:      CUDA0 buffer size =  2373.81 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to try to be as good as one can be.
-Margaret Mead
P.S. I love Margaret.
P.S. I love Margaret.
(I am so fortunate to have her as a mentor, friend and advisor).
"This is the story of a boy who was sad and lonely. He was the only child of a widow and her husband. They lived in a small house with no grass on its lawn or flowers in its garden."
I'm a little late, but I have to say that I totally agree with this! I'm an aunt to two boys now and it's such a fun time!
I can't wait to have kids.
I love Margaret as well!
I love Margaret, too!
I am a day late, but I have to say that I totally agree with this! I'm an aunt to two boys now and it's such a fun time!
I think it's so important to have children. I love being an aunt.
I think it's so important to have children. I love being an aunt.
I'm an aunt to two boys now and it's such a fun time!
I can'
llama_print_timings:        load time =    1027.63 ms
llama_print_timings:      sample time =       9.33 ms /   256 runs   (    0.04 ms per token, 27441.31 tokens per second)
llama_print_timings: prompt eval time =      33.31 ms /     8 tokens (    4.16 ms per token,   240.19 tokens per second)
llama_print_timings:        eval time =    2591.42 ms /   255 runs   (   10.16 ms per token,    98.40 tokens per second)
llama_print_timings:       total time =    2720.26 ms /   263 tokens
Log end

real	0m3.961s
user	0m3.235s
sys	0m0.723s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 12
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q3_K:  129 tensors
llama_model_loader: - type q4_K:   92 tensors
llama_model_loader: - type q5_K:    4 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.07 GiB (3.91 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    53.71 MiB
llm_load_tensors:      CUDA0 buffer size =  3090.81 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to be useful and to enjoy the journey.
I believe the meaning of life is to be useful and to enjoy the journey.
-Maya Angelou
The meaning of life is to be useful and to enjoy the journey.
The meaning of life is to be useful and to enjoy the journey.
-Maya Angelou
I believe the meaning of life is to be useful and to enjoy the journey.
-Maya Angelou
The meaning of life is to be useful and to enjoy the journey.
-Maya Angelou
As I write this, I’m thinking about life. Specifically, about death.
I know I’m not the only one who thinks about it.
I believe it’s a natural human impulse.
I believe it’s a natural human impulse.
As much as I may be preoccupied with the meaning of life, I’ve never been able to figure it out.
And I’m not even sure what I’m looking for.
Or I’m not even sure what I’m looking for.
Maybe it’s a purpose. Maybe it’s something to do.
Maybe it’s a purpose. Maybe it’s something to
llama_print_timings:        load time =    1149.35 ms
llama_print_timings:      sample time =      10.49 ms /   256 runs   (    0.04 ms per token, 24401.87 tokens per second)
llama_print_timings: prompt eval time =      30.17 ms /     8 tokens (    3.77 ms per token,   265.18 tokens per second)
llama_print_timings:        eval time =    2935.97 ms /   255 runs   (   11.51 ms per token,    86.85 tokens per second)
llama_print_timings:       total time =    3061.90 ms /   263 tokens
Log end

real	0m4.439s
user	0m3.647s
sys	0m0.789s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3820.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to find something you’re passionate about and then find a way to make money at it.
–Jim Carrey
I believe the meaning of life is to find something you’re passionate about and then find a way to make money at it.
–Jim Carrey
I believe the meaning of life is to find something you’re passionate about and then find a way to make money at it.
–Jim Carrey
Life is not about finding yourself. Life is about creating yourself.
–George Bernard Shaw
Life is not a matter of holding good cards, but of playing a poor hand well.
–Robert Louis Stevenson
Life is not the way you are. It is the way you think.
–Norman Vincent Peale
Life is not about finding yourself. Life is about creating yourself.
–George Bernard Shaw
Life is not a problem to be solved, but a reality to be experienced.
–Soren Kierkegaard
Life is not a problem to be solved, but a reality to be experienced.
–Soren Kierkegaard
Life is what happens to you while you’re busy making other plans.
–John Lennon
llama_print_timings:        load time =    1298.42 ms
llama_print_timings:      sample time =       9.53 ms /   256 runs   (    0.04 ms per token, 26865.36 tokens per second)
llama_print_timings: prompt eval time =      26.51 ms /     8 tokens (    3.31 ms per token,   301.75 tokens per second)
llama_print_timings:        eval time =    2385.49 ms /   255 runs   (    9.35 ms per token,   106.90 tokens per second)
llama_print_timings:       total time =    2500.81 ms /   263 tokens
Log end

real	0m4.011s
user	0m3.245s
sys	0m0.762s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 17
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4474.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to find meaning in life, to find joy in life, to find fulfillment in life.
I believe the meaning of life is to find meaning in life, to find joy in life, to find fulfillment in life.
You can find meaning in life by being the change you want to see in the world.
You can find joy in life by being with the ones you love and respect.
You can find fulfillment in life by being true to yourself.
You can find meaning in life by being the change you want to see in the world.
You can find joy in life by being with the ones you love and respect.
You can find fulfillment in life by being true to yourself.
I believe that it is important to take time to reflect on life and to think about what is really important in life and to consider how you can make your life more meaningful and joyful.
I believe that it is important to be true to yourself and to be honest and authentic.
I believe that you should always be looking for ways to make your life better.
I believe that you should always try to make your life better.
I believe in the power of positive thinking and that positive thinking can change the way you think and feel.
I believe
llama_print_timings:        load time =    1426.69 ms
llama_print_timings:      sample time =       9.71 ms /   256 runs   (    0.04 ms per token, 26364.57 tokens per second)
llama_print_timings: prompt eval time =      27.64 ms /     8 tokens (    3.45 ms per token,   289.45 tokens per second)
llama_print_timings:        eval time =    2620.57 ms /   255 runs   (   10.28 ms per token,    97.31 tokens per second)
llama_print_timings:       total time =    2741.73 ms /   263 tokens
Log end

real	0m4.382s
user	0m3.531s
sys	0m0.848s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 18
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q6_K:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   102.54 MiB
llm_load_tensors:      CUDA0 buffer size =  5169.81 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 256, n_keep = 1


 I believe the meaning of life is to be happy and live to the fullest, and to be happy you must be happy with yourself.
When you don’t like yourself, it’s hard to be happy because you’re always thinking about yourself and what others think about you. It’s hard to be happy with yourself when you don’t like yourself.
I believe that people are not happy with themselves because they don’t like who they are and they don’t like what they do. It’s hard to be happy if you don’t like who you are and what you do.
You must be happy with yourself to be happy.
I believe that people are not happy with themselves because they are not happy with what they do, they are not happy with who they are. I believe that people are not happy with themselves because they don’t like who they are and what they do.
I believe that people are not happy with themselves because they don’t like who they are and what they do. I believe that people are not happy with themselves because they don’t like who they are and what they do.
You must be happy with yourself to be happy.
I believe that people are not happy with themselves because they don’t like
llama_print_timings:        load time =    1564.90 ms
llama_print_timings:      sample time =       9.99 ms /   256 runs   (    0.04 ms per token, 25617.93 tokens per second)
llama_print_timings: prompt eval time =      23.47 ms /     8 tokens (    2.93 ms per token,   340.89 tokens per second)
llama_print_timings:        eval time =    2933.64 ms /   255 runs   (   11.50 ms per token,    86.92 tokens per second)
llama_print_timings:       total time =    3055.98 ms /   263 tokens
Log end

real	0m4.848s
user	0m3.956s
sys	0m0.887s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215934
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1164.58 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.70 seconds per pass - ETA 0.03 minutes
[1]5.5314,[2]6.8315,[3]7.4349,[4]6.8779,
llama_print_timings:        load time =    3207.47 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2399.45 ms /  8192 tokens (    0.29 ms per token,  3414.12 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    3877.98 ms /  8193 tokens

Final estimate: PPL = 6.8779 +/- 0.25577

real	0m7.325s
user	0m6.035s
sys	0m1.915s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215941
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   132.81 MiB
llm_load_tensors:      CUDA0 buffer size =  6695.84 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1210.16 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.82 seconds per pass - ETA 0.05 minutes
[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,
llama_print_timings:        load time =    1811.67 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2812.84 ms /  8192 tokens (    0.34 ms per token,  2912.36 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4360.18 ms /  8193 tokens

Final estimate: PPL = 6.8806 +/- 0.25603

real	0m6.391s
user	0m5.629s
sys	0m1.388s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215947
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1022.36 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.92 seconds per pass - ETA 0.05 minutes
[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,
llama_print_timings:        load time =    1183.00 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3255.62 ms /  8192 tokens (    0.40 ms per token,  2516.27 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4590.31 ms /  8193 tokens

Final estimate: PPL = 7.0138 +/- 0.26116

real	0m5.991s
user	0m5.428s
sys	0m1.175s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215953
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 3
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.95 GiB (5.03 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    78.12 MiB
llm_load_tensors:      CUDA0 buffer size =  3963.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1132.44 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.94 seconds per pass - ETA 0.05 minutes
[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,
llama_print_timings:        load time =    1251.20 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3262.68 ms /  8192 tokens (    0.40 ms per token,  2510.82 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4731.99 ms /  8193 tokens

Final estimate: PPL = 7.0158 +/- 0.26045

real	0m6.233s
user	0m5.578s
sys	0m1.296s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215960
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 8
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.33 GiB (5.52 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4349.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1184.08 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.82 seconds per pass - ETA 0.05 minutes
[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,
llama_print_timings:        load time =    1470.49 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2809.83 ms /  8192 tokens (    0.34 ms per token,  2915.48 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4311.31 ms /  8193 tokens

Final estimate: PPL = 6.9230 +/- 0.25774

real	0m6.014s
user	0m5.284s
sys	0m1.337s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215966
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 9
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.72 GiB (6.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    93.75 MiB
llm_load_tensors:      CUDA0 buffer size =  4735.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1121.48 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.83 seconds per pass - ETA 0.05 minutes
[1]5.5659,[2]6.8475,[3]7.4558,[4]6.9074,
llama_print_timings:        load time =    1396.00 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2825.48 ms /  8192 tokens (    0.34 ms per token,  2899.33 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4290.69 ms /  8193 tokens

Final estimate: PPL = 6.9074 +/- 0.25709

real	0m5.917s
user	0m5.239s
sys	0m1.310s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215972
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 10
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q2_K:  129 tensors
llama_model_loader: - type q3_K:   96 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 2.36 GiB (3.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    41.02 MiB
llm_load_tensors:      CUDA0 buffer size =  2373.81 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1052.47 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.89 seconds per pass - ETA 0.05 minutes
[1]6.5021,[2]7.9561,[3]8.7000,[4]8.2106,
llama_print_timings:        load time =     967.12 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3119.37 ms /  8192 tokens (    0.38 ms per token,  2626.17 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4484.99 ms /  8193 tokens

Final estimate: PPL = 8.2106 +/- 0.30950

real	0m5.665s
user	0m5.013s
sys	0m1.272s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215977
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 12
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q3_K:  129 tensors
llama_model_loader: - type q4_K:   92 tensors
llama_model_loader: - type q5_K:    4 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.07 GiB (3.91 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    53.71 MiB
llm_load_tensors:      CUDA0 buffer size =  3090.81 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1056.87 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.92 seconds per pass - ETA 0.05 minutes
[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,
llama_print_timings:        load time =    1092.44 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3250.32 ms /  8192 tokens (    0.40 ms per token,  2520.37 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4636.98 ms /  8193 tokens

Final estimate: PPL = 7.1090 +/- 0.26581

real	0m5.948s
user	0m5.372s
sys	0m1.207s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215983
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3820.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1071.95 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.91 seconds per pass - ETA 0.05 minutes
[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,
llama_print_timings:        load time =    1209.05 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3220.66 ms /  8192 tokens (    0.39 ms per token,  2543.58 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4610.49 ms /  8193 tokens

Final estimate: PPL = 6.9337 +/- 0.25755

real	0m6.035s
user	0m5.343s
sys	0m1.307s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215989
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 17
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4474.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1036.86 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.87 seconds per pass - ETA 0.05 minutes
[1]5.5200,[2]6.8175,[3]7.4358,[4]6.8916,
llama_print_timings:        load time =    1346.90 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3049.39 ms /  8192 tokens (    0.37 ms per token,  2686.44 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4418.46 ms /  8193 tokens

Final estimate: PPL = 6.8916 +/- 0.25650

real	0m5.999s
user	0m5.268s
sys	0m1.353s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711215995
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 18
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q6_K:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   102.54 MiB
llm_load_tensors:      CUDA0 buffer size =  5169.81 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1057.85 ms
perplexity: calculating perplexity over 4 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.88 seconds per pass - ETA 0.05 minutes
[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,
llama_print_timings:        load time =    1571.99 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3081.13 ms /  8192 tokens (    0.38 ms per token,  2658.76 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4447.27 ms /  8193 tokens

Final estimate: PPL = 6.8939 +/- 0.25650

real	0m6.247s
user	0m5.494s
sys	0m1.352s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-imatrix.log
+ ./bin/imatrix --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711216001
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
compute_imatrix: tokenizing the input ..
compute_imatrix: tokenization took 1045.62 ms
compute_imatrix: computing over 4 chunks with batch_size 512
compute_imatrix: 3.06 seconds per pass - ETA 0.20 minutes
[1]5.5314,[2]6.8315,
save_imatrix: stored collected data after 10 chunks in imatrix.dat
[3]7.4349,[4]6.8779,
save_imatrix: stored collected data after 16 chunks in imatrix.dat

llama_print_timings:        load time =    4706.60 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   11541.90 ms /  8192 tokens (    1.41 ms per token,   709.76 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   16370.49 ms /  8193 tokens

Final estimate: PPL = 6.8779 +/- 0.25577

real	0m16.600s
user	0m15.313s
sys	0m1.914s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-save-load-state.log
+ ./bin/save-load-state --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/33 layers to GPU
llm_load_tensors:        CPU buffer size =  3647.87 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   206.04 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    17.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 356
main : serialized state into 68164281 out of a maximum of 334045228 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   206.04 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    17.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 356
main : deserialized state from 68164281 out of a maximum of 334045228 bytes

main : success

first run: The quick brown fox jumps over the linen colored Korean War GI
Longtail fled ahead of us


second run: The quick brown fox jumps over the linen colored Korean War GI
Longtail fled ahead of us

real	0m11.103s
user	0m28.128s
sys	0m1.425s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]5.5314,[2]6.8315,[3]7.4349,[4]6.8779,'
+ qnt=f16
++ echo '[1]5.5314,[2]6.8315,[3]7.4349,[4]6.8779,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8779
++ echo '6.8779 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 6.8779
+ return 0
  - f16 @ 6.8779 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,'
+ qnt=q8_0
++ echo '[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8806
++ echo '6.8806 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 6.8806
+ return 0
  - q8_0 @ 6.8806 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,'
+ qnt=q4_0
++ echo '[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.0138
++ echo '7.0138 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 7.0138
+ return 0
  - q4_0 @ 7.0138 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,'
+ qnt=q4_1
++ echo '[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.0158
++ echo '7.0158 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 7.0158
+ return 0
  - q4_1 @ 7.0158 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,'
+ qnt=q5_0
++ echo '[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.9230
++ echo '6.9230 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 6.9230
+ return 0
  - q5_0 @ 6.9230 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]5.5659,[2]6.8475,[3]7.4558,[4]6.9074,'
+ qnt=q5_1
++ echo '[1]5.5659,[2]6.8475,[3]7.4558,[4]6.9074,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.9074
++ echo '6.9074 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 6.9074
+ return 0
  - q5_1 @ 6.9074 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
++ grep '^\[1\]'
+ check_ppl q2_k '[1]6.5021,[2]7.9561,[3]8.7000,[4]8.2106,'
+ qnt=q2_k
++ echo '[1]6.5021,[2]7.9561,[3]8.7000,[4]8.2106,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=8.2106
++ echo '8.2106 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q2_k 8.2106
+ return 0
  - q2_k @ 8.2106 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,'
+ qnt=q3_k
++ echo '[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.1090
++ echo '7.1090 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 7.1090
+ return 0
  - q3_k @ 7.1090 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,'
+ qnt=q4_k
++ echo '[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.9337
++ echo '6.9337 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 6.9337
+ return 0
  - q4_k @ 6.9337 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]5.5200,[2]6.8175,[3]7.4358,[4]6.8916,'
+ qnt=q5_k
++ echo '[1]5.5200,[2]6.8175,[3]7.4358,[4]6.8916,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8916
++ echo '6.8916 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 6.8916
+ return 0
  - q5_k @ 6.8916 OK
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,'
+ qnt=q6_k
++ echo '[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8939
++ echo '6.8939 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 6.8939
+ return 0
  - q6_k @ 6.8939 OK
+ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-imatrix.log
+ grep Final
+ path_lora=../models-mnt/open-llama/7B-v2/lora
+ path_shakespeare=../models-mnt/shakespeare
+ shakespeare=../models-mnt/shakespeare/shakespeare.txt
+ lora_shakespeare=../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin
+ gg_wget ../models-mnt/open-llama/7B-v2/lora https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
+ local out=../models-mnt/open-llama/7B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/7B-v2/lora
+ cd ../models-mnt/open-llama/7B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:47:09 URL:https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json [457/457] -> "adapter_config.json" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/open-llama/7B-v2/lora https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ local out=../models-mnt/open-llama/7B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/7B-v2/lora
+ cd ../models-mnt/open-llama/7B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/shakespeare https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
+ local out=../models-mnt/shakespeare
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/shakespeare
+ cd ../models-mnt/shakespeare
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
Last-modified header missing -- time-stamps turned off.
2024-03-23 17:47:10 URL:https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt [94275/94275] -> "shakespeare.txt" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ python3 ../convert-lora-to-ggml.py ../models-mnt/open-llama/7B-v2/lora
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraB (4096, 64) float32 1.00MB
Converted ../models-mnt/open-llama/7B-v2/lora/adapter_config.json and ../models-mnt/open-llama/7B-v2/lora/adapter_model.bin to ../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt -t 1 -ngl 999 -c 2048 -b 512 --chunks 3
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711216033
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 63.429 ms
perplexity: calculating perplexity over 3 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.73 seconds per pass - ETA 0.03 minutes
[1]14.1610,[2]11.8140,[3]11.2243,
llama_print_timings:        load time =    3295.72 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    1813.80 ms /  6144 tokens (    0.30 ms per token,  3387.37 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    2144.45 ms /  6145 tokens

Final estimate: PPL = 11.2243 +/- 0.52147

real	0m5.689s
user	0m4.299s
sys	0m1.839s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-lora-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin -t 1 -ngl 999 -c 2048 -b 512 --chunks 3
main: build = 2514 (f482bb2e)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1711216039
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:  CUDA_Host buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB
llama_new_context_with_model: graph nodes  = 1060
llama_new_context_with_model: graph splits = 2
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
................ done (18771.05 ms)

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 61.751 ms
perplexity: calculating perplexity over 3 chunks, n_ctx=2048, batch_size=512, n_seq=1
perplexity: 0.70 seconds per pass - ETA 0.03 minutes
[1]11.5881,[2]9.1813,[3]8.7575,
llama_print_timings:        load time =   24867.76 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    1803.35 ms /  6144 tokens (    0.29 ms per token,  3406.99 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    2115.79 ms /  6145 tokens

Final estimate: PPL = 8.7575 +/- 0.39478

real	0m27.240s
user	0m19.900s
sys	0m7.809s
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-f16.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-lora-f16.log
++ grep '^\[1\]'
+ compare_ppl 'f16 shakespeare' '[1]14.1610,[2]11.8140,[3]11.2243,' '[1]11.5881,[2]9.1813,[3]8.7575,'
+ qnt='f16 shakespeare'
++ echo '[1]14.1610,[2]11.8140,[3]11.2243,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=11.2243
++ echo '[1]11.5881,[2]9.1813,[3]8.7575,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=8.7575
++ echo '11.2243 < 8.7575'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'f16 shakespeare' 11.2243 8.7575
+ return 0
  - f16 shakespeare @ 11.2243 8.7575 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /home/ggml/work/llama.cpp
+ local model
+ tee /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_with_model_debug.log
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ [[ -s /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-debug
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    9.16 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    1.90 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =  11.06 sec*proc (2 tests)

Total Test time (real) =  11.06 sec
0.55user 10.52system 0:11.09elapsed 99%CPU (0avgtext+0avgdata 13760584maxresident)k
0inputs+48outputs (0major+3604627minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_with_model_release.log
+ local model
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ [[ -s /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/f4/82bb2e4920e544651fb832f2e0bcb4d2ff69ab/ggml-4-x86-cuda-v100/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-release
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    8.95 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    1.77 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =  10.72 sec*proc (2 tests)

Total Test time (real) =  10.72 sec
0.21user 10.50system 0:10.75elapsed 99%CPU (0avgtext+0avgdata 13760264maxresident)k
0inputs+48outputs (0major+3604374minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
