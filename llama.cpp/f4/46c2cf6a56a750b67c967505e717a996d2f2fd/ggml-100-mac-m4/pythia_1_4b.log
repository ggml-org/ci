Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.463s
user	0m0.895s
sys	0m1.155s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-sampling
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Built target test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Linking CXX executable ../bin/test-gguf
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-quantize-fns
[ 64%] Built target test-backend-ops
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-autorelease
[ 64%] Built target test-rope
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-barrier
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Built target llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-gbnf-validator
[ 76%] Built target llama-gguf-split
[ 76%] Built target llama-bench
[ 76%] Built target llama-imatrix
[ 76%] Built target llama-infill
[ 76%] Built target llama-gritlm
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Built target llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-lookup-stats
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-merge
[ 84%] Generating loading.html.hpp
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Built target llama-lookup-stats
[ 86%] Generating index.html.gz.hpp
[ 86%] Built target llama-parallel
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-passkey
[ 86%] Built target llama-retrieval
[ 86%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-cli
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Built target llama-run
[ 88%] Built target llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-speculative
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-gen-docs
[ 94%] Built target llama-tts
[ 94%] Built target llama-speculative-simple
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Built target llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.961s
user	0m6.264s
sys	0m10.743s

main: quantize time =  4320.56 ms
main:    total time =  4320.56 ms

main: quantize time =  1834.86 ms
main:    total time =  1834.86 ms

main: quantize time =  1664.13 ms
main:    total time =  1664.13 ms

main: quantize time =  2121.26 ms
main:    total time =  2121.26 ms

main: quantize time =  1606.76 ms
main:    total time =  1606.76 ms

main: quantize time =  5016.96 ms
main:    total time =  5016.96 ms

main: quantize time =  5639.43 ms
main:    total time =  5639.43 ms

main: quantize time =  7056.75 ms
main:    total time =  7056.75 ms

main: quantize time =  5838.95 ms
main:    total time =  5838.95 ms

main: quantize time =  4545.95 ms
main:    total time =  4545.95 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.244 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.360 I main: llama backend init
0.00.000.367 I main: load the model and apply lora adapter, if any
0.00.040.196 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.052.942 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.052.958 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.052.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.052.967 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.052.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.052.969 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.052.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.052.972 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.052.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.052.973 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.052.974 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.052.974 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.052.975 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.052.975 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.052.981 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.052.981 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.052.982 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.060.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.069.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.069.285 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.069.285 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.069.286 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.069.287 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.069.289 I llama_model_loader: - type  f32:  194 tensors
0.00.069.289 I llama_model_loader: - type  f16:   98 tensors
0.00.069.291 I print_info: file format = GGUF V3 (latest)
0.00.069.293 I print_info: file type   = all F32 (guessed)
0.00.069.297 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.626 I load: special tokens cache size = 25
0.00.114.565 I load: token to piece cache size = 0.2984 MB
0.00.114.569 I print_info: arch             = gptneox
0.00.114.569 I print_info: vocab_only       = 0
0.00.114.569 I print_info: n_ctx_train      = 2048
0.00.114.569 I print_info: n_embd           = 2048
0.00.114.569 I print_info: n_layer          = 24
0.00.114.573 I print_info: n_head           = 16
0.00.114.574 I print_info: n_head_kv        = 16
0.00.114.574 I print_info: n_rot            = 32
0.00.114.575 I print_info: n_swa            = 0
0.00.114.575 I print_info: n_embd_head_k    = 128
0.00.114.575 I print_info: n_embd_head_v    = 128
0.00.114.576 I print_info: n_gqa            = 1
0.00.114.576 I print_info: n_embd_k_gqa     = 2048
0.00.114.577 I print_info: n_embd_v_gqa     = 2048
0.00.114.577 I print_info: f_norm_eps       = 1.0e-05
0.00.114.578 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.114.578 I print_info: f_clamp_kqv      = 0.0e+00
0.00.114.578 I print_info: f_max_alibi_bias = 0.0e+00
0.00.114.578 I print_info: f_logit_scale    = 0.0e+00
0.00.114.579 I print_info: n_ff             = 8192
0.00.114.579 I print_info: n_expert         = 0
0.00.114.579 I print_info: n_expert_used    = 0
0.00.114.580 I print_info: causal attn      = 1
0.00.114.580 I print_info: pooling type     = 0
0.00.114.580 I print_info: rope type        = 2
0.00.114.580 I print_info: rope scaling     = linear
0.00.114.581 I print_info: freq_base_train  = 10000.0
0.00.114.581 I print_info: freq_scale_train = 1
0.00.114.581 I print_info: n_ctx_orig_yarn  = 2048
0.00.114.581 I print_info: rope_finetuned   = unknown
0.00.114.581 I print_info: ssm_d_conv       = 0
0.00.114.582 I print_info: ssm_d_inner      = 0
0.00.114.582 I print_info: ssm_d_state      = 0
0.00.114.582 I print_info: ssm_dt_rank      = 0
0.00.114.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.114.582 I print_info: model type       = 1.4B
0.00.114.583 I print_info: model params     = 1.41 B
0.00.114.583 I print_info: general.name     = 1.4B
0.00.114.583 I print_info: vocab type       = BPE
0.00.114.583 I print_info: n_vocab          = 50304
0.00.114.584 I print_info: n_merges         = 50009
0.00.114.586 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.114.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.114.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.114.587 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.114.587 I print_info: LF token         = 128 'Ä'
0.00.114.587 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.114.587 I print_info: max token length = 1024
0.00.117.293 I load_tensors: offloading 24 repeating layers to GPU
0.00.117.293 I load_tensors: offloading output layer to GPU
0.00.117.293 I load_tensors: offloaded 25/25 layers to GPU
0.00.117.313 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.117.314 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.117.668 I llama_init_from_model: n_seq_max     = 1
0.00.117.669 I llama_init_from_model: n_ctx         = 2048
0.00.117.669 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.117.669 I llama_init_from_model: n_batch       = 2048
0.00.117.669 I llama_init_from_model: n_ubatch      = 512
0.00.117.670 I llama_init_from_model: flash_attn    = 0
0.00.117.670 I llama_init_from_model: freq_base     = 10000.0
0.00.117.670 I llama_init_from_model: freq_scale    = 1
0.00.117.671 I ggml_metal_init: allocating
0.00.117.674 I ggml_metal_init: found device: Apple M4
0.00.117.676 I ggml_metal_init: picking default device: Apple M4
0.00.118.389 I ggml_metal_init: using embedded metal library
0.00.131.182 I ggml_metal_init: GPU name:   Apple M4
0.00.131.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.131.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.131.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.131.185 I ggml_metal_init: simdgroup reduction   = true
0.00.131.185 I ggml_metal_init: simdgroup matrix mul. = true
0.00.131.185 I ggml_metal_init: has bfloat            = true
0.00.131.185 I ggml_metal_init: use bfloat            = true
0.00.131.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.131.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.166.334 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.189.417 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.189.422 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.189.443 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.190.477 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.190.478 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.190.479 I llama_init_from_model: graph nodes  = 967
0.00.190.479 I llama_init_from_model: graph splits = 2
0.00.190.482 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.190.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.190.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.279.435 I main: llama threadpool init, n_threads = 4
0.00.279.486 I 
0.00.279.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.279.516 I 
0.00.279.842 I sampler seed: 1234
0.00.279.852 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.279.892 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.279.894 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.279.894 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.109.185 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.02.109.186 I llama_perf_context_print:        load time =     239.22 ms
0.02.109.187 I llama_perf_context_print: prompt eval time =      44.11 ms /     7 tokens (    6.30 ms per token,   158.69 tokens per second)
0.02.109.188 I llama_perf_context_print:        eval time =    1782.26 ms /    63 runs   (   28.29 ms per token,    35.35 tokens per second)
0.02.109.188 I llama_perf_context_print:       total time =    1829.76 ms /    70 tokens
0.02.109.420 I ggml_metal_free: deallocating

real	0m2.451s
user	0m0.153s
sys	0m0.110s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.000 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.554 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.561 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.564 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.564 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.565 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.568 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.570 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.571 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.574 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.574 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.574 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.426 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.164 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.165 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.165 I llama_model_loader: - type  f32:  194 tensors
0.00.036.166 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.167 I print_info: file format = GGUF V3 (latest)
0.00.036.167 I print_info: file type   = Q8_0
0.00.036.169 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.729 I load: special tokens cache size = 25
0.00.063.600 I load: token to piece cache size = 0.2984 MB
0.00.063.604 I print_info: arch             = gptneox
0.00.063.605 I print_info: vocab_only       = 0
0.00.063.605 I print_info: n_ctx_train      = 2048
0.00.063.605 I print_info: n_embd           = 2048
0.00.063.605 I print_info: n_layer          = 24
0.00.063.610 I print_info: n_head           = 16
0.00.063.611 I print_info: n_head_kv        = 16
0.00.063.611 I print_info: n_rot            = 32
0.00.063.612 I print_info: n_swa            = 0
0.00.063.612 I print_info: n_embd_head_k    = 128
0.00.063.612 I print_info: n_embd_head_v    = 128
0.00.063.613 I print_info: n_gqa            = 1
0.00.063.613 I print_info: n_embd_k_gqa     = 2048
0.00.063.614 I print_info: n_embd_v_gqa     = 2048
0.00.063.614 I print_info: f_norm_eps       = 1.0e-05
0.00.063.615 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.616 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.617 I print_info: f_logit_scale    = 0.0e+00
0.00.063.617 I print_info: n_ff             = 8192
0.00.063.618 I print_info: n_expert         = 0
0.00.063.618 I print_info: n_expert_used    = 0
0.00.063.618 I print_info: causal attn      = 1
0.00.063.618 I print_info: pooling type     = 0
0.00.063.620 I print_info: rope type        = 2
0.00.063.621 I print_info: rope scaling     = linear
0.00.063.621 I print_info: freq_base_train  = 10000.0
0.00.063.622 I print_info: freq_scale_train = 1
0.00.063.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.622 I print_info: rope_finetuned   = unknown
0.00.063.622 I print_info: ssm_d_conv       = 0
0.00.063.622 I print_info: ssm_d_inner      = 0
0.00.063.623 I print_info: ssm_d_state      = 0
0.00.063.623 I print_info: ssm_dt_rank      = 0
0.00.063.623 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.623 I print_info: model type       = 1.4B
0.00.063.624 I print_info: model params     = 1.41 B
0.00.063.624 I print_info: general.name     = 1.4B
0.00.063.625 I print_info: vocab type       = BPE
0.00.063.625 I print_info: n_vocab          = 50304
0.00.063.626 I print_info: n_merges         = 50009
0.00.063.626 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.627 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.627 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.627 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.627 I print_info: LF token         = 128 'Ä'
0.00.063.627 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.628 I print_info: max token length = 1024
0.00.066.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.040 I load_tensors: offloading output layer to GPU
0.00.066.040 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.052 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.054 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.066.390 I llama_init_from_model: n_seq_max     = 1
0.00.066.390 I llama_init_from_model: n_ctx         = 2048
0.00.066.391 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.066.391 I llama_init_from_model: n_batch       = 2048
0.00.066.391 I llama_init_from_model: n_ubatch      = 512
0.00.066.391 I llama_init_from_model: flash_attn    = 0
0.00.066.392 I llama_init_from_model: freq_base     = 10000.0
0.00.066.392 I llama_init_from_model: freq_scale    = 1
0.00.066.392 I ggml_metal_init: allocating
0.00.066.396 I ggml_metal_init: found device: Apple M4
0.00.066.398 I ggml_metal_init: picking default device: Apple M4
0.00.067.183 I ggml_metal_init: using embedded metal library
0.00.069.918 I ggml_metal_init: GPU name:   Apple M4
0.00.069.920 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.920 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.921 I ggml_metal_init: simdgroup reduction   = true
0.00.069.921 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.921 I ggml_metal_init: has bfloat            = true
0.00.069.921 I ggml_metal_init: use bfloat            = true
0.00.069.922 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.290 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.710 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.719 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.743 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.007 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.107.009 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.107.010 I llama_init_from_model: graph nodes  = 967
0.00.107.010 I llama_init_from_model: graph splits = 2
0.00.107.014 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.142 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.143 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.185.316 I main: llama threadpool init, n_threads = 4
0.01.185.352 I 
0.01.185.375 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.185.377 I 
0.01.185.630 I sampler seed: 1234
0.01.185.634 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.185.672 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.185.673 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.185.673 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.267.215 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.02.267.216 I llama_perf_context_print:        load time =    1175.31 ms
0.02.267.217 I llama_perf_context_print: prompt eval time =      39.85 ms /     7 tokens (    5.69 ms per token,   175.65 tokens per second)
0.02.267.217 I llama_perf_context_print:        eval time =    1038.82 ms /    63 runs   (   16.49 ms per token,    60.65 tokens per second)
0.02.267.218 I llama_perf_context_print:       total time =    1081.90 ms /    70 tokens
0.02.267.460 I ggml_metal_free: deallocating

real	0m2.286s
user	0m0.114s
sys	0m0.243s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.021.541 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.047 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.055 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.056 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.061 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.062 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.063 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.321 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.417 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.661 I llama_model_loader: - type  f32:  194 tensors
0.00.044.662 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.662 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.663 I print_info: file format = GGUF V3 (latest)
0.00.044.663 I print_info: file type   = Q4_0
0.00.044.665 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.070.865 I load: special tokens cache size = 25
0.00.080.868 I load: token to piece cache size = 0.2984 MB
0.00.080.872 I print_info: arch             = gptneox
0.00.080.872 I print_info: vocab_only       = 0
0.00.080.872 I print_info: n_ctx_train      = 2048
0.00.080.873 I print_info: n_embd           = 2048
0.00.080.873 I print_info: n_layer          = 24
0.00.080.877 I print_info: n_head           = 16
0.00.080.878 I print_info: n_head_kv        = 16
0.00.080.879 I print_info: n_rot            = 32
0.00.080.879 I print_info: n_swa            = 0
0.00.080.879 I print_info: n_embd_head_k    = 128
0.00.080.882 I print_info: n_embd_head_v    = 128
0.00.080.883 I print_info: n_gqa            = 1
0.00.080.884 I print_info: n_embd_k_gqa     = 2048
0.00.080.885 I print_info: n_embd_v_gqa     = 2048
0.00.080.886 I print_info: f_norm_eps       = 1.0e-05
0.00.080.887 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.080.887 I print_info: f_clamp_kqv      = 0.0e+00
0.00.080.887 I print_info: f_max_alibi_bias = 0.0e+00
0.00.080.887 I print_info: f_logit_scale    = 0.0e+00
0.00.080.889 I print_info: n_ff             = 8192
0.00.080.889 I print_info: n_expert         = 0
0.00.080.889 I print_info: n_expert_used    = 0
0.00.080.890 I print_info: causal attn      = 1
0.00.080.892 I print_info: pooling type     = 0
0.00.080.892 I print_info: rope type        = 2
0.00.080.893 I print_info: rope scaling     = linear
0.00.080.893 I print_info: freq_base_train  = 10000.0
0.00.080.894 I print_info: freq_scale_train = 1
0.00.080.894 I print_info: n_ctx_orig_yarn  = 2048
0.00.080.895 I print_info: rope_finetuned   = unknown
0.00.080.895 I print_info: ssm_d_conv       = 0
0.00.080.895 I print_info: ssm_d_inner      = 0
0.00.080.895 I print_info: ssm_d_state      = 0
0.00.080.895 I print_info: ssm_dt_rank      = 0
0.00.080.895 I print_info: ssm_dt_b_c_rms   = 0
0.00.080.896 I print_info: model type       = 1.4B
0.00.080.896 I print_info: model params     = 1.41 B
0.00.080.902 I print_info: general.name     = 1.4B
0.00.080.903 I print_info: vocab type       = BPE
0.00.080.903 I print_info: n_vocab          = 50304
0.00.080.904 I print_info: n_merges         = 50009
0.00.080.904 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.080.904 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.080.904 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.080.905 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.080.905 I print_info: LF token         = 128 'Ä'
0.00.080.906 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.080.906 I print_info: max token length = 1024
0.00.083.698 I load_tensors: offloading 24 repeating layers to GPU
0.00.083.698 I load_tensors: offloading output layer to GPU
0.00.083.699 I load_tensors: offloaded 25/25 layers to GPU
0.00.083.706 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.083.707 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.084.188 I llama_init_from_model: n_seq_max     = 1
0.00.084.189 I llama_init_from_model: n_ctx         = 2048
0.00.084.189 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.084.190 I llama_init_from_model: n_batch       = 2048
0.00.084.190 I llama_init_from_model: n_ubatch      = 512
0.00.084.190 I llama_init_from_model: flash_attn    = 0
0.00.084.191 I llama_init_from_model: freq_base     = 10000.0
0.00.084.191 I llama_init_from_model: freq_scale    = 1
0.00.084.192 I ggml_metal_init: allocating
0.00.084.196 I ggml_metal_init: found device: Apple M4
0.00.084.199 I ggml_metal_init: picking default device: Apple M4
0.00.085.210 I ggml_metal_init: using embedded metal library
0.00.089.422 I ggml_metal_init: GPU name:   Apple M4
0.00.089.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.425 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.426 I ggml_metal_init: simdgroup reduction   = true
0.00.089.426 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.427 I ggml_metal_init: has bfloat            = true
0.00.089.427 I ggml_metal_init: use bfloat            = true
0.00.089.427 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.428 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.108 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.127.372 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.127.382 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.127.408 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.128.502 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.128.505 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.128.505 I llama_init_from_model: graph nodes  = 967
0.00.128.505 I llama_init_from_model: graph splits = 2
0.00.128.509 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.128.638 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.128.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.540 I main: llama threadpool init, n_threads = 4
0.00.649.583 I 
0.00.649.608 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.608 I 
0.00.649.838 I sampler seed: 1234
0.00.649.842 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.649.892 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.649.894 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.649.894 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.335.583 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.01.335.583 I llama_perf_context_print:        load time =     627.99 ms
0.01.335.584 I llama_perf_context_print: prompt eval time =      47.23 ms /     7 tokens (    6.75 ms per token,   148.22 tokens per second)
0.01.335.585 I llama_perf_context_print:        eval time =     635.57 ms /    63 runs   (   10.09 ms per token,    99.12 tokens per second)
0.01.335.585 I llama_perf_context_print:       total time =     686.05 ms /    70 tokens
0.01.335.789 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.131s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.924 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.138 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.143 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.145 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.145 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.146 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.153 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.154 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.154 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.912 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.952 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.624 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.625 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.626 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.626 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.626 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.627 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.627 I llama_model_loader: - type  f32:  194 tensors
0.00.033.628 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.628 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.628 I print_info: file format = GGUF V3 (latest)
0.00.033.629 I print_info: file type   = Q4_1
0.00.033.630 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.054.713 I load: special tokens cache size = 25
0.00.060.691 I load: token to piece cache size = 0.2984 MB
0.00.060.694 I print_info: arch             = gptneox
0.00.060.694 I print_info: vocab_only       = 0
0.00.060.694 I print_info: n_ctx_train      = 2048
0.00.060.695 I print_info: n_embd           = 2048
0.00.060.695 I print_info: n_layer          = 24
0.00.060.698 I print_info: n_head           = 16
0.00.060.698 I print_info: n_head_kv        = 16
0.00.060.698 I print_info: n_rot            = 32
0.00.060.699 I print_info: n_swa            = 0
0.00.060.701 I print_info: n_embd_head_k    = 128
0.00.060.701 I print_info: n_embd_head_v    = 128
0.00.060.702 I print_info: n_gqa            = 1
0.00.060.703 I print_info: n_embd_k_gqa     = 2048
0.00.060.703 I print_info: n_embd_v_gqa     = 2048
0.00.060.709 I print_info: f_norm_eps       = 1.0e-05
0.00.060.713 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.713 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.713 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.713 I print_info: f_logit_scale    = 0.0e+00
0.00.060.714 I print_info: n_ff             = 8192
0.00.060.714 I print_info: n_expert         = 0
0.00.060.715 I print_info: n_expert_used    = 0
0.00.060.715 I print_info: causal attn      = 1
0.00.060.715 I print_info: pooling type     = 0
0.00.060.716 I print_info: rope type        = 2
0.00.060.718 I print_info: rope scaling     = linear
0.00.060.718 I print_info: freq_base_train  = 10000.0
0.00.060.718 I print_info: freq_scale_train = 1
0.00.060.718 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.718 I print_info: rope_finetuned   = unknown
0.00.060.719 I print_info: ssm_d_conv       = 0
0.00.060.719 I print_info: ssm_d_inner      = 0
0.00.060.719 I print_info: ssm_d_state      = 0
0.00.060.719 I print_info: ssm_dt_rank      = 0
0.00.060.719 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.719 I print_info: model type       = 1.4B
0.00.060.720 I print_info: model params     = 1.41 B
0.00.060.720 I print_info: general.name     = 1.4B
0.00.060.720 I print_info: vocab type       = BPE
0.00.060.720 I print_info: n_vocab          = 50304
0.00.060.720 I print_info: n_merges         = 50009
0.00.060.721 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.721 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.721 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.721 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.721 I print_info: LF token         = 128 'Ä'
0.00.060.722 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.722 I print_info: max token length = 1024
0.00.062.521 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.521 I load_tensors: offloading output layer to GPU
0.00.062.521 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.526 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.062.527 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.062.798 I llama_init_from_model: n_seq_max     = 1
0.00.062.798 I llama_init_from_model: n_ctx         = 2048
0.00.062.799 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.799 I llama_init_from_model: n_batch       = 2048
0.00.062.799 I llama_init_from_model: n_ubatch      = 512
0.00.062.799 I llama_init_from_model: flash_attn    = 0
0.00.062.799 I llama_init_from_model: freq_base     = 10000.0
0.00.062.800 I llama_init_from_model: freq_scale    = 1
0.00.062.800 I ggml_metal_init: allocating
0.00.062.803 I ggml_metal_init: found device: Apple M4
0.00.062.805 I ggml_metal_init: picking default device: Apple M4
0.00.063.400 I ggml_metal_init: using embedded metal library
0.00.065.717 I ggml_metal_init: GPU name:   Apple M4
0.00.065.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.720 I ggml_metal_init: simdgroup reduction   = true
0.00.065.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.720 I ggml_metal_init: has bfloat            = true
0.00.065.720 I ggml_metal_init: use bfloat            = true
0.00.065.720 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.721 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.279 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.232 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.238 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.257 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.382 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.384 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.384 I llama_init_from_model: graph nodes  = 967
0.00.097.384 I llama_init_from_model: graph splits = 2
0.00.097.388 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.530 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.900.999 I main: llama threadpool init, n_threads = 4
0.00.901.039 I 
0.00.901.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.901.063 I 
0.00.901.289 I sampler seed: 1234
0.00.901.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.901.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.901.305 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.901.305 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.631.598 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62999.11 tokens per second)
0.01.631.598 I llama_perf_context_print:        load time =     892.07 ms
0.01.631.599 I llama_perf_context_print: prompt eval time =      43.54 ms /     7 tokens (    6.22 ms per token,   160.76 tokens per second)
0.01.631.600 I llama_perf_context_print:        eval time =     683.87 ms /    63 runs   (   10.86 ms per token,    92.12 tokens per second)
0.01.631.601 I llama_perf_context_print:       total time =     730.60 ms /    70 tokens
0.01.631.833 I ggml_metal_free: deallocating

real	0m1.647s
user	0m0.110s
sys	0m0.147s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.013.807 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.136 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.142 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.143 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.144 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.144 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.144 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.146 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.147 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.147 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.148 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.148 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.149 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.150 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.151 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.151 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.926 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.228 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.037.229 I llama_model_loader: - type  f32:  194 tensors
0.00.037.229 I llama_model_loader: - type q5_0:   97 tensors
0.00.037.229 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.230 I print_info: file format = GGUF V3 (latest)
0.00.037.235 I print_info: file type   = Q5_0
0.00.037.236 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.072.382 I load: special tokens cache size = 25
0.00.082.463 I load: token to piece cache size = 0.2984 MB
0.00.082.467 I print_info: arch             = gptneox
0.00.082.467 I print_info: vocab_only       = 0
0.00.082.467 I print_info: n_ctx_train      = 2048
0.00.082.467 I print_info: n_embd           = 2048
0.00.082.468 I print_info: n_layer          = 24
0.00.082.471 I print_info: n_head           = 16
0.00.082.472 I print_info: n_head_kv        = 16
0.00.082.473 I print_info: n_rot            = 32
0.00.082.473 I print_info: n_swa            = 0
0.00.082.473 I print_info: n_embd_head_k    = 128
0.00.082.473 I print_info: n_embd_head_v    = 128
0.00.082.474 I print_info: n_gqa            = 1
0.00.082.475 I print_info: n_embd_k_gqa     = 2048
0.00.082.476 I print_info: n_embd_v_gqa     = 2048
0.00.082.477 I print_info: f_norm_eps       = 1.0e-05
0.00.082.477 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.477 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.478 I print_info: f_logit_scale    = 0.0e+00
0.00.082.479 I print_info: n_ff             = 8192
0.00.082.479 I print_info: n_expert         = 0
0.00.082.479 I print_info: n_expert_used    = 0
0.00.082.479 I print_info: causal attn      = 1
0.00.082.479 I print_info: pooling type     = 0
0.00.082.479 I print_info: rope type        = 2
0.00.082.482 I print_info: rope scaling     = linear
0.00.082.482 I print_info: freq_base_train  = 10000.0
0.00.082.483 I print_info: freq_scale_train = 1
0.00.082.483 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.483 I print_info: rope_finetuned   = unknown
0.00.082.484 I print_info: ssm_d_conv       = 0
0.00.082.484 I print_info: ssm_d_inner      = 0
0.00.082.484 I print_info: ssm_d_state      = 0
0.00.082.484 I print_info: ssm_dt_rank      = 0
0.00.082.484 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.484 I print_info: model type       = 1.4B
0.00.082.485 I print_info: model params     = 1.41 B
0.00.082.485 I print_info: general.name     = 1.4B
0.00.082.486 I print_info: vocab type       = BPE
0.00.082.486 I print_info: n_vocab          = 50304
0.00.082.486 I print_info: n_merges         = 50009
0.00.082.486 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.487 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.487 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.489 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.489 I print_info: LF token         = 128 'Ä'
0.00.082.489 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.489 I print_info: max token length = 1024
0.00.085.181 I load_tensors: offloading 24 repeating layers to GPU
0.00.085.181 I load_tensors: offloading output layer to GPU
0.00.085.181 I load_tensors: offloaded 25/25 layers to GPU
0.00.085.193 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.085.195 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.085.580 I llama_init_from_model: n_seq_max     = 1
0.00.085.581 I llama_init_from_model: n_ctx         = 2048
0.00.085.581 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.085.582 I llama_init_from_model: n_batch       = 2048
0.00.085.582 I llama_init_from_model: n_ubatch      = 512
0.00.085.582 I llama_init_from_model: flash_attn    = 0
0.00.085.583 I llama_init_from_model: freq_base     = 10000.0
0.00.085.583 I llama_init_from_model: freq_scale    = 1
0.00.085.584 I ggml_metal_init: allocating
0.00.085.587 I ggml_metal_init: found device: Apple M4
0.00.085.590 I ggml_metal_init: picking default device: Apple M4
0.00.086.426 I ggml_metal_init: using embedded metal library
0.00.089.882 I ggml_metal_init: GPU name:   Apple M4
0.00.089.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.885 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.885 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.886 I ggml_metal_init: simdgroup reduction   = true
0.00.089.886 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.886 I ggml_metal_init: has bfloat            = true
0.00.089.886 I ggml_metal_init: use bfloat            = true
0.00.089.887 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.734 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.122.721 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.728 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.748 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.123.778 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.123.780 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.123.780 I llama_init_from_model: graph nodes  = 967
0.00.123.780 I llama_init_from_model: graph splits = 2
0.00.123.784 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.123.918 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.123.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.900 I main: llama threadpool init, n_threads = 4
0.00.802.986 I 
0.00.803.033 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.035 I 
0.00.803.505 I sampler seed: 1234
0.00.803.515 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.545 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.547 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.547 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.599.833 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54826.25 tokens per second)
0.01.599.834 I llama_perf_context_print:        load time =     789.08 ms
0.01.599.835 I llama_perf_context_print: prompt eval time =      48.47 ms /     7 tokens (    6.92 ms per token,   144.42 tokens per second)
0.01.599.836 I llama_perf_context_print:        eval time =     744.68 ms /    63 runs   (   11.82 ms per token,    84.60 tokens per second)
0.01.599.836 I llama_perf_context_print:       total time =     796.94 ms /    70 tokens
0.01.600.093 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.143s
sys	0m0.173s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.726 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.235 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.022.239 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.246 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.246 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.248 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.253 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.898 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.893 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.669 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.671 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.030.672 I llama_model_loader: - type  f32:  194 tensors
0.00.030.672 I llama_model_loader: - type q5_1:   97 tensors
0.00.030.673 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.673 I print_info: file format = GGUF V3 (latest)
0.00.030.674 I print_info: file type   = Q5_1
0.00.030.674 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.050.302 I load: special tokens cache size = 25
0.00.056.028 I load: token to piece cache size = 0.2984 MB
0.00.056.030 I print_info: arch             = gptneox
0.00.056.031 I print_info: vocab_only       = 0
0.00.056.031 I print_info: n_ctx_train      = 2048
0.00.056.031 I print_info: n_embd           = 2048
0.00.056.031 I print_info: n_layer          = 24
0.00.056.034 I print_info: n_head           = 16
0.00.056.035 I print_info: n_head_kv        = 16
0.00.056.035 I print_info: n_rot            = 32
0.00.056.035 I print_info: n_swa            = 0
0.00.056.036 I print_info: n_embd_head_k    = 128
0.00.056.036 I print_info: n_embd_head_v    = 128
0.00.056.037 I print_info: n_gqa            = 1
0.00.056.037 I print_info: n_embd_k_gqa     = 2048
0.00.056.038 I print_info: n_embd_v_gqa     = 2048
0.00.056.039 I print_info: f_norm_eps       = 1.0e-05
0.00.056.039 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.039 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.039 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.039 I print_info: f_logit_scale    = 0.0e+00
0.00.056.040 I print_info: n_ff             = 8192
0.00.056.040 I print_info: n_expert         = 0
0.00.056.041 I print_info: n_expert_used    = 0
0.00.056.041 I print_info: causal attn      = 1
0.00.056.041 I print_info: pooling type     = 0
0.00.056.041 I print_info: rope type        = 2
0.00.056.041 I print_info: rope scaling     = linear
0.00.056.042 I print_info: freq_base_train  = 10000.0
0.00.056.042 I print_info: freq_scale_train = 1
0.00.056.042 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.042 I print_info: rope_finetuned   = unknown
0.00.056.043 I print_info: ssm_d_conv       = 0
0.00.056.043 I print_info: ssm_d_inner      = 0
0.00.056.045 I print_info: ssm_d_state      = 0
0.00.056.045 I print_info: ssm_dt_rank      = 0
0.00.056.045 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.045 I print_info: model type       = 1.4B
0.00.056.046 I print_info: model params     = 1.41 B
0.00.056.046 I print_info: general.name     = 1.4B
0.00.056.046 I print_info: vocab type       = BPE
0.00.056.047 I print_info: n_vocab          = 50304
0.00.056.047 I print_info: n_merges         = 50009
0.00.056.047 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.047 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.047 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.047 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.048 I print_info: LF token         = 128 'Ä'
0.00.056.048 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.048 I print_info: max token length = 1024
0.00.057.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.979 I load_tensors: offloading output layer to GPU
0.00.057.979 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.990 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.057.991 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.058.286 I llama_init_from_model: n_seq_max     = 1
0.00.058.286 I llama_init_from_model: n_ctx         = 2048
0.00.058.287 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.287 I llama_init_from_model: n_batch       = 2048
0.00.058.287 I llama_init_from_model: n_ubatch      = 512
0.00.058.287 I llama_init_from_model: flash_attn    = 0
0.00.058.288 I llama_init_from_model: freq_base     = 10000.0
0.00.058.288 I llama_init_from_model: freq_scale    = 1
0.00.058.288 I ggml_metal_init: allocating
0.00.058.291 I ggml_metal_init: found device: Apple M4
0.00.058.294 I ggml_metal_init: picking default device: Apple M4
0.00.058.884 I ggml_metal_init: using embedded metal library
0.00.061.183 I ggml_metal_init: GPU name:   Apple M4
0.00.061.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.185 I ggml_metal_init: simdgroup reduction   = true
0.00.061.185 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.185 I ggml_metal_init: has bfloat            = true
0.00.061.186 I ggml_metal_init: use bfloat            = true
0.00.061.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.783 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.821 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.828 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.845 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.989 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.990 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.990 I llama_init_from_model: graph nodes  = 967
0.00.090.991 I llama_init_from_model: graph splits = 2
0.00.090.994 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.122 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.122 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.895.957 I main: llama threadpool init, n_threads = 4
0.00.896.052 I 
0.00.896.097 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.896.097 I 
0.00.896.638 I sampler seed: 1234
0.00.896.649 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.896.679 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.896.682 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.896.682 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.744.851 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48730.27 tokens per second)
0.01.744.852 I llama_perf_context_print:        load time =     887.22 ms
0.01.744.853 I llama_perf_context_print: prompt eval time =      53.35 ms /     7 tokens (    7.62 ms per token,   131.21 tokens per second)
0.01.744.853 I llama_perf_context_print:        eval time =     791.65 ms /    63 runs   (   12.57 ms per token,    79.58 tokens per second)
0.01.744.853 I llama_perf_context_print:       total time =     848.91 ms /    70 tokens
0.01.745.107 I ggml_metal_free: deallocating

real	0m1.762s
user	0m0.119s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.015.602 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.102 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.103 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.105 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.106 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.108 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.109 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.679 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.219 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.220 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.220 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.220 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.221 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.034.221 I llama_model_loader: - type  f32:  194 tensors
0.00.034.222 I llama_model_loader: - type q2_K:   49 tensors
0.00.034.222 I llama_model_loader: - type q3_K:   48 tensors
0.00.034.222 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.223 I print_info: file format = GGUF V3 (latest)
0.00.034.223 I print_info: file type   = Q2_K - Medium
0.00.034.224 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.062.336 I load: special tokens cache size = 25
0.00.072.266 I load: token to piece cache size = 0.2984 MB
0.00.072.270 I print_info: arch             = gptneox
0.00.072.270 I print_info: vocab_only       = 0
0.00.072.271 I print_info: n_ctx_train      = 2048
0.00.072.271 I print_info: n_embd           = 2048
0.00.072.271 I print_info: n_layer          = 24
0.00.072.274 I print_info: n_head           = 16
0.00.072.276 I print_info: n_head_kv        = 16
0.00.072.276 I print_info: n_rot            = 32
0.00.072.276 I print_info: n_swa            = 0
0.00.072.276 I print_info: n_embd_head_k    = 128
0.00.072.277 I print_info: n_embd_head_v    = 128
0.00.072.278 I print_info: n_gqa            = 1
0.00.072.279 I print_info: n_embd_k_gqa     = 2048
0.00.072.283 I print_info: n_embd_v_gqa     = 2048
0.00.072.284 I print_info: f_norm_eps       = 1.0e-05
0.00.072.284 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.284 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.285 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.285 I print_info: f_logit_scale    = 0.0e+00
0.00.072.286 I print_info: n_ff             = 8192
0.00.072.286 I print_info: n_expert         = 0
0.00.072.286 I print_info: n_expert_used    = 0
0.00.072.286 I print_info: causal attn      = 1
0.00.072.288 I print_info: pooling type     = 0
0.00.072.289 I print_info: rope type        = 2
0.00.072.289 I print_info: rope scaling     = linear
0.00.072.289 I print_info: freq_base_train  = 10000.0
0.00.072.290 I print_info: freq_scale_train = 1
0.00.072.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.290 I print_info: rope_finetuned   = unknown
0.00.072.290 I print_info: ssm_d_conv       = 0
0.00.072.291 I print_info: ssm_d_inner      = 0
0.00.072.291 I print_info: ssm_d_state      = 0
0.00.072.291 I print_info: ssm_dt_rank      = 0
0.00.072.291 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.292 I print_info: model type       = 1.4B
0.00.072.298 I print_info: model params     = 1.41 B
0.00.072.298 I print_info: general.name     = 1.4B
0.00.072.299 I print_info: vocab type       = BPE
0.00.072.299 I print_info: n_vocab          = 50304
0.00.072.299 I print_info: n_merges         = 50009
0.00.072.300 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.300 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.300 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.300 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.301 I print_info: LF token         = 128 'Ä'
0.00.072.303 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.303 I print_info: max token length = 1024
0.00.074.982 I load_tensors: offloading 24 repeating layers to GPU
0.00.074.982 I load_tensors: offloading output layer to GPU
0.00.074.982 I load_tensors: offloaded 25/25 layers to GPU
0.00.074.994 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.074.995 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.075.442 I llama_init_from_model: n_seq_max     = 1
0.00.075.443 I llama_init_from_model: n_ctx         = 2048
0.00.075.444 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.075.444 I llama_init_from_model: n_batch       = 2048
0.00.075.444 I llama_init_from_model: n_ubatch      = 512
0.00.075.444 I llama_init_from_model: flash_attn    = 0
0.00.075.445 I llama_init_from_model: freq_base     = 10000.0
0.00.075.445 I llama_init_from_model: freq_scale    = 1
0.00.075.446 I ggml_metal_init: allocating
0.00.075.450 I ggml_metal_init: found device: Apple M4
0.00.075.453 I ggml_metal_init: picking default device: Apple M4
0.00.076.318 I ggml_metal_init: using embedded metal library
0.00.080.022 I ggml_metal_init: GPU name:   Apple M4
0.00.080.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.025 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.026 I ggml_metal_init: simdgroup reduction   = true
0.00.080.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.026 I ggml_metal_init: has bfloat            = true
0.00.080.026 I ggml_metal_init: use bfloat            = true
0.00.080.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.028 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.737 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.112.884 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.893 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.924 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.113.939 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.113.941 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.113.941 I llama_init_from_model: graph nodes  = 967
0.00.113.941 I llama_init_from_model: graph splits = 2
0.00.113.945 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.469 I main: llama threadpool init, n_threads = 4
0.00.629.512 I 
0.00.629.533 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.535 I 
0.00.629.768 I sampler seed: 1234
0.00.629.772 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.813 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.813 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.813 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.307.859 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.307.860 I llama_perf_context_print:        load time =     613.86 ms
0.01.307.861 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.82 tokens per second)
0.01.307.861 I llama_perf_context_print:        eval time =     639.27 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.307.862 I llama_perf_context_print:       total time =     678.40 ms /    70 tokens
0.01.308.055 I ggml_metal_free: deallocating

real	0m1.342s
user	0m0.129s
sys	0m0.125s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.017.358 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.108 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.027.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.126 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.128 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.128 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.128 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.973 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.006 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.036.007 I llama_model_loader: - type  f32:  194 tensors
0.00.036.007 I llama_model_loader: - type q3_K:   25 tensors
0.00.036.007 I llama_model_loader: - type q4_K:   71 tensors
0.00.036.008 I llama_model_loader: - type q5_K:    1 tensors
0.00.036.008 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.008 I print_info: file format = GGUF V3 (latest)
0.00.036.009 I print_info: file type   = Q3_K - Medium
0.00.036.010 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.057.015 I load: special tokens cache size = 25
0.00.062.913 I load: token to piece cache size = 0.2984 MB
0.00.062.916 I print_info: arch             = gptneox
0.00.062.916 I print_info: vocab_only       = 0
0.00.062.916 I print_info: n_ctx_train      = 2048
0.00.062.916 I print_info: n_embd           = 2048
0.00.062.917 I print_info: n_layer          = 24
0.00.062.920 I print_info: n_head           = 16
0.00.062.921 I print_info: n_head_kv        = 16
0.00.062.921 I print_info: n_rot            = 32
0.00.062.921 I print_info: n_swa            = 0
0.00.062.921 I print_info: n_embd_head_k    = 128
0.00.062.923 I print_info: n_embd_head_v    = 128
0.00.062.924 I print_info: n_gqa            = 1
0.00.062.925 I print_info: n_embd_k_gqa     = 2048
0.00.062.925 I print_info: n_embd_v_gqa     = 2048
0.00.062.926 I print_info: f_norm_eps       = 1.0e-05
0.00.062.926 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.926 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.926 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.926 I print_info: f_logit_scale    = 0.0e+00
0.00.062.927 I print_info: n_ff             = 8192
0.00.062.927 I print_info: n_expert         = 0
0.00.062.927 I print_info: n_expert_used    = 0
0.00.062.927 I print_info: causal attn      = 1
0.00.062.927 I print_info: pooling type     = 0
0.00.062.928 I print_info: rope type        = 2
0.00.062.928 I print_info: rope scaling     = linear
0.00.062.928 I print_info: freq_base_train  = 10000.0
0.00.062.928 I print_info: freq_scale_train = 1
0.00.062.929 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.929 I print_info: rope_finetuned   = unknown
0.00.062.929 I print_info: ssm_d_conv       = 0
0.00.062.929 I print_info: ssm_d_inner      = 0
0.00.062.930 I print_info: ssm_d_state      = 0
0.00.062.930 I print_info: ssm_dt_rank      = 0
0.00.062.930 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.930 I print_info: model type       = 1.4B
0.00.062.930 I print_info: model params     = 1.41 B
0.00.062.930 I print_info: general.name     = 1.4B
0.00.062.931 I print_info: vocab type       = BPE
0.00.062.931 I print_info: n_vocab          = 50304
0.00.062.931 I print_info: n_merges         = 50009
0.00.062.932 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.932 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.932 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.932 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.932 I print_info: LF token         = 128 'Ä'
0.00.062.933 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.933 I print_info: max token length = 1024
0.00.064.719 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.719 I load_tensors: offloading output layer to GPU
0.00.064.719 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.725 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.064.725 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.065.115 I llama_init_from_model: n_seq_max     = 1
0.00.065.116 I llama_init_from_model: n_ctx         = 2048
0.00.065.116 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.065.116 I llama_init_from_model: n_batch       = 2048
0.00.065.116 I llama_init_from_model: n_ubatch      = 512
0.00.065.116 I llama_init_from_model: flash_attn    = 0
0.00.065.117 I llama_init_from_model: freq_base     = 10000.0
0.00.065.117 I llama_init_from_model: freq_scale    = 1
0.00.065.117 I ggml_metal_init: allocating
0.00.065.120 I ggml_metal_init: found device: Apple M4
0.00.065.122 I ggml_metal_init: picking default device: Apple M4
0.00.065.714 I ggml_metal_init: using embedded metal library
0.00.068.034 I ggml_metal_init: GPU name:   Apple M4
0.00.068.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.036 I ggml_metal_init: simdgroup reduction   = true
0.00.068.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.036 I ggml_metal_init: has bfloat            = true
0.00.068.037 I ggml_metal_init: use bfloat            = true
0.00.068.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.602 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.435 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.440 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.458 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.099.587 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.099.588 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.099.589 I llama_init_from_model: graph nodes  = 967
0.00.099.589 I llama_init_from_model: graph splits = 2
0.00.099.592 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.404 I main: llama threadpool init, n_threads = 4
0.00.582.442 I 
0.00.582.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.465 I 
0.00.582.677 I sampler seed: 1234
0.00.582.681 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.582.723 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.582.724 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.582.724 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.331.250 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59915.61 tokens per second)
0.01.331.251 I llama_perf_context_print:        load time =     565.04 ms
0.01.331.251 I llama_perf_context_print: prompt eval time =      44.89 ms /     7 tokens (    6.41 ms per token,   155.93 tokens per second)
0.01.331.252 I llama_perf_context_print:        eval time =     700.71 ms /    63 runs   (   11.12 ms per token,    89.91 tokens per second)
0.01.331.253 I llama_perf_context_print:       total time =     748.85 ms /    70 tokens
0.01.331.482 I ggml_metal_free: deallocating

real	0m1.347s
user	0m0.110s
sys	0m0.124s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.808 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.024.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.543 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.517 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.399 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.400 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.401 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.401 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.033.402 I llama_model_loader: - type  f32:  194 tensors
0.00.033.402 I llama_model_loader: - type q4_K:   61 tensors
0.00.033.402 I llama_model_loader: - type q5_K:   24 tensors
0.00.033.402 I llama_model_loader: - type q6_K:   13 tensors
0.00.033.403 I print_info: file format = GGUF V3 (latest)
0.00.033.403 I print_info: file type   = Q4_K - Medium
0.00.033.406 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.053.827 I load: special tokens cache size = 25
0.00.059.697 I load: token to piece cache size = 0.2984 MB
0.00.059.700 I print_info: arch             = gptneox
0.00.059.700 I print_info: vocab_only       = 0
0.00.059.700 I print_info: n_ctx_train      = 2048
0.00.059.701 I print_info: n_embd           = 2048
0.00.059.701 I print_info: n_layer          = 24
0.00.059.704 I print_info: n_head           = 16
0.00.059.705 I print_info: n_head_kv        = 16
0.00.059.706 I print_info: n_rot            = 32
0.00.059.706 I print_info: n_swa            = 0
0.00.059.706 I print_info: n_embd_head_k    = 128
0.00.059.707 I print_info: n_embd_head_v    = 128
0.00.059.708 I print_info: n_gqa            = 1
0.00.059.709 I print_info: n_embd_k_gqa     = 2048
0.00.059.710 I print_info: n_embd_v_gqa     = 2048
0.00.059.715 I print_info: f_norm_eps       = 1.0e-05
0.00.059.715 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.715 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.715 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.715 I print_info: f_logit_scale    = 0.0e+00
0.00.059.716 I print_info: n_ff             = 8192
0.00.059.716 I print_info: n_expert         = 0
0.00.059.716 I print_info: n_expert_used    = 0
0.00.059.717 I print_info: causal attn      = 1
0.00.059.718 I print_info: pooling type     = 0
0.00.059.720 I print_info: rope type        = 2
0.00.059.720 I print_info: rope scaling     = linear
0.00.059.720 I print_info: freq_base_train  = 10000.0
0.00.059.721 I print_info: freq_scale_train = 1
0.00.059.721 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.721 I print_info: rope_finetuned   = unknown
0.00.059.721 I print_info: ssm_d_conv       = 0
0.00.059.721 I print_info: ssm_d_inner      = 0
0.00.059.722 I print_info: ssm_d_state      = 0
0.00.059.722 I print_info: ssm_dt_rank      = 0
0.00.059.722 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.722 I print_info: model type       = 1.4B
0.00.059.724 I print_info: model params     = 1.41 B
0.00.059.724 I print_info: general.name     = 1.4B
0.00.059.724 I print_info: vocab type       = BPE
0.00.059.725 I print_info: n_vocab          = 50304
0.00.059.725 I print_info: n_merges         = 50009
0.00.059.726 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.726 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.727 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.727 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.727 I print_info: LF token         = 128 'Ä'
0.00.059.728 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.728 I print_info: max token length = 1024
0.00.061.661 I load_tensors: offloading 24 repeating layers to GPU
0.00.061.661 I load_tensors: offloading output layer to GPU
0.00.061.662 I load_tensors: offloaded 25/25 layers to GPU
0.00.061.673 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.061.674 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.061.961 I llama_init_from_model: n_seq_max     = 1
0.00.061.962 I llama_init_from_model: n_ctx         = 2048
0.00.061.962 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.061.962 I llama_init_from_model: n_batch       = 2048
0.00.061.962 I llama_init_from_model: n_ubatch      = 512
0.00.061.962 I llama_init_from_model: flash_attn    = 0
0.00.061.963 I llama_init_from_model: freq_base     = 10000.0
0.00.061.963 I llama_init_from_model: freq_scale    = 1
0.00.061.963 I ggml_metal_init: allocating
0.00.061.966 I ggml_metal_init: found device: Apple M4
0.00.061.968 I ggml_metal_init: picking default device: Apple M4
0.00.062.567 I ggml_metal_init: using embedded metal library
0.00.064.878 I ggml_metal_init: GPU name:   Apple M4
0.00.064.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.880 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.880 I ggml_metal_init: simdgroup reduction   = true
0.00.064.881 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.881 I ggml_metal_init: has bfloat            = true
0.00.064.881 I ggml_metal_init: use bfloat            = true
0.00.064.881 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.305 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.215 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.222 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.241 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.318 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.320 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.320 I llama_init_from_model: graph nodes  = 967
0.00.097.320 I llama_init_from_model: graph splits = 2
0.00.097.323 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.463 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.201 I main: llama threadpool init, n_threads = 4
0.00.708.249 I 
0.00.708.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.274 I 
0.00.708.507 I sampler seed: 1234
0.00.708.511 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.708.545 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.708.565 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.708.568 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.475.474 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.475.474 I llama_perf_context_print:        load time =     699.39 ms
0.01.475.475 I llama_perf_context_print: prompt eval time =      53.53 ms /     7 tokens (    7.65 ms per token,   130.76 tokens per second)
0.01.475.476 I llama_perf_context_print:        eval time =     710.27 ms /    63 runs   (   11.27 ms per token,    88.70 tokens per second)
0.01.475.476 I llama_perf_context_print:       total time =     767.28 ms /    70 tokens
0.01.475.683 I ggml_metal_free: deallocating

real	0m1.493s
user	0m0.110s
sys	0m0.145s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.015.036 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.085 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.035.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.092 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.092 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.098 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.099 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.099 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.099 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.100 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.104 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.104 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.702 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.704 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.705 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.046.705 I llama_model_loader: - type  f32:  194 tensors
0.00.046.706 I llama_model_loader: - type q5_K:   61 tensors
0.00.046.706 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.707 I print_info: file format = GGUF V3 (latest)
0.00.046.708 I print_info: file type   = Q5_K - Medium
0.00.046.709 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.080.030 I load: special tokens cache size = 25
0.00.089.677 I load: token to piece cache size = 0.2984 MB
0.00.089.682 I print_info: arch             = gptneox
0.00.089.682 I print_info: vocab_only       = 0
0.00.089.682 I print_info: n_ctx_train      = 2048
0.00.089.682 I print_info: n_embd           = 2048
0.00.089.683 I print_info: n_layer          = 24
0.00.089.686 I print_info: n_head           = 16
0.00.089.689 I print_info: n_head_kv        = 16
0.00.089.689 I print_info: n_rot            = 32
0.00.089.689 I print_info: n_swa            = 0
0.00.089.689 I print_info: n_embd_head_k    = 128
0.00.089.690 I print_info: n_embd_head_v    = 128
0.00.089.690 I print_info: n_gqa            = 1
0.00.089.691 I print_info: n_embd_k_gqa     = 2048
0.00.089.692 I print_info: n_embd_v_gqa     = 2048
0.00.089.693 I print_info: f_norm_eps       = 1.0e-05
0.00.089.693 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.693 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.694 I print_info: f_logit_scale    = 0.0e+00
0.00.089.695 I print_info: n_ff             = 8192
0.00.089.695 I print_info: n_expert         = 0
0.00.089.695 I print_info: n_expert_used    = 0
0.00.089.695 I print_info: causal attn      = 1
0.00.089.696 I print_info: pooling type     = 0
0.00.089.697 I print_info: rope type        = 2
0.00.089.700 I print_info: rope scaling     = linear
0.00.089.700 I print_info: freq_base_train  = 10000.0
0.00.089.701 I print_info: freq_scale_train = 1
0.00.089.701 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.701 I print_info: rope_finetuned   = unknown
0.00.089.701 I print_info: ssm_d_conv       = 0
0.00.089.701 I print_info: ssm_d_inner      = 0
0.00.089.702 I print_info: ssm_d_state      = 0
0.00.089.702 I print_info: ssm_dt_rank      = 0
0.00.089.702 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.702 I print_info: model type       = 1.4B
0.00.089.703 I print_info: model params     = 1.41 B
0.00.089.708 I print_info: general.name     = 1.4B
0.00.089.708 I print_info: vocab type       = BPE
0.00.089.709 I print_info: n_vocab          = 50304
0.00.089.709 I print_info: n_merges         = 50009
0.00.089.709 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.709 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.710 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.710 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.711 I print_info: LF token         = 128 'Ä'
0.00.089.711 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.711 I print_info: max token length = 1024
0.00.092.476 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.476 I load_tensors: offloading output layer to GPU
0.00.092.477 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.488 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.092.490 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.092.907 I llama_init_from_model: n_seq_max     = 1
0.00.092.908 I llama_init_from_model: n_ctx         = 2048
0.00.092.908 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.092.909 I llama_init_from_model: n_batch       = 2048
0.00.092.909 I llama_init_from_model: n_ubatch      = 512
0.00.092.909 I llama_init_from_model: flash_attn    = 0
0.00.092.910 I llama_init_from_model: freq_base     = 10000.0
0.00.092.910 I llama_init_from_model: freq_scale    = 1
0.00.092.910 I ggml_metal_init: allocating
0.00.092.914 I ggml_metal_init: found device: Apple M4
0.00.092.917 I ggml_metal_init: picking default device: Apple M4
0.00.093.788 I ggml_metal_init: using embedded metal library
0.00.097.316 I ggml_metal_init: GPU name:   Apple M4
0.00.097.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.319 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.320 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.320 I ggml_metal_init: simdgroup reduction   = true
0.00.097.320 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.320 I ggml_metal_init: has bfloat            = true
0.00.097.320 I ggml_metal_init: use bfloat            = true
0.00.097.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.322 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.416 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.131.583 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.131.590 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.131.612 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.132.715 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.132.716 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.132.717 I llama_init_from_model: graph nodes  = 967
0.00.132.717 I llama_init_from_model: graph splits = 2
0.00.132.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.132.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.132.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.520 I main: llama threadpool init, n_threads = 4
0.00.847.622 I 
0.00.847.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.681 I 
0.00.848.193 I sampler seed: 1234
0.00.848.202 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.848.234 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.848.236 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.848.236 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.697.802 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.01.697.803 I llama_perf_context_print:        load time =     832.48 ms
0.01.697.804 I llama_perf_context_print: prompt eval time =      52.24 ms /     7 tokens (    7.46 ms per token,   134.00 tokens per second)
0.01.697.804 I llama_perf_context_print:        eval time =     794.33 ms /    63 runs   (   12.61 ms per token,    79.31 tokens per second)
0.01.697.805 I llama_perf_context_print:       total time =     850.29 ms /    70 tokens
0.01.698.040 I ggml_metal_free: deallocating

real	0m1.732s
user	0m0.144s
sys	0m0.195s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.639 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.493 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.497 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.505 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.509 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.214 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.927 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.928 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.929 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.930 I llama_model_loader: - type  f32:  194 tensors
0.00.024.930 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.931 I print_info: file format = GGUF V3 (latest)
0.00.024.931 I print_info: file type   = Q6_K
0.00.024.932 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.244 I load: special tokens cache size = 25
0.00.050.146 I load: token to piece cache size = 0.2984 MB
0.00.050.149 I print_info: arch             = gptneox
0.00.050.149 I print_info: vocab_only       = 0
0.00.050.149 I print_info: n_ctx_train      = 2048
0.00.050.149 I print_info: n_embd           = 2048
0.00.050.149 I print_info: n_layer          = 24
0.00.050.153 I print_info: n_head           = 16
0.00.050.153 I print_info: n_head_kv        = 16
0.00.050.154 I print_info: n_rot            = 32
0.00.050.154 I print_info: n_swa            = 0
0.00.050.156 I print_info: n_embd_head_k    = 128
0.00.050.156 I print_info: n_embd_head_v    = 128
0.00.050.157 I print_info: n_gqa            = 1
0.00.050.158 I print_info: n_embd_k_gqa     = 2048
0.00.050.159 I print_info: n_embd_v_gqa     = 2048
0.00.050.159 I print_info: f_norm_eps       = 1.0e-05
0.00.050.160 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.161 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.161 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.161 I print_info: f_logit_scale    = 0.0e+00
0.00.050.162 I print_info: n_ff             = 8192
0.00.050.162 I print_info: n_expert         = 0
0.00.050.162 I print_info: n_expert_used    = 0
0.00.050.162 I print_info: causal attn      = 1
0.00.050.162 I print_info: pooling type     = 0
0.00.050.163 I print_info: rope type        = 2
0.00.050.163 I print_info: rope scaling     = linear
0.00.050.163 I print_info: freq_base_train  = 10000.0
0.00.050.164 I print_info: freq_scale_train = 1
0.00.050.164 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.164 I print_info: rope_finetuned   = unknown
0.00.050.164 I print_info: ssm_d_conv       = 0
0.00.050.164 I print_info: ssm_d_inner      = 0
0.00.050.165 I print_info: ssm_d_state      = 0
0.00.050.165 I print_info: ssm_dt_rank      = 0
0.00.050.165 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.165 I print_info: model type       = 1.4B
0.00.050.166 I print_info: model params     = 1.41 B
0.00.050.166 I print_info: general.name     = 1.4B
0.00.050.166 I print_info: vocab type       = BPE
0.00.050.166 I print_info: n_vocab          = 50304
0.00.050.167 I print_info: n_merges         = 50009
0.00.050.167 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.167 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.167 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.168 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.168 I print_info: LF token         = 128 'Ä'
0.00.050.168 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.168 I print_info: max token length = 1024
0.00.052.152 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.152 I load_tensors: offloading output layer to GPU
0.00.052.153 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.163 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.165 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.440 I llama_init_from_model: n_seq_max     = 1
0.00.052.441 I llama_init_from_model: n_ctx         = 2048
0.00.052.441 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.441 I llama_init_from_model: n_batch       = 2048
0.00.052.442 I llama_init_from_model: n_ubatch      = 512
0.00.052.442 I llama_init_from_model: flash_attn    = 0
0.00.052.442 I llama_init_from_model: freq_base     = 10000.0
0.00.052.442 I llama_init_from_model: freq_scale    = 1
0.00.052.443 I ggml_metal_init: allocating
0.00.052.446 I ggml_metal_init: found device: Apple M4
0.00.052.448 I ggml_metal_init: picking default device: Apple M4
0.00.053.020 I ggml_metal_init: using embedded metal library
0.00.055.331 I ggml_metal_init: GPU name:   Apple M4
0.00.055.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.332 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.333 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.333 I ggml_metal_init: simdgroup reduction   = true
0.00.055.333 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.333 I ggml_metal_init: has bfloat            = true
0.00.055.333 I ggml_metal_init: use bfloat            = true
0.00.055.334 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.961 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.114 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.120 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.102 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.103 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.104 I llama_init_from_model: graph nodes  = 967
0.00.085.104 I llama_init_from_model: graph splits = 2
0.00.085.107 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.236 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.127 I main: llama threadpool init, n_threads = 4
0.00.723.172 I 
0.00.723.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.192 I 
0.00.723.349 I sampler seed: 1234
0.00.723.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.723.391 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.723.392 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.723.392 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.605.188 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.605.189 I llama_perf_context_print:        load time =     714.48 ms
0.01.605.189 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.71 tokens per second)
0.01.605.190 I llama_perf_context_print:        eval time =     824.44 ms /    63 runs   (   13.09 ms per token,    76.42 tokens per second)
0.01.605.193 I llama_perf_context_print:       total time =     882.07 ms /    70 tokens
0.01.605.448 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.108s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.518 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.377 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.047 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.065 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.074 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.075 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.076 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.077 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.077 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.080 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.080 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.092 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.332 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.680 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.348 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.349 I llama_model_loader: - type  f32:  194 tensors
0.00.056.350 I llama_model_loader: - type  f16:   98 tensors
0.00.056.351 I print_info: file format = GGUF V3 (latest)
0.00.056.352 I print_info: file type   = all F32 (guessed)
0.00.056.354 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.458 I load: special tokens cache size = 25
0.00.091.267 I load: token to piece cache size = 0.2984 MB
0.00.091.270 I print_info: arch             = gptneox
0.00.091.270 I print_info: vocab_only       = 0
0.00.091.270 I print_info: n_ctx_train      = 2048
0.00.091.270 I print_info: n_embd           = 2048
0.00.091.270 I print_info: n_layer          = 24
0.00.091.273 I print_info: n_head           = 16
0.00.091.274 I print_info: n_head_kv        = 16
0.00.091.274 I print_info: n_rot            = 32
0.00.091.275 I print_info: n_swa            = 0
0.00.091.275 I print_info: n_embd_head_k    = 128
0.00.091.275 I print_info: n_embd_head_v    = 128
0.00.091.275 I print_info: n_gqa            = 1
0.00.091.276 I print_info: n_embd_k_gqa     = 2048
0.00.091.277 I print_info: n_embd_v_gqa     = 2048
0.00.091.277 I print_info: f_norm_eps       = 1.0e-05
0.00.091.278 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.278 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.278 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.278 I print_info: f_logit_scale    = 0.0e+00
0.00.091.279 I print_info: n_ff             = 8192
0.00.091.279 I print_info: n_expert         = 0
0.00.091.279 I print_info: n_expert_used    = 0
0.00.091.279 I print_info: causal attn      = 1
0.00.091.279 I print_info: pooling type     = 0
0.00.091.282 I print_info: rope type        = 2
0.00.091.282 I print_info: rope scaling     = linear
0.00.091.282 I print_info: freq_base_train  = 10000.0
0.00.091.283 I print_info: freq_scale_train = 1
0.00.091.283 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.283 I print_info: rope_finetuned   = unknown
0.00.091.283 I print_info: ssm_d_conv       = 0
0.00.091.283 I print_info: ssm_d_inner      = 0
0.00.091.284 I print_info: ssm_d_state      = 0
0.00.091.284 I print_info: ssm_dt_rank      = 0
0.00.091.284 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.284 I print_info: model type       = 1.4B
0.00.091.284 I print_info: model params     = 1.41 B
0.00.091.284 I print_info: general.name     = 1.4B
0.00.091.285 I print_info: vocab type       = BPE
0.00.091.290 I print_info: n_vocab          = 50304
0.00.091.290 I print_info: n_merges         = 50009
0.00.091.290 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.290 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.290 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.291 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.291 I print_info: LF token         = 128 'Ä'
0.00.091.291 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.291 I print_info: max token length = 1024
0.00.093.856 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.857 I load_tensors: offloading output layer to GPU
0.00.093.857 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.867 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.869 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.163 I llama_init_from_model: n_seq_max     = 1
0.00.094.164 I llama_init_from_model: n_ctx         = 128
0.00.094.164 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.164 I llama_init_from_model: n_batch       = 128
0.00.094.164 I llama_init_from_model: n_ubatch      = 128
0.00.094.164 I llama_init_from_model: flash_attn    = 0
0.00.094.165 I llama_init_from_model: freq_base     = 10000.0
0.00.094.165 I llama_init_from_model: freq_scale    = 1
0.00.094.165 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.166 I ggml_metal_init: allocating
0.00.094.168 I ggml_metal_init: found device: Apple M4
0.00.094.170 I ggml_metal_init: picking default device: Apple M4
0.00.094.793 I ggml_metal_init: using embedded metal library
0.00.097.452 I ggml_metal_init: GPU name:   Apple M4
0.00.097.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.454 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.455 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.455 I ggml_metal_init: simdgroup reduction   = true
0.00.097.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.455 I ggml_metal_init: has bfloat            = true
0.00.097.455 I ggml_metal_init: use bfloat            = true
0.00.097.456 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.854 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.099 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.103 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.119 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.987 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.108.988 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.108.988 I llama_init_from_model: graph nodes  = 967
0.00.108.989 I llama_init_from_model: graph splits = 2
0.00.108.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.990 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.363.715 I 
0.01.363.759 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.363.763 I perplexity: tokenizing the input ..
0.01.376.554 I perplexity: tokenization took 12.785 ms
0.01.376.559 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.498.121 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.499.742 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.499.794 I llama_perf_context_print:        load time =    1339.32 ms
0.01.499.795 I llama_perf_context_print: prompt eval time =     121.02 ms /   128 tokens (    0.95 ms per token,  1057.67 tokens per second)
0.01.499.796 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.499.797 I llama_perf_context_print:       total time =     136.09 ms /   129 tokens
0.01.500.553 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.122s
sys	0m0.241s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.136 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.248 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.209 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.210 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.212 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.214 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.215 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.219 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.558 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.091 I llama_model_loader: - type  f32:  194 tensors
0.00.033.092 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.092 I print_info: file format = GGUF V3 (latest)
0.00.033.093 I print_info: file type   = Q8_0
0.00.033.094 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.055.998 I load: special tokens cache size = 25
0.00.062.514 I load: token to piece cache size = 0.2984 MB
0.00.062.517 I print_info: arch             = gptneox
0.00.062.518 I print_info: vocab_only       = 0
0.00.062.518 I print_info: n_ctx_train      = 2048
0.00.062.518 I print_info: n_embd           = 2048
0.00.062.518 I print_info: n_layer          = 24
0.00.062.522 I print_info: n_head           = 16
0.00.062.523 I print_info: n_head_kv        = 16
0.00.062.526 I print_info: n_rot            = 32
0.00.062.526 I print_info: n_swa            = 0
0.00.062.526 I print_info: n_embd_head_k    = 128
0.00.062.527 I print_info: n_embd_head_v    = 128
0.00.062.527 I print_info: n_gqa            = 1
0.00.062.528 I print_info: n_embd_k_gqa     = 2048
0.00.062.529 I print_info: n_embd_v_gqa     = 2048
0.00.062.529 I print_info: f_norm_eps       = 1.0e-05
0.00.062.530 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.530 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.530 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.531 I print_info: f_logit_scale    = 0.0e+00
0.00.062.531 I print_info: n_ff             = 8192
0.00.062.531 I print_info: n_expert         = 0
0.00.062.532 I print_info: n_expert_used    = 0
0.00.062.532 I print_info: causal attn      = 1
0.00.062.532 I print_info: pooling type     = 0
0.00.062.532 I print_info: rope type        = 2
0.00.062.532 I print_info: rope scaling     = linear
0.00.062.533 I print_info: freq_base_train  = 10000.0
0.00.062.533 I print_info: freq_scale_train = 1
0.00.062.533 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.533 I print_info: rope_finetuned   = unknown
0.00.062.534 I print_info: ssm_d_conv       = 0
0.00.062.534 I print_info: ssm_d_inner      = 0
0.00.062.534 I print_info: ssm_d_state      = 0
0.00.062.534 I print_info: ssm_dt_rank      = 0
0.00.062.534 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.534 I print_info: model type       = 1.4B
0.00.062.535 I print_info: model params     = 1.41 B
0.00.062.535 I print_info: general.name     = 1.4B
0.00.062.535 I print_info: vocab type       = BPE
0.00.062.536 I print_info: n_vocab          = 50304
0.00.062.536 I print_info: n_merges         = 50009
0.00.062.536 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.536 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.537 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.538 I print_info: LF token         = 128 'Ä'
0.00.062.539 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.539 I print_info: max token length = 1024
0.00.064.816 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.816 I load_tensors: offloading output layer to GPU
0.00.064.816 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.828 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.829 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.122 I llama_init_from_model: n_seq_max     = 1
0.00.065.123 I llama_init_from_model: n_ctx         = 128
0.00.065.123 I llama_init_from_model: n_ctx_per_seq = 128
0.00.065.123 I llama_init_from_model: n_batch       = 128
0.00.065.123 I llama_init_from_model: n_ubatch      = 128
0.00.065.123 I llama_init_from_model: flash_attn    = 0
0.00.065.124 I llama_init_from_model: freq_base     = 10000.0
0.00.065.124 I llama_init_from_model: freq_scale    = 1
0.00.065.124 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.125 I ggml_metal_init: allocating
0.00.065.128 I ggml_metal_init: found device: Apple M4
0.00.065.130 I ggml_metal_init: picking default device: Apple M4
0.00.065.770 I ggml_metal_init: using embedded metal library
0.00.068.298 I ggml_metal_init: GPU name:   Apple M4
0.00.068.299 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.300 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.300 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.300 I ggml_metal_init: simdgroup reduction   = true
0.00.068.301 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.301 I ggml_metal_init: has bfloat            = true
0.00.068.301 I ggml_metal_init: use bfloat            = true
0.00.068.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.166 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.079.524 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.527 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.541 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.080.518 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.080.519 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.080.520 I llama_init_from_model: graph nodes  = 967
0.00.080.520 I llama_init_from_model: graph splits = 2
0.00.080.521 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.363 I 
0.00.874.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.874.426 I perplexity: tokenizing the input ..
0.00.882.156 I perplexity: tokenization took 7.728 ms
0.00.882.159 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.006.860 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.008.024 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.008.052 I llama_perf_context_print:        load time =     863.11 ms
0.01.008.054 I llama_perf_context_print: prompt eval time =     124.45 ms /   128 tokens (    0.97 ms per token,  1028.48 tokens per second)
0.01.008.055 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.008.056 I llama_perf_context_print:       total time =     133.69 ms /   129 tokens
0.01.008.613 I ggml_metal_free: deallocating

real	0m1.025s
user	0m0.091s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.378 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.229 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.236 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.237 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.238 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.239 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.239 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.240 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.242 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.624 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.625 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.626 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.627 I llama_model_loader: - type  f32:  194 tensors
0.00.024.627 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.627 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.628 I print_info: file format = GGUF V3 (latest)
0.00.024.628 I print_info: file type   = Q4_0
0.00.024.629 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.043.726 I load: special tokens cache size = 25
0.00.049.789 I load: token to piece cache size = 0.2984 MB
0.00.049.792 I print_info: arch             = gptneox
0.00.049.792 I print_info: vocab_only       = 0
0.00.049.792 I print_info: n_ctx_train      = 2048
0.00.049.792 I print_info: n_embd           = 2048
0.00.049.792 I print_info: n_layer          = 24
0.00.049.796 I print_info: n_head           = 16
0.00.049.797 I print_info: n_head_kv        = 16
0.00.049.797 I print_info: n_rot            = 32
0.00.049.797 I print_info: n_swa            = 0
0.00.049.797 I print_info: n_embd_head_k    = 128
0.00.049.797 I print_info: n_embd_head_v    = 128
0.00.049.799 I print_info: n_gqa            = 1
0.00.049.800 I print_info: n_embd_k_gqa     = 2048
0.00.049.800 I print_info: n_embd_v_gqa     = 2048
0.00.049.801 I print_info: f_norm_eps       = 1.0e-05
0.00.049.801 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.801 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.802 I print_info: f_logit_scale    = 0.0e+00
0.00.049.802 I print_info: n_ff             = 8192
0.00.049.802 I print_info: n_expert         = 0
0.00.049.802 I print_info: n_expert_used    = 0
0.00.049.803 I print_info: causal attn      = 1
0.00.049.803 I print_info: pooling type     = 0
0.00.049.803 I print_info: rope type        = 2
0.00.049.803 I print_info: rope scaling     = linear
0.00.049.803 I print_info: freq_base_train  = 10000.0
0.00.049.804 I print_info: freq_scale_train = 1
0.00.049.804 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.804 I print_info: rope_finetuned   = unknown
0.00.049.804 I print_info: ssm_d_conv       = 0
0.00.049.805 I print_info: ssm_d_inner      = 0
0.00.049.805 I print_info: ssm_d_state      = 0
0.00.049.805 I print_info: ssm_dt_rank      = 0
0.00.049.807 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.807 I print_info: model type       = 1.4B
0.00.049.808 I print_info: model params     = 1.41 B
0.00.049.808 I print_info: general.name     = 1.4B
0.00.049.808 I print_info: vocab type       = BPE
0.00.049.809 I print_info: n_vocab          = 50304
0.00.049.809 I print_info: n_merges         = 50009
0.00.049.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.813 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.813 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: LF token         = 128 'Ä'
0.00.049.814 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: max token length = 1024
0.00.051.755 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.755 I load_tensors: offloading output layer to GPU
0.00.051.755 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.766 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.767 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.069 I llama_init_from_model: n_seq_max     = 1
0.00.052.069 I llama_init_from_model: n_ctx         = 128
0.00.052.070 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.070 I llama_init_from_model: n_batch       = 128
0.00.052.070 I llama_init_from_model: n_ubatch      = 128
0.00.052.070 I llama_init_from_model: flash_attn    = 0
0.00.052.070 I llama_init_from_model: freq_base     = 10000.0
0.00.052.071 I llama_init_from_model: freq_scale    = 1
0.00.052.071 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.071 I ggml_metal_init: allocating
0.00.052.075 I ggml_metal_init: found device: Apple M4
0.00.052.077 I ggml_metal_init: picking default device: Apple M4
0.00.052.653 I ggml_metal_init: using embedded metal library
0.00.054.959 I ggml_metal_init: GPU name:   Apple M4
0.00.054.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.962 I ggml_metal_init: simdgroup reduction   = true
0.00.054.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.962 I ggml_metal_init: has bfloat            = true
0.00.054.962 I ggml_metal_init: use bfloat            = true
0.00.054.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.676 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.922 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.924 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.939 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.899 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.900 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.901 I llama_init_from_model: graph nodes  = 967
0.00.066.901 I llama_init_from_model: graph splits = 2
0.00.066.902 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.546.292 I 
0.00.546.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.546.336 I perplexity: tokenizing the input ..
0.00.554.002 I perplexity: tokenization took 7.663 ms
0.00.554.006 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.677.062 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.678.266 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.678.297 I llama_perf_context_print:        load time =     536.91 ms
0.00.678.298 I llama_perf_context_print: prompt eval time =     122.83 ms /   128 tokens (    0.96 ms per token,  1042.10 tokens per second)
0.00.678.299 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.678.299 I llama_perf_context_print:       total time =     132.01 ms /   129 tokens
0.00.678.881 I ggml_metal_free: deallocating

real	0m0.694s
user	0m0.077s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.649 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.578 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.582 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.583 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.583 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.584 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.237 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.231 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.890 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.891 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.892 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.892 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.892 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.893 I llama_model_loader: - type  f32:  194 tensors
0.00.023.893 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.893 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.894 I print_info: file format = GGUF V3 (latest)
0.00.023.894 I print_info: file type   = Q4_1
0.00.023.895 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.114 I load: special tokens cache size = 25
0.00.048.087 I load: token to piece cache size = 0.2984 MB
0.00.048.090 I print_info: arch             = gptneox
0.00.048.090 I print_info: vocab_only       = 0
0.00.048.091 I print_info: n_ctx_train      = 2048
0.00.048.091 I print_info: n_embd           = 2048
0.00.048.091 I print_info: n_layer          = 24
0.00.048.094 I print_info: n_head           = 16
0.00.048.095 I print_info: n_head_kv        = 16
0.00.048.095 I print_info: n_rot            = 32
0.00.048.095 I print_info: n_swa            = 0
0.00.048.095 I print_info: n_embd_head_k    = 128
0.00.048.095 I print_info: n_embd_head_v    = 128
0.00.048.096 I print_info: n_gqa            = 1
0.00.048.097 I print_info: n_embd_k_gqa     = 2048
0.00.048.098 I print_info: n_embd_v_gqa     = 2048
0.00.048.099 I print_info: f_norm_eps       = 1.0e-05
0.00.048.099 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.099 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.099 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.101 I print_info: f_logit_scale    = 0.0e+00
0.00.048.101 I print_info: n_ff             = 8192
0.00.048.101 I print_info: n_expert         = 0
0.00.048.102 I print_info: n_expert_used    = 0
0.00.048.103 I print_info: causal attn      = 1
0.00.048.104 I print_info: pooling type     = 0
0.00.048.104 I print_info: rope type        = 2
0.00.048.104 I print_info: rope scaling     = linear
0.00.048.104 I print_info: freq_base_train  = 10000.0
0.00.048.105 I print_info: freq_scale_train = 1
0.00.048.105 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.105 I print_info: rope_finetuned   = unknown
0.00.048.105 I print_info: ssm_d_conv       = 0
0.00.048.105 I print_info: ssm_d_inner      = 0
0.00.048.106 I print_info: ssm_d_state      = 0
0.00.048.106 I print_info: ssm_dt_rank      = 0
0.00.048.106 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.106 I print_info: model type       = 1.4B
0.00.048.106 I print_info: model params     = 1.41 B
0.00.048.106 I print_info: general.name     = 1.4B
0.00.048.107 I print_info: vocab type       = BPE
0.00.048.111 I print_info: n_vocab          = 50304
0.00.048.111 I print_info: n_merges         = 50009
0.00.048.111 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.112 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.112 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.113 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.114 I print_info: LF token         = 128 'Ä'
0.00.048.114 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.114 I print_info: max token length = 1024
0.00.050.030 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.031 I load_tensors: offloading output layer to GPU
0.00.050.031 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.041 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.043 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.050.333 I llama_init_from_model: n_seq_max     = 1
0.00.050.334 I llama_init_from_model: n_ctx         = 128
0.00.050.334 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.334 I llama_init_from_model: n_batch       = 128
0.00.050.334 I llama_init_from_model: n_ubatch      = 128
0.00.050.335 I llama_init_from_model: flash_attn    = 0
0.00.050.335 I llama_init_from_model: freq_base     = 10000.0
0.00.050.335 I llama_init_from_model: freq_scale    = 1
0.00.050.335 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.336 I ggml_metal_init: allocating
0.00.050.338 I ggml_metal_init: found device: Apple M4
0.00.050.340 I ggml_metal_init: picking default device: Apple M4
0.00.050.908 I ggml_metal_init: using embedded metal library
0.00.053.221 I ggml_metal_init: GPU name:   Apple M4
0.00.053.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.223 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.223 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.223 I ggml_metal_init: simdgroup reduction   = true
0.00.053.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.224 I ggml_metal_init: has bfloat            = true
0.00.053.224 I ggml_metal_init: use bfloat            = true
0.00.053.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.225 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.485 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.770 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.774 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.791 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.730 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.731 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.731 I llama_init_from_model: graph nodes  = 967
0.00.063.732 I llama_init_from_model: graph splits = 2
0.00.063.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.733 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.667 I 
0.00.629.701 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.705 I perplexity: tokenizing the input ..
0.00.637.980 I perplexity: tokenization took 8.273 ms
0.00.637.983 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.587 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.761.766 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.761.791 I llama_perf_context_print:        load time =     621.01 ms
0.00.761.792 I llama_perf_context_print: prompt eval time =     122.38 ms /   128 tokens (    0.96 ms per token,  1045.95 tokens per second)
0.00.761.793 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.793 I llama_perf_context_print:       total time =     132.13 ms /   129 tokens
0.00.762.143 I ggml_metal_free: deallocating

real	0m0.775s
user	0m0.075s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.993 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.835 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.835 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.836 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.837 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.838 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.839 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.839 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.839 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.840 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.840 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.841 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.842 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.843 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.509 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.179 I llama_model_loader: - type  f32:  194 tensors
0.00.025.179 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.180 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.180 I print_info: file format = GGUF V3 (latest)
0.00.025.181 I print_info: file type   = Q5_0
0.00.025.182 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.102 I load: special tokens cache size = 25
0.00.049.915 I load: token to piece cache size = 0.2984 MB
0.00.049.918 I print_info: arch             = gptneox
0.00.049.918 I print_info: vocab_only       = 0
0.00.049.919 I print_info: n_ctx_train      = 2048
0.00.049.919 I print_info: n_embd           = 2048
0.00.049.919 I print_info: n_layer          = 24
0.00.049.922 I print_info: n_head           = 16
0.00.049.926 I print_info: n_head_kv        = 16
0.00.049.926 I print_info: n_rot            = 32
0.00.049.926 I print_info: n_swa            = 0
0.00.049.926 I print_info: n_embd_head_k    = 128
0.00.049.926 I print_info: n_embd_head_v    = 128
0.00.049.927 I print_info: n_gqa            = 1
0.00.049.928 I print_info: n_embd_k_gqa     = 2048
0.00.049.929 I print_info: n_embd_v_gqa     = 2048
0.00.049.929 I print_info: f_norm_eps       = 1.0e-05
0.00.049.929 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.930 I print_info: f_logit_scale    = 0.0e+00
0.00.049.935 I print_info: n_ff             = 8192
0.00.049.935 I print_info: n_expert         = 0
0.00.049.935 I print_info: n_expert_used    = 0
0.00.049.935 I print_info: causal attn      = 1
0.00.049.935 I print_info: pooling type     = 0
0.00.049.936 I print_info: rope type        = 2
0.00.049.937 I print_info: rope scaling     = linear
0.00.049.937 I print_info: freq_base_train  = 10000.0
0.00.049.938 I print_info: freq_scale_train = 1
0.00.049.938 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.938 I print_info: rope_finetuned   = unknown
0.00.049.938 I print_info: ssm_d_conv       = 0
0.00.049.939 I print_info: ssm_d_inner      = 0
0.00.049.939 I print_info: ssm_d_state      = 0
0.00.049.939 I print_info: ssm_dt_rank      = 0
0.00.049.939 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.939 I print_info: model type       = 1.4B
0.00.049.940 I print_info: model params     = 1.41 B
0.00.049.940 I print_info: general.name     = 1.4B
0.00.049.940 I print_info: vocab type       = BPE
0.00.049.940 I print_info: n_vocab          = 50304
0.00.049.941 I print_info: n_merges         = 50009
0.00.049.943 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.944 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.944 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.944 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.944 I print_info: LF token         = 128 'Ä'
0.00.049.945 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.946 I print_info: max token length = 1024
0.00.051.934 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.934 I load_tensors: offloading output layer to GPU
0.00.051.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.945 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.946 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.223 I llama_init_from_model: n_seq_max     = 1
0.00.052.224 I llama_init_from_model: n_ctx         = 128
0.00.052.224 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.225 I llama_init_from_model: n_batch       = 128
0.00.052.225 I llama_init_from_model: n_ubatch      = 128
0.00.052.225 I llama_init_from_model: flash_attn    = 0
0.00.052.225 I llama_init_from_model: freq_base     = 10000.0
0.00.052.225 I llama_init_from_model: freq_scale    = 1
0.00.052.226 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.226 I ggml_metal_init: allocating
0.00.052.228 I ggml_metal_init: found device: Apple M4
0.00.052.230 I ggml_metal_init: picking default device: Apple M4
0.00.052.792 I ggml_metal_init: using embedded metal library
0.00.055.121 I ggml_metal_init: GPU name:   Apple M4
0.00.055.122 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.123 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.123 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.123 I ggml_metal_init: simdgroup reduction   = true
0.00.055.123 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.123 I ggml_metal_init: has bfloat            = true
0.00.055.123 I ggml_metal_init: use bfloat            = true
0.00.055.124 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.124 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.797 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.099 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.101 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.125 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.067 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.068 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.068 I llama_init_from_model: graph nodes  = 967
0.00.066.068 I llama_init_from_model: graph splits = 2
0.00.066.069 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.438 I 
0.00.666.490 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.528 I perplexity: tokenizing the input ..
0.00.674.278 I perplexity: tokenization took 7.749 ms
0.00.674.282 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.230 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.810.376 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.810.406 I llama_perf_context_print:        load time =     656.44 ms
0.00.810.407 I llama_perf_context_print: prompt eval time =     134.72 ms /   128 tokens (    1.05 ms per token,   950.10 tokens per second)
0.00.810.407 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.408 I llama_perf_context_print:       total time =     143.97 ms /   129 tokens
0.00.810.947 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.076s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.868 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.647 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.658 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.659 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.661 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.662 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.663 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.664 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.665 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.974 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.975 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.975 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.976 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.976 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.976 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.977 I llama_model_loader: - type  f32:  194 tensors
0.00.023.977 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.977 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.978 I print_info: file format = GGUF V3 (latest)
0.00.023.978 I print_info: file type   = Q5_1
0.00.023.979 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.211 I load: special tokens cache size = 25
0.00.048.118 I load: token to piece cache size = 0.2984 MB
0.00.048.121 I print_info: arch             = gptneox
0.00.048.121 I print_info: vocab_only       = 0
0.00.048.122 I print_info: n_ctx_train      = 2048
0.00.048.122 I print_info: n_embd           = 2048
0.00.048.122 I print_info: n_layer          = 24
0.00.048.125 I print_info: n_head           = 16
0.00.048.125 I print_info: n_head_kv        = 16
0.00.048.126 I print_info: n_rot            = 32
0.00.048.126 I print_info: n_swa            = 0
0.00.048.126 I print_info: n_embd_head_k    = 128
0.00.048.126 I print_info: n_embd_head_v    = 128
0.00.048.127 I print_info: n_gqa            = 1
0.00.048.128 I print_info: n_embd_k_gqa     = 2048
0.00.048.129 I print_info: n_embd_v_gqa     = 2048
0.00.048.129 I print_info: f_norm_eps       = 1.0e-05
0.00.048.129 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.130 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.130 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.131 I print_info: f_logit_scale    = 0.0e+00
0.00.048.131 I print_info: n_ff             = 8192
0.00.048.132 I print_info: n_expert         = 0
0.00.048.132 I print_info: n_expert_used    = 0
0.00.048.132 I print_info: causal attn      = 1
0.00.048.132 I print_info: pooling type     = 0
0.00.048.132 I print_info: rope type        = 2
0.00.048.132 I print_info: rope scaling     = linear
0.00.048.133 I print_info: freq_base_train  = 10000.0
0.00.048.133 I print_info: freq_scale_train = 1
0.00.048.133 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.134 I print_info: rope_finetuned   = unknown
0.00.048.134 I print_info: ssm_d_conv       = 0
0.00.048.134 I print_info: ssm_d_inner      = 0
0.00.048.134 I print_info: ssm_d_state      = 0
0.00.048.134 I print_info: ssm_dt_rank      = 0
0.00.048.134 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.135 I print_info: model type       = 1.4B
0.00.048.135 I print_info: model params     = 1.41 B
0.00.048.135 I print_info: general.name     = 1.4B
0.00.048.136 I print_info: vocab type       = BPE
0.00.048.136 I print_info: n_vocab          = 50304
0.00.048.138 I print_info: n_merges         = 50009
0.00.048.138 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.138 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.139 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.139 I print_info: LF token         = 128 'Ä'
0.00.048.139 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.140 I print_info: max token length = 1024
0.00.050.125 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.125 I load_tensors: offloading output layer to GPU
0.00.050.125 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.136 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.137 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.050.451 I llama_init_from_model: n_seq_max     = 1
0.00.050.452 I llama_init_from_model: n_ctx         = 128
0.00.050.452 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.452 I llama_init_from_model: n_batch       = 128
0.00.050.453 I llama_init_from_model: n_ubatch      = 128
0.00.050.453 I llama_init_from_model: flash_attn    = 0
0.00.050.453 I llama_init_from_model: freq_base     = 10000.0
0.00.050.453 I llama_init_from_model: freq_scale    = 1
0.00.050.454 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.454 I ggml_metal_init: allocating
0.00.050.456 I ggml_metal_init: found device: Apple M4
0.00.050.458 I ggml_metal_init: picking default device: Apple M4
0.00.051.015 I ggml_metal_init: using embedded metal library
0.00.053.340 I ggml_metal_init: GPU name:   Apple M4
0.00.053.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.343 I ggml_metal_init: simdgroup reduction   = true
0.00.053.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.343 I ggml_metal_init: has bfloat            = true
0.00.053.343 I ggml_metal_init: use bfloat            = true
0.00.053.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.725 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.038 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.040 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.054 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.956 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.957 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.957 I llama_init_from_model: graph nodes  = 967
0.00.063.958 I llama_init_from_model: graph splits = 2
0.00.063.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.175 I 
0.00.758.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.218 I perplexity: tokenizing the input ..
0.00.766.479 I perplexity: tokenization took 8.26 ms
0.00.766.483 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.901.555 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.902.721 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.902.745 I llama_perf_context_print:        load time =     749.30 ms
0.00.902.746 I llama_perf_context_print: prompt eval time =     134.84 ms /   128 tokens (    1.05 ms per token,   949.25 tokens per second)
0.00.902.747 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.902.747 I llama_perf_context_print:       total time =     144.57 ms /   129 tokens
0.00.903.222 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.075s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.224 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.165 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.170 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.172 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.172 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.173 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.181 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.893 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.549 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.551 I llama_model_loader: - type  f32:  194 tensors
0.00.025.551 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.552 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.552 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.552 I print_info: file format = GGUF V3 (latest)
0.00.025.553 I print_info: file type   = Q2_K - Medium
0.00.025.554 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.554 I load: special tokens cache size = 25
0.00.050.542 I load: token to piece cache size = 0.2984 MB
0.00.050.545 I print_info: arch             = gptneox
0.00.050.545 I print_info: vocab_only       = 0
0.00.050.545 I print_info: n_ctx_train      = 2048
0.00.050.545 I print_info: n_embd           = 2048
0.00.050.545 I print_info: n_layer          = 24
0.00.050.548 I print_info: n_head           = 16
0.00.050.549 I print_info: n_head_kv        = 16
0.00.050.549 I print_info: n_rot            = 32
0.00.050.549 I print_info: n_swa            = 0
0.00.050.549 I print_info: n_embd_head_k    = 128
0.00.050.550 I print_info: n_embd_head_v    = 128
0.00.050.550 I print_info: n_gqa            = 1
0.00.050.551 I print_info: n_embd_k_gqa     = 2048
0.00.050.552 I print_info: n_embd_v_gqa     = 2048
0.00.050.552 I print_info: f_norm_eps       = 1.0e-05
0.00.050.553 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.553 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.553 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.555 I print_info: f_logit_scale    = 0.0e+00
0.00.050.555 I print_info: n_ff             = 8192
0.00.050.556 I print_info: n_expert         = 0
0.00.050.556 I print_info: n_expert_used    = 0
0.00.050.556 I print_info: causal attn      = 1
0.00.050.556 I print_info: pooling type     = 0
0.00.050.556 I print_info: rope type        = 2
0.00.050.557 I print_info: rope scaling     = linear
0.00.050.557 I print_info: freq_base_train  = 10000.0
0.00.050.557 I print_info: freq_scale_train = 1
0.00.050.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.558 I print_info: rope_finetuned   = unknown
0.00.050.558 I print_info: ssm_d_conv       = 0
0.00.050.558 I print_info: ssm_d_inner      = 0
0.00.050.558 I print_info: ssm_d_state      = 0
0.00.050.558 I print_info: ssm_dt_rank      = 0
0.00.050.558 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.559 I print_info: model type       = 1.4B
0.00.050.559 I print_info: model params     = 1.41 B
0.00.050.559 I print_info: general.name     = 1.4B
0.00.050.560 I print_info: vocab type       = BPE
0.00.050.560 I print_info: n_vocab          = 50304
0.00.050.560 I print_info: n_merges         = 50009
0.00.050.561 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: LF token         = 128 'Ä'
0.00.050.562 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.562 I print_info: max token length = 1024
0.00.052.408 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.408 I load_tensors: offloading output layer to GPU
0.00.052.408 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.419 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.420 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.694 I llama_init_from_model: n_seq_max     = 1
0.00.052.695 I llama_init_from_model: n_ctx         = 128
0.00.052.695 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.695 I llama_init_from_model: n_batch       = 128
0.00.052.695 I llama_init_from_model: n_ubatch      = 128
0.00.052.696 I llama_init_from_model: flash_attn    = 0
0.00.052.696 I llama_init_from_model: freq_base     = 10000.0
0.00.052.696 I llama_init_from_model: freq_scale    = 1
0.00.052.696 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.697 I ggml_metal_init: allocating
0.00.052.700 I ggml_metal_init: found device: Apple M4
0.00.052.702 I ggml_metal_init: picking default device: Apple M4
0.00.053.276 I ggml_metal_init: using embedded metal library
0.00.055.614 I ggml_metal_init: GPU name:   Apple M4
0.00.055.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.616 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.617 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.617 I ggml_metal_init: simdgroup reduction   = true
0.00.055.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.617 I ggml_metal_init: has bfloat            = true
0.00.055.617 I ggml_metal_init: use bfloat            = true
0.00.055.618 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.180 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.406 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.422 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.308 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.309 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.309 I llama_init_from_model: graph nodes  = 967
0.00.067.310 I llama_init_from_model: graph splits = 2
0.00.067.311 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.469.568 I 
0.00.469.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.469.601 I perplexity: tokenizing the input ..
0.00.477.730 I perplexity: tokenization took 8.127 ms
0.00.477.734 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.609.870 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.611.041 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.611.068 I llama_perf_context_print:        load time =     459.34 ms
0.00.611.069 I llama_perf_context_print: prompt eval time =     131.91 ms /   128 tokens (    1.03 ms per token,   970.35 tokens per second)
0.00.611.070 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.611.071 I llama_perf_context_print:       total time =     141.50 ms /   129 tokens
0.00.611.555 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.077s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.880 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.907 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.909 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.909 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.909 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.910 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.910 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.911 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.911 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.914 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.917 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.536 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.550 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.165 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.165 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.166 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.166 I llama_model_loader: - type  f32:  194 tensors
0.00.024.167 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.167 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.167 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.167 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.167 I print_info: file format = GGUF V3 (latest)
0.00.024.168 I print_info: file type   = Q3_K - Medium
0.00.024.169 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.412 I load: special tokens cache size = 25
0.00.048.230 I load: token to piece cache size = 0.2984 MB
0.00.048.233 I print_info: arch             = gptneox
0.00.048.233 I print_info: vocab_only       = 0
0.00.048.233 I print_info: n_ctx_train      = 2048
0.00.048.233 I print_info: n_embd           = 2048
0.00.048.234 I print_info: n_layer          = 24
0.00.048.236 I print_info: n_head           = 16
0.00.048.237 I print_info: n_head_kv        = 16
0.00.048.237 I print_info: n_rot            = 32
0.00.048.237 I print_info: n_swa            = 0
0.00.048.239 I print_info: n_embd_head_k    = 128
0.00.048.239 I print_info: n_embd_head_v    = 128
0.00.048.240 I print_info: n_gqa            = 1
0.00.048.241 I print_info: n_embd_k_gqa     = 2048
0.00.048.247 I print_info: n_embd_v_gqa     = 2048
0.00.048.248 I print_info: f_norm_eps       = 1.0e-05
0.00.048.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.249 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.249 I print_info: f_logit_scale    = 0.0e+00
0.00.048.250 I print_info: n_ff             = 8192
0.00.048.250 I print_info: n_expert         = 0
0.00.048.250 I print_info: n_expert_used    = 0
0.00.048.250 I print_info: causal attn      = 1
0.00.048.251 I print_info: pooling type     = 0
0.00.048.251 I print_info: rope type        = 2
0.00.048.251 I print_info: rope scaling     = linear
0.00.048.251 I print_info: freq_base_train  = 10000.0
0.00.048.252 I print_info: freq_scale_train = 1
0.00.048.252 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.252 I print_info: rope_finetuned   = unknown
0.00.048.252 I print_info: ssm_d_conv       = 0
0.00.048.253 I print_info: ssm_d_inner      = 0
0.00.048.253 I print_info: ssm_d_state      = 0
0.00.048.253 I print_info: ssm_dt_rank      = 0
0.00.048.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.253 I print_info: model type       = 1.4B
0.00.048.254 I print_info: model params     = 1.41 B
0.00.048.254 I print_info: general.name     = 1.4B
0.00.048.254 I print_info: vocab type       = BPE
0.00.048.255 I print_info: n_vocab          = 50304
0.00.048.255 I print_info: n_merges         = 50009
0.00.048.255 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.255 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.256 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.256 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.256 I print_info: LF token         = 128 'Ä'
0.00.048.256 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.256 I print_info: max token length = 1024
0.00.049.867 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.867 I load_tensors: offloading output layer to GPU
0.00.049.867 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.877 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.049.878 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.147 I llama_init_from_model: n_seq_max     = 1
0.00.050.148 I llama_init_from_model: n_ctx         = 128
0.00.050.148 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.148 I llama_init_from_model: n_batch       = 128
0.00.050.148 I llama_init_from_model: n_ubatch      = 128
0.00.050.148 I llama_init_from_model: flash_attn    = 0
0.00.050.149 I llama_init_from_model: freq_base     = 10000.0
0.00.050.149 I llama_init_from_model: freq_scale    = 1
0.00.050.149 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.150 I ggml_metal_init: allocating
0.00.050.155 I ggml_metal_init: found device: Apple M4
0.00.050.157 I ggml_metal_init: picking default device: Apple M4
0.00.050.741 I ggml_metal_init: using embedded metal library
0.00.053.073 I ggml_metal_init: GPU name:   Apple M4
0.00.053.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.076 I ggml_metal_init: simdgroup reduction   = true
0.00.053.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.076 I ggml_metal_init: has bfloat            = true
0.00.053.076 I ggml_metal_init: use bfloat            = true
0.00.053.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.607 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.900 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.904 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.928 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.767 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.768 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.768 I llama_init_from_model: graph nodes  = 967
0.00.063.769 I llama_init_from_model: graph splits = 2
0.00.063.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.814 I 
0.00.564.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.850 I perplexity: tokenizing the input ..
0.00.572.526 I perplexity: tokenization took 7.674 ms
0.00.572.529 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.704.779 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.705.929 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.705.955 I llama_perf_context_print:        load time =     555.93 ms
0.00.705.958 I llama_perf_context_print: prompt eval time =     132.02 ms /   128 tokens (    1.03 ms per token,   969.52 tokens per second)
0.00.705.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.705.959 I llama_perf_context_print:       total time =     141.14 ms /   129 tokens
0.00.706.426 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.075s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.821 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.939 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.950 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.951 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.951 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.952 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.952 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.953 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.954 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.956 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.601 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.161 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.161 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.162 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.162 I llama_model_loader: - type  f32:  194 tensors
0.00.024.163 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.163 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.163 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.164 I print_info: file format = GGUF V3 (latest)
0.00.024.164 I print_info: file type   = Q4_K - Medium
0.00.024.165 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.248 I load: special tokens cache size = 25
0.00.049.354 I load: token to piece cache size = 0.2984 MB
0.00.049.357 I print_info: arch             = gptneox
0.00.049.357 I print_info: vocab_only       = 0
0.00.049.357 I print_info: n_ctx_train      = 2048
0.00.049.357 I print_info: n_embd           = 2048
0.00.049.358 I print_info: n_layer          = 24
0.00.049.360 I print_info: n_head           = 16
0.00.049.361 I print_info: n_head_kv        = 16
0.00.049.361 I print_info: n_rot            = 32
0.00.049.361 I print_info: n_swa            = 0
0.00.049.362 I print_info: n_embd_head_k    = 128
0.00.049.362 I print_info: n_embd_head_v    = 128
0.00.049.362 I print_info: n_gqa            = 1
0.00.049.363 I print_info: n_embd_k_gqa     = 2048
0.00.049.364 I print_info: n_embd_v_gqa     = 2048
0.00.049.364 I print_info: f_norm_eps       = 1.0e-05
0.00.049.365 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.365 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.365 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.365 I print_info: f_logit_scale    = 0.0e+00
0.00.049.366 I print_info: n_ff             = 8192
0.00.049.366 I print_info: n_expert         = 0
0.00.049.367 I print_info: n_expert_used    = 0
0.00.049.367 I print_info: causal attn      = 1
0.00.049.367 I print_info: pooling type     = 0
0.00.049.367 I print_info: rope type        = 2
0.00.049.369 I print_info: rope scaling     = linear
0.00.049.370 I print_info: freq_base_train  = 10000.0
0.00.049.370 I print_info: freq_scale_train = 1
0.00.049.370 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.371 I print_info: rope_finetuned   = unknown
0.00.049.371 I print_info: ssm_d_conv       = 0
0.00.049.371 I print_info: ssm_d_inner      = 0
0.00.049.371 I print_info: ssm_d_state      = 0
0.00.049.372 I print_info: ssm_dt_rank      = 0
0.00.049.373 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.373 I print_info: model type       = 1.4B
0.00.049.373 I print_info: model params     = 1.41 B
0.00.049.373 I print_info: general.name     = 1.4B
0.00.049.374 I print_info: vocab type       = BPE
0.00.049.374 I print_info: n_vocab          = 50304
0.00.049.374 I print_info: n_merges         = 50009
0.00.049.375 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.375 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.379 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.379 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.379 I print_info: LF token         = 128 'Ä'
0.00.049.380 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.380 I print_info: max token length = 1024
0.00.051.354 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.354 I load_tensors: offloading output layer to GPU
0.00.051.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.364 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.366 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.646 I llama_init_from_model: n_seq_max     = 1
0.00.051.646 I llama_init_from_model: n_ctx         = 128
0.00.051.647 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.647 I llama_init_from_model: n_batch       = 128
0.00.051.647 I llama_init_from_model: n_ubatch      = 128
0.00.051.647 I llama_init_from_model: flash_attn    = 0
0.00.051.647 I llama_init_from_model: freq_base     = 10000.0
0.00.051.647 I llama_init_from_model: freq_scale    = 1
0.00.051.648 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.648 I ggml_metal_init: allocating
0.00.051.651 I ggml_metal_init: found device: Apple M4
0.00.051.652 I ggml_metal_init: picking default device: Apple M4
0.00.052.226 I ggml_metal_init: using embedded metal library
0.00.054.571 I ggml_metal_init: GPU name:   Apple M4
0.00.054.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.574 I ggml_metal_init: simdgroup reduction   = true
0.00.054.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.574 I ggml_metal_init: has bfloat            = true
0.00.054.574 I ggml_metal_init: use bfloat            = true
0.00.054.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.119 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.359 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.361 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.376 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.324 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.325 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.325 I llama_init_from_model: graph nodes  = 967
0.00.066.325 I llama_init_from_model: graph splits = 2
0.00.066.326 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.293 I 
0.00.545.330 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.339 I perplexity: tokenizing the input ..
0.00.553.378 I perplexity: tokenization took 8.037 ms
0.00.553.381 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.687.240 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.688.431 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.688.454 I llama_perf_context_print:        load time =     536.47 ms
0.00.688.455 I llama_perf_context_print: prompt eval time =     133.63 ms /   128 tokens (    1.04 ms per token,   957.85 tokens per second)
0.00.688.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.688.456 I llama_perf_context_print:       total time =     143.16 ms /   129 tokens
0.00.688.927 I ggml_metal_free: deallocating

real	0m0.701s
user	0m0.077s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.223 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.834 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.839 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.841 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.842 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.842 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.842 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.843 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.844 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.846 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.848 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.849 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.849 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.148 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.149 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.149 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.149 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.150 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.150 I llama_model_loader: - type  f32:  194 tensors
0.00.025.150 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.150 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.151 I print_info: file format = GGUF V3 (latest)
0.00.025.151 I print_info: file type   = Q5_K - Medium
0.00.025.152 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.360 I load: special tokens cache size = 25
0.00.049.241 I load: token to piece cache size = 0.2984 MB
0.00.049.244 I print_info: arch             = gptneox
0.00.049.244 I print_info: vocab_only       = 0
0.00.049.244 I print_info: n_ctx_train      = 2048
0.00.049.245 I print_info: n_embd           = 2048
0.00.049.245 I print_info: n_layer          = 24
0.00.049.247 I print_info: n_head           = 16
0.00.049.248 I print_info: n_head_kv        = 16
0.00.049.248 I print_info: n_rot            = 32
0.00.049.248 I print_info: n_swa            = 0
0.00.049.248 I print_info: n_embd_head_k    = 128
0.00.049.248 I print_info: n_embd_head_v    = 128
0.00.049.249 I print_info: n_gqa            = 1
0.00.049.250 I print_info: n_embd_k_gqa     = 2048
0.00.049.252 I print_info: n_embd_v_gqa     = 2048
0.00.049.252 I print_info: f_norm_eps       = 1.0e-05
0.00.049.253 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.253 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.253 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.253 I print_info: f_logit_scale    = 0.0e+00
0.00.049.254 I print_info: n_ff             = 8192
0.00.049.254 I print_info: n_expert         = 0
0.00.049.254 I print_info: n_expert_used    = 0
0.00.049.254 I print_info: causal attn      = 1
0.00.049.255 I print_info: pooling type     = 0
0.00.049.255 I print_info: rope type        = 2
0.00.049.255 I print_info: rope scaling     = linear
0.00.049.257 I print_info: freq_base_train  = 10000.0
0.00.049.259 I print_info: freq_scale_train = 1
0.00.049.259 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.259 I print_info: rope_finetuned   = unknown
0.00.049.260 I print_info: ssm_d_conv       = 0
0.00.049.260 I print_info: ssm_d_inner      = 0
0.00.049.260 I print_info: ssm_d_state      = 0
0.00.049.260 I print_info: ssm_dt_rank      = 0
0.00.049.260 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.265 I print_info: model type       = 1.4B
0.00.049.265 I print_info: model params     = 1.41 B
0.00.049.265 I print_info: general.name     = 1.4B
0.00.049.266 I print_info: vocab type       = BPE
0.00.049.266 I print_info: n_vocab          = 50304
0.00.049.266 I print_info: n_merges         = 50009
0.00.049.266 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.266 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.268 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.268 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.268 I print_info: LF token         = 128 'Ä'
0.00.049.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.268 I print_info: max token length = 1024
0.00.051.236 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.236 I load_tensors: offloading output layer to GPU
0.00.051.237 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.247 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.248 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.534 I llama_init_from_model: n_seq_max     = 1
0.00.051.535 I llama_init_from_model: n_ctx         = 128
0.00.051.535 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.535 I llama_init_from_model: n_batch       = 128
0.00.051.536 I llama_init_from_model: n_ubatch      = 128
0.00.051.536 I llama_init_from_model: flash_attn    = 0
0.00.051.536 I llama_init_from_model: freq_base     = 10000.0
0.00.051.536 I llama_init_from_model: freq_scale    = 1
0.00.051.537 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.537 I ggml_metal_init: allocating
0.00.051.539 I ggml_metal_init: found device: Apple M4
0.00.051.541 I ggml_metal_init: picking default device: Apple M4
0.00.052.103 I ggml_metal_init: using embedded metal library
0.00.054.433 I ggml_metal_init: GPU name:   Apple M4
0.00.054.434 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.435 I ggml_metal_init: simdgroup reduction   = true
0.00.054.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.435 I ggml_metal_init: has bfloat            = true
0.00.054.435 I ggml_metal_init: use bfloat            = true
0.00.054.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.436 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.831 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.167 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.170 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.186 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.059 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.060 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.060 I llama_init_from_model: graph nodes  = 967
0.00.065.061 I llama_init_from_model: graph splits = 2
0.00.065.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.344 I 
0.00.662.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.395 I perplexity: tokenizing the input ..
0.00.671.234 I perplexity: tokenization took 8.838 ms
0.00.671.238 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.144 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.812.549 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.812.575 I llama_perf_context_print:        load time =     652.12 ms
0.00.812.576 I llama_perf_context_print: prompt eval time =     139.67 ms /   128 tokens (    1.09 ms per token,   916.46 tokens per second)
0.00.812.576 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.577 I llama_perf_context_print:       total time =     150.23 ms /   129 tokens
0.00.812.930 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.076s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.256 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.003 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.009 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.011 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.011 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.012 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.012 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.014 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.014 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.015 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.015 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.018 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.018 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.611 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.549 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.549 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.550 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.551 I llama_model_loader: - type  f32:  194 tensors
0.00.024.552 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.552 I print_info: file format = GGUF V3 (latest)
0.00.024.553 I print_info: file type   = Q6_K
0.00.024.554 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.750 I load: special tokens cache size = 25
0.00.049.838 I load: token to piece cache size = 0.2984 MB
0.00.049.841 I print_info: arch             = gptneox
0.00.049.841 I print_info: vocab_only       = 0
0.00.049.842 I print_info: n_ctx_train      = 2048
0.00.049.842 I print_info: n_embd           = 2048
0.00.049.842 I print_info: n_layer          = 24
0.00.049.846 I print_info: n_head           = 16
0.00.049.848 I print_info: n_head_kv        = 16
0.00.049.848 I print_info: n_rot            = 32
0.00.049.848 I print_info: n_swa            = 0
0.00.049.848 I print_info: n_embd_head_k    = 128
0.00.049.848 I print_info: n_embd_head_v    = 128
0.00.049.849 I print_info: n_gqa            = 1
0.00.049.850 I print_info: n_embd_k_gqa     = 2048
0.00.049.851 I print_info: n_embd_v_gqa     = 2048
0.00.049.851 I print_info: f_norm_eps       = 1.0e-05
0.00.049.852 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.852 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.852 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.852 I print_info: f_logit_scale    = 0.0e+00
0.00.049.853 I print_info: n_ff             = 8192
0.00.049.853 I print_info: n_expert         = 0
0.00.049.853 I print_info: n_expert_used    = 0
0.00.049.853 I print_info: causal attn      = 1
0.00.049.853 I print_info: pooling type     = 0
0.00.049.853 I print_info: rope type        = 2
0.00.049.854 I print_info: rope scaling     = linear
0.00.049.854 I print_info: freq_base_train  = 10000.0
0.00.049.854 I print_info: freq_scale_train = 1
0.00.049.854 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.855 I print_info: rope_finetuned   = unknown
0.00.049.855 I print_info: ssm_d_conv       = 0
0.00.049.855 I print_info: ssm_d_inner      = 0
0.00.049.855 I print_info: ssm_d_state      = 0
0.00.049.855 I print_info: ssm_dt_rank      = 0
0.00.049.855 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.855 I print_info: model type       = 1.4B
0.00.049.856 I print_info: model params     = 1.41 B
0.00.049.856 I print_info: general.name     = 1.4B
0.00.049.856 I print_info: vocab type       = BPE
0.00.049.856 I print_info: n_vocab          = 50304
0.00.049.857 I print_info: n_merges         = 50009
0.00.049.857 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.857 I print_info: LF token         = 128 'Ä'
0.00.049.858 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.858 I print_info: max token length = 1024
0.00.051.877 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.877 I load_tensors: offloading output layer to GPU
0.00.051.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.889 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.890 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.165 I llama_init_from_model: n_seq_max     = 1
0.00.052.166 I llama_init_from_model: n_ctx         = 128
0.00.052.166 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.166 I llama_init_from_model: n_batch       = 128
0.00.052.166 I llama_init_from_model: n_ubatch      = 128
0.00.052.166 I llama_init_from_model: flash_attn    = 0
0.00.052.167 I llama_init_from_model: freq_base     = 10000.0
0.00.052.167 I llama_init_from_model: freq_scale    = 1
0.00.052.167 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.167 I ggml_metal_init: allocating
0.00.052.170 I ggml_metal_init: found device: Apple M4
0.00.052.171 I ggml_metal_init: picking default device: Apple M4
0.00.052.786 I ggml_metal_init: using embedded metal library
0.00.055.392 I ggml_metal_init: GPU name:   Apple M4
0.00.055.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.395 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.395 I ggml_metal_init: simdgroup reduction   = true
0.00.055.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.395 I ggml_metal_init: has bfloat            = true
0.00.055.396 I ggml_metal_init: use bfloat            = true
0.00.055.396 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.677 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.679 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.695 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.635 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.636 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.636 I llama_init_from_model: graph nodes  = 967
0.00.066.636 I llama_init_from_model: graph splits = 2
0.00.066.637 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.241 I 
0.00.631.287 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.296 I perplexity: tokenizing the input ..
0.00.638.804 I perplexity: tokenization took 7.506 ms
0.00.638.808 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.668 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.780.115 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.780.135 I llama_perf_context_print:        load time =     621.98 ms
0.00.780.136 I llama_perf_context_print: prompt eval time =     139.61 ms /   128 tokens (    1.09 ms per token,   916.83 tokens per second)
0.00.780.137 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.137 I llama_perf_context_print:       total time =     148.90 ms /   129 tokens
0.00.780.518 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.078s
sys	0m0.096s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.208 I build: 4485 (f446c2cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.957 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.427 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.459 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.461 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.466 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.467 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.467 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.468 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.699 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.701 I llama_model_loader: - type  f32:  194 tensors
0.00.051.701 I llama_model_loader: - type  f16:   98 tensors
0.00.051.702 I print_info: file format = GGUF V3 (latest)
0.00.051.703 I print_info: file type   = all F32 (guessed)
0.00.051.707 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.309 I load: special tokens cache size = 25
0.00.086.868 I load: token to piece cache size = 0.2984 MB
0.00.086.871 I print_info: arch             = gptneox
0.00.086.871 I print_info: vocab_only       = 0
0.00.086.871 I print_info: n_ctx_train      = 2048
0.00.086.871 I print_info: n_embd           = 2048
0.00.086.872 I print_info: n_layer          = 24
0.00.086.875 I print_info: n_head           = 16
0.00.086.876 I print_info: n_head_kv        = 16
0.00.086.876 I print_info: n_rot            = 32
0.00.086.876 I print_info: n_swa            = 0
0.00.086.877 I print_info: n_embd_head_k    = 128
0.00.086.877 I print_info: n_embd_head_v    = 128
0.00.086.877 I print_info: n_gqa            = 1
0.00.086.878 I print_info: n_embd_k_gqa     = 2048
0.00.086.881 I print_info: n_embd_v_gqa     = 2048
0.00.086.881 I print_info: f_norm_eps       = 1.0e-05
0.00.086.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.883 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.883 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.883 I print_info: f_logit_scale    = 0.0e+00
0.00.086.884 I print_info: n_ff             = 8192
0.00.086.884 I print_info: n_expert         = 0
0.00.086.884 I print_info: n_expert_used    = 0
0.00.086.884 I print_info: causal attn      = 1
0.00.086.884 I print_info: pooling type     = 0
0.00.086.884 I print_info: rope type        = 2
0.00.086.885 I print_info: rope scaling     = linear
0.00.086.885 I print_info: freq_base_train  = 10000.0
0.00.086.885 I print_info: freq_scale_train = 1
0.00.086.885 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.886 I print_info: rope_finetuned   = unknown
0.00.086.886 I print_info: ssm_d_conv       = 0
0.00.086.886 I print_info: ssm_d_inner      = 0
0.00.086.887 I print_info: ssm_d_state      = 0
0.00.086.887 I print_info: ssm_dt_rank      = 0
0.00.086.888 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.888 I print_info: model type       = 1.4B
0.00.086.888 I print_info: model params     = 1.41 B
0.00.086.888 I print_info: general.name     = 1.4B
0.00.086.889 I print_info: vocab type       = BPE
0.00.086.889 I print_info: n_vocab          = 50304
0.00.086.889 I print_info: n_merges         = 50009
0.00.086.889 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.890 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.890 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.890 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.894 I print_info: LF token         = 128 'Ä'
0.00.086.894 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.894 I print_info: max token length = 1024
0.00.089.736 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.737 I load_tensors: offloading output layer to GPU
0.00.089.737 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.748 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.749 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.069 I llama_init_from_model: n_seq_max     = 1
0.00.090.070 I llama_init_from_model: n_ctx         = 128
0.00.090.070 I llama_init_from_model: n_ctx_per_seq = 128
0.00.090.070 I llama_init_from_model: n_batch       = 128
0.00.090.071 I llama_init_from_model: n_ubatch      = 128
0.00.090.071 I llama_init_from_model: flash_attn    = 0
0.00.090.071 I llama_init_from_model: freq_base     = 10000.0
0.00.090.071 I llama_init_from_model: freq_scale    = 1
0.00.090.072 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.072 I ggml_metal_init: allocating
0.00.090.076 I ggml_metal_init: found device: Apple M4
0.00.090.079 I ggml_metal_init: picking default device: Apple M4
0.00.090.757 I ggml_metal_init: using embedded metal library
0.00.093.918 I ggml_metal_init: GPU name:   Apple M4
0.00.093.920 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.920 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.921 I ggml_metal_init: simdgroup reduction   = true
0.00.093.921 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.921 I ggml_metal_init: has bfloat            = true
0.00.093.922 I ggml_metal_init: use bfloat            = true
0.00.093.922 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.390 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.704 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.707 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.721 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.656 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.105.657 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.105.657 I llama_init_from_model: graph nodes  = 967
0.00.105.658 I llama_init_from_model: graph splits = 2
0.00.105.659 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.659 I 
0.00.105.689 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.690 I compute_imatrix: tokenizing the input ..
0.00.113.415 I compute_imatrix: tokenization took 7.723 ms
0.00.113.418 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.577.145 I compute_imatrix: 1.46 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.581.935 I llama_perf_context_print:        load time =    1554.19 ms
0.01.581.936 I llama_perf_context_print: prompt eval time =    1463.07 ms /   128 tokens (   11.43 ms per token,    87.49 tokens per second)
0.01.581.937 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.581.937 I llama_perf_context_print:       total time =    1558.97 ms /   129 tokens
0.01.582.526 I ggml_metal_free: deallocating

real	0m1.763s
user	0m0.168s
sys	0m0.226s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4485 (f446c2cf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105607040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105607790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105607c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105608070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1056084e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105608950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105608dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105609230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1056096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105609b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105609f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10560a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10560b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10560b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10560c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10560c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10560cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10560d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10560dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10560e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10560ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10560f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10560fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105610350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105610a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105610d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105610ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105611460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105611b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105611f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1056123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105612980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105612df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1056130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105613520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105613dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105614090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105614500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105614970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105615250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1056156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105615b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105615fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105616880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105616cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105617720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1056179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105617e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1056182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105618730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105618ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105619010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105619480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105619b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105619fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10561a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10561a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10561add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10561b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10561b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10561b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10561be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10561c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10561c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10561cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10561d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10561d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10561dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10561e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10561e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10561eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10561f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10561f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10561fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1056201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105620750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105620d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1056212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105621860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105621e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1056223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105622970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105622f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1056234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105623a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105624030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1056245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105625140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1056256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105626250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105626800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105626db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105627360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105627ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105627f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1056283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105628950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1056294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105629a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10562a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10562a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10562ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10562b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10562b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10562bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10562c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10562c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10562cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10562d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10562d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10562dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10562e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10562e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10562eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10562f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10562f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10562fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10562ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105630490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105630990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105630e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105631390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105631d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105632290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105632790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105632c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105633190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105633690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105633b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105634090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105634a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105634f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105635490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105635990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105635e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105636390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105636890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105637290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105637790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105637c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105638190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105638690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105638b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105639090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105639590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105639a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105639f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10563a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10563a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10563ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10563b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10563b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10563bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10563c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10563c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10563cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10563d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10563d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10563db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10563e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10563e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10563ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10563ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10563f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10563f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10563fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105640390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105640890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105641290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105641790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105641c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105642190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105642b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105643090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105643590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105643a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105643f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105644490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105644990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105644e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105645390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105645890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105645d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105646340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1056468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105646ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105647450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105647a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105648070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105648680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105648e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105649310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1056495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105649be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10564a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10564a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10564ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10564b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10564b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10564bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10564c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10564ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10564cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10564d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10564da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10564df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10564e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10564e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10564ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10564f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10564f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10564ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105650480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1056509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105650f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105651470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1056519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105651f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105652460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1056529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105652f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105653450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1056539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105653ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105654440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105654990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105654ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105655430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105655980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105655ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105656420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105656970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105656ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105657410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105657960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105657eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105658400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105658950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105658ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1056593f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105659940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105659e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10565a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10565a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10565ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10565b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10565b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10565be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10565c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10565c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10565ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10565d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10565d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10565de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10565e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10565e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10565ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10565f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10565f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10565fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105660010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1056604b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105660950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105660df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105661290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105661730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105661bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105662070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105662510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1056629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105662e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1056633a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105663ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1056641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105664900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105665020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1056652e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105665ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105665d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1056663a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.121.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.121.609 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10561feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10561f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105624e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10561f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105627070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1056248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10562bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10562b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10562b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105626ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105621570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105629770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105646600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105626510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105620fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1056242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105622c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1056291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105646050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10562ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105625f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105620a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105622680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105628c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10562a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1056259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105620460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105623790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105628660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10562a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1056231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105629d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105666050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105647710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105648330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105649ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10560fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105616fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1056137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10560a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105611720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105619740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10561a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1056655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105627620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10564a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105648940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105666800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105666ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105666d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105667040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105667300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1056675c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105667880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105667b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105667e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1056680c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105668380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105668640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105668900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105668bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105668e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105669140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105669400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1056696c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105669980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105669c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105669f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10566a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10566a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10566a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10566aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10566acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10566af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10566b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10566b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10566b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10566ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10566bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10566c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10566c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10566c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10566c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10566cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10566cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10566d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10566d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10566d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10566d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10566db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10566de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10566e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10566e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10566e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10566e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10566ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10566eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10566f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10566f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10566f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10566f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10566fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10566ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105670200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1056704c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105670780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105670a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105670d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105670fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105671280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105671540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105671800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105671ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105671d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105672040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105672300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1056725c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105672880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105672b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105672e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1056730c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105673380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105673640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105673900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105673bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105673e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105674140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105674400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1056746c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105674980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105674c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105674f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1056751c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105675480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105675740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105675a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105675cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105675f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105676240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105676500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1056767c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105676a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105676d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105677000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1056772c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105677580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105677840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105677b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105677dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105678080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105678340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105678600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1056788c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105678b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105678e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105679100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1056793c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105679680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105679940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105679c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105679ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10567a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10567a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10567a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10567a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10567ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10567af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10567b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10567b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10567b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10567ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10567bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10567bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10567c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10567c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10567c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10567cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10567cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10567d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10567d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10567d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10567d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10567db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10567de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10567e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10567e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10567e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10567e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10567ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10567ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10567f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10567f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10567f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10567f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10567fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10567ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1056801c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105680480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105680740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105680a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105680cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105680f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105681240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105681500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1056817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105681a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105681d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105682000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1056825d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105682890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105682de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105683330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105683880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105683dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105684320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105684870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105684dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105685310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105685860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105685db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105686300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105686850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105686da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1056872f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105687840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105687d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1056882e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105688830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105688d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1056892d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105689820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105689d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10568a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10568a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10568ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10568b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10568b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10568bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10568c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10568c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10568cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10568d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10568d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10568dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10568e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10568e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10568ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10568f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10568f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10568fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105690260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1056907b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105690d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105691250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1056917a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105691cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105692240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105692790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105692ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105693230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105693780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105693cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105694220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105694770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105694cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105694f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105695240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105695500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105695970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105695de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105696250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1056966c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105696b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105696fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105697410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105697880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105697cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105698160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1056985d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105698a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105698eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105699320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10569a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10569a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10569ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10569b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10569b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10569bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10569c1d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105699860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10569be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10569b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10569c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10569c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10569cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10569ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10569d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10569d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10569d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10569d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10569dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10569e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10569e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10569ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10569f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10569f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10569f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10569f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10569fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10569fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1056a0140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1056a0400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1056a06c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1056a0980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1056a0c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1056a0f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1056a11c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1056a1480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1056a1740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1056a1a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1056a1cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1056a1f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1056a2240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1056a2500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1056a27c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1056a2a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1056a2d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1056a3000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1056a32c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1056a3580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1056a3840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1056a3b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1056a3dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1056a4080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1056a4340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1056a4600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1056a48c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1056a4b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1056a4e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1056a5100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1056a53c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1056a5680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1056a5940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1056a5c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1056a5ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1056a6180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1056a6440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1056a6700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1056a69c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1056a6c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1056a6f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1056a7200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1056a74c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1056a7780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1056a7a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1056a7d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1056a7fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1056a8280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1056a8540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1056a8800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1056a8ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1056a8d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1056a9040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10df04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10df044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10df04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10df04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10df05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10df056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10df05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10df05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10df06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10df06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10df06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10df07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10df075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10df07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10df07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10df08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10df08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10df08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10df09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10df094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10df09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10df09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10df0a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10df0a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10df0ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10df0af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10df0b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10df0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10df0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10df0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10df0ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10df0cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10df0d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10df0db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10df0e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10df0e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10df0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10df0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10df0f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10df0fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10df10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10df10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10df10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10df11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10df11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10df11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10df12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10df12530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10df12a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10df12f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10df13430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10df13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10df13e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10df14330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10df14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10df14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10df15230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10df15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10df15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10df16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10df16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10df16b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10df17030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10df17530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10df17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10df17f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10df18430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10df18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10df18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10df19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10df19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10df19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10df1a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10df1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10df1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10df1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10df1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10df1bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10df1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10df1c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10df1ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10df1cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10df1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10df1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10df1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10df1e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10df1e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10df1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10df1f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10df1f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10df1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10df20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10df20630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10df20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10df21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10df21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10df21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10df21f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10df22430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10df22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10df22e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10df23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10df23830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10df23d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10df24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10df24730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10df24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10df25130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10df25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10df25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10df26030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10df26530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10df26a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10df26f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10df27430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10df27930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10df27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10df28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10df28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10df28d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10df292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10df29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10df29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10df2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10df2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10df2b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10df2b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10df2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10df2c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10df2c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10df2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10df2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10df2d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10df2de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10df2e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10df2e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10df2ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10df2f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10df2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10df2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10df30450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10df309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10df30ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10df31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10df31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10df31ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10df32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10df32980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10df32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10df33420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10df33970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10df33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10df34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10df34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10df34eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10df35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10df35950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10df35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10df363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10df36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10df36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10df373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10df37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10df37e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10df383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10df38920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10df38e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10df393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10df39910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10df39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10df3a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10df3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10df3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10df3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10df3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10df3be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10df3c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10df3c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10df3ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10df3d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10df3d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10df3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10df3e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10df3e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10df3ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10df3f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10df3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10df3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10df40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10df408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10df40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10df41340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10df41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10df41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10df421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10df42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10df42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10df42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10df43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10df438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10df43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10df44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10df446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10df44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10df45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10df454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10df45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10df45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10df46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10df46a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10df47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10df478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10df47fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10df48280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10df48a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10df48d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10df49340 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.759s
user	0m0.292s
sys	0m0.281s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4485 (f446c2cf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138e0ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138e0e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138e0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138e0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138e0f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138e0fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138e10170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138e10720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138e10cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138e111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138e116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138e11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138e126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138e12ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138e136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138e13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138e144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138e14c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138e15330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138e15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138e16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138e16940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138e17060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138e17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138e18020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138e182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138e188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138e19560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138e19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138e19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138e1a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138e1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138e1ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138e1b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138e1b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138e1b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138e1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138e1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138e1c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138e1cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138e1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138e1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138e1da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138e1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138e1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138e1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138e1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138e1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138e1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138e20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138e20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138e20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138e21540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138e21b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138e22340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138e227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138e22c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138e22f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138e23550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138e23d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138e24000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138e244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138e24940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138e24de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138e25280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138e25720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138e25bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138e26060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138e26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138e269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138e26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138e272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138e27780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138e27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138e28220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138e28770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138e28cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138e29210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138e29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138e29cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138e2a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138e2a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138e2aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138e2b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138e2b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138e2bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138e2c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138e2c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138e2cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138e2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138e2d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138e2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138e2e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138e2e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138e2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138e2f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138e2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138e1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138e2fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138e30320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138e30870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138e30dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138e31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138e31860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138e31db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138e32300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138e32850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138e32da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138e332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138e33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138e33d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138e342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138e34830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138e34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138e35170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138e35610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138e35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138e35f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138e363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138e36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138e36d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138e371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138e37670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138e37b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138e37fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138e38450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138e388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138e38d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138e39230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138e396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138e39b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138e3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138e3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138e3a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138e3adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138e3b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138e3b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138e3bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138e3c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138e3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138e3c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138e3ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138e3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138e3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138e3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138e3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138e3e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138e3ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138e3eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138e3f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138e3f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138e3fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138e40130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138e405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138e40a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138e40f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138e413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138e41850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138e41cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138e42190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138e42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138e42ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138e42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138e43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138e438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138e43d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138e441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138e44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138e44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138e44fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138e45470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138e45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138e45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138e46250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138e466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138e46b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138e47030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138e474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138e47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138e47e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138e482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138e48750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138e48bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138e49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138e49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138e499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138e49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138e4a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138e4a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138e4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138e4b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138e4b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138e4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138e4bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138e4c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138e4ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138e4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138e4d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138e4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138e4de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138e4e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138e4ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138e4f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138e4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138e4f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138e4ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138e507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138e50c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138e51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138e515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138e51d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138e522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138e527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138e52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138e53290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138e537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138e53d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138e54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138e547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138e54d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138e55270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138e557c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138e55d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138e56260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138e567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138e56d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138e57250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138e577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138e57cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138e58240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138e58790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138e58ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138e59230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138e59780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138e59cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138e5a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138e5a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138e5acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138e5b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138e5b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138e5bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138e5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138e5c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138e5cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138e5d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138e5d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138e5dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138e5e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138e5e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138e5ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138e5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138e5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138e5fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138e601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138e60710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138e60c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138e611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138e61700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138e61c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138e621a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138e626f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138e62c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138e63190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138e636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138e63c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138e64180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138e646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138e64b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138e65010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138e654b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138e65950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138e65df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138e66290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138e66730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138e66bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138e67070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138e67510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138e679b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138e67e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138e682f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138e68790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138e68c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138e69180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138e698a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138e69fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138e6a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138e6ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138e6b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138e6b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138e6bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138e6c180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.126.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.126.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13de04bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13de05030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13de054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13de05910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13de05d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13de061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13de06660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13de06ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13de06f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13de073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de07820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de08a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de0e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de0e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de0eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de0f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de0f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de0fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de10e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de11ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de12d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de13620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de13a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de13f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de14370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de19950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de1a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de1af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de1bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de1ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de1d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de1e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de1eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de21e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de22bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de25820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de26100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de26570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de26e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de28010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de28480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de2a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de2cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de2e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de2ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de2ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de2f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de31b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de31fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de32440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de33190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de33a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de33ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de35cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de37cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de38160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de38a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de38eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de3a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de3adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de3bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de3c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de3da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de3ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de3fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de41b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de42950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de44050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de44bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de45750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de47410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de48b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de49c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de4a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de4a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de4ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de4b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de4ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de4ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de50f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de51ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de53d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de57550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de57a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de57f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de58950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de58e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de59850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de59d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de5a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de5a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de5ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de5b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de5c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de5cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de5d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de5d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13de5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13de5e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de5e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13de5b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13de4c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13de4b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13de48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13de45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13de55150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13de52910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13de50690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13de4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13de46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13de43d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13de48dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13de49f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13de4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13de4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13de54010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13de46b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13de4ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13de49950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13de42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13de4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13de48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13de52ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13de4de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13de43790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13de45450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13de55cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13de4b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13de53490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13de49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13de4bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13de4fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13de4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13de47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13de517d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13de45fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13de545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13de51d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13de4d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13de56850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13de44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13de56290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13de44310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13de54b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13de4e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13de50c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13de53a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13de52350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13de4a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13de41de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13de04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13de5db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13de07ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13de5eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13de5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13de5f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13de5f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13de5f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13de5fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13de5fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13de600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13de603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13de60660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13de60920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13de60be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13de60ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13de61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13de61420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13de616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13de619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13de61c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13de61f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13de621e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13de62730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13de629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13de62cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13de62f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13de63230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13de634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13de637b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13de63a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13de63d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13de63ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13de642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13de64570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13de64830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13de64af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13de64db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13de65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13de65330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13de655f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13de658b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13de65b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13de65e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13de660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13de663b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13de66670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13de66930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13de66bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13de66eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13de67170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13de67430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13de676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13de679b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13de67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13de67f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13de681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13de684b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13de68770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13de68a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13de68cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13de68fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13de69270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13de69530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13de697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13de69ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13de69d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13de6a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13de6a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13de6a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13de6a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13de6ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13de6adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13de6b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13de6b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13de6b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13de6b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13de6bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13de6be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13de6c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13de6c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13de6c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13de6c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13de6cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13de6cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13de6d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13de6d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13de6d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13de6d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13de6dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13de6df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13de6e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13de6e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13de6e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13de6ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13de6ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13de6eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13de6f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13de6f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13de6f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13de6faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13de6fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13de70070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13de70330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13de705f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13de708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13de70b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13de70e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13de710f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13de713b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13de71670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13de71930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13de71bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13de71eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13de72170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13de72430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13de726f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13de729b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13de72c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13de72f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13de731f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13de734b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13de73770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13de73a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13de73cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13de73fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13de74270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13de74530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13de747f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13de74ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13de74d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13de75030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13de752f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13de755b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13de75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13de75b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13de75df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13de760b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13de76370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13de76630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13de768f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13de76bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13de76e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13de77130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13de773f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13de776b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13de77970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13de77c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13de77ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13de781b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13de78470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13de78730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13de789f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13de78cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13de78f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13de79230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13de794f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13de797b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13de79a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13de79d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13de7a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13de7a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13de7a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13de7ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13de7ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13de7b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13de7b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13de7b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13de7bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13de7c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13de7c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13de7cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13de7d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13de7d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13de7db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13de7e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13de7e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13de7eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13de7f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13de7f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13de7fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13de800a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13de805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13de80b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13de81090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13de815e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13de81b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13de82080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13de825d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13de82b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13de83070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13de835c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13de83b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13de84060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13de845b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13de84b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13de85050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13de855a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13de85af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13de86040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13de86590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13de86ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13de87030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13de87580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13de87ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13de88020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13de88570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13de88ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13de89010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13de89560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13de89ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13de8a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13de8a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13de8aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13de8aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13de8b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13de8ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13de8bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13de8c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13de8c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13de8c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13de8cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13de8d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13de8d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13de8d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13de8dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13de8e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13de8e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13de8eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13de8ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13de8f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13de8f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13de8fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13de900f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13de90de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13de91500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13de91c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13de91ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13de92350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13de92950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13de92f60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.981s
user	0m0.264s
sys	0m0.142s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
