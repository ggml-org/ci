Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.688s
user	0m0.719s
sys	0m1.026s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  7%] Built target build_info
[  7%] Built target sha1
[  7%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target xxhash
[  7%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 24%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Built target llava
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX static library libllava_static.a
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 33%] Linking CXX static library libcommon.a
[ 33%] Built target test-c
[ 33%] Built target llama-simple-chat
[ 33%] Built target llama-simple
[ 33%] Built target llama-quantize-stats
[ 33%] Built target llava_static
[ 33%] Built target common
[ 33%] Built target llava_shared
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-0
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 44%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Built target test-log
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Built target test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Built target test-autorelease
[ 62%] Linking CXX executable ../bin/test-rope
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Built target test-barrier
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Built target llama-batched-bench
[ 66%] Built target test-quantize-perf
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 66%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target test-rope
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Built target llama-batched
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Built target llama-infill
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Built target llama-bench
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Built target llama-lookahead
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Built target llama-lookup
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Generating loading.html.hpp
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Generating index.html.hpp
[ 84%] Built target llama-lookup-merge
[ 84%] Built target llama-lookup-create
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-stats
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 87%] Built target llama-parallel
[ 87%] Built target llama-perplexity
[ 87%] Built target llama-quantize
[ 87%] Built target llama-passkey
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-tokenize
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-speculative-simple
[ 94%] Built target llama-speculative
[ 94%] Built target llama-run
[ 94%] Built target llama-save-load-state
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-tokenize
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Built target llama-export-lora
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[100%] Built target llama-q8dot
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.389s
user	0m5.460s
sys	0m8.902s

main: quantize time =  2967.23 ms
main:    total time =  2967.23 ms

main: quantize time =  1539.16 ms
main:    total time =  1539.16 ms

main: quantize time =  1362.91 ms
main:    total time =  1362.91 ms

main: quantize time =  2153.72 ms
main:    total time =  2153.72 ms

main: quantize time =  3124.28 ms
main:    total time =  3124.28 ms

main: quantize time =  5082.98 ms
main:    total time =  5082.98 ms

main: quantize time =  5734.78 ms
main:    total time =  5734.78 ms

main: quantize time =  6766.77 ms
main:    total time =  6766.77 ms

main: quantize time =  6021.28 ms
main:    total time =  6021.28 ms

main: quantize time =  4541.05 ms
main:    total time =  4541.05 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.216 I main: llama backend init
0.00.000.222 I main: load the model and apply lora adapter, if any
0.00.042.242 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.053.535 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.053.549 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.053.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.053.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.053.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.053.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.053.556 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.053.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.053.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.053.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.053.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.053.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.053.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.053.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.053.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.053.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.053.569 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.060.532 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.062.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.070.429 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.070.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.070.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.070.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.070.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.070.441 I llama_model_loader: - type  f32:  194 tensors
0.00.070.442 I llama_model_loader: - type  f16:   98 tensors
0.00.107.286 I llm_load_vocab: special tokens cache size = 25
0.00.115.038 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.115.041 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.115.041 I llm_load_print_meta: arch             = gptneox
0.00.115.042 I llm_load_print_meta: vocab type       = BPE
0.00.115.042 I llm_load_print_meta: n_vocab          = 50304
0.00.115.042 I llm_load_print_meta: n_merges         = 50009
0.00.115.042 I llm_load_print_meta: vocab_only       = 0
0.00.115.042 I llm_load_print_meta: n_ctx_train      = 2048
0.00.115.042 I llm_load_print_meta: n_embd           = 2048
0.00.115.043 I llm_load_print_meta: n_layer          = 24
0.00.115.046 I llm_load_print_meta: n_head           = 16
0.00.115.047 I llm_load_print_meta: n_head_kv        = 16
0.00.115.047 I llm_load_print_meta: n_rot            = 32
0.00.115.047 I llm_load_print_meta: n_swa            = 0
0.00.115.047 I llm_load_print_meta: n_embd_head_k    = 128
0.00.115.047 I llm_load_print_meta: n_embd_head_v    = 128
0.00.115.050 I llm_load_print_meta: n_gqa            = 1
0.00.115.051 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.115.052 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.115.052 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.115.053 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.115.053 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.115.053 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.115.054 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.115.054 I llm_load_print_meta: n_ff             = 8192
0.00.115.054 I llm_load_print_meta: n_expert         = 0
0.00.115.055 I llm_load_print_meta: n_expert_used    = 0
0.00.115.055 I llm_load_print_meta: causal attn      = 1
0.00.115.055 I llm_load_print_meta: pooling type     = 0
0.00.115.055 I llm_load_print_meta: rope type        = 2
0.00.115.055 I llm_load_print_meta: rope scaling     = linear
0.00.115.056 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.115.058 I llm_load_print_meta: freq_scale_train = 1
0.00.115.058 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.115.058 I llm_load_print_meta: rope_finetuned   = unknown
0.00.115.058 I llm_load_print_meta: ssm_d_conv       = 0
0.00.115.058 I llm_load_print_meta: ssm_d_inner      = 0
0.00.115.058 I llm_load_print_meta: ssm_d_state      = 0
0.00.115.059 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.115.059 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.115.059 I llm_load_print_meta: model type       = 1.4B
0.00.115.060 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.115.060 I llm_load_print_meta: model params     = 1.41 B
0.00.115.060 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.115.061 I llm_load_print_meta: general.name     = 1.4B
0.00.115.066 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.115.067 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.115.067 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.115.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.115.068 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.115.068 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.115.068 I llm_load_print_meta: max token length = 1024
0.00.117.797 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.117.797 I llm_load_tensors: offloading output layer to GPU
0.00.117.797 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.117.816 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.117.817 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.118.849 I llama_new_context_with_model: n_seq_max     = 1
0.00.118.850 I llama_new_context_with_model: n_ctx         = 2048
0.00.118.850 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.118.850 I llama_new_context_with_model: n_batch       = 2048
0.00.118.850 I llama_new_context_with_model: n_ubatch      = 512
0.00.118.850 I llama_new_context_with_model: flash_attn    = 0
0.00.118.851 I llama_new_context_with_model: freq_base     = 10000.0
0.00.118.851 I llama_new_context_with_model: freq_scale    = 1
0.00.118.852 I ggml_metal_init: allocating
0.00.118.860 I ggml_metal_init: found device: Apple M4
0.00.118.862 I ggml_metal_init: picking default device: Apple M4
0.00.119.562 I ggml_metal_init: using embedded metal library
0.00.129.649 I ggml_metal_init: GPU name:   Apple M4
0.00.129.651 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.129.652 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.129.652 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.129.652 I ggml_metal_init: simdgroup reduction   = true
0.00.129.653 I ggml_metal_init: simdgroup matrix mul. = true
0.00.129.653 I ggml_metal_init: has bfloat            = true
0.00.129.653 I ggml_metal_init: use bfloat            = true
0.00.129.653 I ggml_metal_init: hasUnifiedMemory      = true
0.00.129.654 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.174.974 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.174.980 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.175.000 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.176.041 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.176.043 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.176.043 I llama_new_context_with_model: graph nodes  = 967
0.00.176.043 I llama_new_context_with_model: graph splits = 2
0.00.176.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.255.592 I main: llama threadpool init, n_threads = 4
0.00.255.626 I 
0.00.255.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.255.662 I 
0.00.255.741 I sampler seed: 1234
0.00.255.746 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.255.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.255.771 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.255.771 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.103.952 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.02.103.953 I llama_perf_context_print:        load time =     213.34 ms
0.02.103.954 I llama_perf_context_print: prompt eval time =      43.80 ms /     7 tokens (    6.26 ms per token,   159.81 tokens per second)
0.02.103.954 I llama_perf_context_print:        eval time =    1801.49 ms /    63 runs   (   28.60 ms per token,    34.97 tokens per second)
0.02.103.955 I llama_perf_context_print:       total time =    1848.36 ms /    70 tokens
0.02.104.147 I ggml_metal_free: deallocating

real	0m2.394s
user	0m0.148s
sys	0m0.103s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.689 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.428 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.436 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.437 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.438 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.439 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.439 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.440 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.440 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.440 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.442 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.443 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.399 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.590 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.878 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.881 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.882 I llama_model_loader: - type  f32:  194 tensors
0.00.034.882 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.701 I llm_load_vocab: special tokens cache size = 25
0.00.063.643 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.647 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.648 I llm_load_print_meta: arch             = gptneox
0.00.063.648 I llm_load_print_meta: vocab type       = BPE
0.00.063.649 I llm_load_print_meta: n_vocab          = 50304
0.00.063.649 I llm_load_print_meta: n_merges         = 50009
0.00.063.649 I llm_load_print_meta: vocab_only       = 0
0.00.063.651 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.651 I llm_load_print_meta: n_embd           = 2048
0.00.063.651 I llm_load_print_meta: n_layer          = 24
0.00.063.656 I llm_load_print_meta: n_head           = 16
0.00.063.658 I llm_load_print_meta: n_head_kv        = 16
0.00.063.658 I llm_load_print_meta: n_rot            = 32
0.00.063.658 I llm_load_print_meta: n_swa            = 0
0.00.063.658 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.658 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.661 I llm_load_print_meta: n_gqa            = 1
0.00.063.662 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.662 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.663 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.664 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.664 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.664 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.666 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.666 I llm_load_print_meta: n_ff             = 8192
0.00.063.667 I llm_load_print_meta: n_expert         = 0
0.00.063.667 I llm_load_print_meta: n_expert_used    = 0
0.00.063.667 I llm_load_print_meta: causal attn      = 1
0.00.063.667 I llm_load_print_meta: pooling type     = 0
0.00.063.667 I llm_load_print_meta: rope type        = 2
0.00.063.668 I llm_load_print_meta: rope scaling     = linear
0.00.063.668 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.668 I llm_load_print_meta: freq_scale_train = 1
0.00.063.669 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.669 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.669 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.669 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.669 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.669 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.670 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.671 I llm_load_print_meta: model type       = 1.4B
0.00.063.671 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.671 I llm_load_print_meta: model params     = 1.41 B
0.00.063.672 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.672 I llm_load_print_meta: general.name     = 1.4B
0.00.063.672 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.672 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.672 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.672 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.673 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.673 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.673 I llm_load_print_meta: max token length = 1024
0.00.065.770 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.771 I llm_load_tensors: offloading output layer to GPU
0.00.065.771 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.777 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.779 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.764 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.765 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.765 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.765 I llama_new_context_with_model: n_batch       = 2048
0.00.066.766 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.766 I llama_new_context_with_model: flash_attn    = 0
0.00.066.766 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.767 I llama_new_context_with_model: freq_scale    = 1
0.00.066.767 I ggml_metal_init: allocating
0.00.066.771 I ggml_metal_init: found device: Apple M4
0.00.066.773 I ggml_metal_init: picking default device: Apple M4
0.00.067.503 I ggml_metal_init: using embedded metal library
0.00.070.058 I ggml_metal_init: GPU name:   Apple M4
0.00.070.060 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.060 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.061 I ggml_metal_init: simdgroup reduction   = true
0.00.070.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.061 I ggml_metal_init: has bfloat            = true
0.00.070.061 I ggml_metal_init: use bfloat            = true
0.00.070.062 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.286 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.294 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.322 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.410 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.412 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.412 I llama_new_context_with_model: graph nodes  = 967
0.00.106.412 I llama_new_context_with_model: graph splits = 2
0.00.106.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.476.295 I main: llama threadpool init, n_threads = 4
0.01.476.344 I 
0.01.476.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.476.394 I 
0.01.476.719 I sampler seed: 1234
0.01.476.727 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.476.767 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.476.773 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.476.774 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.578.963 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49754.73 tokens per second)
0.02.578.964 I llama_perf_context_print:        load time =    1466.60 ms
0.02.578.965 I llama_perf_context_print: prompt eval time =      50.52 ms /     7 tokens (    7.22 ms per token,   138.57 tokens per second)
0.02.578.966 I llama_perf_context_print:        eval time =    1048.81 ms /    63 runs   (   16.65 ms per token,    60.07 tokens per second)
0.02.578.967 I llama_perf_context_print:       total time =    1102.67 ms /    70 tokens
0.02.579.174 I ggml_metal_free: deallocating

real	0m2.598s
user	0m0.119s
sys	0m0.261s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.015.178 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.108 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.114 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.119 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.120 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.120 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.122 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.134 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.808 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.858 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.859 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.860 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.860 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.861 I llama_model_loader: - type  f32:  194 tensors
0.00.045.862 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.862 I llama_model_loader: - type q6_K:    1 tensors
0.00.079.149 I llm_load_vocab: special tokens cache size = 25
0.00.089.548 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.554 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.555 I llm_load_print_meta: arch             = gptneox
0.00.089.555 I llm_load_print_meta: vocab type       = BPE
0.00.089.556 I llm_load_print_meta: n_vocab          = 50304
0.00.089.556 I llm_load_print_meta: n_merges         = 50009
0.00.089.556 I llm_load_print_meta: vocab_only       = 0
0.00.089.556 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.557 I llm_load_print_meta: n_embd           = 2048
0.00.089.557 I llm_load_print_meta: n_layer          = 24
0.00.089.562 I llm_load_print_meta: n_head           = 16
0.00.089.563 I llm_load_print_meta: n_head_kv        = 16
0.00.089.563 I llm_load_print_meta: n_rot            = 32
0.00.089.565 I llm_load_print_meta: n_swa            = 0
0.00.089.565 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.565 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.566 I llm_load_print_meta: n_gqa            = 1
0.00.089.568 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.571 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.572 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.572 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.572 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.573 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.573 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.574 I llm_load_print_meta: n_ff             = 8192
0.00.089.574 I llm_load_print_meta: n_expert         = 0
0.00.089.574 I llm_load_print_meta: n_expert_used    = 0
0.00.089.574 I llm_load_print_meta: causal attn      = 1
0.00.089.575 I llm_load_print_meta: pooling type     = 0
0.00.089.575 I llm_load_print_meta: rope type        = 2
0.00.089.575 I llm_load_print_meta: rope scaling     = linear
0.00.089.576 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.580 I llm_load_print_meta: freq_scale_train = 1
0.00.089.581 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.581 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.581 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.582 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.582 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.582 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.583 I llm_load_print_meta: model type       = 1.4B
0.00.089.583 I llm_load_print_meta: model ftype      = Q4_0
0.00.089.584 I llm_load_print_meta: model params     = 1.41 B
0.00.089.584 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.089.584 I llm_load_print_meta: general.name     = 1.4B
0.00.089.585 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.585 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.585 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.585 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.587 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.587 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.588 I llm_load_print_meta: max token length = 1024
0.00.092.392 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.392 I llm_load_tensors: offloading output layer to GPU
0.00.092.392 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.405 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.092.407 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.093.800 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.801 I llama_new_context_with_model: n_ctx         = 2048
0.00.093.801 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.093.801 I llama_new_context_with_model: n_batch       = 2048
0.00.093.802 I llama_new_context_with_model: n_ubatch      = 512
0.00.093.802 I llama_new_context_with_model: flash_attn    = 0
0.00.093.803 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.803 I llama_new_context_with_model: freq_scale    = 1
0.00.093.804 I ggml_metal_init: allocating
0.00.093.812 I ggml_metal_init: found device: Apple M4
0.00.093.816 I ggml_metal_init: picking default device: Apple M4
0.00.094.779 I ggml_metal_init: using embedded metal library
0.00.098.565 I ggml_metal_init: GPU name:   Apple M4
0.00.098.567 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.568 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.569 I ggml_metal_init: simdgroup reduction   = true
0.00.098.569 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.569 I ggml_metal_init: has bfloat            = true
0.00.098.569 I ggml_metal_init: use bfloat            = true
0.00.098.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.571 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.137.526 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.137.535 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.137.575 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.138.638 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.138.640 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.138.640 I llama_new_context_with_model: graph nodes  = 967
0.00.138.641 I llama_new_context_with_model: graph splits = 2
0.00.138.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.108 I main: llama threadpool init, n_threads = 4
0.00.790.196 I 
0.00.790.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.265 I 
0.00.790.794 I sampler seed: 1234
0.00.790.802 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.860 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.861 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.861 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.478.553 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.478.556 I llama_perf_context_print:        load time =     774.92 ms
0.01.478.557 I llama_perf_context_print: prompt eval time =      48.48 ms /     7 tokens (    6.93 ms per token,   144.38 tokens per second)
0.01.478.558 I llama_perf_context_print:        eval time =     636.19 ms /    63 runs   (   10.10 ms per token,    99.03 tokens per second)
0.01.478.558 I llama_perf_context_print:       total time =     688.45 ms /    70 tokens
0.01.478.753 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.146s
sys	0m0.208s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.684 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.537 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.543 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.543 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.544 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.544 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.544 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.545 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.546 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.547 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.547 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.547 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.550 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.550 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.551 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.650 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.470 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.471 I llama_model_loader: - type  f32:  194 tensors
0.00.025.471 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.471 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.676 I llm_load_vocab: special tokens cache size = 25
0.00.051.670 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.673 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.673 I llm_load_print_meta: arch             = gptneox
0.00.051.674 I llm_load_print_meta: vocab type       = BPE
0.00.051.674 I llm_load_print_meta: n_vocab          = 50304
0.00.051.674 I llm_load_print_meta: n_merges         = 50009
0.00.051.674 I llm_load_print_meta: vocab_only       = 0
0.00.051.674 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.675 I llm_load_print_meta: n_embd           = 2048
0.00.051.675 I llm_load_print_meta: n_layer          = 24
0.00.051.677 I llm_load_print_meta: n_head           = 16
0.00.051.678 I llm_load_print_meta: n_head_kv        = 16
0.00.051.678 I llm_load_print_meta: n_rot            = 32
0.00.051.679 I llm_load_print_meta: n_swa            = 0
0.00.051.679 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.679 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.680 I llm_load_print_meta: n_gqa            = 1
0.00.051.681 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.681 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.682 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.682 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.682 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.682 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.683 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.683 I llm_load_print_meta: n_ff             = 8192
0.00.051.683 I llm_load_print_meta: n_expert         = 0
0.00.051.685 I llm_load_print_meta: n_expert_used    = 0
0.00.051.686 I llm_load_print_meta: causal attn      = 1
0.00.051.688 I llm_load_print_meta: pooling type     = 0
0.00.051.688 I llm_load_print_meta: rope type        = 2
0.00.051.688 I llm_load_print_meta: rope scaling     = linear
0.00.051.689 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.689 I llm_load_print_meta: freq_scale_train = 1
0.00.051.689 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.689 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.689 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.690 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.690 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.690 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.690 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.690 I llm_load_print_meta: model type       = 1.4B
0.00.051.691 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.691 I llm_load_print_meta: model params     = 1.41 B
0.00.051.692 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.692 I llm_load_print_meta: general.name     = 1.4B
0.00.051.692 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.696 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.696 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.696 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.697 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.697 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.697 I llm_load_print_meta: max token length = 1024
0.00.053.673 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.674 I llm_load_tensors: offloading output layer to GPU
0.00.053.674 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.684 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.686 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.635 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.636 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.636 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.636 I llama_new_context_with_model: n_batch       = 2048
0.00.054.636 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.636 I llama_new_context_with_model: flash_attn    = 0
0.00.054.637 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.637 I llama_new_context_with_model: freq_scale    = 1
0.00.054.637 I ggml_metal_init: allocating
0.00.054.644 I ggml_metal_init: found device: Apple M4
0.00.054.646 I ggml_metal_init: picking default device: Apple M4
0.00.055.256 I ggml_metal_init: using embedded metal library
0.00.057.575 I ggml_metal_init: GPU name:   Apple M4
0.00.057.576 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.577 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.577 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.577 I ggml_metal_init: simdgroup reduction   = true
0.00.057.577 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.578 I ggml_metal_init: has bfloat            = true
0.00.057.579 I ggml_metal_init: use bfloat            = true
0.00.057.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.241 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.246 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.264 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.277 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.279 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.279 I llama_new_context_with_model: graph nodes  = 967
0.00.087.279 I llama_new_context_with_model: graph splits = 2
0.00.087.294 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.713.463 I main: llama threadpool init, n_threads = 4
0.00.713.503 I 
0.00.713.532 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.713.533 I 
0.00.713.769 I sampler seed: 1234
0.00.713.774 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.713.786 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.713.786 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.713.787 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.448.406 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.448.408 I llama_perf_context_print:        load time =     704.77 ms
0.01.448.408 I llama_perf_context_print: prompt eval time =      46.55 ms /     7 tokens (    6.65 ms per token,   150.37 tokens per second)
0.01.448.409 I llama_perf_context_print:        eval time =     685.07 ms /    63 runs   (   10.87 ms per token,    91.96 tokens per second)
0.01.448.410 I llama_perf_context_print:       total time =     734.95 ms /    70 tokens
0.01.448.596 I ggml_metal_free: deallocating

real	0m1.464s
user	0m0.108s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.446 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.884 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.890 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.891 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.901 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.867 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.946 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.947 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.947 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.948 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.948 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.948 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.949 I llama_model_loader: - type  f32:  194 tensors
0.00.024.949 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.949 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.098 I llm_load_vocab: special tokens cache size = 25
0.00.052.309 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.311 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.312 I llm_load_print_meta: arch             = gptneox
0.00.052.312 I llm_load_print_meta: vocab type       = BPE
0.00.052.312 I llm_load_print_meta: n_vocab          = 50304
0.00.052.312 I llm_load_print_meta: n_merges         = 50009
0.00.052.313 I llm_load_print_meta: vocab_only       = 0
0.00.052.313 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.313 I llm_load_print_meta: n_embd           = 2048
0.00.052.313 I llm_load_print_meta: n_layer          = 24
0.00.052.316 I llm_load_print_meta: n_head           = 16
0.00.052.317 I llm_load_print_meta: n_head_kv        = 16
0.00.052.317 I llm_load_print_meta: n_rot            = 32
0.00.052.317 I llm_load_print_meta: n_swa            = 0
0.00.052.317 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.317 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.318 I llm_load_print_meta: n_gqa            = 1
0.00.052.319 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.320 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.320 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.320 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.321 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.321 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.321 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.322 I llm_load_print_meta: n_ff             = 8192
0.00.052.322 I llm_load_print_meta: n_expert         = 0
0.00.052.322 I llm_load_print_meta: n_expert_used    = 0
0.00.052.324 I llm_load_print_meta: causal attn      = 1
0.00.052.326 I llm_load_print_meta: pooling type     = 0
0.00.052.326 I llm_load_print_meta: rope type        = 2
0.00.052.326 I llm_load_print_meta: rope scaling     = linear
0.00.052.326 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.327 I llm_load_print_meta: freq_scale_train = 1
0.00.052.327 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.327 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.327 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.328 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.328 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.328 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.328 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.328 I llm_load_print_meta: model type       = 1.4B
0.00.052.329 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.329 I llm_load_print_meta: model params     = 1.41 B
0.00.052.330 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.330 I llm_load_print_meta: general.name     = 1.4B
0.00.052.332 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.332 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.332 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.332 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.332 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.333 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.333 I llm_load_print_meta: max token length = 1024
0.00.054.365 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.366 I llm_load_tensors: offloading output layer to GPU
0.00.054.366 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.376 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.378 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.289 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.289 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.290 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.290 I llama_new_context_with_model: n_batch       = 2048
0.00.055.290 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.290 I llama_new_context_with_model: flash_attn    = 0
0.00.055.291 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.291 I llama_new_context_with_model: freq_scale    = 1
0.00.055.291 I ggml_metal_init: allocating
0.00.055.295 I ggml_metal_init: found device: Apple M4
0.00.055.297 I ggml_metal_init: picking default device: Apple M4
0.00.055.911 I ggml_metal_init: using embedded metal library
0.00.058.255 I ggml_metal_init: GPU name:   Apple M4
0.00.058.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.257 I ggml_metal_init: simdgroup reduction   = true
0.00.058.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.257 I ggml_metal_init: has bfloat            = true
0.00.058.257 I ggml_metal_init: use bfloat            = true
0.00.058.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.889 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.896 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.931 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.911 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.913 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.913 I llama_new_context_with_model: graph nodes  = 967
0.00.089.913 I llama_new_context_with_model: graph splits = 2
0.00.089.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.615 I main: llama threadpool init, n_threads = 4
0.00.782.650 I 
0.00.782.683 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.683 I 
0.00.782.910 I sampler seed: 1234
0.00.782.915 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.947 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.949 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.950 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.577.099 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.01.577.100 I llama_perf_context_print:        load time =     773.16 ms
0.01.577.101 I llama_perf_context_print: prompt eval time =      49.20 ms /     7 tokens (    7.03 ms per token,   142.26 tokens per second)
0.01.577.102 I llama_perf_context_print:        eval time =     742.08 ms /    63 runs   (   11.78 ms per token,    84.90 tokens per second)
0.01.577.102 I llama_perf_context_print:       total time =     794.49 ms /    70 tokens
0.01.577.310 I ggml_metal_free: deallocating

real	0m1.597s
user	0m0.111s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.754 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.447 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.460 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.461 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.461 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.462 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.462 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.463 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.463 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.463 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.464 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.464 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.466 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.466 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.466 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.413 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.415 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.415 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.415 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.416 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.416 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.417 I llama_model_loader: - type  f32:  194 tensors
0.00.024.417 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.417 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.470 I llm_load_vocab: special tokens cache size = 25
0.00.051.391 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.394 I llm_load_print_meta: arch             = gptneox
0.00.051.394 I llm_load_print_meta: vocab type       = BPE
0.00.051.395 I llm_load_print_meta: n_vocab          = 50304
0.00.051.395 I llm_load_print_meta: n_merges         = 50009
0.00.051.395 I llm_load_print_meta: vocab_only       = 0
0.00.051.395 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.395 I llm_load_print_meta: n_embd           = 2048
0.00.051.396 I llm_load_print_meta: n_layer          = 24
0.00.051.398 I llm_load_print_meta: n_head           = 16
0.00.051.401 I llm_load_print_meta: n_head_kv        = 16
0.00.051.401 I llm_load_print_meta: n_rot            = 32
0.00.051.402 I llm_load_print_meta: n_swa            = 0
0.00.051.402 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.403 I llm_load_print_meta: n_gqa            = 1
0.00.051.403 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.404 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.405 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.405 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.405 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.406 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.406 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.406 I llm_load_print_meta: n_ff             = 8192
0.00.051.407 I llm_load_print_meta: n_expert         = 0
0.00.051.407 I llm_load_print_meta: n_expert_used    = 0
0.00.051.407 I llm_load_print_meta: causal attn      = 1
0.00.051.407 I llm_load_print_meta: pooling type     = 0
0.00.051.407 I llm_load_print_meta: rope type        = 2
0.00.051.407 I llm_load_print_meta: rope scaling     = linear
0.00.051.408 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.408 I llm_load_print_meta: freq_scale_train = 1
0.00.051.408 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.408 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.409 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.409 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.409 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.409 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.409 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.409 I llm_load_print_meta: model type       = 1.4B
0.00.051.410 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.410 I llm_load_print_meta: model params     = 1.41 B
0.00.051.411 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.411 I llm_load_print_meta: general.name     = 1.4B
0.00.051.413 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.413 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.414 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.414 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.414 I llm_load_print_meta: max token length = 1024
0.00.053.475 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.475 I llm_load_tensors: offloading output layer to GPU
0.00.053.475 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.486 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.487 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.418 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.419 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.419 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.419 I llama_new_context_with_model: n_batch       = 2048
0.00.054.419 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.420 I llama_new_context_with_model: flash_attn    = 0
0.00.054.420 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.420 I llama_new_context_with_model: freq_scale    = 1
0.00.054.421 I ggml_metal_init: allocating
0.00.054.427 I ggml_metal_init: found device: Apple M4
0.00.054.431 I ggml_metal_init: picking default device: Apple M4
0.00.055.025 I ggml_metal_init: using embedded metal library
0.00.057.362 I ggml_metal_init: GPU name:   Apple M4
0.00.057.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.364 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.364 I ggml_metal_init: simdgroup reduction   = true
0.00.057.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.364 I ggml_metal_init: has bfloat            = true
0.00.057.365 I ggml_metal_init: use bfloat            = true
0.00.057.365 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.366 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.865 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.870 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.887 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.881 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.883 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.883 I llama_new_context_with_model: graph nodes  = 967
0.00.086.883 I llama_new_context_with_model: graph splits = 2
0.00.086.897 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.199 I main: llama threadpool init, n_threads = 4
0.00.708.240 I 
0.00.708.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.265 I 
0.00.708.533 I sampler seed: 1234
0.00.708.536 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.708.550 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.708.550 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.708.550 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.550.391 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51152.74 tokens per second)
0.01.550.392 I llama_perf_context_print:        load time =     699.44 ms
0.01.550.393 I llama_perf_context_print: prompt eval time =      42.33 ms /     7 tokens (    6.05 ms per token,   165.38 tokens per second)
0.01.550.394 I llama_perf_context_print:        eval time =     796.81 ms /    63 runs   (   12.65 ms per token,    79.07 tokens per second)
0.01.550.394 I llama_perf_context_print:       total time =     842.20 ms /    70 tokens
0.01.550.629 I ggml_metal_free: deallocating

real	0m1.567s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.748 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.252 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.259 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.260 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.260 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.260 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.261 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.261 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.263 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.263 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.263 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.264 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.264 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.264 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.268 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.045 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.154 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.940 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.942 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.943 I llama_model_loader: - type  f32:  194 tensors
0.00.023.943 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.943 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.944 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.045 I llm_load_vocab: special tokens cache size = 25
0.00.050.034 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.037 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.037 I llm_load_print_meta: arch             = gptneox
0.00.050.038 I llm_load_print_meta: vocab type       = BPE
0.00.050.038 I llm_load_print_meta: n_vocab          = 50304
0.00.050.038 I llm_load_print_meta: n_merges         = 50009
0.00.050.038 I llm_load_print_meta: vocab_only       = 0
0.00.050.039 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.039 I llm_load_print_meta: n_embd           = 2048
0.00.050.039 I llm_load_print_meta: n_layer          = 24
0.00.050.041 I llm_load_print_meta: n_head           = 16
0.00.050.042 I llm_load_print_meta: n_head_kv        = 16
0.00.050.044 I llm_load_print_meta: n_rot            = 32
0.00.050.044 I llm_load_print_meta: n_swa            = 0
0.00.050.044 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.044 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.045 I llm_load_print_meta: n_gqa            = 1
0.00.050.046 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.046 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.047 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.047 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.047 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.048 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.048 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.049 I llm_load_print_meta: n_ff             = 8192
0.00.050.049 I llm_load_print_meta: n_expert         = 0
0.00.050.049 I llm_load_print_meta: n_expert_used    = 0
0.00.050.049 I llm_load_print_meta: causal attn      = 1
0.00.050.049 I llm_load_print_meta: pooling type     = 0
0.00.050.049 I llm_load_print_meta: rope type        = 2
0.00.050.049 I llm_load_print_meta: rope scaling     = linear
0.00.050.050 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.050 I llm_load_print_meta: freq_scale_train = 1
0.00.050.050 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.051 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.051 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.051 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.053 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.053 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.053 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.053 I llm_load_print_meta: model type       = 1.4B
0.00.050.054 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.054 I llm_load_print_meta: model params     = 1.41 B
0.00.050.055 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.055 I llm_load_print_meta: general.name     = 1.4B
0.00.050.055 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.055 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.056 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.056 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.056 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.057 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.057 I llm_load_print_meta: max token length = 1024
0.00.051.916 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.916 I llm_load_tensors: offloading output layer to GPU
0.00.051.916 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.927 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.928 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.841 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.841 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.842 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.842 I llama_new_context_with_model: n_batch       = 2048
0.00.052.842 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.842 I llama_new_context_with_model: flash_attn    = 0
0.00.052.843 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.843 I llama_new_context_with_model: freq_scale    = 1
0.00.052.843 I ggml_metal_init: allocating
0.00.052.847 I ggml_metal_init: found device: Apple M4
0.00.052.849 I ggml_metal_init: picking default device: Apple M4
0.00.053.468 I ggml_metal_init: using embedded metal library
0.00.055.836 I ggml_metal_init: GPU name:   Apple M4
0.00.055.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.838 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.838 I ggml_metal_init: simdgroup reduction   = true
0.00.055.839 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.839 I ggml_metal_init: has bfloat            = true
0.00.055.839 I ggml_metal_init: use bfloat            = true
0.00.055.839 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.658 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.664 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.684 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.712 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.714 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.714 I llama_new_context_with_model: graph nodes  = 967
0.00.085.715 I llama_new_context_with_model: graph splits = 2
0.00.085.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.440.197 I main: llama threadpool init, n_threads = 4
0.00.440.236 I 
0.00.440.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.440.268 I 
0.00.440.508 I sampler seed: 1234
0.00.440.514 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.440.524 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.440.525 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.440.525 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.119.711 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61846.69 tokens per second)
0.01.119.711 I llama_perf_context_print:        load time =     430.44 ms
0.01.119.712 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.85 tokens per second)
0.01.119.713 I llama_perf_context_print:        eval time =     640.53 ms /    63 runs   (   10.17 ms per token,    98.36 tokens per second)
0.01.119.713 I llama_perf_context_print:       total time =     679.52 ms /    70 tokens
0.01.119.903 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.109s
sys	0m0.111s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.756 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.149 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.154 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.156 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.158 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.203 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.073 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.074 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.074 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.074 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.075 I llama_model_loader: - type  f32:  194 tensors
0.00.024.075 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.075 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.075 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.076 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.028 I llm_load_vocab: special tokens cache size = 25
0.00.050.936 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.939 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.940 I llm_load_print_meta: arch             = gptneox
0.00.050.940 I llm_load_print_meta: vocab type       = BPE
0.00.050.940 I llm_load_print_meta: n_vocab          = 50304
0.00.050.940 I llm_load_print_meta: n_merges         = 50009
0.00.050.941 I llm_load_print_meta: vocab_only       = 0
0.00.050.941 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.941 I llm_load_print_meta: n_embd           = 2048
0.00.050.941 I llm_load_print_meta: n_layer          = 24
0.00.050.944 I llm_load_print_meta: n_head           = 16
0.00.050.945 I llm_load_print_meta: n_head_kv        = 16
0.00.050.945 I llm_load_print_meta: n_rot            = 32
0.00.050.945 I llm_load_print_meta: n_swa            = 0
0.00.050.946 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.946 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.946 I llm_load_print_meta: n_gqa            = 1
0.00.050.947 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.948 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.948 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.949 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.949 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.949 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.949 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.950 I llm_load_print_meta: n_ff             = 8192
0.00.050.951 I llm_load_print_meta: n_expert         = 0
0.00.050.951 I llm_load_print_meta: n_expert_used    = 0
0.00.050.951 I llm_load_print_meta: causal attn      = 1
0.00.050.951 I llm_load_print_meta: pooling type     = 0
0.00.050.951 I llm_load_print_meta: rope type        = 2
0.00.050.952 I llm_load_print_meta: rope scaling     = linear
0.00.050.952 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.952 I llm_load_print_meta: freq_scale_train = 1
0.00.050.953 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.954 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.954 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.954 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.954 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.954 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.956 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.956 I llm_load_print_meta: model type       = 1.4B
0.00.050.957 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.957 I llm_load_print_meta: model params     = 1.41 B
0.00.050.958 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.958 I llm_load_print_meta: general.name     = 1.4B
0.00.050.958 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.958 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.958 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.959 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.959 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.963 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.963 I llm_load_print_meta: max token length = 1024
0.00.052.834 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.834 I llm_load_tensors: offloading output layer to GPU
0.00.052.834 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.845 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.846 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.750 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.751 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.751 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.751 I llama_new_context_with_model: n_batch       = 2048
0.00.053.752 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.752 I llama_new_context_with_model: flash_attn    = 0
0.00.053.752 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.753 I llama_new_context_with_model: freq_scale    = 1
0.00.053.753 I ggml_metal_init: allocating
0.00.053.759 I ggml_metal_init: found device: Apple M4
0.00.053.762 I ggml_metal_init: picking default device: Apple M4
0.00.054.333 I ggml_metal_init: using embedded metal library
0.00.056.645 I ggml_metal_init: GPU name:   Apple M4
0.00.056.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.647 I ggml_metal_init: simdgroup reduction   = true
0.00.056.647 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.647 I ggml_metal_init: has bfloat            = true
0.00.056.647 I ggml_metal_init: use bfloat            = true
0.00.056.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.891 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.900 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.919 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.914 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.916 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.916 I llama_new_context_with_model: graph nodes  = 967
0.00.085.916 I llama_new_context_with_model: graph splits = 2
0.00.085.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.084 I main: llama threadpool init, n_threads = 4
0.00.544.125 I 
0.00.544.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.544.159 I 
0.00.544.407 I sampler seed: 1234
0.00.544.411 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.544.495 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.544.499 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.544.500 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.292.938 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62335.38 tokens per second)
0.01.292.938 I llama_perf_context_print:        load time =     534.32 ms
0.01.292.940 I llama_perf_context_print: prompt eval time =      44.38 ms /     7 tokens (    6.34 ms per token,   157.75 tokens per second)
0.01.292.940 I llama_perf_context_print:        eval time =     701.17 ms /    63 runs   (   11.13 ms per token,    89.85 tokens per second)
0.01.292.941 I llama_perf_context_print:       total time =     748.86 ms /    70 tokens
0.01.293.136 I ggml_metal_free: deallocating

real	0m1.310s
user	0m0.110s
sys	0m0.129s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.708 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.322 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.327 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.328 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.329 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.329 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.329 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.335 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.335 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.336 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.274 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.276 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.276 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.276 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.277 I llama_model_loader: - type  f32:  194 tensors
0.00.024.277 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.277 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.278 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.176 I llm_load_vocab: special tokens cache size = 25
0.00.051.141 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.143 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.144 I llm_load_print_meta: arch             = gptneox
0.00.051.144 I llm_load_print_meta: vocab type       = BPE
0.00.051.145 I llm_load_print_meta: n_vocab          = 50304
0.00.051.145 I llm_load_print_meta: n_merges         = 50009
0.00.051.145 I llm_load_print_meta: vocab_only       = 0
0.00.051.145 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.145 I llm_load_print_meta: n_embd           = 2048
0.00.051.145 I llm_load_print_meta: n_layer          = 24
0.00.051.148 I llm_load_print_meta: n_head           = 16
0.00.051.149 I llm_load_print_meta: n_head_kv        = 16
0.00.051.149 I llm_load_print_meta: n_rot            = 32
0.00.051.149 I llm_load_print_meta: n_swa            = 0
0.00.051.149 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.149 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.150 I llm_load_print_meta: n_gqa            = 1
0.00.051.151 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.152 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.152 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.154 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.154 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.154 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.154 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.155 I llm_load_print_meta: n_ff             = 8192
0.00.051.155 I llm_load_print_meta: n_expert         = 0
0.00.051.157 I llm_load_print_meta: n_expert_used    = 0
0.00.051.158 I llm_load_print_meta: causal attn      = 1
0.00.051.158 I llm_load_print_meta: pooling type     = 0
0.00.051.158 I llm_load_print_meta: rope type        = 2
0.00.051.159 I llm_load_print_meta: rope scaling     = linear
0.00.051.159 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.159 I llm_load_print_meta: freq_scale_train = 1
0.00.051.159 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.160 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.160 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.160 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.160 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.160 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.160 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.161 I llm_load_print_meta: model type       = 1.4B
0.00.051.161 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.161 I llm_load_print_meta: model params     = 1.41 B
0.00.051.162 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.164 I llm_load_print_meta: general.name     = 1.4B
0.00.051.164 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.164 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.164 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.165 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.165 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.165 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.170 I llm_load_print_meta: max token length = 1024
0.00.052.941 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.941 I llm_load_tensors: offloading output layer to GPU
0.00.052.941 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.947 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.947 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.847 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.847 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.848 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.848 I llama_new_context_with_model: n_batch       = 2048
0.00.053.848 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.848 I llama_new_context_with_model: flash_attn    = 0
0.00.053.848 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.849 I llama_new_context_with_model: freq_scale    = 1
0.00.053.849 I ggml_metal_init: allocating
0.00.053.852 I ggml_metal_init: found device: Apple M4
0.00.053.854 I ggml_metal_init: picking default device: Apple M4
0.00.054.470 I ggml_metal_init: using embedded metal library
0.00.056.776 I ggml_metal_init: GPU name:   Apple M4
0.00.056.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.778 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.780 I ggml_metal_init: simdgroup reduction   = true
0.00.056.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.780 I ggml_metal_init: has bfloat            = true
0.00.056.780 I ggml_metal_init: use bfloat            = true
0.00.056.781 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.782 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.875 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.882 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.910 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.800 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.801 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.802 I llama_new_context_with_model: graph nodes  = 967
0.00.085.802 I llama_new_context_with_model: graph splits = 2
0.00.085.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.791 I main: llama threadpool init, n_threads = 4
0.00.613.830 I 
0.00.613.877 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.879 I 
0.00.614.111 I sampler seed: 1234
0.00.614.115 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.137 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.137 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.137 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.374.260 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.374.260 I llama_perf_context_print:        load time =     605.08 ms
0.01.374.264 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.59 tokens per second)
0.01.374.265 I llama_perf_context_print:        eval time =     709.89 ms /    63 runs   (   11.27 ms per token,    88.75 tokens per second)
0.01.374.265 I llama_perf_context_print:       total time =     760.47 ms /    70 tokens
0.01.374.450 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.109s
sys	0m0.138s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.009.594 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.217 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.231 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.231 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.235 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.235 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.238 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.238 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.080 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.081 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.082 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.082 I llama_model_loader: - type  f32:  194 tensors
0.00.025.083 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.083 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.129 I llm_load_vocab: special tokens cache size = 25
0.00.051.076 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.078 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.079 I llm_load_print_meta: arch             = gptneox
0.00.051.079 I llm_load_print_meta: vocab type       = BPE
0.00.051.079 I llm_load_print_meta: n_vocab          = 50304
0.00.051.080 I llm_load_print_meta: n_merges         = 50009
0.00.051.080 I llm_load_print_meta: vocab_only       = 0
0.00.051.080 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.080 I llm_load_print_meta: n_embd           = 2048
0.00.051.080 I llm_load_print_meta: n_layer          = 24
0.00.051.082 I llm_load_print_meta: n_head           = 16
0.00.051.083 I llm_load_print_meta: n_head_kv        = 16
0.00.051.084 I llm_load_print_meta: n_rot            = 32
0.00.051.084 I llm_load_print_meta: n_swa            = 0
0.00.051.084 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.084 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.085 I llm_load_print_meta: n_gqa            = 1
0.00.051.086 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.086 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.087 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.087 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.087 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.087 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.088 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.088 I llm_load_print_meta: n_ff             = 8192
0.00.051.088 I llm_load_print_meta: n_expert         = 0
0.00.051.088 I llm_load_print_meta: n_expert_used    = 0
0.00.051.089 I llm_load_print_meta: causal attn      = 1
0.00.051.089 I llm_load_print_meta: pooling type     = 0
0.00.051.089 I llm_load_print_meta: rope type        = 2
0.00.051.089 I llm_load_print_meta: rope scaling     = linear
0.00.051.089 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.090 I llm_load_print_meta: freq_scale_train = 1
0.00.051.090 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.090 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.090 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.090 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.091 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.091 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.091 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.091 I llm_load_print_meta: model type       = 1.4B
0.00.051.091 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.092 I llm_load_print_meta: model params     = 1.41 B
0.00.051.092 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.093 I llm_load_print_meta: general.name     = 1.4B
0.00.051.093 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.093 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.093 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.093 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.094 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.096 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.096 I llm_load_print_meta: max token length = 1024
0.00.053.097 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.098 I llm_load_tensors: offloading output layer to GPU
0.00.053.098 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.108 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.109 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.039 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.040 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.040 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.040 I llama_new_context_with_model: n_batch       = 2048
0.00.054.040 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.040 I llama_new_context_with_model: flash_attn    = 0
0.00.054.041 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.041 I llama_new_context_with_model: freq_scale    = 1
0.00.054.041 I ggml_metal_init: allocating
0.00.054.044 I ggml_metal_init: found device: Apple M4
0.00.054.046 I ggml_metal_init: picking default device: Apple M4
0.00.054.638 I ggml_metal_init: using embedded metal library
0.00.056.911 I ggml_metal_init: GPU name:   Apple M4
0.00.056.912 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.913 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.913 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.913 I ggml_metal_init: simdgroup reduction   = true
0.00.056.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.913 I ggml_metal_init: has bfloat            = true
0.00.056.914 I ggml_metal_init: use bfloat            = true
0.00.056.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.914 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.842 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.848 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.866 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.875 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.876 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.877 I llama_new_context_with_model: graph nodes  = 967
0.00.086.877 I llama_new_context_with_model: graph splits = 2
0.00.086.886 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.365 I main: llama threadpool init, n_threads = 4
0.00.695.412 I 
0.00.695.447 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.450 I 
0.00.695.689 I sampler seed: 1234
0.00.695.694 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.695.732 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.695.736 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.695.736 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.541.593 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50105.86 tokens per second)
0.01.541.594 I llama_perf_context_print:        load time =     685.76 ms
0.01.541.594 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.61 tokens per second)
0.01.541.595 I llama_perf_context_print:        eval time =     791.64 ms /    63 runs   (   12.57 ms per token,    79.58 tokens per second)
0.01.541.595 I llama_perf_context_print:       total time =     846.24 ms /    70 tokens
0.01.541.822 I ggml_metal_free: deallocating

real	0m1.559s
user	0m0.107s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.673 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.711 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.723 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.726 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.726 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.573 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.576 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.577 I llama_model_loader: - type  f32:  194 tensors
0.00.024.577 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.514 I llm_load_vocab: special tokens cache size = 25
0.00.051.575 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.578 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.578 I llm_load_print_meta: arch             = gptneox
0.00.051.578 I llm_load_print_meta: vocab type       = BPE
0.00.051.579 I llm_load_print_meta: n_vocab          = 50304
0.00.051.579 I llm_load_print_meta: n_merges         = 50009
0.00.051.579 I llm_load_print_meta: vocab_only       = 0
0.00.051.579 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.579 I llm_load_print_meta: n_embd           = 2048
0.00.051.579 I llm_load_print_meta: n_layer          = 24
0.00.051.582 I llm_load_print_meta: n_head           = 16
0.00.051.583 I llm_load_print_meta: n_head_kv        = 16
0.00.051.583 I llm_load_print_meta: n_rot            = 32
0.00.051.583 I llm_load_print_meta: n_swa            = 0
0.00.051.584 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.586 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.587 I llm_load_print_meta: n_gqa            = 1
0.00.051.587 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.589 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.590 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.590 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.591 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.591 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.591 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.592 I llm_load_print_meta: n_ff             = 8192
0.00.051.592 I llm_load_print_meta: n_expert         = 0
0.00.051.592 I llm_load_print_meta: n_expert_used    = 0
0.00.051.592 I llm_load_print_meta: causal attn      = 1
0.00.051.594 I llm_load_print_meta: pooling type     = 0
0.00.051.595 I llm_load_print_meta: rope type        = 2
0.00.051.596 I llm_load_print_meta: rope scaling     = linear
0.00.051.596 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.596 I llm_load_print_meta: freq_scale_train = 1
0.00.051.597 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.597 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.597 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.597 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.597 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.597 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.597 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.601 I llm_load_print_meta: model type       = 1.4B
0.00.051.602 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.602 I llm_load_print_meta: model params     = 1.41 B
0.00.051.602 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.602 I llm_load_print_meta: general.name     = 1.4B
0.00.051.603 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.603 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.603 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.603 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.604 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.604 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.604 I llm_load_print_meta: max token length = 1024
0.00.053.656 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.656 I llm_load_tensors: offloading output layer to GPU
0.00.053.656 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.667 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.668 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.594 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.595 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.595 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.595 I llama_new_context_with_model: n_batch       = 2048
0.00.054.595 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.595 I llama_new_context_with_model: flash_attn    = 0
0.00.054.596 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.596 I llama_new_context_with_model: freq_scale    = 1
0.00.054.596 I ggml_metal_init: allocating
0.00.054.600 I ggml_metal_init: found device: Apple M4
0.00.054.602 I ggml_metal_init: picking default device: Apple M4
0.00.055.204 I ggml_metal_init: using embedded metal library
0.00.057.553 I ggml_metal_init: GPU name:   Apple M4
0.00.057.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.555 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.555 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.556 I ggml_metal_init: simdgroup reduction   = true
0.00.057.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.557 I ggml_metal_init: has bfloat            = true
0.00.057.557 I ggml_metal_init: use bfloat            = true
0.00.057.558 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.868 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.875 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.893 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.943 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.944 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.945 I llama_new_context_with_model: graph nodes  = 967
0.00.088.945 I llama_new_context_with_model: graph splits = 2
0.00.088.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.389 I main: llama threadpool init, n_threads = 4
0.00.767.425 I 
0.00.767.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.468 I 
0.00.767.701 I sampler seed: 1234
0.00.767.707 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.719 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.719 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.719 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.648.018 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 60996.56 tokens per second)
0.01.648.018 I llama_perf_context_print:        load time =     758.71 ms
0.01.648.019 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.59 tokens per second)
0.01.648.020 I llama_perf_context_print:        eval time =     823.01 ms /    63 runs   (   13.06 ms per token,    76.55 tokens per second)
0.01.648.020 I llama_perf_context_print:       total time =     880.63 ms /    70 tokens
0.01.648.236 I ggml_metal_free: deallocating

real	0m1.665s
user	0m0.111s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.706 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.725 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.731 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.733 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.733 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.733 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.734 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.748 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.749 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.138 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.232 I llama_model_loader: - type  f32:  194 tensors
0.00.054.233 I llama_model_loader: - type  f16:   98 tensors
0.00.083.621 I llm_load_vocab: special tokens cache size = 25
0.00.090.344 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.347 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.347 I llm_load_print_meta: arch             = gptneox
0.00.090.348 I llm_load_print_meta: vocab type       = BPE
0.00.090.348 I llm_load_print_meta: n_vocab          = 50304
0.00.090.348 I llm_load_print_meta: n_merges         = 50009
0.00.090.348 I llm_load_print_meta: vocab_only       = 0
0.00.090.348 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.348 I llm_load_print_meta: n_embd           = 2048
0.00.090.348 I llm_load_print_meta: n_layer          = 24
0.00.090.351 I llm_load_print_meta: n_head           = 16
0.00.090.352 I llm_load_print_meta: n_head_kv        = 16
0.00.090.354 I llm_load_print_meta: n_rot            = 32
0.00.090.354 I llm_load_print_meta: n_swa            = 0
0.00.090.354 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.354 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.355 I llm_load_print_meta: n_gqa            = 1
0.00.090.355 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.356 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.357 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.357 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.357 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.357 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.357 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.358 I llm_load_print_meta: n_ff             = 8192
0.00.090.358 I llm_load_print_meta: n_expert         = 0
0.00.090.358 I llm_load_print_meta: n_expert_used    = 0
0.00.090.358 I llm_load_print_meta: causal attn      = 1
0.00.090.358 I llm_load_print_meta: pooling type     = 0
0.00.090.358 I llm_load_print_meta: rope type        = 2
0.00.090.359 I llm_load_print_meta: rope scaling     = linear
0.00.090.360 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.360 I llm_load_print_meta: freq_scale_train = 1
0.00.090.360 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.360 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.360 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.361 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.361 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.362 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.362 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.362 I llm_load_print_meta: model type       = 1.4B
0.00.090.363 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.363 I llm_load_print_meta: model params     = 1.41 B
0.00.090.365 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.365 I llm_load_print_meta: general.name     = 1.4B
0.00.090.366 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.366 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.367 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.367 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.367 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.367 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.368 I llm_load_print_meta: max token length = 1024
0.00.092.835 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.836 I llm_load_tensors: offloading output layer to GPU
0.00.092.836 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.846 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.847 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.826 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.827 I llama_new_context_with_model: n_ctx         = 128
0.00.093.827 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.827 I llama_new_context_with_model: n_batch       = 128
0.00.093.828 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.828 I llama_new_context_with_model: flash_attn    = 0
0.00.093.828 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.828 I llama_new_context_with_model: freq_scale    = 1
0.00.093.829 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.829 I ggml_metal_init: allocating
0.00.093.837 I ggml_metal_init: found device: Apple M4
0.00.093.841 I ggml_metal_init: picking default device: Apple M4
0.00.094.454 I ggml_metal_init: using embedded metal library
0.00.097.000 I ggml_metal_init: GPU name:   Apple M4
0.00.097.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.002 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.003 I ggml_metal_init: simdgroup reduction   = true
0.00.097.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.003 I ggml_metal_init: has bfloat            = true
0.00.097.003 I ggml_metal_init: use bfloat            = true
0.00.097.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.004 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.702 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.704 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.717 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.593 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.594 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.594 I llama_new_context_with_model: graph nodes  = 967
0.00.108.595 I llama_new_context_with_model: graph splits = 2
0.00.108.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.254.471 I 
0.01.254.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.254.596 I perplexity: tokenizing the input ..
0.01.267.922 I perplexity: tokenization took 13.322 ms
0.01.267.959 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.402.884 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.404.582 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.404.612 I llama_perf_context_print:        load time =    1230.82 ms
0.01.404.613 I llama_perf_context_print: prompt eval time =     133.98 ms /   128 tokens (    1.05 ms per token,   955.37 tokens per second)
0.01.404.622 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.404.623 I llama_perf_context_print:       total time =     150.15 ms /   129 tokens
0.01.405.156 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.123s
sys	0m0.242s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.312 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.314 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.315 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.318 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.816 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.559 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.564 I llama_model_loader: - type  f32:  194 tensors
0.00.030.565 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.404 I llm_load_vocab: special tokens cache size = 25
0.00.060.604 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.608 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.608 I llm_load_print_meta: arch             = gptneox
0.00.060.608 I llm_load_print_meta: vocab type       = BPE
0.00.060.609 I llm_load_print_meta: n_vocab          = 50304
0.00.060.609 I llm_load_print_meta: n_merges         = 50009
0.00.060.609 I llm_load_print_meta: vocab_only       = 0
0.00.060.609 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.609 I llm_load_print_meta: n_embd           = 2048
0.00.060.609 I llm_load_print_meta: n_layer          = 24
0.00.060.614 I llm_load_print_meta: n_head           = 16
0.00.060.615 I llm_load_print_meta: n_head_kv        = 16
0.00.060.615 I llm_load_print_meta: n_rot            = 32
0.00.060.616 I llm_load_print_meta: n_swa            = 0
0.00.060.616 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.616 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.617 I llm_load_print_meta: n_gqa            = 1
0.00.060.617 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.618 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.619 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.619 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.619 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.620 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.620 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.620 I llm_load_print_meta: n_ff             = 8192
0.00.060.621 I llm_load_print_meta: n_expert         = 0
0.00.060.621 I llm_load_print_meta: n_expert_used    = 0
0.00.060.621 I llm_load_print_meta: causal attn      = 1
0.00.060.621 I llm_load_print_meta: pooling type     = 0
0.00.060.621 I llm_load_print_meta: rope type        = 2
0.00.060.622 I llm_load_print_meta: rope scaling     = linear
0.00.060.622 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.622 I llm_load_print_meta: freq_scale_train = 1
0.00.060.623 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.623 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.623 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.623 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.623 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.626 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.626 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.627 I llm_load_print_meta: model type       = 1.4B
0.00.060.627 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.628 I llm_load_print_meta: model params     = 1.41 B
0.00.060.628 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.628 I llm_load_print_meta: general.name     = 1.4B
0.00.060.629 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.629 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.633 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.633 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.634 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.634 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.634 I llm_load_print_meta: max token length = 1024
0.00.063.026 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.026 I llm_load_tensors: offloading output layer to GPU
0.00.063.026 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.037 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.039 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.047 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.048 I llama_new_context_with_model: n_ctx         = 128
0.00.064.048 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.048 I llama_new_context_with_model: n_batch       = 128
0.00.064.048 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.049 I llama_new_context_with_model: flash_attn    = 0
0.00.064.049 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.049 I llama_new_context_with_model: freq_scale    = 1
0.00.064.050 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.050 I ggml_metal_init: allocating
0.00.064.056 I ggml_metal_init: found device: Apple M4
0.00.064.059 I ggml_metal_init: picking default device: Apple M4
0.00.064.756 I ggml_metal_init: using embedded metal library
0.00.067.294 I ggml_metal_init: GPU name:   Apple M4
0.00.067.296 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.296 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.297 I ggml_metal_init: simdgroup reduction   = true
0.00.067.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.297 I ggml_metal_init: has bfloat            = true
0.00.067.297 I ggml_metal_init: use bfloat            = true
0.00.067.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.225 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.228 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.245 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.305 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.306 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.306 I llama_new_context_with_model: graph nodes  = 967
0.00.080.306 I llama_new_context_with_model: graph splits = 2
0.00.080.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.953.649 I 
0.00.953.677 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.953.686 I perplexity: tokenizing the input ..
0.00.961.473 I perplexity: tokenization took 7.786 ms
0.00.961.483 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.085.275 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.086.432 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.086.444 I llama_perf_context_print:        load time =     942.33 ms
0.01.086.445 I llama_perf_context_print: prompt eval time =     123.57 ms /   128 tokens (    0.97 ms per token,  1035.88 tokens per second)
0.01.086.446 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.086.447 I llama_perf_context_print:       total time =     132.80 ms /   129 tokens
0.01.086.764 I ggml_metal_free: deallocating

real	0m1.103s
user	0m0.089s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.196 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.655 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.661 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.667 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.668 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.669 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.671 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.673 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.674 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.674 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.651 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.484 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.485 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.485 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.486 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.486 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.486 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.487 I llama_model_loader: - type  f32:  194 tensors
0.00.023.487 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.487 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.588 I llm_load_vocab: special tokens cache size = 25
0.00.049.717 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.719 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.719 I llm_load_print_meta: arch             = gptneox
0.00.049.720 I llm_load_print_meta: vocab type       = BPE
0.00.049.720 I llm_load_print_meta: n_vocab          = 50304
0.00.049.720 I llm_load_print_meta: n_merges         = 50009
0.00.049.720 I llm_load_print_meta: vocab_only       = 0
0.00.049.720 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.720 I llm_load_print_meta: n_embd           = 2048
0.00.049.721 I llm_load_print_meta: n_layer          = 24
0.00.049.724 I llm_load_print_meta: n_head           = 16
0.00.049.725 I llm_load_print_meta: n_head_kv        = 16
0.00.049.725 I llm_load_print_meta: n_rot            = 32
0.00.049.725 I llm_load_print_meta: n_swa            = 0
0.00.049.725 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.725 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.729 I llm_load_print_meta: n_gqa            = 1
0.00.049.730 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.730 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.731 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.731 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.731 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.731 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.732 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.732 I llm_load_print_meta: n_ff             = 8192
0.00.049.734 I llm_load_print_meta: n_expert         = 0
0.00.049.734 I llm_load_print_meta: n_expert_used    = 0
0.00.049.734 I llm_load_print_meta: causal attn      = 1
0.00.049.734 I llm_load_print_meta: pooling type     = 0
0.00.049.734 I llm_load_print_meta: rope type        = 2
0.00.049.734 I llm_load_print_meta: rope scaling     = linear
0.00.049.735 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.735 I llm_load_print_meta: freq_scale_train = 1
0.00.049.735 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.735 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.736 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.736 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.736 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.737 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.737 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.738 I llm_load_print_meta: model type       = 1.4B
0.00.049.738 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.738 I llm_load_print_meta: model params     = 1.41 B
0.00.049.739 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.739 I llm_load_print_meta: general.name     = 1.4B
0.00.049.739 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.740 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.740 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.740 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.740 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.741 I llm_load_print_meta: max token length = 1024
0.00.051.619 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.620 I llm_load_tensors: offloading output layer to GPU
0.00.051.620 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.630 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.631 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.506 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.507 I llama_new_context_with_model: n_ctx         = 128
0.00.052.507 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.507 I llama_new_context_with_model: n_batch       = 128
0.00.052.507 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.507 I llama_new_context_with_model: flash_attn    = 0
0.00.052.508 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.508 I llama_new_context_with_model: freq_scale    = 1
0.00.052.508 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.509 I ggml_metal_init: allocating
0.00.052.511 I ggml_metal_init: found device: Apple M4
0.00.052.513 I ggml_metal_init: picking default device: Apple M4
0.00.053.091 I ggml_metal_init: using embedded metal library
0.00.055.425 I ggml_metal_init: GPU name:   Apple M4
0.00.055.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.427 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.428 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.428 I ggml_metal_init: simdgroup reduction   = true
0.00.055.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.428 I ggml_metal_init: has bfloat            = true
0.00.055.428 I ggml_metal_init: use bfloat            = true
0.00.055.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.430 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.277 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.279 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.294 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.190 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.191 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.191 I llama_new_context_with_model: graph nodes  = 967
0.00.067.191 I llama_new_context_with_model: graph splits = 2
0.00.067.204 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.232 I 
0.00.640.266 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.275 I perplexity: tokenizing the input ..
0.00.647.897 I perplexity: tokenization took 7.62 ms
0.00.647.913 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.514 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.770.944 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.770.958 I llama_perf_context_print:        load time =     631.03 ms
0.00.770.960 I llama_perf_context_print: prompt eval time =     121.37 ms /   128 tokens (    0.95 ms per token,  1054.67 tokens per second)
0.00.770.960 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.961 I llama_perf_context_print:       total time =     130.73 ms /   129 tokens
0.00.771.297 I ggml_metal_free: deallocating

real	0m0.787s
user	0m0.077s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.767 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.453 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.459 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.467 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.468 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.468 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.471 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.481 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.427 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.428 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.429 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.430 I llama_model_loader: - type  f32:  194 tensors
0.00.023.430 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.431 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.442 I llm_load_vocab: special tokens cache size = 25
0.00.051.669 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.676 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.677 I llm_load_print_meta: arch             = gptneox
0.00.051.677 I llm_load_print_meta: vocab type       = BPE
0.00.051.677 I llm_load_print_meta: n_vocab          = 50304
0.00.051.677 I llm_load_print_meta: n_merges         = 50009
0.00.051.678 I llm_load_print_meta: vocab_only       = 0
0.00.051.678 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.678 I llm_load_print_meta: n_embd           = 2048
0.00.051.678 I llm_load_print_meta: n_layer          = 24
0.00.051.682 I llm_load_print_meta: n_head           = 16
0.00.051.683 I llm_load_print_meta: n_head_kv        = 16
0.00.051.683 I llm_load_print_meta: n_rot            = 32
0.00.051.683 I llm_load_print_meta: n_swa            = 0
0.00.051.683 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.683 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.684 I llm_load_print_meta: n_gqa            = 1
0.00.051.685 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.685 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.686 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.686 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.686 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.686 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.686 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.687 I llm_load_print_meta: n_ff             = 8192
0.00.051.687 I llm_load_print_meta: n_expert         = 0
0.00.051.687 I llm_load_print_meta: n_expert_used    = 0
0.00.051.687 I llm_load_print_meta: causal attn      = 1
0.00.051.687 I llm_load_print_meta: pooling type     = 0
0.00.051.688 I llm_load_print_meta: rope type        = 2
0.00.051.689 I llm_load_print_meta: rope scaling     = linear
0.00.051.691 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.692 I llm_load_print_meta: freq_scale_train = 1
0.00.051.692 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.692 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.692 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.692 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.692 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.693 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.693 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.693 I llm_load_print_meta: model type       = 1.4B
0.00.051.693 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.694 I llm_load_print_meta: model params     = 1.41 B
0.00.051.694 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.694 I llm_load_print_meta: general.name     = 1.4B
0.00.051.695 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.695 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.696 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.696 I llm_load_print_meta: max token length = 1024
0.00.053.544 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.544 I llm_load_tensors: offloading output layer to GPU
0.00.053.545 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.556 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.557 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.438 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.439 I llama_new_context_with_model: n_ctx         = 128
0.00.054.439 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.439 I llama_new_context_with_model: n_batch       = 128
0.00.054.439 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.439 I llama_new_context_with_model: flash_attn    = 0
0.00.054.440 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.440 I llama_new_context_with_model: freq_scale    = 1
0.00.054.440 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.441 I ggml_metal_init: allocating
0.00.054.445 I ggml_metal_init: found device: Apple M4
0.00.054.447 I ggml_metal_init: picking default device: Apple M4
0.00.055.103 I ggml_metal_init: using embedded metal library
0.00.057.586 I ggml_metal_init: GPU name:   Apple M4
0.00.057.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.589 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.589 I ggml_metal_init: simdgroup reduction   = true
0.00.057.589 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.589 I ggml_metal_init: has bfloat            = true
0.00.057.589 I ggml_metal_init: use bfloat            = true
0.00.057.590 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.346 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.350 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.368 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.192 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.193 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.193 I llama_new_context_with_model: graph nodes  = 967
0.00.069.194 I llama_new_context_with_model: graph splits = 2
0.00.069.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.269 I 
0.00.655.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.307 I perplexity: tokenizing the input ..
0.00.663.352 I perplexity: tokenization took 8.044 ms
0.00.663.362 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.543 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.787.830 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.787.848 I llama_perf_context_print:        load time =     646.50 ms
0.00.787.849 I llama_perf_context_print: prompt eval time =     122.95 ms /   128 tokens (    0.96 ms per token,  1041.05 tokens per second)
0.00.787.850 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.850 I llama_perf_context_print:       total time =     132.58 ms /   129 tokens
0.00.788.268 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.079s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.149 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.841 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.845 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.848 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.849 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.849 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.849 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.850 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.851 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.851 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.851 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.852 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.852 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.853 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.855 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.856 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.856 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.433 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.433 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.433 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.434 I llama_model_loader: - type  f32:  194 tensors
0.00.023.434 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.435 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.598 I llm_load_vocab: special tokens cache size = 25
0.00.049.476 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.478 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.479 I llm_load_print_meta: arch             = gptneox
0.00.049.479 I llm_load_print_meta: vocab type       = BPE
0.00.049.479 I llm_load_print_meta: n_vocab          = 50304
0.00.049.480 I llm_load_print_meta: n_merges         = 50009
0.00.049.480 I llm_load_print_meta: vocab_only       = 0
0.00.049.480 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.480 I llm_load_print_meta: n_embd           = 2048
0.00.049.480 I llm_load_print_meta: n_layer          = 24
0.00.049.483 I llm_load_print_meta: n_head           = 16
0.00.049.484 I llm_load_print_meta: n_head_kv        = 16
0.00.049.485 I llm_load_print_meta: n_rot            = 32
0.00.049.485 I llm_load_print_meta: n_swa            = 0
0.00.049.487 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.487 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.488 I llm_load_print_meta: n_gqa            = 1
0.00.049.488 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.489 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.490 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.490 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.490 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.490 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.490 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.491 I llm_load_print_meta: n_ff             = 8192
0.00.049.491 I llm_load_print_meta: n_expert         = 0
0.00.049.491 I llm_load_print_meta: n_expert_used    = 0
0.00.049.491 I llm_load_print_meta: causal attn      = 1
0.00.049.492 I llm_load_print_meta: pooling type     = 0
0.00.049.492 I llm_load_print_meta: rope type        = 2
0.00.049.492 I llm_load_print_meta: rope scaling     = linear
0.00.049.492 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.497 I llm_load_print_meta: freq_scale_train = 1
0.00.049.497 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.498 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.498 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.498 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.499 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.499 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.499 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.499 I llm_load_print_meta: model type       = 1.4B
0.00.049.500 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.500 I llm_load_print_meta: model params     = 1.41 B
0.00.049.501 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.501 I llm_load_print_meta: general.name     = 1.4B
0.00.049.501 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.501 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.501 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.502 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.502 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.507 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.508 I llm_load_print_meta: max token length = 1024
0.00.051.450 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.450 I llm_load_tensors: offloading output layer to GPU
0.00.051.450 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.461 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.462 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.376 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.377 I llama_new_context_with_model: n_ctx         = 128
0.00.052.377 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.377 I llama_new_context_with_model: n_batch       = 128
0.00.052.378 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.378 I llama_new_context_with_model: flash_attn    = 0
0.00.052.378 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.378 I llama_new_context_with_model: freq_scale    = 1
0.00.052.379 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.379 I ggml_metal_init: allocating
0.00.052.382 I ggml_metal_init: found device: Apple M4
0.00.052.384 I ggml_metal_init: picking default device: Apple M4
0.00.052.940 I ggml_metal_init: using embedded metal library
0.00.055.323 I ggml_metal_init: GPU name:   Apple M4
0.00.055.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.326 I ggml_metal_init: simdgroup reduction   = true
0.00.055.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.326 I ggml_metal_init: has bfloat            = true
0.00.055.326 I ggml_metal_init: use bfloat            = true
0.00.055.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.078 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.085 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.103 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.973 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.974 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.974 I llama_new_context_with_model: graph nodes  = 967
0.00.066.974 I llama_new_context_with_model: graph splits = 2
0.00.066.987 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.276 I 
0.00.709.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.322 I perplexity: tokenizing the input ..
0.00.717.390 I perplexity: tokenization took 8.066 ms
0.00.717.401 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.948 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.853.226 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.853.243 I llama_perf_context_print:        load time =     700.12 ms
0.00.853.244 I llama_perf_context_print: prompt eval time =     134.32 ms /   128 tokens (    1.05 ms per token,   952.94 tokens per second)
0.00.853.244 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.248 I llama_perf_context_print:       total time =     143.97 ms /   129 tokens
0.00.853.649 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.077s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.077 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.682 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.618 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.622 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.625 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.625 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.626 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.626 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.627 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.627 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.627 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.628 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.629 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.629 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.631 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.631 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.349 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.350 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.350 I llama_model_loader: - type  f32:  194 tensors
0.00.023.351 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.351 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.427 I llm_load_vocab: special tokens cache size = 25
0.00.049.668 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.671 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.671 I llm_load_print_meta: arch             = gptneox
0.00.049.671 I llm_load_print_meta: vocab type       = BPE
0.00.049.672 I llm_load_print_meta: n_vocab          = 50304
0.00.049.672 I llm_load_print_meta: n_merges         = 50009
0.00.049.672 I llm_load_print_meta: vocab_only       = 0
0.00.049.672 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.672 I llm_load_print_meta: n_embd           = 2048
0.00.049.672 I llm_load_print_meta: n_layer          = 24
0.00.049.675 I llm_load_print_meta: n_head           = 16
0.00.049.676 I llm_load_print_meta: n_head_kv        = 16
0.00.049.676 I llm_load_print_meta: n_rot            = 32
0.00.049.678 I llm_load_print_meta: n_swa            = 0
0.00.049.678 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.678 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.679 I llm_load_print_meta: n_gqa            = 1
0.00.049.680 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.681 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.681 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.682 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.682 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.682 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.682 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.683 I llm_load_print_meta: n_ff             = 8192
0.00.049.683 I llm_load_print_meta: n_expert         = 0
0.00.049.683 I llm_load_print_meta: n_expert_used    = 0
0.00.049.683 I llm_load_print_meta: causal attn      = 1
0.00.049.684 I llm_load_print_meta: pooling type     = 0
0.00.049.689 I llm_load_print_meta: rope type        = 2
0.00.049.689 I llm_load_print_meta: rope scaling     = linear
0.00.049.690 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.690 I llm_load_print_meta: freq_scale_train = 1
0.00.049.690 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.690 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.690 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.691 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.691 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.691 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.691 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.694 I llm_load_print_meta: model type       = 1.4B
0.00.049.695 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.695 I llm_load_print_meta: model params     = 1.41 B
0.00.049.696 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.696 I llm_load_print_meta: general.name     = 1.4B
0.00.049.696 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.696 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.696 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.697 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.697 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.697 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.697 I llm_load_print_meta: max token length = 1024
0.00.051.499 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.499 I llm_load_tensors: offloading output layer to GPU
0.00.051.499 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.505 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.505 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.414 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.415 I llama_new_context_with_model: n_ctx         = 128
0.00.052.415 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.415 I llama_new_context_with_model: n_batch       = 128
0.00.052.416 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.416 I llama_new_context_with_model: flash_attn    = 0
0.00.052.416 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.416 I llama_new_context_with_model: freq_scale    = 1
0.00.052.417 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.417 I ggml_metal_init: allocating
0.00.052.420 I ggml_metal_init: found device: Apple M4
0.00.052.422 I ggml_metal_init: picking default device: Apple M4
0.00.052.979 I ggml_metal_init: using embedded metal library
0.00.055.317 I ggml_metal_init: GPU name:   Apple M4
0.00.055.318 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.319 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.319 I ggml_metal_init: simdgroup reduction   = true
0.00.055.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.319 I ggml_metal_init: has bfloat            = true
0.00.055.320 I ggml_metal_init: use bfloat            = true
0.00.055.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.989 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.994 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.009 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.855 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.856 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.856 I llama_new_context_with_model: graph nodes  = 967
0.00.066.856 I llama_new_context_with_model: graph splits = 2
0.00.066.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.852 I 
0.00.654.947 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.965 I perplexity: tokenizing the input ..
0.00.662.897 I perplexity: tokenization took 7.931 ms
0.00.662.907 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.653 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.798.810 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.798.828 I llama_perf_context_print:        load time =     646.15 ms
0.00.798.829 I llama_perf_context_print: prompt eval time =     134.52 ms /   128 tokens (    1.05 ms per token,   951.52 tokens per second)
0.00.798.830 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.830 I llama_perf_context_print:       total time =     143.99 ms /   129 tokens
0.00.799.277 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.078s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.972 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.489 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.492 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.493 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.494 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.494 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.495 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.495 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.496 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.499 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.499 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.499 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.453 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.197 I llama_model_loader: - type  f32:  194 tensors
0.00.024.197 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.197 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.045 I llm_load_vocab: special tokens cache size = 25
0.00.051.091 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.093 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.094 I llm_load_print_meta: arch             = gptneox
0.00.051.094 I llm_load_print_meta: vocab type       = BPE
0.00.051.094 I llm_load_print_meta: n_vocab          = 50304
0.00.051.094 I llm_load_print_meta: n_merges         = 50009
0.00.051.095 I llm_load_print_meta: vocab_only       = 0
0.00.051.095 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.095 I llm_load_print_meta: n_embd           = 2048
0.00.051.095 I llm_load_print_meta: n_layer          = 24
0.00.051.098 I llm_load_print_meta: n_head           = 16
0.00.051.101 I llm_load_print_meta: n_head_kv        = 16
0.00.051.101 I llm_load_print_meta: n_rot            = 32
0.00.051.102 I llm_load_print_meta: n_swa            = 0
0.00.051.102 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.102 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.103 I llm_load_print_meta: n_gqa            = 1
0.00.051.103 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.104 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.105 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.105 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.105 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.105 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.105 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.106 I llm_load_print_meta: n_ff             = 8192
0.00.051.107 I llm_load_print_meta: n_expert         = 0
0.00.051.108 I llm_load_print_meta: n_expert_used    = 0
0.00.051.108 I llm_load_print_meta: causal attn      = 1
0.00.051.108 I llm_load_print_meta: pooling type     = 0
0.00.051.108 I llm_load_print_meta: rope type        = 2
0.00.051.108 I llm_load_print_meta: rope scaling     = linear
0.00.051.109 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.109 I llm_load_print_meta: freq_scale_train = 1
0.00.051.109 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.109 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.109 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.110 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.110 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.110 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.111 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.111 I llm_load_print_meta: model type       = 1.4B
0.00.051.111 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.112 I llm_load_print_meta: model params     = 1.41 B
0.00.051.112 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.113 I llm_load_print_meta: general.name     = 1.4B
0.00.051.113 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.113 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.113 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.113 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.114 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.114 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.115 I llm_load_print_meta: max token length = 1024
0.00.052.999 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.999 I llm_load_tensors: offloading output layer to GPU
0.00.053.000 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.010 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.011 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.900 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.901 I llama_new_context_with_model: n_ctx         = 128
0.00.053.901 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.901 I llama_new_context_with_model: n_batch       = 128
0.00.053.901 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.902 I llama_new_context_with_model: flash_attn    = 0
0.00.053.902 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.902 I llama_new_context_with_model: freq_scale    = 1
0.00.053.902 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.903 I ggml_metal_init: allocating
0.00.053.906 I ggml_metal_init: found device: Apple M4
0.00.053.907 I ggml_metal_init: picking default device: Apple M4
0.00.054.470 I ggml_metal_init: using embedded metal library
0.00.056.788 I ggml_metal_init: GPU name:   Apple M4
0.00.056.800 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.800 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.802 I ggml_metal_init: simdgroup reduction   = true
0.00.056.802 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.802 I ggml_metal_init: has bfloat            = true
0.00.056.802 I ggml_metal_init: use bfloat            = true
0.00.056.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.598 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.601 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.618 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.553 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.554 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.554 I llama_new_context_with_model: graph nodes  = 967
0.00.068.554 I llama_new_context_with_model: graph splits = 2
0.00.068.567 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.388.242 I 
0.00.388.275 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.388.283 I perplexity: tokenizing the input ..
0.00.396.474 I perplexity: tokenization took 8.19 ms
0.00.396.490 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.529.033 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.530.231 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.530.240 I llama_perf_context_print:        load time =     378.27 ms
0.00.530.242 I llama_perf_context_print: prompt eval time =     132.31 ms /   128 tokens (    1.03 ms per token,   967.45 tokens per second)
0.00.530.243 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.530.243 I llama_perf_context_print:       total time =     142.00 ms /   129 tokens
0.00.530.679 I ggml_metal_free: deallocating

real	0m0.547s
user	0m0.078s
sys	0m0.072s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.655 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.236 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.238 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.239 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.239 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.240 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.242 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.660 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.661 I llama_model_loader: - type  f32:  194 tensors
0.00.022.661 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.662 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.662 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.662 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.298 I llm_load_vocab: special tokens cache size = 25
0.00.049.358 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.361 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.361 I llm_load_print_meta: arch             = gptneox
0.00.049.362 I llm_load_print_meta: vocab type       = BPE
0.00.049.362 I llm_load_print_meta: n_vocab          = 50304
0.00.049.362 I llm_load_print_meta: n_merges         = 50009
0.00.049.362 I llm_load_print_meta: vocab_only       = 0
0.00.049.362 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.362 I llm_load_print_meta: n_embd           = 2048
0.00.049.363 I llm_load_print_meta: n_layer          = 24
0.00.049.366 I llm_load_print_meta: n_head           = 16
0.00.049.366 I llm_load_print_meta: n_head_kv        = 16
0.00.049.367 I llm_load_print_meta: n_rot            = 32
0.00.049.367 I llm_load_print_meta: n_swa            = 0
0.00.049.367 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.367 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.368 I llm_load_print_meta: n_gqa            = 1
0.00.049.369 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.369 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.370 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.370 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.371 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.371 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.371 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.372 I llm_load_print_meta: n_ff             = 8192
0.00.049.372 I llm_load_print_meta: n_expert         = 0
0.00.049.372 I llm_load_print_meta: n_expert_used    = 0
0.00.049.372 I llm_load_print_meta: causal attn      = 1
0.00.049.372 I llm_load_print_meta: pooling type     = 0
0.00.049.373 I llm_load_print_meta: rope type        = 2
0.00.049.373 I llm_load_print_meta: rope scaling     = linear
0.00.049.373 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.374 I llm_load_print_meta: freq_scale_train = 1
0.00.049.374 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.374 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.374 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.374 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.374 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.375 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.375 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.375 I llm_load_print_meta: model type       = 1.4B
0.00.049.375 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.376 I llm_load_print_meta: model params     = 1.41 B
0.00.049.376 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.377 I llm_load_print_meta: general.name     = 1.4B
0.00.049.377 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.377 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.377 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.377 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.378 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.378 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.378 I llm_load_print_meta: max token length = 1024
0.00.051.299 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.300 I llm_load_tensors: offloading output layer to GPU
0.00.051.300 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.311 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.312 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.192 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.192 I llama_new_context_with_model: n_ctx         = 128
0.00.052.193 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.193 I llama_new_context_with_model: n_batch       = 128
0.00.052.193 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.193 I llama_new_context_with_model: flash_attn    = 0
0.00.052.193 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.194 I llama_new_context_with_model: freq_scale    = 1
0.00.052.194 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.195 I ggml_metal_init: allocating
0.00.052.198 I ggml_metal_init: found device: Apple M4
0.00.052.200 I ggml_metal_init: picking default device: Apple M4
0.00.052.776 I ggml_metal_init: using embedded metal library
0.00.055.066 I ggml_metal_init: GPU name:   Apple M4
0.00.055.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.068 I ggml_metal_init: simdgroup reduction   = true
0.00.055.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.068 I ggml_metal_init: has bfloat            = true
0.00.055.068 I ggml_metal_init: use bfloat            = true
0.00.055.069 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.069 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.940 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.942 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.956 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.862 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.863 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.864 I llama_new_context_with_model: graph nodes  = 967
0.00.066.864 I llama_new_context_with_model: graph splits = 2
0.00.066.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.485.368 I 
0.00.485.406 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.485.413 I perplexity: tokenizing the input ..
0.00.493.273 I perplexity: tokenization took 7.858 ms
0.00.493.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.625.603 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.626.837 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.626.853 I llama_perf_context_print:        load time =     476.71 ms
0.00.626.853 I llama_perf_context_print: prompt eval time =     132.09 ms /   128 tokens (    1.03 ms per token,   969.02 tokens per second)
0.00.626.854 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.626.854 I llama_perf_context_print:       total time =     141.49 ms /   129 tokens
0.00.627.303 I ggml_metal_free: deallocating

real	0m0.640s
user	0m0.077s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.821 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.490 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.491 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.492 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.492 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.493 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.493 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.495 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.495 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.308 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.421 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.218 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.219 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.219 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.220 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.220 I llama_model_loader: - type  f32:  194 tensors
0.00.023.220 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.221 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.221 I llama_model_loader: - type q6_K:   13 tensors
0.00.042.974 I llm_load_vocab: special tokens cache size = 25
0.00.048.880 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.883 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.883 I llm_load_print_meta: arch             = gptneox
0.00.048.883 I llm_load_print_meta: vocab type       = BPE
0.00.048.884 I llm_load_print_meta: n_vocab          = 50304
0.00.048.884 I llm_load_print_meta: n_merges         = 50009
0.00.048.884 I llm_load_print_meta: vocab_only       = 0
0.00.048.884 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.884 I llm_load_print_meta: n_embd           = 2048
0.00.048.885 I llm_load_print_meta: n_layer          = 24
0.00.048.887 I llm_load_print_meta: n_head           = 16
0.00.048.888 I llm_load_print_meta: n_head_kv        = 16
0.00.048.888 I llm_load_print_meta: n_rot            = 32
0.00.048.888 I llm_load_print_meta: n_swa            = 0
0.00.048.888 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.889 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.889 I llm_load_print_meta: n_gqa            = 1
0.00.048.891 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.892 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.892 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.893 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.893 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.893 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.893 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.894 I llm_load_print_meta: n_ff             = 8192
0.00.048.894 I llm_load_print_meta: n_expert         = 0
0.00.048.894 I llm_load_print_meta: n_expert_used    = 0
0.00.048.894 I llm_load_print_meta: causal attn      = 1
0.00.048.894 I llm_load_print_meta: pooling type     = 0
0.00.048.895 I llm_load_print_meta: rope type        = 2
0.00.048.895 I llm_load_print_meta: rope scaling     = linear
0.00.048.897 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.897 I llm_load_print_meta: freq_scale_train = 1
0.00.048.897 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.898 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.898 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.898 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.898 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.898 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.898 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.899 I llm_load_print_meta: model type       = 1.4B
0.00.048.899 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.900 I llm_load_print_meta: model params     = 1.41 B
0.00.048.904 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.904 I llm_load_print_meta: general.name     = 1.4B
0.00.048.905 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.906 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.906 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.907 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.907 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.907 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.907 I llm_load_print_meta: max token length = 1024
0.00.050.824 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.824 I llm_load_tensors: offloading output layer to GPU
0.00.050.824 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.835 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.836 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.796 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.797 I llama_new_context_with_model: n_ctx         = 128
0.00.051.797 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.797 I llama_new_context_with_model: n_batch       = 128
0.00.051.797 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.798 I llama_new_context_with_model: flash_attn    = 0
0.00.051.798 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.798 I llama_new_context_with_model: freq_scale    = 1
0.00.051.798 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.799 I ggml_metal_init: allocating
0.00.051.802 I ggml_metal_init: found device: Apple M4
0.00.051.804 I ggml_metal_init: picking default device: Apple M4
0.00.052.358 I ggml_metal_init: using embedded metal library
0.00.054.707 I ggml_metal_init: GPU name:   Apple M4
0.00.054.708 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.709 I ggml_metal_init: simdgroup reduction   = true
0.00.054.709 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.709 I ggml_metal_init: has bfloat            = true
0.00.054.709 I ggml_metal_init: use bfloat            = true
0.00.054.710 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.396 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.398 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.413 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.342 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.343 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.344 I llama_new_context_with_model: graph nodes  = 967
0.00.066.344 I llama_new_context_with_model: graph splits = 2
0.00.066.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.559.495 I 
0.00.559.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.559.561 I perplexity: tokenizing the input ..
0.00.567.662 I perplexity: tokenization took 8.1 ms
0.00.567.672 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.701.734 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.702.916 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.702.931 I llama_perf_context_print:        load time =     550.67 ms
0.00.702.933 I llama_perf_context_print: prompt eval time =     133.84 ms /   128 tokens (    1.05 ms per token,   956.40 tokens per second)
0.00.702.934 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.702.934 I llama_perf_context_print:       total time =     143.44 ms /   129 tokens
0.00.703.481 I ggml_metal_free: deallocating

real	0m0.716s
user	0m0.077s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.242 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.839 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.845 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.846 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.848 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.849 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.851 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.613 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.614 I llama_model_loader: - type  f32:  194 tensors
0.00.023.614 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.615 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.457 I llm_load_vocab: special tokens cache size = 25
0.00.049.364 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.366 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.366 I llm_load_print_meta: arch             = gptneox
0.00.049.367 I llm_load_print_meta: vocab type       = BPE
0.00.049.367 I llm_load_print_meta: n_vocab          = 50304
0.00.049.367 I llm_load_print_meta: n_merges         = 50009
0.00.049.367 I llm_load_print_meta: vocab_only       = 0
0.00.049.368 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.368 I llm_load_print_meta: n_embd           = 2048
0.00.049.368 I llm_load_print_meta: n_layer          = 24
0.00.049.370 I llm_load_print_meta: n_head           = 16
0.00.049.371 I llm_load_print_meta: n_head_kv        = 16
0.00.049.371 I llm_load_print_meta: n_rot            = 32
0.00.049.371 I llm_load_print_meta: n_swa            = 0
0.00.049.374 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.374 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.375 I llm_load_print_meta: n_gqa            = 1
0.00.049.375 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.376 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.377 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.377 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.377 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.377 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.377 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.378 I llm_load_print_meta: n_ff             = 8192
0.00.049.378 I llm_load_print_meta: n_expert         = 0
0.00.049.378 I llm_load_print_meta: n_expert_used    = 0
0.00.049.378 I llm_load_print_meta: causal attn      = 1
0.00.049.378 I llm_load_print_meta: pooling type     = 0
0.00.049.379 I llm_load_print_meta: rope type        = 2
0.00.049.379 I llm_load_print_meta: rope scaling     = linear
0.00.049.379 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.383 I llm_load_print_meta: freq_scale_train = 1
0.00.049.383 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.384 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.384 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.384 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.384 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.384 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.385 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.385 I llm_load_print_meta: model type       = 1.4B
0.00.049.385 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.386 I llm_load_print_meta: model params     = 1.41 B
0.00.049.387 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.387 I llm_load_print_meta: general.name     = 1.4B
0.00.049.387 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.387 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.387 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.388 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.388 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.388 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.388 I llm_load_print_meta: max token length = 1024
0.00.051.340 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.340 I llm_load_tensors: offloading output layer to GPU
0.00.051.340 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.351 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.352 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.228 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.229 I llama_new_context_with_model: n_ctx         = 128
0.00.052.229 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.229 I llama_new_context_with_model: n_batch       = 128
0.00.052.229 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.230 I llama_new_context_with_model: flash_attn    = 0
0.00.052.230 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.230 I llama_new_context_with_model: freq_scale    = 1
0.00.052.231 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.231 I ggml_metal_init: allocating
0.00.052.234 I ggml_metal_init: found device: Apple M4
0.00.052.236 I ggml_metal_init: picking default device: Apple M4
0.00.052.794 I ggml_metal_init: using embedded metal library
0.00.055.069 I ggml_metal_init: GPU name:   Apple M4
0.00.055.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.071 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.071 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.072 I ggml_metal_init: simdgroup reduction   = true
0.00.055.072 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.072 I ggml_metal_init: has bfloat            = true
0.00.055.072 I ggml_metal_init: use bfloat            = true
0.00.055.073 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.073 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.809 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.811 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.824 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.741 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.742 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.742 I llama_new_context_with_model: graph nodes  = 967
0.00.066.743 I llama_new_context_with_model: graph splits = 2
0.00.066.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.931 I 
0.00.655.965 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.973 I perplexity: tokenizing the input ..
0.00.664.251 I perplexity: tokenization took 8.277 ms
0.00.664.261 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.624 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.805.800 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.805.816 I llama_perf_context_print:        load time =     646.68 ms
0.00.805.817 I llama_perf_context_print: prompt eval time =     140.14 ms /   128 tokens (    1.09 ms per token,   913.40 tokens per second)
0.00.805.818 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.819 I llama_perf_context_print:       total time =     149.89 ms /   129 tokens
0.00.806.282 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.078s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.077 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.782 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.357 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.362 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.364 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.364 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.364 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.365 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.366 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.366 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.366 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.367 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.367 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.015 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.015 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.016 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.016 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.016 I llama_model_loader: - type  f32:  194 tensors
0.00.023.017 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.892 I llm_load_vocab: special tokens cache size = 25
0.00.048.823 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.826 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.826 I llm_load_print_meta: arch             = gptneox
0.00.048.827 I llm_load_print_meta: vocab type       = BPE
0.00.048.827 I llm_load_print_meta: n_vocab          = 50304
0.00.048.827 I llm_load_print_meta: n_merges         = 50009
0.00.048.827 I llm_load_print_meta: vocab_only       = 0
0.00.048.827 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.827 I llm_load_print_meta: n_embd           = 2048
0.00.048.828 I llm_load_print_meta: n_layer          = 24
0.00.048.830 I llm_load_print_meta: n_head           = 16
0.00.048.831 I llm_load_print_meta: n_head_kv        = 16
0.00.048.831 I llm_load_print_meta: n_rot            = 32
0.00.048.832 I llm_load_print_meta: n_swa            = 0
0.00.048.832 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.832 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.833 I llm_load_print_meta: n_gqa            = 1
0.00.048.833 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.834 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.834 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.835 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.835 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.835 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.835 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.836 I llm_load_print_meta: n_ff             = 8192
0.00.048.836 I llm_load_print_meta: n_expert         = 0
0.00.048.836 I llm_load_print_meta: n_expert_used    = 0
0.00.048.836 I llm_load_print_meta: causal attn      = 1
0.00.048.836 I llm_load_print_meta: pooling type     = 0
0.00.048.837 I llm_load_print_meta: rope type        = 2
0.00.048.837 I llm_load_print_meta: rope scaling     = linear
0.00.048.837 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.837 I llm_load_print_meta: freq_scale_train = 1
0.00.048.838 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.838 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.838 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.838 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.838 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.838 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.838 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.839 I llm_load_print_meta: model type       = 1.4B
0.00.048.839 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.840 I llm_load_print_meta: model params     = 1.41 B
0.00.048.840 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.840 I llm_load_print_meta: general.name     = 1.4B
0.00.048.840 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.841 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.841 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.841 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.841 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.842 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.842 I llm_load_print_meta: max token length = 1024
0.00.050.824 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.824 I llm_load_tensors: offloading output layer to GPU
0.00.050.825 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.835 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.836 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.715 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.716 I llama_new_context_with_model: n_ctx         = 128
0.00.051.716 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.716 I llama_new_context_with_model: n_batch       = 128
0.00.051.717 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.717 I llama_new_context_with_model: flash_attn    = 0
0.00.051.717 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.717 I llama_new_context_with_model: freq_scale    = 1
0.00.051.718 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.718 I ggml_metal_init: allocating
0.00.051.724 I ggml_metal_init: found device: Apple M4
0.00.051.726 I ggml_metal_init: picking default device: Apple M4
0.00.052.281 I ggml_metal_init: using embedded metal library
0.00.054.583 I ggml_metal_init: GPU name:   Apple M4
0.00.054.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.585 I ggml_metal_init: simdgroup reduction   = true
0.00.054.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.585 I ggml_metal_init: has bfloat            = true
0.00.054.585 I ggml_metal_init: use bfloat            = true
0.00.054.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.587 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.225 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.229 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.241 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.081 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.082 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.082 I llama_new_context_with_model: graph nodes  = 967
0.00.066.082 I llama_new_context_with_model: graph splits = 2
0.00.066.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.730 I 
0.00.487.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.770 I perplexity: tokenizing the input ..
0.00.495.911 I perplexity: tokenization took 8.139 ms
0.00.495.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.636.529 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.637.782 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.637.795 I llama_perf_context_print:        load time =     478.94 ms
0.00.637.796 I llama_perf_context_print: prompt eval time =     140.38 ms /   128 tokens (    1.10 ms per token,   911.81 tokens per second)
0.00.637.797 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.797 I llama_perf_context_print:       total time =     150.07 ms /   129 tokens
0.00.638.245 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.076s
sys	0m0.097s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.177 I build: 4329 (89d604f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.741 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.875 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.888 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.896 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.897 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.902 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.840 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.960 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.798 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.799 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.799 I llama_model_loader: - type  f32:  194 tensors
0.00.042.800 I llama_model_loader: - type  f16:   98 tensors
0.00.065.899 I llm_load_vocab: special tokens cache size = 25
0.00.071.926 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.929 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.929 I llm_load_print_meta: arch             = gptneox
0.00.071.930 I llm_load_print_meta: vocab type       = BPE
0.00.071.930 I llm_load_print_meta: n_vocab          = 50304
0.00.071.930 I llm_load_print_meta: n_merges         = 50009
0.00.071.930 I llm_load_print_meta: vocab_only       = 0
0.00.071.930 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.930 I llm_load_print_meta: n_embd           = 2048
0.00.071.931 I llm_load_print_meta: n_layer          = 24
0.00.071.935 I llm_load_print_meta: n_head           = 16
0.00.071.935 I llm_load_print_meta: n_head_kv        = 16
0.00.071.936 I llm_load_print_meta: n_rot            = 32
0.00.071.936 I llm_load_print_meta: n_swa            = 0
0.00.071.936 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.937 I llm_load_print_meta: n_gqa            = 1
0.00.071.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.938 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.938 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.939 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.939 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.939 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.939 I llm_load_print_meta: n_ff             = 8192
0.00.071.940 I llm_load_print_meta: n_expert         = 0
0.00.071.940 I llm_load_print_meta: n_expert_used    = 0
0.00.071.940 I llm_load_print_meta: causal attn      = 1
0.00.071.940 I llm_load_print_meta: pooling type     = 0
0.00.071.940 I llm_load_print_meta: rope type        = 2
0.00.071.940 I llm_load_print_meta: rope scaling     = linear
0.00.071.940 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.941 I llm_load_print_meta: freq_scale_train = 1
0.00.071.941 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.941 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.941 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.941 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.941 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.941 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.941 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.942 I llm_load_print_meta: model type       = 1.4B
0.00.071.942 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.071.943 I llm_load_print_meta: model params     = 1.41 B
0.00.071.943 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.071.943 I llm_load_print_meta: general.name     = 1.4B
0.00.071.944 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.944 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.944 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.944 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.944 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.071.944 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.945 I llm_load_print_meta: max token length = 1024
0.00.074.486 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.487 I llm_load_tensors: offloading output layer to GPU
0.00.074.487 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.498 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.074.500 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.075.422 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.423 I llama_new_context_with_model: n_ctx         = 128
0.00.075.423 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.075.423 I llama_new_context_with_model: n_batch       = 128
0.00.075.424 I llama_new_context_with_model: n_ubatch      = 128
0.00.075.424 I llama_new_context_with_model: flash_attn    = 0
0.00.075.424 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.425 I llama_new_context_with_model: freq_scale    = 1
0.00.075.425 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.075.426 I ggml_metal_init: allocating
0.00.075.430 I ggml_metal_init: found device: Apple M4
0.00.075.432 I ggml_metal_init: picking default device: Apple M4
0.00.076.100 I ggml_metal_init: using embedded metal library
0.00.078.804 I ggml_metal_init: GPU name:   Apple M4
0.00.078.805 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.806 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.806 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.806 I ggml_metal_init: simdgroup reduction   = true
0.00.078.807 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.807 I ggml_metal_init: has bfloat            = true
0.00.078.807 I ggml_metal_init: use bfloat            = true
0.00.078.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.808 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.320 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.089.325 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.089.340 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.218 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.090.219 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.090.220 I llama_new_context_with_model: graph nodes  = 967
0.00.090.220 I llama_new_context_with_model: graph splits = 2
0.00.090.234 I 
0.00.090.268 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.090.270 I compute_imatrix: tokenizing the input ..
0.00.098.112 I compute_imatrix: tokenization took 7.841 ms
0.00.098.115 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.627.302 I compute_imatrix: 1.53 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.629.733 I llama_perf_context_print:        load time =    1609.56 ms
0.01.629.734 I llama_perf_context_print: prompt eval time =    1528.48 ms /   128 tokens (   11.94 ms per token,    83.74 tokens per second)
0.01.629.735 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.629.736 I llama_perf_context_print:       total time =    1611.99 ms /   129 tokens
0.01.630.284 I ggml_metal_free: deallocating

real	0m1.814s
user	0m0.139s
sys	0m0.275s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4329 (89d604f2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e60a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e60a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e60aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e60b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e60ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e60bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e60c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e60cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e60d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e60d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e60daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e60dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e60eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e60f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e60fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e6101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e611030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e611750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e611f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e612640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e612d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e613480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e615ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e616180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e6168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e617170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e6176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e617e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e6182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e618750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e619090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e6199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e619e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e61a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e61a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e61abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e61b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e61bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e61c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e61c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e61cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e61d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e61d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e61df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e61e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e61ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e61f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e61f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e61f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e620160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e6208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e620d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e6216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e621fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e622920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e623260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e623ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e6240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e6250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e625b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e6260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e626620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e6270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e627610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e6280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e6290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e6295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e629b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e62a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e62a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e62ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e62b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e62b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e62bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e61b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e62bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e62c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e62cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e62d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e62d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e62dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e62e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e62e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e62ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e62f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e62f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e62fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e6301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e630700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e6310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e6335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e6343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e6351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e6368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e6376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e63a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e63a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e63a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e63ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e63b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e63b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e63bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e63c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e63c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e63c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e63ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e63d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e63d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e63dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e63e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e63e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e63ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e63eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e63f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e63f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e63fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e640f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e6413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e641d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e6421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e642fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e6438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e6446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e644b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e645010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e6454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e645950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e645df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e646bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e647070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e647510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e6479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e647e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e6483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e6488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e648e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e64a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e64a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e64b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e64b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e64b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e64bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e64c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e64cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e64d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e64d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e64d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e64e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e64e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e64ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e64f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e64f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e64fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e650150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e6506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e650bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e651690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e651be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e652130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e652680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e652bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e653120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e653670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e653bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e654110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e654660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e654bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e655100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e655650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e655ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e6560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e656b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e6570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e657630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e6580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e658620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e658b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e6590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e659610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e65a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e65a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e65ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e65b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e65b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e65bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e65c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e65c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e65cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e65d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e65d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e65db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e65e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e65e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e65eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e65f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e65f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e65fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e660050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e6605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e6618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e661d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e662210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e6626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e662b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e662ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e663490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e663930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e663dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e664270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e664710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e664bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e6655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e665cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e6663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e666b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e667220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e6674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e6685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.152.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e625360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e6257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e6260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e626520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e626990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e626e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e627270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e6276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e627b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e6285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e628e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e629610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e629df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e62a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e62abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e62b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e62b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e62c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e62ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e62d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e62d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e62def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e62e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e62ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e62eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e62f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e62f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e62fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e630080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e6304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e630960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e630c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e631090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e631500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e631970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e631de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e632250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e6326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e632b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e632fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e633410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e633880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e633cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e634160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e6345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e634eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e635320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e635790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e635c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e636070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e6364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e636950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e636dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e6376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e637b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e637f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e6383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e638860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e638cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e639140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e6395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e639a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e639e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e63a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e63a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e63abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e63b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e63b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e63b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e63bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e63c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e63c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e63caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e63cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e63d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e63d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e63dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e63e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e63e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e63ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e63ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e63f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e63f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e63fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e640030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e6404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e640910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e640d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e6411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e641660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e641ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e641f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e6423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e642820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e642c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e643100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e6439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e643e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e6442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e644730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e644ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e645010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e645480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e6458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e645d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e6461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e646640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e646ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e646f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e647390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e647800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e647c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e6480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e648550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e6489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e648e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e6492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e649710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e649b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e649ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e64a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e64a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e64ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e64b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e64b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e64ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e64bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e64c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e64c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e64cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e64d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e64d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e64d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e64de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e64e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e64e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e64eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e64efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e64f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e64f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e64fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e650190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e650600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e650a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e650ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e651350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e6517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e651c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e6520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e652510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e652980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e652df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e653260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e6536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e653b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e653fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e654420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e654890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e654d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e655170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e6555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e655a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e656330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e6567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e656c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e657080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e6574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e657960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e657dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e658240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e6586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e658b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e658f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e659400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e659870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e659ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e65a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e65a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e65aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e65aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e65b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e65b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e65bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e65c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e65c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e65c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e65cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e65d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e65d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e65db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e65df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e65e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e65e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e65ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e65f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e65f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e65fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e65fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e6602f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e660760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e660bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e661040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e6614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e661920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e6620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e662510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e662980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e662df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e663260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e6636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e663b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e663fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e664420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e664890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e664d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e6655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e665a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e666330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e6667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e666c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e667080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e6674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e667960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e667dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e668240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e6686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e60b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e60aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e60a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e6176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e617960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e617dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e618240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e6186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e618b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e618f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e619400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e619870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e619ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e61a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e61a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e61aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e61aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e61b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e61b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e61bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e61c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e61c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e61c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e61cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e61d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e61d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e61db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e61df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e61e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e61e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e61ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e61f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e61f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e61fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e61fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e6202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e620760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e620bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e621040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e6214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e621920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e621d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e622200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e622670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e622ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e622f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e6233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e623830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e623ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e616130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e616820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e616f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e60d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e60da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e60de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e60e300 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e615eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e616320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e616790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e616c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e617070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e60a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e617850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e617cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e618130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e6185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e618a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e618ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e6198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e61a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e61a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e61af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e61b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e61bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e61c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e61cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e61d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e61db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e61e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e61e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e61f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e61f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e61f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e61fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e6201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e620660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e620ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e620f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e6213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e621670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e621ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e621f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e6223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e622830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e622ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e623110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e623580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e6239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e623e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e6242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e60ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e60b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e624f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e6251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e625aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e625f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e6267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e626c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e6270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e627540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e6279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e627e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e628290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e628700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e628b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e628fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e629450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e6298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e629d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e62a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e62a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e62aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e62aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e62b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e62b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e62bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e62c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e62c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e62c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e62ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e62d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e62d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e62db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e62dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e62e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e62e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e62ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e62f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e62f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e62fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e62fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e630340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e6307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e630c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e631090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e631500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e631970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e631de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e632250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e6326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e632b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e632fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e633410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e633880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e633cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e634160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e6345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e634a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e634eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e635320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e635790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e635c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e636070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e6364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e636950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e636dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e6376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e637b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e637f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e6383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e638860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e638cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e639140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e6395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e639a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e639e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e63a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e63a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e63abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e63b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e63b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e63b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e63bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e63c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e63c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e63caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e63cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e63d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e63d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e63dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e63e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e63e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e63ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e63ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e63f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e63f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e63fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e640030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e6404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e640910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e640d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e6411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e641660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e641ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e641f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e6423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e642820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e642c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e643100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e6439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e643e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e6442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e644730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e644ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e645010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e645480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e6458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e645d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e6461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e646640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e646ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e646f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e647390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e647800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e647c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e6480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e648550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e6489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e648e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e6492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e649710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e649b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e649ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e64a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e64a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e64ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e64b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e64b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e64ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e64bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e64c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e64c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e64cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e64d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e64d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e64d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e64de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e64e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e64e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e64eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e64efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e64f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e64f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e64fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e650190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e650600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e650a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e650ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e651350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e6517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e651c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e6520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e652820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e652c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e653100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e653570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e6539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e653e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e6542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e654730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e654ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e655010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e655480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e6558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e655d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e6561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e656ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e656f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e657800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e657c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e6580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e658550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e6589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e658e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e6592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e659710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e659b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e659ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e65a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e65a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e65ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e65b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e65b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e65ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e65bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e65c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e65c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e65cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e65d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e65d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e65d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e65de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e65e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e65e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e65eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e65efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e65f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e65f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e65fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e660190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e660600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e660a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e660ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e661350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e6617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e661c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e6620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e662510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e662980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e662df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e663260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e6636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e663b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e663fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e664420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e664890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e664d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e6655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e665a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e666330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e6667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e667000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e6676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e667de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e6684d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e60d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e60da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e60de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e60e300 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.850s
user	0m0.312s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4329 (89d604f2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146f102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146f109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146f10f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146f11530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146f11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146f12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146f12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146f12bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146f131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146f136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146f13ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146f140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146f14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146f15370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146f15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146f162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146f169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146f170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146f17800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146f17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146f186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146f18e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146f19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146f19dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146f1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146f1a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146f1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146f1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146f1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146f1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146f1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146f1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146f1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146f1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146f1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146f1dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146f1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146f1e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146f1eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146f1f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146f1f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146f1fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146f1ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146f203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146f20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146f20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146f212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146f21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146f227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146f22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146f23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146f23a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146f24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146f24810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146f24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146f25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146f25410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146f25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146f26210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146f264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146f26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146f26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146f272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146f27750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146f27bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146f28090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146f28530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146f289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146f28e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146f29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146f297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146f29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146f2a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146f2ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146f2b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146f2b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146f2bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146f2c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146f2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146f2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146f2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146f2d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146f2dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146f2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146f2e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146f2ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146f2f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146f2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146f2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146f30140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146f30690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146f30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146f31130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146f31680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146f31bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146f218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146f32040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146f327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146f32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146f33290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146f337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146f33d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146f34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146f347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146f34d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146f35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146f357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146f35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146f36260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146f367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146f36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146f371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146f37640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146f37ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146f37f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146f38420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146f388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146f38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146f39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146f396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146f39b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146f39fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146f3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146f3a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146f3adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146f3b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146f3b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146f3bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146f3c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146f3c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146f3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146f3ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146f3d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146f3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146f3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146f3e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146f3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146f3e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146f3ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146f3f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146f3f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146f3fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146f40100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146f405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146f40a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146f40ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146f41380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146f41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146f41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146f42160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146f42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146f42aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146f42f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146f433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146f43880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146f43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146f441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146f44660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146f44b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146f44fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146f45440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146f458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146f45d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146f46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146f466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146f46b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146f47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146f474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146f47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146f47de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146f48280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146f48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146f48bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146f49060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146f49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146f499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146f49e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146f4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146f4a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146f4ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146f4b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146f4b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146f4ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146f4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146f4c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146f4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146f4cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146f4d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146f4d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146f4da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146f4df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146f4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146f4e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146f4eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146f4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146f4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146f4fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146f50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146f50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146f51120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146f515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146f51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146f51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146f524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146f52c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146f53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146f535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146f53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146f54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146f54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146f54cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146f55210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146f55760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146f55cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146f56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146f56750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146f56ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146f571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146f57740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146f57c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146f581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146f58730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146f58c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146f591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146f59720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146f59c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146f5a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146f5a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146f5ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146f5b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146f5b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146f5bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146f5c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146f5c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146f5cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146f5d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146f5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146f5dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146f5e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146f5e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146f5ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146f5f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146f5f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146f5fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146f60160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146f606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146f60c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146f61150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146f616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146f61bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146f62140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146f62690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146f62be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146f63130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146f63680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146f63bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146f64120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146f64670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146f64bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146f65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146f65660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146f65bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146f66100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146f66650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146f66ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146f67040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146f674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146f67980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146f67e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146f682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146f68760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146f68c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146f690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146f69540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146f699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146f69e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146f6a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146f6a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146f6ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146f6b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146f6b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146f6bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146f6c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146f6cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146f6d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146f6d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146f6dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146f6e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146f6e650 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146f2b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146f2b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146f2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146f2c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146f2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146f2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146f2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146f2d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146f2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146f2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146f2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146f2e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146f2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146f2f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146f2fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146f30590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146f30c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146f31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146f31a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146f323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146f32ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146f331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146f338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146f33fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146f34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146f34b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146f34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146f353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146f35850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146f35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146f36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146f365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146f36a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146f36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146f37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146f375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146f37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146f37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146f38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146f38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146f39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146f394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146f39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146f39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146f3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146f3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146f3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146f3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146f3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146f3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146f3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146f3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146f3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146f3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146f3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146f3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146f3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146f3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146f3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146f3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146f3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146f3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146f3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146f3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146f3fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146f3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146f403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146f40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146f40c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146f41100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146f41570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146f419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146f41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146f422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146f42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146f42ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146f43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146f43480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146f438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146f43d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146f441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146f44640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146f44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146f44f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146f45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146f45800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146f45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146f460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146f46550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146f469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146f46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146f472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146f47710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146f47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146f47ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146f48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146f488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146f48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146f491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146f49620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146f49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146f49f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146f4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146f4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146f4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146f4b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146f4b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146f4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146f4be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146f4c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146f4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146f4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146f4cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146f4d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146f4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146f4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146f4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146f4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146f4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146f4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146f4f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146f4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146f500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146f50510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146f50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146f50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146f51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146f516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146f51b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146f51fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146f52420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146f52890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146f52d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146f53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146f535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146f53a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146f53ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146f54330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146f547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146f54c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146f55080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146f554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146f55960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146f55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146f56240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146f566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146f56b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146f56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146f57400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146f57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146f57ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146f58150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146f585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146f58a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146f58ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146f59310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146f59780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146f59bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146f5a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146f5a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146f5a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146f5adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146f5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146f5b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146f5bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146f5bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146f5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146f5c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146f5ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146f5d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146f5d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146f5da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146f5de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146f5e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146f5e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146f5ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146f5f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146f5f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146f5f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146f5fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146f60200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146f60670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146f60ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146f60f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146f613c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146f61830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146f61ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146f62110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146f62580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146f629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146f62e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146f632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146f63740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146f63bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146f64020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146f64490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146f64900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146f64d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146f651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146f65650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146f65ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146f65f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146f663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146f66810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146f66c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146f670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146f67560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146f679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146f68150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146f685c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146f68a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146f68ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146f69310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146f69780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146f69bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146f6a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146f6a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146f6a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146f6adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146f6b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146f6b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146f6bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146f6bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146f6c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146f6c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146f6ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146f6d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146f6d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146f6da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146f6de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146f6e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146f6e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146f11520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146f10f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146f102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146f1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146f1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146f1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146f1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146f1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146f1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146f1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146f20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146f20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146f20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146f21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146f22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146f22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146f229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146f22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146f232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146f23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146f23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146f24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146f24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146f24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146f24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146f251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146f25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146f25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146f25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146f263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146f26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146f26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146f270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146f27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146f279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146f27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146f282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146f28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146f28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146f29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146f29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146f298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146f29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146f2a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146f1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146f1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146f1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146f13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146f13ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146f13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146f143b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146e053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146e05820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146e05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146e06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146e06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146e069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146e06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146e072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146e07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146e07cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146e08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146e087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146e092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146e09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146e0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146e0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146e0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146e0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146e0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146e0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146e0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146e0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146e0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146e0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146e0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146e0ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146e0eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146e0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146e0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146e0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146e101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146e106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146e10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146e10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146e11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146e116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146e11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146e11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146e12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146e128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146e12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146e13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146e13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146e13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146e13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146e14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146e147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146e14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146e150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146e15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146e15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146e15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146e16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146e166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146e16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146e16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146e17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146e17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146e17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146e18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146e18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146e18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146e19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146e194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146e19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146e19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146e1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146e1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146e1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146e1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146e1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146e1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146e1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146e1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146e1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146e1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146e1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146e1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146e1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146e1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146e1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146e1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146e1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146e1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146e1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146e1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146e1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146e1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146e203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146e20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146e20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146e21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146e21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146e219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146e21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146e222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146e22730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146e22ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146e23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146e23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146e238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146e23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146e241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146e24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146e24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146e24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146e25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146e25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146e25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146e260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146e26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146e269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146e26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146e272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146e27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146e27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146e27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146e28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146e288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146e28d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146e291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146e29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146e29a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146e29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146e2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146e2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146e2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146e2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146e2b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146e2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146e2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146e2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146e2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146e2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146e2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146e2d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146e2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146e2dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146e2e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146e2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146e2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146e2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146e2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146e2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146e2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146e300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146e30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146e30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146e30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146e31260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146e316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146e31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146e31fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146e32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146e32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146e32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146e33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146e335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146e33a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146e33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146e34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146e347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146e34c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146e35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146e354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146e35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146e35dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146e36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146e366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146e36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146e36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146e37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146e37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146e37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146e38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146e385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146e38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146e38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146e39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146e39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146e39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146e3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146e3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146e3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146e3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146e3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146e3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146e3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146e3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146e3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146e3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146e3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146e3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146e3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146e3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146e3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146e3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146e3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146e3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146e3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146e3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146e3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146e3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146e40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146e40670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146e40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146e40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146e414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146e42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146e42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146e42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146e43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146e43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146e43be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146e44050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146e444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146e44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146e44da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146e45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146e45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146e45af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146e45f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146e463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146e46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146e46cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146e47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146e47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146e47a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146e47e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146e482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146e48750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146e48bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146e49030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146e494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146e49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146e49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146e4a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146e4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146e4aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146e4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146e4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146e4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146e4c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146e4c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146e4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146e4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146e4d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146e4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146e4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146e4ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146e4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146e4e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146e4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146e4ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146e4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146e4f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146e4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146e50140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146e505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146e50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146e50e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146e51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146e51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146e51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146e52050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146e524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146e52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146e52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146e53310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146e53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146e53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146e54060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146e544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146e54940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146e54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146e55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146e55690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146e55b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146e55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146e563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146e56850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146e573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146e57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146e581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146e58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146e58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146e58e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146e592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146e59760 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.971s
user	0m0.245s
sys	0m0.141s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
