### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.21 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.61 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.62 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.39 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.30 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.89 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.30 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.30 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.88 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  177.63 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.98 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.36 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 220.18 sec*proc (28 tests)

Total Test time (real) = 220.19 sec

real	3m40.219s
user	7m31.742s
sys	0m6.343s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.22 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.39 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.36 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.13 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.57 sec*proc (28 tests)

Total Test time (real) =  51.58 sec

real	0m51.596s
user	1m11.663s
sys	0m5.694s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.100 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.795 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.448 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.459 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.460 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.461 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.462 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.465 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.465 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.466 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.467 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.467 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.471 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.471 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.472 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.473 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.473 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.474 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.474 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.909 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.910 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.911 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.911 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.912 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.030.912 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.913 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.030.914 I llama_model_loader: - type  f32:  124 tensors
0.00.030.914 I llama_model_loader: - type  f16:   73 tensors
0.00.030.915 I print_info: file format = GGUF V3 (latest)
0.00.030.916 I print_info: file type   = F16
0.00.030.917 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.035.310 I load: special tokens cache size = 5
0.00.037.588 I load: token to piece cache size = 0.2032 MB
0.00.037.618 I print_info: arch             = bert
0.00.037.619 I print_info: n_vocab (hp)     = 30522
0.00.037.620 I print_info: vocab_only       = 0
0.00.037.620 I print_info: n_ctx_train      = 512
0.00.037.621 I print_info: n_embd           = 384
0.00.037.621 I print_info: n_layer          = 12
0.00.037.624 I print_info: n_head           = 12
0.00.037.625 I print_info: n_head_kv        = 12
0.00.037.625 I print_info: n_rot            = 32
0.00.037.625 I print_info: n_swa            = 0
0.00.037.627 I print_info: n_embd_head_k    = 32
0.00.037.627 I print_info: n_embd_head_v    = 32
0.00.037.628 I print_info: n_gqa            = 1
0.00.037.629 I print_info: n_embd_k_gqa     = 384
0.00.037.630 I print_info: n_embd_v_gqa     = 384
0.00.037.631 I print_info: f_norm_eps       = 1.0e-12
0.00.037.632 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.632 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.632 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.633 I print_info: f_logit_scale    = 0.0e+00
0.00.037.636 I print_info: n_ff             = 1536
0.00.037.636 I print_info: n_expert         = 0
0.00.037.637 I print_info: n_expert_used    = 0
0.00.037.637 I print_info: causal attn      = 0
0.00.037.637 I print_info: pooling type     = 2
0.00.037.637 I print_info: rope type        = 2
0.00.037.638 I print_info: rope scaling     = linear
0.00.037.638 I print_info: freq_base_train  = 10000.0
0.00.037.639 I print_info: freq_scale_train = 1
0.00.037.639 I print_info: n_ctx_orig_yarn  = 512
0.00.037.639 I print_info: rope_finetuned   = unknown
0.00.037.646 I print_info: ssm_d_conv       = 0
0.00.037.646 I print_info: ssm_d_inner      = 0
0.00.037.646 I print_info: ssm_d_state      = 0
0.00.037.646 I print_info: ssm_dt_rank      = 0
0.00.037.647 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.647 I print_info: model type       = 33M
0.00.037.648 I print_info: model params     = 33.21 M
0.00.037.648 I print_info: general.name     = Bge Small
0.00.037.648 I print_info: vocab type       = WPM
0.00.037.649 I print_info: n_vocab          = 30522
0.00.037.649 I print_info: n_merges         = 0
0.00.037.649 I print_info: UNK token        = 100 '[UNK]'
0.00.037.650 I print_info: SEP token        = 102 '[SEP]'
0.00.037.650 I print_info: PAD token        = 0 '[PAD]'
0.00.037.654 I print_info: CLS token        = 101 '[CLS]'
0.00.037.654 I print_info: MASK token       = 103 '[MASK]'
0.00.037.654 I print_info: LF token         = 0 '[PAD]'
0.00.037.655 I print_info: max token length = 21
0.00.039.806 I load_tensors: offloading 12 repeating layers to GPU
0.00.039.812 I load_tensors: offloading output layer to GPU
0.00.039.813 I load_tensors: offloaded 13/13 layers to GPU
0.00.039.839 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.841 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.040.082 I llama_new_context_with_model: n_seq_max     = 1
0.00.040.083 I llama_new_context_with_model: n_ctx         = 512
0.00.040.083 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.040.083 I llama_new_context_with_model: n_batch       = 2048
0.00.040.084 I llama_new_context_with_model: n_ubatch      = 2048
0.00.040.084 I llama_new_context_with_model: flash_attn    = 0
0.00.040.085 I llama_new_context_with_model: freq_base     = 10000.0
0.00.040.085 I llama_new_context_with_model: freq_scale    = 1
0.00.040.085 I ggml_metal_init: allocating
0.00.040.089 I ggml_metal_init: found device: Apple M4
0.00.040.092 I ggml_metal_init: picking default device: Apple M4
0.00.040.950 I ggml_metal_init: using embedded metal library
0.00.045.225 I ggml_metal_init: GPU name:   Apple M4
0.00.045.228 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.229 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.229 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.229 I ggml_metal_init: simdgroup reduction   = true
0.00.045.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.230 I ggml_metal_init: has bfloat            = true
0.00.045.230 I ggml_metal_init: use bfloat            = true
0.00.045.230 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.231 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.733 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.058.309 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.311 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.331 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.059.152 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.059.154 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.059.154 I llama_new_context_with_model: graph nodes  = 429
0.00.059.154 I llama_new_context_with_model: graph splits = 2
0.00.059.155 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.059.155 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.597 I 
0.00.065.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.260 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.069.807 I llama_perf_context_print:        load time =      46.79 ms
0.00.069.808 I llama_perf_context_print: prompt eval time =       3.40 ms /     9 tokens (    0.38 ms per token,  2647.06 tokens per second)
0.00.069.809 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.069.809 I llama_perf_context_print:       total time =       4.21 ms /    10 tokens
0.00.069.935 I ggml_metal_free: deallocating

real	0m0.248s
user	0m0.051s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.033 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.261 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.023 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.029 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.030 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.030 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.030 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.031 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.031 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.032 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.032 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.033 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.036 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.036 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.037 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.037 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.037 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.038 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.533 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.197 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.199 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.199 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.199 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.200 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.200 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.200 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.015.200 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.015.201 I llama_model_loader: - type  f32:  124 tensors
0.00.015.201 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.202 I print_info: file format = GGUF V3 (latest)
0.00.015.202 I print_info: file type   = Q8_0
0.00.015.203 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.764 I load: special tokens cache size = 5
0.00.019.081 I load: token to piece cache size = 0.2032 MB
0.00.019.090 I print_info: arch             = bert
0.00.019.091 I print_info: n_vocab (hp)     = 30522
0.00.019.091 I print_info: vocab_only       = 0
0.00.019.092 I print_info: n_ctx_train      = 512
0.00.019.092 I print_info: n_embd           = 384
0.00.019.092 I print_info: n_layer          = 12
0.00.019.095 I print_info: n_head           = 12
0.00.019.095 I print_info: n_head_kv        = 12
0.00.019.096 I print_info: n_rot            = 32
0.00.019.096 I print_info: n_swa            = 0
0.00.019.096 I print_info: n_embd_head_k    = 32
0.00.019.096 I print_info: n_embd_head_v    = 32
0.00.019.096 I print_info: n_gqa            = 1
0.00.019.097 I print_info: n_embd_k_gqa     = 384
0.00.019.098 I print_info: n_embd_v_gqa     = 384
0.00.019.098 I print_info: f_norm_eps       = 1.0e-12
0.00.019.098 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.099 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.099 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.099 I print_info: f_logit_scale    = 0.0e+00
0.00.019.099 I print_info: n_ff             = 1536
0.00.019.100 I print_info: n_expert         = 0
0.00.019.100 I print_info: n_expert_used    = 0
0.00.019.102 I print_info: causal attn      = 0
0.00.019.102 I print_info: pooling type     = 2
0.00.019.102 I print_info: rope type        = 2
0.00.019.103 I print_info: rope scaling     = linear
0.00.019.103 I print_info: freq_base_train  = 10000.0
0.00.019.103 I print_info: freq_scale_train = 1
0.00.019.103 I print_info: n_ctx_orig_yarn  = 512
0.00.019.103 I print_info: rope_finetuned   = unknown
0.00.019.104 I print_info: ssm_d_conv       = 0
0.00.019.104 I print_info: ssm_d_inner      = 0
0.00.019.105 I print_info: ssm_d_state      = 0
0.00.019.105 I print_info: ssm_dt_rank      = 0
0.00.019.105 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.105 I print_info: model type       = 33M
0.00.019.105 I print_info: model params     = 33.21 M
0.00.019.106 I print_info: general.name     = Bge Small
0.00.019.106 I print_info: vocab type       = WPM
0.00.019.106 I print_info: n_vocab          = 30522
0.00.019.106 I print_info: n_merges         = 0
0.00.019.107 I print_info: UNK token        = 100 '[UNK]'
0.00.019.107 I print_info: SEP token        = 102 '[SEP]'
0.00.019.109 I print_info: PAD token        = 0 '[PAD]'
0.00.019.109 I print_info: CLS token        = 101 '[CLS]'
0.00.019.109 I print_info: MASK token       = 103 '[MASK]'
0.00.019.110 I print_info: LF token         = 0 '[PAD]'
0.00.019.110 I print_info: max token length = 21
0.00.020.361 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.361 I load_tensors: offloading output layer to GPU
0.00.020.362 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.370 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.370 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.513 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.514 I llama_new_context_with_model: n_ctx         = 512
0.00.020.514 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.514 I llama_new_context_with_model: n_batch       = 2048
0.00.020.514 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.514 I llama_new_context_with_model: flash_attn    = 0
0.00.020.515 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.515 I llama_new_context_with_model: freq_scale    = 1
0.00.020.515 I ggml_metal_init: allocating
0.00.020.518 I ggml_metal_init: found device: Apple M4
0.00.020.520 I ggml_metal_init: picking default device: Apple M4
0.00.021.103 I ggml_metal_init: using embedded metal library
0.00.023.613 I ggml_metal_init: GPU name:   Apple M4
0.00.023.615 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.616 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.616 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.616 I ggml_metal_init: simdgroup reduction   = true
0.00.023.617 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.617 I ggml_metal_init: has bfloat            = true
0.00.023.617 I ggml_metal_init: use bfloat            = true
0.00.023.617 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.770 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.299 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.301 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.314 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.888 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.889 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.889 I llama_new_context_with_model: graph nodes  = 429
0.00.034.890 I llama_new_context_with_model: graph splits = 2
0.00.034.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.891 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.107 I 
0.00.039.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.635 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.100 I llama_perf_context_print:        load time =      29.84 ms
0.00.043.101 I llama_perf_context_print: prompt eval time =       3.35 ms /     9 tokens (    0.37 ms per token,  2686.57 tokens per second)
0.00.043.102 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.102 I llama_perf_context_print:       total time =       3.99 ms /    10 tokens
0.00.043.242 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.031s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.169 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.028 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.127 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.134 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.136 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.137 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.137 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.139 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.140 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.140 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.141 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.142 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.145 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.145 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.146 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.147 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.274 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.276 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.277 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.277 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.277 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.278 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.278 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.051.278 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.279 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.279 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.279 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.280 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.051.280 I llama_model_loader: - type  f32:   40 tensors
0.00.051.286 I llama_model_loader: - type  f16:   30 tensors
0.00.051.287 I print_info: file format = GGUF V3 (latest)
0.00.051.288 I print_info: file type   = F16
0.00.051.289 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.068.057 W load: empty token at index 5
0.00.072.585 W load: model vocab missing newline token, using special_pad_id instead
0.00.073.931 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.073.957 I load: special tokens cache size = 5
0.00.337.154 I load: token to piece cache size = 1.5060 MB
0.00.337.192 I print_info: arch             = jina-bert-v2
0.00.337.193 I print_info: n_vocab (hp)     = 61056
0.00.337.194 I print_info: vocab_only       = 0
0.00.337.194 I print_info: n_ctx_train      = 8192
0.00.337.194 I print_info: n_embd           = 384
0.00.337.194 I print_info: n_layer          = 4
0.00.337.200 I print_info: n_head           = 12
0.00.337.200 I print_info: n_head_kv        = 12
0.00.337.200 I print_info: n_rot            = 32
0.00.337.200 I print_info: n_swa            = 0
0.00.337.201 I print_info: n_embd_head_k    = 32
0.00.337.201 I print_info: n_embd_head_v    = 32
0.00.337.202 I print_info: n_gqa            = 1
0.00.337.202 I print_info: n_embd_k_gqa     = 384
0.00.337.203 I print_info: n_embd_v_gqa     = 384
0.00.337.204 I print_info: f_norm_eps       = 1.0e-12
0.00.337.204 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.337.204 I print_info: f_clamp_kqv      = 0.0e+00
0.00.337.205 I print_info: f_max_alibi_bias = 8.0e+00
0.00.337.205 I print_info: f_logit_scale    = 0.0e+00
0.00.337.206 I print_info: n_ff             = 1536
0.00.337.206 I print_info: n_expert         = 0
0.00.337.206 I print_info: n_expert_used    = 0
0.00.337.206 I print_info: causal attn      = 0
0.00.337.207 I print_info: pooling type     = -1
0.00.337.207 I print_info: rope type        = -1
0.00.337.207 I print_info: rope scaling     = linear
0.00.337.207 I print_info: freq_base_train  = 10000.0
0.00.337.208 I print_info: freq_scale_train = 1
0.00.337.208 I print_info: n_ctx_orig_yarn  = 8192
0.00.337.208 I print_info: rope_finetuned   = unknown
0.00.337.208 I print_info: ssm_d_conv       = 0
0.00.337.208 I print_info: ssm_d_inner      = 0
0.00.337.209 I print_info: ssm_d_state      = 0
0.00.337.209 I print_info: ssm_dt_rank      = 0
0.00.337.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.337.209 I print_info: model type       = 33M
0.00.337.210 I print_info: model params     = 32.90 M
0.00.337.210 I print_info: general.name     = Jina Bert Implementation
0.00.337.211 I print_info: vocab type       = BPE
0.00.337.211 I print_info: n_vocab          = 61056
0.00.337.211 I print_info: n_merges         = 39382
0.00.337.211 I print_info: BOS token        = 0 '<s>'
0.00.337.212 I print_info: EOS token        = 2 '</s>'
0.00.337.212 I print_info: UNK token        = 3 '<unk>'
0.00.337.212 I print_info: SEP token        = 2 '</s>'
0.00.337.212 I print_info: PAD token        = 1 '<pad>'
0.00.337.212 I print_info: CLS token        = 0 '<s>'
0.00.337.212 I print_info: MASK token       = 4 '<mask>'
0.00.337.215 I print_info: EOG token        = 2 '</s>'
0.00.337.215 I print_info: max token length = 45
0.00.338.469 I load_tensors: offloading 4 repeating layers to GPU
0.00.338.469 I load_tensors: offloading output layer to GPU
0.00.338.469 I load_tensors: offloaded 5/5 layers to GPU
0.00.338.497 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.498 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.338.896 I llama_new_context_with_model: n_seq_max     = 1
0.00.338.897 I llama_new_context_with_model: n_ctx         = 8192
0.00.338.897 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.338.898 I llama_new_context_with_model: n_batch       = 2048
0.00.338.898 I llama_new_context_with_model: n_ubatch      = 2048
0.00.338.898 I llama_new_context_with_model: flash_attn    = 0
0.00.338.898 I llama_new_context_with_model: freq_base     = 10000.0
0.00.338.899 I llama_new_context_with_model: freq_scale    = 1
0.00.338.899 I ggml_metal_init: allocating
0.00.338.903 I ggml_metal_init: found device: Apple M4
0.00.338.904 I ggml_metal_init: picking default device: Apple M4
0.00.339.840 I ggml_metal_init: using embedded metal library
0.00.342.975 I ggml_metal_init: GPU name:   Apple M4
0.00.342.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.978 I ggml_metal_init: simdgroup reduction   = true
0.00.342.978 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.978 I ggml_metal_init: has bfloat            = true
0.00.342.978 I ggml_metal_init: use bfloat            = true
0.00.342.979 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.352.641 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.355.106 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.355.110 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.355.125 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.355.633 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.355.634 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.355.634 I llama_new_context_with_model: graph nodes  = 154
0.00.355.634 I llama_new_context_with_model: graph splits = 2
0.00.355.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.355.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.367.523 I 
0.00.367.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.367.793 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.367.794 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.367.802 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.367.802 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.367.804 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.367.804 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.368.314 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.371.977 I llama_perf_context_print:        load time =     344.49 ms
0.00.371.978 I llama_perf_context_print: prompt eval time =       3.65 ms /    62 tokens (    0.06 ms per token, 16972.35 tokens per second)
0.00.371.982 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.371.983 I llama_perf_context_print:       total time =       4.45 ms /    63 tokens
0.00.372.139 I ggml_metal_free: deallocating

real	0m1.091s
user	0m0.344s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.148 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.269 I main: llama backend init
0.00.000.277 I main: load the model and apply lora adapter, if any
0.00.029.420 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.209 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.224 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.227 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.232 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.232 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.233 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.236 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.237 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.238 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.564 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.568 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.569 I llama_model_loader: - type  f32:  194 tensors
0.00.061.570 I llama_model_loader: - type  f16:   98 tensors
0.00.061.571 I print_info: file format = GGUF V3 (latest)
0.00.061.572 I print_info: file type   = all F32 (guessed)
0.00.061.573 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.088.805 I load: special tokens cache size = 25
0.00.095.759 I load: token to piece cache size = 0.2984 MB
0.00.095.781 I print_info: arch             = gptneox
0.00.095.782 I print_info: n_vocab (hp)     = 50304
0.00.095.782 I print_info: vocab_only       = 0
0.00.095.782 I print_info: n_ctx_train      = 2048
0.00.095.782 I print_info: n_embd           = 2048
0.00.095.782 I print_info: n_layer          = 24
0.00.095.787 I print_info: n_head           = 16
0.00.095.787 I print_info: n_head_kv        = 16
0.00.095.788 I print_info: n_rot            = 32
0.00.095.788 I print_info: n_swa            = 0
0.00.095.788 I print_info: n_embd_head_k    = 128
0.00.095.790 I print_info: n_embd_head_v    = 128
0.00.095.790 I print_info: n_gqa            = 1
0.00.095.791 I print_info: n_embd_k_gqa     = 2048
0.00.095.791 I print_info: n_embd_v_gqa     = 2048
0.00.095.796 I print_info: f_norm_eps       = 1.0e-05
0.00.095.796 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.095.796 I print_info: f_clamp_kqv      = 0.0e+00
0.00.095.797 I print_info: f_max_alibi_bias = 0.0e+00
0.00.095.797 I print_info: f_logit_scale    = 0.0e+00
0.00.095.798 I print_info: n_ff             = 8192
0.00.095.798 I print_info: n_expert         = 0
0.00.095.798 I print_info: n_expert_used    = 0
0.00.095.798 I print_info: causal attn      = 1
0.00.095.799 I print_info: pooling type     = 0
0.00.095.799 I print_info: rope type        = 2
0.00.095.799 I print_info: rope scaling     = linear
0.00.095.800 I print_info: freq_base_train  = 10000.0
0.00.095.801 I print_info: freq_scale_train = 1
0.00.095.801 I print_info: n_ctx_orig_yarn  = 2048
0.00.095.801 I print_info: rope_finetuned   = unknown
0.00.095.801 I print_info: ssm_d_conv       = 0
0.00.095.801 I print_info: ssm_d_inner      = 0
0.00.095.802 I print_info: ssm_d_state      = 0
0.00.095.802 I print_info: ssm_dt_rank      = 0
0.00.095.802 I print_info: ssm_dt_b_c_rms   = 0
0.00.095.802 I print_info: model type       = 1.4B
0.00.095.802 I print_info: model params     = 1.41 B
0.00.095.802 I print_info: general.name     = 1.4B
0.00.095.803 I print_info: vocab type       = BPE
0.00.095.803 I print_info: n_vocab          = 50304
0.00.095.804 I print_info: n_merges         = 50009
0.00.095.804 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.095.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.095.804 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.095.805 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.095.805 I print_info: LF token         = 128 ''
0.00.095.805 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.095.806 I print_info: max token length = 1024
0.00.098.351 I load_tensors: offloading 24 repeating layers to GPU
0.00.098.351 I load_tensors: offloading output layer to GPU
0.00.098.352 I load_tensors: offloaded 25/25 layers to GPU
0.00.098.371 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.372 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.098.626 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.626 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.627 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.627 I llama_new_context_with_model: n_batch       = 2048
0.00.098.627 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.627 I llama_new_context_with_model: flash_attn    = 0
0.00.098.627 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.628 I llama_new_context_with_model: freq_scale    = 1
0.00.098.628 I ggml_metal_init: allocating
0.00.098.631 I ggml_metal_init: found device: Apple M4
0.00.098.633 I ggml_metal_init: picking default device: Apple M4
0.00.099.292 I ggml_metal_init: using embedded metal library
0.00.109.591 I ggml_metal_init: GPU name:   Apple M4
0.00.109.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.109.593 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.109.594 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.109.594 I ggml_metal_init: simdgroup reduction   = true
0.00.109.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.109.594 I ggml_metal_init: has bfloat            = true
0.00.109.594 I ggml_metal_init: use bfloat            = true
0.00.109.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.109.595 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.132.847 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.152.348 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.152.355 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.152.395 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.153.343 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.153.345 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.153.345 I llama_new_context_with_model: graph nodes  = 967
0.00.153.345 I llama_new_context_with_model: graph splits = 2
0.00.153.348 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.153.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.153.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.236.115 I main: llama threadpool init, n_threads = 4
0.00.236.156 I 
0.00.236.175 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.236.176 I 
0.00.236.258 I sampler seed: 1234
0.00.236.263 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.236.286 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.236.288 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.236.288 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.064.817 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.02.064.817 I llama_perf_context_print:        load time =     206.68 ms
0.02.064.818 I llama_perf_context_print: prompt eval time =      43.58 ms /     7 tokens (    6.23 ms per token,   160.61 tokens per second)
0.02.064.820 I llama_perf_context_print:        eval time =    1782.04 ms /    63 runs   (   28.29 ms per token,    35.35 tokens per second)
0.02.064.820 I llama_perf_context_print:       total time =    1828.70 ms /    70 tokens
0.02.065.019 I ggml_metal_free: deallocating

real	0m2.375s
user	0m0.141s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.508 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.867 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.230 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.240 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.243 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.243 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.244 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.246 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.249 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.049.028 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.028 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.029 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.029 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.030 I llama_model_loader: - type  f32:  194 tensors
0.00.049.030 I llama_model_loader: - type  f16:   98 tensors
0.00.049.031 I print_info: file format = GGUF V3 (latest)
0.00.049.032 I print_info: file type   = all F32 (guessed)
0.00.049.033 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.074.743 I load: special tokens cache size = 25
0.00.081.220 I load: token to piece cache size = 0.2984 MB
0.00.081.235 I print_info: arch             = gptneox
0.00.081.236 I print_info: n_vocab (hp)     = 50304
0.00.081.236 I print_info: vocab_only       = 0
0.00.081.236 I print_info: n_ctx_train      = 2048
0.00.081.236 I print_info: n_embd           = 2048
0.00.081.236 I print_info: n_layer          = 24
0.00.081.240 I print_info: n_head           = 16
0.00.081.240 I print_info: n_head_kv        = 16
0.00.081.241 I print_info: n_rot            = 32
0.00.081.241 I print_info: n_swa            = 0
0.00.081.243 I print_info: n_embd_head_k    = 128
0.00.081.243 I print_info: n_embd_head_v    = 128
0.00.081.244 I print_info: n_gqa            = 1
0.00.081.244 I print_info: n_embd_k_gqa     = 2048
0.00.081.252 I print_info: n_embd_v_gqa     = 2048
0.00.081.254 I print_info: f_norm_eps       = 1.0e-05
0.00.081.255 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.255 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.255 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.255 I print_info: f_logit_scale    = 0.0e+00
0.00.081.262 I print_info: n_ff             = 8192
0.00.081.263 I print_info: n_expert         = 0
0.00.081.263 I print_info: n_expert_used    = 0
0.00.081.264 I print_info: causal attn      = 1
0.00.081.264 I print_info: pooling type     = 0
0.00.081.264 I print_info: rope type        = 2
0.00.081.264 I print_info: rope scaling     = linear
0.00.081.265 I print_info: freq_base_train  = 10000.0
0.00.081.266 I print_info: freq_scale_train = 1
0.00.081.266 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.266 I print_info: rope_finetuned   = unknown
0.00.081.266 I print_info: ssm_d_conv       = 0
0.00.081.267 I print_info: ssm_d_inner      = 0
0.00.081.267 I print_info: ssm_d_state      = 0
0.00.081.267 I print_info: ssm_dt_rank      = 0
0.00.081.268 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.268 I print_info: model type       = 1.4B
0.00.081.268 I print_info: model params     = 1.41 B
0.00.081.269 I print_info: general.name     = 1.4B
0.00.081.269 I print_info: vocab type       = BPE
0.00.081.269 I print_info: n_vocab          = 50304
0.00.081.269 I print_info: n_merges         = 50009
0.00.081.269 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.270 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.271 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.271 I print_info: LF token         = 128 ''
0.00.081.272 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.272 I print_info: max token length = 1024
0.00.083.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.083.979 I load_tensors: offloading output layer to GPU
0.00.083.980 I load_tensors: offloaded 25/25 layers to GPU
0.00.083.991 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.083.992 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.084.360 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.361 I llama_new_context_with_model: n_ctx         = 128
0.00.084.361 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.084.361 I llama_new_context_with_model: n_batch       = 128
0.00.084.362 I llama_new_context_with_model: n_ubatch      = 128
0.00.084.362 I llama_new_context_with_model: flash_attn    = 0
0.00.084.362 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.362 I llama_new_context_with_model: freq_scale    = 1
0.00.084.363 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.084.363 I ggml_metal_init: allocating
0.00.084.366 I ggml_metal_init: found device: Apple M4
0.00.084.368 I ggml_metal_init: picking default device: Apple M4
0.00.084.984 I ggml_metal_init: using embedded metal library
0.00.087.569 I ggml_metal_init: GPU name:   Apple M4
0.00.087.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.571 I ggml_metal_init: simdgroup reduction   = true
0.00.087.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.572 I ggml_metal_init: has bfloat            = true
0.00.087.572 I ggml_metal_init: use bfloat            = true
0.00.087.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.572 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.937 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.310 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.098.315 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.098.345 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.288 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.099.289 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.099.290 I llama_new_context_with_model: graph nodes  = 967
0.00.099.290 I llama_new_context_with_model: graph splits = 2
0.00.099.291 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.099.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.094.322 I 
0.01.094.368 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.094.397 I perplexity: tokenizing the input ..
0.01.108.468 I perplexity: tokenization took 14.07 ms
0.01.108.475 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.229.354 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.230.781 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.230.811 I llama_perf_context_print:        load time =    1076.44 ms
0.01.230.812 I llama_perf_context_print: prompt eval time =     119.89 ms /   128 tokens (    0.94 ms per token,  1067.64 tokens per second)
0.01.230.813 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.230.813 I llama_perf_context_print:       total time =     136.49 ms /   129 tokens
0.01.231.190 I ggml_metal_free: deallocating

real	0m1.413s
user	0m0.114s
sys	0m0.211s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.847 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.677 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.678 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.679 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.679 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.680 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.680 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.681 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.681 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.682 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.682 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.682 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.683 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.683 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.685 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.335 I llama_model_loader: - type  f32:  194 tensors
0.00.034.335 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.336 I print_info: file format = GGUF V3 (latest)
0.00.034.337 I print_info: file type   = Q8_0
0.00.034.338 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.054.917 I load: special tokens cache size = 25
0.00.061.061 I load: token to piece cache size = 0.2984 MB
0.00.061.078 I print_info: arch             = gptneox
0.00.061.079 I print_info: n_vocab (hp)     = 50304
0.00.061.080 I print_info: vocab_only       = 0
0.00.061.080 I print_info: n_ctx_train      = 2048
0.00.061.080 I print_info: n_embd           = 2048
0.00.061.080 I print_info: n_layer          = 24
0.00.061.087 I print_info: n_head           = 16
0.00.061.087 I print_info: n_head_kv        = 16
0.00.061.088 I print_info: n_rot            = 32
0.00.061.088 I print_info: n_swa            = 0
0.00.061.088 I print_info: n_embd_head_k    = 128
0.00.061.088 I print_info: n_embd_head_v    = 128
0.00.061.089 I print_info: n_gqa            = 1
0.00.061.090 I print_info: n_embd_k_gqa     = 2048
0.00.061.090 I print_info: n_embd_v_gqa     = 2048
0.00.061.091 I print_info: f_norm_eps       = 1.0e-05
0.00.061.092 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.093 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.094 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.094 I print_info: f_logit_scale    = 0.0e+00
0.00.061.094 I print_info: n_ff             = 8192
0.00.061.095 I print_info: n_expert         = 0
0.00.061.095 I print_info: n_expert_used    = 0
0.00.061.095 I print_info: causal attn      = 1
0.00.061.095 I print_info: pooling type     = 0
0.00.061.095 I print_info: rope type        = 2
0.00.061.095 I print_info: rope scaling     = linear
0.00.061.096 I print_info: freq_base_train  = 10000.0
0.00.061.096 I print_info: freq_scale_train = 1
0.00.061.096 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.096 I print_info: rope_finetuned   = unknown
0.00.061.096 I print_info: ssm_d_conv       = 0
0.00.061.097 I print_info: ssm_d_inner      = 0
0.00.061.097 I print_info: ssm_d_state      = 0
0.00.061.097 I print_info: ssm_dt_rank      = 0
0.00.061.097 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.097 I print_info: model type       = 1.4B
0.00.061.097 I print_info: model params     = 1.41 B
0.00.061.097 I print_info: general.name     = 1.4B
0.00.061.098 I print_info: vocab type       = BPE
0.00.061.098 I print_info: n_vocab          = 50304
0.00.061.098 I print_info: n_merges         = 50009
0.00.061.099 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.099 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.099 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.099 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.099 I print_info: LF token         = 128 ''
0.00.061.100 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.100 I print_info: max token length = 1024
0.00.063.483 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.483 I load_tensors: offloading output layer to GPU
0.00.063.483 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.494 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.495 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.063.820 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.821 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.821 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.821 I llama_new_context_with_model: n_batch       = 2048
0.00.063.821 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.821 I llama_new_context_with_model: flash_attn    = 0
0.00.063.822 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.822 I llama_new_context_with_model: freq_scale    = 1
0.00.063.822 I ggml_metal_init: allocating
0.00.063.826 I ggml_metal_init: found device: Apple M4
0.00.063.828 I ggml_metal_init: picking default device: Apple M4
0.00.064.551 I ggml_metal_init: using embedded metal library
0.00.067.129 I ggml_metal_init: GPU name:   Apple M4
0.00.067.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.132 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.132 I ggml_metal_init: simdgroup reduction   = true
0.00.067.132 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.132 I ggml_metal_init: has bfloat            = true
0.00.067.132 I ggml_metal_init: use bfloat            = true
0.00.067.133 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.528 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.289 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.303 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.339 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.443 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.444 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.445 I llama_new_context_with_model: graph nodes  = 967
0.00.104.445 I llama_new_context_with_model: graph splits = 2
0.00.104.449 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.590 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.591 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.472.961 I main: llama threadpool init, n_threads = 4
0.01.472.999 I 
0.01.473.018 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.473.018 I 
0.01.473.255 I sampler seed: 1234
0.01.473.259 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.473.297 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.473.317 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.473.318 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.577.085 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.02.577.085 I llama_perf_context_print:        load time =    1463.11 ms
0.02.577.086 I llama_perf_context_print: prompt eval time =      39.82 ms /     7 tokens (    5.69 ms per token,   175.80 tokens per second)
0.02.577.087 I llama_perf_context_print:        eval time =    1061.15 ms /    63 runs   (   16.84 ms per token,    59.37 tokens per second)
0.02.577.087 I llama_perf_context_print:       total time =    1104.13 ms /    70 tokens
0.02.577.300 I ggml_metal_free: deallocating

real	0m2.597s
user	0m0.114s
sys	0m0.221s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.451 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.916 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.922 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.931 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.932 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.933 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.933 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.933 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.935 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.935 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.937 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.978 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.957 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.957 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.958 I llama_model_loader: - type  f32:  194 tensors
0.00.025.959 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.959 I print_info: file format = GGUF V3 (latest)
0.00.025.960 I print_info: file type   = Q8_0
0.00.025.961 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.666 I load: special tokens cache size = 25
0.00.052.754 I load: token to piece cache size = 0.2984 MB
0.00.052.771 I print_info: arch             = gptneox
0.00.052.772 I print_info: n_vocab (hp)     = 50304
0.00.052.772 I print_info: vocab_only       = 0
0.00.052.772 I print_info: n_ctx_train      = 2048
0.00.052.773 I print_info: n_embd           = 2048
0.00.052.773 I print_info: n_layer          = 24
0.00.052.776 I print_info: n_head           = 16
0.00.052.776 I print_info: n_head_kv        = 16
0.00.052.778 I print_info: n_rot            = 32
0.00.052.779 I print_info: n_swa            = 0
0.00.052.779 I print_info: n_embd_head_k    = 128
0.00.052.779 I print_info: n_embd_head_v    = 128
0.00.052.779 I print_info: n_gqa            = 1
0.00.052.780 I print_info: n_embd_k_gqa     = 2048
0.00.052.780 I print_info: n_embd_v_gqa     = 2048
0.00.052.781 I print_info: f_norm_eps       = 1.0e-05
0.00.052.781 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.781 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.782 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.782 I print_info: f_logit_scale    = 0.0e+00
0.00.052.782 I print_info: n_ff             = 8192
0.00.052.783 I print_info: n_expert         = 0
0.00.052.783 I print_info: n_expert_used    = 0
0.00.052.783 I print_info: causal attn      = 1
0.00.052.783 I print_info: pooling type     = 0
0.00.052.783 I print_info: rope type        = 2
0.00.052.783 I print_info: rope scaling     = linear
0.00.052.784 I print_info: freq_base_train  = 10000.0
0.00.052.784 I print_info: freq_scale_train = 1
0.00.052.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.784 I print_info: rope_finetuned   = unknown
0.00.052.784 I print_info: ssm_d_conv       = 0
0.00.052.784 I print_info: ssm_d_inner      = 0
0.00.052.785 I print_info: ssm_d_state      = 0
0.00.052.785 I print_info: ssm_dt_rank      = 0
0.00.052.785 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.785 I print_info: model type       = 1.4B
0.00.052.785 I print_info: model params     = 1.41 B
0.00.052.785 I print_info: general.name     = 1.4B
0.00.052.786 I print_info: vocab type       = BPE
0.00.052.786 I print_info: n_vocab          = 50304
0.00.052.786 I print_info: n_merges         = 50009
0.00.052.786 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.786 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.787 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.787 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.787 I print_info: LF token         = 128 ''
0.00.052.787 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.787 I print_info: max token length = 1024
0.00.054.874 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.874 I load_tensors: offloading output layer to GPU
0.00.054.875 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.886 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.887 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.055.279 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.280 I llama_new_context_with_model: n_ctx         = 128
0.00.055.280 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.280 I llama_new_context_with_model: n_batch       = 128
0.00.055.281 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.281 I llama_new_context_with_model: flash_attn    = 0
0.00.055.281 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.282 I llama_new_context_with_model: freq_scale    = 1
0.00.055.282 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.283 I ggml_metal_init: allocating
0.00.055.287 I ggml_metal_init: found device: Apple M4
0.00.055.290 I ggml_metal_init: picking default device: Apple M4
0.00.055.860 I ggml_metal_init: using embedded metal library
0.00.058.306 I ggml_metal_init: GPU name:   Apple M4
0.00.058.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.308 I ggml_metal_init: simdgroup reduction   = true
0.00.058.309 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.309 I ggml_metal_init: has bfloat            = true
0.00.058.309 I ggml_metal_init: use bfloat            = true
0.00.058.310 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.499 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.812 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.816 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.846 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.724 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.725 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.725 I llama_new_context_with_model: graph nodes  = 967
0.00.070.726 I llama_new_context_with_model: graph splits = 2
0.00.070.727 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.898.068 I 
0.00.898.091 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.898.100 I perplexity: tokenizing the input ..
0.00.906.148 I perplexity: tokenization took 8.047 ms
0.00.906.153 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.029.980 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.031.288 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.031.311 I llama_perf_context_print:        load time =     888.61 ms
0.01.031.312 I llama_perf_context_print: prompt eval time =     123.61 ms /   128 tokens (    0.97 ms per token,  1035.53 tokens per second)
0.01.031.313 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.031.314 I llama_perf_context_print:       total time =     133.24 ms /   129 tokens
0.01.031.673 I ggml_metal_free: deallocating

real	0m1.045s
user	0m0.080s
sys	0m0.139s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.016.378 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.108 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.117 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.119 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.120 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.120 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.121 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.093 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.798 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.799 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.799 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.800 I llama_model_loader: - type  f32:  194 tensors
0.00.044.801 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.801 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.801 I print_info: file format = GGUF V3 (latest)
0.00.044.802 I print_info: file type   = Q4_0
0.00.044.804 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.080.085 I load: special tokens cache size = 25
0.00.090.366 I load: token to piece cache size = 0.2984 MB
0.00.090.383 I print_info: arch             = gptneox
0.00.090.384 I print_info: n_vocab (hp)     = 50304
0.00.090.385 I print_info: vocab_only       = 0
0.00.090.385 I print_info: n_ctx_train      = 2048
0.00.090.385 I print_info: n_embd           = 2048
0.00.090.385 I print_info: n_layer          = 24
0.00.090.390 I print_info: n_head           = 16
0.00.090.391 I print_info: n_head_kv        = 16
0.00.090.391 I print_info: n_rot            = 32
0.00.090.391 I print_info: n_swa            = 0
0.00.090.391 I print_info: n_embd_head_k    = 128
0.00.090.392 I print_info: n_embd_head_v    = 128
0.00.090.393 I print_info: n_gqa            = 1
0.00.090.393 I print_info: n_embd_k_gqa     = 2048
0.00.090.394 I print_info: n_embd_v_gqa     = 2048
0.00.090.395 I print_info: f_norm_eps       = 1.0e-05
0.00.090.395 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.396 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.396 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.396 I print_info: f_logit_scale    = 0.0e+00
0.00.090.397 I print_info: n_ff             = 8192
0.00.090.397 I print_info: n_expert         = 0
0.00.090.399 I print_info: n_expert_used    = 0
0.00.090.401 I print_info: causal attn      = 1
0.00.090.401 I print_info: pooling type     = 0
0.00.090.402 I print_info: rope type        = 2
0.00.090.402 I print_info: rope scaling     = linear
0.00.090.402 I print_info: freq_base_train  = 10000.0
0.00.090.403 I print_info: freq_scale_train = 1
0.00.090.403 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.403 I print_info: rope_finetuned   = unknown
0.00.090.403 I print_info: ssm_d_conv       = 0
0.00.090.404 I print_info: ssm_d_inner      = 0
0.00.090.404 I print_info: ssm_d_state      = 0
0.00.090.404 I print_info: ssm_dt_rank      = 0
0.00.090.404 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.404 I print_info: model type       = 1.4B
0.00.090.405 I print_info: model params     = 1.41 B
0.00.090.405 I print_info: general.name     = 1.4B
0.00.090.410 I print_info: vocab type       = BPE
0.00.090.411 I print_info: n_vocab          = 50304
0.00.090.411 I print_info: n_merges         = 50009
0.00.090.413 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.413 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.413 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.414 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.414 I print_info: LF token         = 128 ''
0.00.090.414 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.414 I print_info: max token length = 1024
0.00.093.242 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.243 I load_tensors: offloading output layer to GPU
0.00.093.243 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.255 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.093.257 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.093.689 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.690 I llama_new_context_with_model: n_ctx         = 2048
0.00.093.690 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.093.691 I llama_new_context_with_model: n_batch       = 2048
0.00.093.691 I llama_new_context_with_model: n_ubatch      = 512
0.00.093.691 I llama_new_context_with_model: flash_attn    = 0
0.00.093.692 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.692 I llama_new_context_with_model: freq_scale    = 1
0.00.093.693 I ggml_metal_init: allocating
0.00.093.697 I ggml_metal_init: found device: Apple M4
0.00.093.699 I ggml_metal_init: picking default device: Apple M4
0.00.094.624 I ggml_metal_init: using embedded metal library
0.00.098.246 I ggml_metal_init: GPU name:   Apple M4
0.00.098.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.250 I ggml_metal_init: simdgroup reduction   = true
0.00.098.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.250 I ggml_metal_init: has bfloat            = true
0.00.098.250 I ggml_metal_init: use bfloat            = true
0.00.098.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.252 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.637 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.133.854 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.133.863 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.133.902 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.134.924 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.134.926 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.134.927 I llama_new_context_with_model: graph nodes  = 967
0.00.134.927 I llama_new_context_with_model: graph splits = 2
0.00.134.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.135.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.135.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.791.994 I main: llama threadpool init, n_threads = 4
0.00.792.075 I 
0.00.792.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.131 I 
0.00.792.663 I sampler seed: 1234
0.00.792.674 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.792.706 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.792.710 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.792.710 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.472.353 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.472.354 I llama_perf_context_print:        load time =     775.61 ms
0.01.472.354 I llama_perf_context_print: prompt eval time =      40.32 ms /     7 tokens (    5.76 ms per token,   173.62 tokens per second)
0.01.472.355 I llama_perf_context_print:        eval time =     636.11 ms /    63 runs   (   10.10 ms per token,    99.04 tokens per second)
0.01.472.355 I llama_perf_context_print:       total time =     680.36 ms /    70 tokens
0.01.472.615 I ggml_metal_free: deallocating

real	0m1.508s
user	0m0.145s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.737 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.596 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.601 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.602 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.602 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.602 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.604 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.606 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.607 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.608 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.608 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.608 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.315 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.005 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.005 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.005 I llama_model_loader: - type  f32:  194 tensors
0.00.025.006 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.006 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.007 I print_info: file format = GGUF V3 (latest)
0.00.025.007 I print_info: file type   = Q4_0
0.00.025.008 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.043.704 I load: special tokens cache size = 25
0.00.049.691 I load: token to piece cache size = 0.2984 MB
0.00.049.701 I print_info: arch             = gptneox
0.00.049.702 I print_info: n_vocab (hp)     = 50304
0.00.049.702 I print_info: vocab_only       = 0
0.00.049.702 I print_info: n_ctx_train      = 2048
0.00.049.702 I print_info: n_embd           = 2048
0.00.049.703 I print_info: n_layer          = 24
0.00.049.705 I print_info: n_head           = 16
0.00.049.706 I print_info: n_head_kv        = 16
0.00.049.706 I print_info: n_rot            = 32
0.00.049.706 I print_info: n_swa            = 0
0.00.049.706 I print_info: n_embd_head_k    = 128
0.00.049.707 I print_info: n_embd_head_v    = 128
0.00.049.707 I print_info: n_gqa            = 1
0.00.049.708 I print_info: n_embd_k_gqa     = 2048
0.00.049.709 I print_info: n_embd_v_gqa     = 2048
0.00.049.709 I print_info: f_norm_eps       = 1.0e-05
0.00.049.711 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.711 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.711 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.711 I print_info: f_logit_scale    = 0.0e+00
0.00.049.712 I print_info: n_ff             = 8192
0.00.049.712 I print_info: n_expert         = 0
0.00.049.712 I print_info: n_expert_used    = 0
0.00.049.712 I print_info: causal attn      = 1
0.00.049.713 I print_info: pooling type     = 0
0.00.049.713 I print_info: rope type        = 2
0.00.049.713 I print_info: rope scaling     = linear
0.00.049.713 I print_info: freq_base_train  = 10000.0
0.00.049.714 I print_info: freq_scale_train = 1
0.00.049.714 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.714 I print_info: rope_finetuned   = unknown
0.00.049.714 I print_info: ssm_d_conv       = 0
0.00.049.714 I print_info: ssm_d_inner      = 0
0.00.049.714 I print_info: ssm_d_state      = 0
0.00.049.715 I print_info: ssm_dt_rank      = 0
0.00.049.715 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.716 I print_info: model type       = 1.4B
0.00.049.717 I print_info: model params     = 1.41 B
0.00.049.717 I print_info: general.name     = 1.4B
0.00.049.717 I print_info: vocab type       = BPE
0.00.049.717 I print_info: n_vocab          = 50304
0.00.049.717 I print_info: n_merges         = 50009
0.00.049.717 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.718 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.718 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.718 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.718 I print_info: LF token         = 128 ''
0.00.049.718 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.719 I print_info: max token length = 1024
0.00.051.495 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.495 I load_tensors: offloading output layer to GPU
0.00.051.495 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.501 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.501 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.765 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.766 I llama_new_context_with_model: n_ctx         = 128
0.00.051.766 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.766 I llama_new_context_with_model: n_batch       = 128
0.00.051.767 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.767 I llama_new_context_with_model: flash_attn    = 0
0.00.051.767 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.767 I llama_new_context_with_model: freq_scale    = 1
0.00.051.768 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.768 I ggml_metal_init: allocating
0.00.051.771 I ggml_metal_init: found device: Apple M4
0.00.051.773 I ggml_metal_init: picking default device: Apple M4
0.00.052.338 I ggml_metal_init: using embedded metal library
0.00.054.681 I ggml_metal_init: GPU name:   Apple M4
0.00.054.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.684 I ggml_metal_init: simdgroup reduction   = true
0.00.054.684 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.684 I ggml_metal_init: has bfloat            = true
0.00.054.684 I ggml_metal_init: use bfloat            = true
0.00.054.685 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.371 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.639 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.641 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.659 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.563 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.564 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.564 I llama_new_context_with_model: graph nodes  = 967
0.00.066.565 I llama_new_context_with_model: graph splits = 2
0.00.066.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.566 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.944 I 
0.00.638.020 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.049 I perplexity: tokenizing the input ..
0.00.646.243 I perplexity: tokenization took 8.192 ms
0.00.646.246 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.768.925 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.770.103 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.770.136 I llama_perf_context_print:        load time =     628.20 ms
0.00.770.137 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.29 tokens per second)
0.00.770.138 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.139 I llama_perf_context_print:       total time =     132.20 ms /   129 tokens
0.00.770.673 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.078s
sys	0m0.112s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.691 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.029.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.226 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.226 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.226 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.228 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.229 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.232 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.088 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.944 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.946 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.946 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.947 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.037.947 I llama_model_loader: - type  f32:  194 tensors
0.00.037.947 I llama_model_loader: - type q4_1:   97 tensors
0.00.037.948 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.948 I print_info: file format = GGUF V3 (latest)
0.00.037.949 I print_info: file type   = Q4_1
0.00.037.953 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.059.678 I load: special tokens cache size = 25
0.00.067.187 I load: token to piece cache size = 0.2984 MB
0.00.067.201 I print_info: arch             = gptneox
0.00.067.202 I print_info: n_vocab (hp)     = 50304
0.00.067.202 I print_info: vocab_only       = 0
0.00.067.202 I print_info: n_ctx_train      = 2048
0.00.067.203 I print_info: n_embd           = 2048
0.00.067.203 I print_info: n_layer          = 24
0.00.067.206 I print_info: n_head           = 16
0.00.067.207 I print_info: n_head_kv        = 16
0.00.067.207 I print_info: n_rot            = 32
0.00.067.207 I print_info: n_swa            = 0
0.00.067.207 I print_info: n_embd_head_k    = 128
0.00.067.207 I print_info: n_embd_head_v    = 128
0.00.067.208 I print_info: n_gqa            = 1
0.00.067.209 I print_info: n_embd_k_gqa     = 2048
0.00.067.209 I print_info: n_embd_v_gqa     = 2048
0.00.067.210 I print_info: f_norm_eps       = 1.0e-05
0.00.067.210 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.210 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.210 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.211 I print_info: f_logit_scale    = 0.0e+00
0.00.067.211 I print_info: n_ff             = 8192
0.00.067.211 I print_info: n_expert         = 0
0.00.067.213 I print_info: n_expert_used    = 0
0.00.067.215 I print_info: causal attn      = 1
0.00.067.215 I print_info: pooling type     = 0
0.00.067.215 I print_info: rope type        = 2
0.00.067.215 I print_info: rope scaling     = linear
0.00.067.216 I print_info: freq_base_train  = 10000.0
0.00.067.216 I print_info: freq_scale_train = 1
0.00.067.216 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.216 I print_info: rope_finetuned   = unknown
0.00.067.216 I print_info: ssm_d_conv       = 0
0.00.067.216 I print_info: ssm_d_inner      = 0
0.00.067.217 I print_info: ssm_d_state      = 0
0.00.067.217 I print_info: ssm_dt_rank      = 0
0.00.067.217 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.217 I print_info: model type       = 1.4B
0.00.067.217 I print_info: model params     = 1.41 B
0.00.067.217 I print_info: general.name     = 1.4B
0.00.067.218 I print_info: vocab type       = BPE
0.00.067.218 I print_info: n_vocab          = 50304
0.00.067.218 I print_info: n_merges         = 50009
0.00.067.218 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.219 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.219 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.219 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.219 I print_info: LF token         = 128 ''
0.00.067.219 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.220 I print_info: max token length = 1024
0.00.069.329 I load_tensors: offloading 24 repeating layers to GPU
0.00.069.329 I load_tensors: offloading output layer to GPU
0.00.069.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.069.340 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.069.341 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.069.646 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.647 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.647 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.647 I llama_new_context_with_model: n_batch       = 2048
0.00.069.647 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.648 I llama_new_context_with_model: flash_attn    = 0
0.00.069.648 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.648 I llama_new_context_with_model: freq_scale    = 1
0.00.069.649 I ggml_metal_init: allocating
0.00.069.652 I ggml_metal_init: found device: Apple M4
0.00.069.654 I ggml_metal_init: picking default device: Apple M4
0.00.070.308 I ggml_metal_init: using embedded metal library
0.00.073.104 I ggml_metal_init: GPU name:   Apple M4
0.00.073.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.107 I ggml_metal_init: simdgroup reduction   = true
0.00.073.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.107 I ggml_metal_init: has bfloat            = true
0.00.073.107 I ggml_metal_init: use bfloat            = true
0.00.073.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.833 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.795 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.802 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.834 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.894 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.895 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.896 I llama_new_context_with_model: graph nodes  = 967
0.00.106.896 I llama_new_context_with_model: graph splits = 2
0.00.106.899 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.047 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.640 I main: llama threadpool init, n_threads = 4
0.00.809.683 I 
0.00.809.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.705 I 
0.00.809.943 I sampler seed: 1234
0.00.809.948 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.003 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.005 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.005 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.536.073 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63677.13 tokens per second)
0.01.536.074 I llama_perf_context_print:        load time =     800.94 ms
0.01.536.075 I llama_perf_context_print: prompt eval time =      45.33 ms /     7 tokens (    6.48 ms per token,   154.43 tokens per second)
0.01.536.075 I llama_perf_context_print:        eval time =     677.87 ms /    63 runs   (   10.76 ms per token,    92.94 tokens per second)
0.01.536.076 I llama_perf_context_print:       total time =     726.44 ms /    70 tokens
0.01.536.331 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.116s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.460 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.467 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.467 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.468 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.473 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.474 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.479 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.479 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.480 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.263 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.273 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.022 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.023 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.024 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.024 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.025 I llama_model_loader: - type  f32:  194 tensors
0.00.024.025 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.025 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.026 I print_info: file format = GGUF V3 (latest)
0.00.024.026 I print_info: file type   = Q4_1
0.00.024.027 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.483 I load: special tokens cache size = 25
0.00.049.439 I load: token to piece cache size = 0.2984 MB
0.00.049.454 I print_info: arch             = gptneox
0.00.049.455 I print_info: n_vocab (hp)     = 50304
0.00.049.455 I print_info: vocab_only       = 0
0.00.049.455 I print_info: n_ctx_train      = 2048
0.00.049.456 I print_info: n_embd           = 2048
0.00.049.456 I print_info: n_layer          = 24
0.00.049.459 I print_info: n_head           = 16
0.00.049.460 I print_info: n_head_kv        = 16
0.00.049.460 I print_info: n_rot            = 32
0.00.049.460 I print_info: n_swa            = 0
0.00.049.460 I print_info: n_embd_head_k    = 128
0.00.049.460 I print_info: n_embd_head_v    = 128
0.00.049.461 I print_info: n_gqa            = 1
0.00.049.462 I print_info: n_embd_k_gqa     = 2048
0.00.049.463 I print_info: n_embd_v_gqa     = 2048
0.00.049.463 I print_info: f_norm_eps       = 1.0e-05
0.00.049.464 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.464 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.464 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.464 I print_info: f_logit_scale    = 0.0e+00
0.00.049.466 I print_info: n_ff             = 8192
0.00.049.466 I print_info: n_expert         = 0
0.00.049.466 I print_info: n_expert_used    = 0
0.00.049.467 I print_info: causal attn      = 1
0.00.049.467 I print_info: pooling type     = 0
0.00.049.467 I print_info: rope type        = 2
0.00.049.468 I print_info: rope scaling     = linear
0.00.049.470 I print_info: freq_base_train  = 10000.0
0.00.049.470 I print_info: freq_scale_train = 1
0.00.049.471 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.471 I print_info: rope_finetuned   = unknown
0.00.049.471 I print_info: ssm_d_conv       = 0
0.00.049.471 I print_info: ssm_d_inner      = 0
0.00.049.471 I print_info: ssm_d_state      = 0
0.00.049.471 I print_info: ssm_dt_rank      = 0
0.00.049.472 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.472 I print_info: model type       = 1.4B
0.00.049.472 I print_info: model params     = 1.41 B
0.00.049.472 I print_info: general.name     = 1.4B
0.00.049.473 I print_info: vocab type       = BPE
0.00.049.473 I print_info: n_vocab          = 50304
0.00.049.473 I print_info: n_merges         = 50009
0.00.049.473 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.474 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.474 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.474 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.474 I print_info: LF token         = 128 ''
0.00.049.479 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.479 I print_info: max token length = 1024
0.00.051.523 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.523 I load_tensors: offloading output layer to GPU
0.00.051.523 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.534 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.535 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.813 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.814 I llama_new_context_with_model: n_ctx         = 128
0.00.051.814 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.814 I llama_new_context_with_model: n_batch       = 128
0.00.051.814 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.814 I llama_new_context_with_model: flash_attn    = 0
0.00.051.815 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.815 I llama_new_context_with_model: freq_scale    = 1
0.00.051.815 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.816 I ggml_metal_init: allocating
0.00.051.819 I ggml_metal_init: found device: Apple M4
0.00.051.821 I ggml_metal_init: picking default device: Apple M4
0.00.052.404 I ggml_metal_init: using embedded metal library
0.00.054.807 I ggml_metal_init: GPU name:   Apple M4
0.00.054.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.809 I ggml_metal_init: simdgroup reduction   = true
0.00.054.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.809 I ggml_metal_init: has bfloat            = true
0.00.054.809 I ggml_metal_init: use bfloat            = true
0.00.054.810 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.678 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.930 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.933 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.959 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.951 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.953 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.953 I llama_new_context_with_model: graph nodes  = 967
0.00.066.953 I llama_new_context_with_model: graph splits = 2
0.00.066.954 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.917 I 
0.00.651.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.953 I perplexity: tokenizing the input ..
0.00.659.691 I perplexity: tokenization took 7.736 ms
0.00.659.694 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.632 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.783.848 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.783.875 I llama_perf_context_print:        load time =     643.06 ms
0.00.783.875 I llama_perf_context_print: prompt eval time =     122.69 ms /   128 tokens (    0.96 ms per token,  1043.28 tokens per second)
0.00.783.876 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.877 I llama_perf_context_print:       total time =     131.96 ms /   129 tokens
0.00.784.229 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.078s
sys	0m0.096s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.012.513 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.548 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.550 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.551 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.556 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.557 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.557 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.561 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.561 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.380 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.382 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.383 I llama_model_loader: - type  f32:  194 tensors
0.00.029.383 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.383 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.384 I print_info: file format = GGUF V3 (latest)
0.00.029.384 I print_info: file type   = Q5_0
0.00.029.385 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.049.115 I load: special tokens cache size = 25
0.00.055.255 I load: token to piece cache size = 0.2984 MB
0.00.055.269 I print_info: arch             = gptneox
0.00.055.270 I print_info: n_vocab (hp)     = 50304
0.00.055.270 I print_info: vocab_only       = 0
0.00.055.270 I print_info: n_ctx_train      = 2048
0.00.055.270 I print_info: n_embd           = 2048
0.00.055.271 I print_info: n_layer          = 24
0.00.055.273 I print_info: n_head           = 16
0.00.055.274 I print_info: n_head_kv        = 16
0.00.055.274 I print_info: n_rot            = 32
0.00.055.275 I print_info: n_swa            = 0
0.00.055.275 I print_info: n_embd_head_k    = 128
0.00.055.275 I print_info: n_embd_head_v    = 128
0.00.055.276 I print_info: n_gqa            = 1
0.00.055.276 I print_info: n_embd_k_gqa     = 2048
0.00.055.277 I print_info: n_embd_v_gqa     = 2048
0.00.055.280 I print_info: f_norm_eps       = 1.0e-05
0.00.055.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.281 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.281 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.281 I print_info: f_logit_scale    = 0.0e+00
0.00.055.282 I print_info: n_ff             = 8192
0.00.055.282 I print_info: n_expert         = 0
0.00.055.283 I print_info: n_expert_used    = 0
0.00.055.283 I print_info: causal attn      = 1
0.00.055.283 I print_info: pooling type     = 0
0.00.055.283 I print_info: rope type        = 2
0.00.055.283 I print_info: rope scaling     = linear
0.00.055.284 I print_info: freq_base_train  = 10000.0
0.00.055.291 I print_info: freq_scale_train = 1
0.00.055.292 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.292 I print_info: rope_finetuned   = unknown
0.00.055.292 I print_info: ssm_d_conv       = 0
0.00.055.292 I print_info: ssm_d_inner      = 0
0.00.055.293 I print_info: ssm_d_state      = 0
0.00.055.294 I print_info: ssm_dt_rank      = 0
0.00.055.294 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.294 I print_info: model type       = 1.4B
0.00.055.295 I print_info: model params     = 1.41 B
0.00.055.295 I print_info: general.name     = 1.4B
0.00.055.295 I print_info: vocab type       = BPE
0.00.055.296 I print_info: n_vocab          = 50304
0.00.055.296 I print_info: n_merges         = 50009
0.00.055.296 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.296 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.296 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.297 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.297 I print_info: LF token         = 128 ''
0.00.055.297 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.297 I print_info: max token length = 1024
0.00.056.883 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.883 I load_tensors: offloading output layer to GPU
0.00.056.883 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.893 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.056.894 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.057.164 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.165 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.165 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.165 I llama_new_context_with_model: n_batch       = 2048
0.00.057.165 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.166 I llama_new_context_with_model: flash_attn    = 0
0.00.057.166 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.166 I llama_new_context_with_model: freq_scale    = 1
0.00.057.167 I ggml_metal_init: allocating
0.00.057.170 I ggml_metal_init: found device: Apple M4
0.00.057.172 I ggml_metal_init: picking default device: Apple M4
0.00.057.787 I ggml_metal_init: using embedded metal library
0.00.060.143 I ggml_metal_init: GPU name:   Apple M4
0.00.060.145 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.146 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.146 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.146 I ggml_metal_init: simdgroup reduction   = true
0.00.060.146 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.146 I ggml_metal_init: has bfloat            = true
0.00.060.147 I ggml_metal_init: use bfloat            = true
0.00.060.147 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.148 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.914 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.151 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.160 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.192 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.396 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.397 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.398 I llama_new_context_with_model: graph nodes  = 967
0.00.092.398 I llama_new_context_with_model: graph splits = 2
0.00.092.401 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.816 I main: llama threadpool init, n_threads = 4
0.00.751.850 I 
0.00.751.871 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.871 I 
0.00.752.107 I sampler seed: 1234
0.00.752.111 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.141 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.142 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.142 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.536.252 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.536.252 I llama_perf_context_print:        load time =     739.30 ms
0.01.536.253 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.19 tokens per second)
0.01.536.253 I llama_perf_context_print:        eval time =     738.07 ms /    63 runs   (   11.72 ms per token,    85.36 tokens per second)
0.01.536.254 I llama_perf_context_print:       total time =     784.44 ms /    70 tokens
0.01.536.429 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.251 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.177 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.181 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.188 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.191 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.774 I llama_model_loader: - type  f32:  194 tensors
0.00.025.774 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.774 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.775 I print_info: file format = GGUF V3 (latest)
0.00.025.775 I print_info: file type   = Q5_0
0.00.025.776 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.410 I load: special tokens cache size = 25
0.00.050.440 I load: token to piece cache size = 0.2984 MB
0.00.050.451 I print_info: arch             = gptneox
0.00.050.452 I print_info: n_vocab (hp)     = 50304
0.00.050.453 I print_info: vocab_only       = 0
0.00.050.453 I print_info: n_ctx_train      = 2048
0.00.050.453 I print_info: n_embd           = 2048
0.00.050.453 I print_info: n_layer          = 24
0.00.050.456 I print_info: n_head           = 16
0.00.050.457 I print_info: n_head_kv        = 16
0.00.050.459 I print_info: n_rot            = 32
0.00.050.459 I print_info: n_swa            = 0
0.00.050.459 I print_info: n_embd_head_k    = 128
0.00.050.460 I print_info: n_embd_head_v    = 128
0.00.050.460 I print_info: n_gqa            = 1
0.00.050.461 I print_info: n_embd_k_gqa     = 2048
0.00.050.462 I print_info: n_embd_v_gqa     = 2048
0.00.050.463 I print_info: f_norm_eps       = 1.0e-05
0.00.050.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.463 I print_info: f_logit_scale    = 0.0e+00
0.00.050.464 I print_info: n_ff             = 8192
0.00.050.465 I print_info: n_expert         = 0
0.00.050.465 I print_info: n_expert_used    = 0
0.00.050.465 I print_info: causal attn      = 1
0.00.050.466 I print_info: pooling type     = 0
0.00.050.466 I print_info: rope type        = 2
0.00.050.466 I print_info: rope scaling     = linear
0.00.050.466 I print_info: freq_base_train  = 10000.0
0.00.050.466 I print_info: freq_scale_train = 1
0.00.050.467 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.467 I print_info: rope_finetuned   = unknown
0.00.050.467 I print_info: ssm_d_conv       = 0
0.00.050.467 I print_info: ssm_d_inner      = 0
0.00.050.467 I print_info: ssm_d_state      = 0
0.00.050.467 I print_info: ssm_dt_rank      = 0
0.00.050.468 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.468 I print_info: model type       = 1.4B
0.00.050.469 I print_info: model params     = 1.41 B
0.00.050.469 I print_info: general.name     = 1.4B
0.00.050.469 I print_info: vocab type       = BPE
0.00.050.470 I print_info: n_vocab          = 50304
0.00.050.471 I print_info: n_merges         = 50009
0.00.050.471 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.471 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.471 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.471 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.472 I print_info: LF token         = 128 ''
0.00.050.472 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.472 I print_info: max token length = 1024
0.00.052.062 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.062 I load_tensors: offloading output layer to GPU
0.00.052.062 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.068 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.069 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.437 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.438 I llama_new_context_with_model: n_ctx         = 128
0.00.052.438 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.438 I llama_new_context_with_model: n_batch       = 128
0.00.052.438 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.438 I llama_new_context_with_model: flash_attn    = 0
0.00.052.439 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.439 I llama_new_context_with_model: freq_scale    = 1
0.00.052.439 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.440 I ggml_metal_init: allocating
0.00.052.443 I ggml_metal_init: found device: Apple M4
0.00.052.445 I ggml_metal_init: picking default device: Apple M4
0.00.053.033 I ggml_metal_init: using embedded metal library
0.00.055.349 I ggml_metal_init: GPU name:   Apple M4
0.00.055.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.351 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.352 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.352 I ggml_metal_init: simdgroup reduction   = true
0.00.055.352 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.352 I ggml_metal_init: has bfloat            = true
0.00.055.352 I ggml_metal_init: use bfloat            = true
0.00.055.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.959 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.356 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.359 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.379 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.220 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.221 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.221 I llama_new_context_with_model: graph nodes  = 967
0.00.067.221 I llama_new_context_with_model: graph splits = 2
0.00.067.222 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.626 I 
0.00.698.659 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.674 I perplexity: tokenizing the input ..
0.00.705.832 I perplexity: tokenization took 7.157 ms
0.00.705.836 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.030 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.842.198 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.842.223 I llama_perf_context_print:        load time =     688.37 ms
0.00.842.224 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.37 tokens per second)
0.00.842.225 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.226 I llama_perf_context_print:       total time =     143.60 ms /   129 tokens
0.00.842.632 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.077s
sys	0m0.097s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.744 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.484 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.486 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.486 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.487 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.487 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.487 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.488 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.489 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.489 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.490 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.490 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.494 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.494 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.494 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.036 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.037 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.038 I llama_model_loader: - type  f32:  194 tensors
0.00.025.038 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.039 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.039 I print_info: file format = GGUF V3 (latest)
0.00.025.039 I print_info: file type   = Q5_1
0.00.025.040 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.957 I load: special tokens cache size = 25
0.00.049.917 I load: token to piece cache size = 0.2984 MB
0.00.049.931 I print_info: arch             = gptneox
0.00.049.932 I print_info: n_vocab (hp)     = 50304
0.00.049.933 I print_info: vocab_only       = 0
0.00.049.933 I print_info: n_ctx_train      = 2048
0.00.049.933 I print_info: n_embd           = 2048
0.00.049.933 I print_info: n_layer          = 24
0.00.049.936 I print_info: n_head           = 16
0.00.049.937 I print_info: n_head_kv        = 16
0.00.049.937 I print_info: n_rot            = 32
0.00.049.937 I print_info: n_swa            = 0
0.00.049.937 I print_info: n_embd_head_k    = 128
0.00.049.938 I print_info: n_embd_head_v    = 128
0.00.049.938 I print_info: n_gqa            = 1
0.00.049.939 I print_info: n_embd_k_gqa     = 2048
0.00.049.940 I print_info: n_embd_v_gqa     = 2048
0.00.049.940 I print_info: f_norm_eps       = 1.0e-05
0.00.049.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.941 I print_info: f_logit_scale    = 0.0e+00
0.00.049.943 I print_info: n_ff             = 8192
0.00.049.943 I print_info: n_expert         = 0
0.00.049.943 I print_info: n_expert_used    = 0
0.00.049.943 I print_info: causal attn      = 1
0.00.049.943 I print_info: pooling type     = 0
0.00.049.944 I print_info: rope type        = 2
0.00.049.944 I print_info: rope scaling     = linear
0.00.049.948 I print_info: freq_base_train  = 10000.0
0.00.049.948 I print_info: freq_scale_train = 1
0.00.049.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.948 I print_info: rope_finetuned   = unknown
0.00.049.948 I print_info: ssm_d_conv       = 0
0.00.049.949 I print_info: ssm_d_inner      = 0
0.00.049.949 I print_info: ssm_d_state      = 0
0.00.049.949 I print_info: ssm_dt_rank      = 0
0.00.049.949 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.949 I print_info: model type       = 1.4B
0.00.049.950 I print_info: model params     = 1.41 B
0.00.049.951 I print_info: general.name     = 1.4B
0.00.049.951 I print_info: vocab type       = BPE
0.00.049.951 I print_info: n_vocab          = 50304
0.00.049.951 I print_info: n_merges         = 50009
0.00.049.951 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.952 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.952 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.952 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.952 I print_info: LF token         = 128 ''
0.00.049.952 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.953 I print_info: max token length = 1024
0.00.051.955 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.956 I load_tensors: offloading output layer to GPU
0.00.051.956 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.966 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.967 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.245 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.246 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.246 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.246 I llama_new_context_with_model: n_batch       = 2048
0.00.052.246 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.246 I llama_new_context_with_model: flash_attn    = 0
0.00.052.247 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.247 I llama_new_context_with_model: freq_scale    = 1
0.00.052.248 I ggml_metal_init: allocating
0.00.052.251 I ggml_metal_init: found device: Apple M4
0.00.052.252 I ggml_metal_init: picking default device: Apple M4
0.00.052.816 I ggml_metal_init: using embedded metal library
0.00.055.182 I ggml_metal_init: GPU name:   Apple M4
0.00.055.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.185 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.185 I ggml_metal_init: simdgroup reduction   = true
0.00.055.185 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.185 I ggml_metal_init: has bfloat            = true
0.00.055.185 I ggml_metal_init: use bfloat            = true
0.00.055.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.880 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.327 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.333 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.362 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.418 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.419 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.419 I llama_new_context_with_model: graph nodes  = 967
0.00.086.420 I llama_new_context_with_model: graph splits = 2
0.00.086.423 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.573 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.090 I main: llama threadpool init, n_threads = 4
0.00.689.133 I 
0.00.689.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.184 I 
0.00.689.424 I sampler seed: 1234
0.00.689.430 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.441 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.441 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.441 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.531.114 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.01.531.115 I llama_perf_context_print:        load time =     680.34 ms
0.01.531.116 I llama_perf_context_print: prompt eval time =      42.19 ms /     7 tokens (    6.03 ms per token,   165.92 tokens per second)
0.01.531.116 I llama_perf_context_print:        eval time =     796.45 ms /    63 runs   (   12.64 ms per token,    79.10 tokens per second)
0.01.531.117 I llama_perf_context_print:       total time =     842.03 ms /    70 tokens
0.01.531.355 I ggml_metal_free: deallocating

real	0m1.548s
user	0m0.108s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.737 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.194 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.202 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.206 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.207 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.207 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.207 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.208 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.208 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.211 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.979 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.712 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.713 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.713 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.713 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.714 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.714 I llama_model_loader: - type  f32:  194 tensors
0.00.023.714 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.714 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.715 I print_info: file format = GGUF V3 (latest)
0.00.023.715 I print_info: file type   = Q5_1
0.00.023.716 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.456 I load: special tokens cache size = 25
0.00.048.353 I load: token to piece cache size = 0.2984 MB
0.00.048.368 I print_info: arch             = gptneox
0.00.048.369 I print_info: n_vocab (hp)     = 50304
0.00.048.369 I print_info: vocab_only       = 0
0.00.048.369 I print_info: n_ctx_train      = 2048
0.00.048.369 I print_info: n_embd           = 2048
0.00.048.369 I print_info: n_layer          = 24
0.00.048.372 I print_info: n_head           = 16
0.00.048.373 I print_info: n_head_kv        = 16
0.00.048.373 I print_info: n_rot            = 32
0.00.048.375 I print_info: n_swa            = 0
0.00.048.375 I print_info: n_embd_head_k    = 128
0.00.048.375 I print_info: n_embd_head_v    = 128
0.00.048.376 I print_info: n_gqa            = 1
0.00.048.377 I print_info: n_embd_k_gqa     = 2048
0.00.048.377 I print_info: n_embd_v_gqa     = 2048
0.00.048.378 I print_info: f_norm_eps       = 1.0e-05
0.00.048.378 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.378 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.379 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.379 I print_info: f_logit_scale    = 0.0e+00
0.00.048.380 I print_info: n_ff             = 8192
0.00.048.380 I print_info: n_expert         = 0
0.00.048.380 I print_info: n_expert_used    = 0
0.00.048.380 I print_info: causal attn      = 1
0.00.048.380 I print_info: pooling type     = 0
0.00.048.380 I print_info: rope type        = 2
0.00.048.381 I print_info: rope scaling     = linear
0.00.048.381 I print_info: freq_base_train  = 10000.0
0.00.048.381 I print_info: freq_scale_train = 1
0.00.048.381 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.382 I print_info: rope_finetuned   = unknown
0.00.048.382 I print_info: ssm_d_conv       = 0
0.00.048.382 I print_info: ssm_d_inner      = 0
0.00.048.382 I print_info: ssm_d_state      = 0
0.00.048.382 I print_info: ssm_dt_rank      = 0
0.00.048.382 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.382 I print_info: model type       = 1.4B
0.00.048.383 I print_info: model params     = 1.41 B
0.00.048.383 I print_info: general.name     = 1.4B
0.00.048.383 I print_info: vocab type       = BPE
0.00.048.384 I print_info: n_vocab          = 50304
0.00.048.384 I print_info: n_merges         = 50009
0.00.048.384 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.384 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.384 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.385 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.385 I print_info: LF token         = 128 ''
0.00.048.385 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.385 I print_info: max token length = 1024
0.00.050.390 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.390 I load_tensors: offloading output layer to GPU
0.00.050.390 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.401 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.402 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.050.673 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.674 I llama_new_context_with_model: n_ctx         = 128
0.00.050.674 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.674 I llama_new_context_with_model: n_batch       = 128
0.00.050.674 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.674 I llama_new_context_with_model: flash_attn    = 0
0.00.050.675 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.675 I llama_new_context_with_model: freq_scale    = 1
0.00.050.675 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.676 I ggml_metal_init: allocating
0.00.050.679 I ggml_metal_init: found device: Apple M4
0.00.050.681 I ggml_metal_init: picking default device: Apple M4
0.00.051.242 I ggml_metal_init: using embedded metal library
0.00.053.571 I ggml_metal_init: GPU name:   Apple M4
0.00.053.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.574 I ggml_metal_init: simdgroup reduction   = true
0.00.053.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.574 I ggml_metal_init: has bfloat            = true
0.00.053.574 I ggml_metal_init: use bfloat            = true
0.00.053.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.057 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.297 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.301 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.326 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.275 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.276 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.276 I llama_new_context_with_model: graph nodes  = 967
0.00.065.277 I llama_new_context_with_model: graph splits = 2
0.00.065.278 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.278 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.447 I 
0.00.661.476 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.488 I perplexity: tokenizing the input ..
0.00.669.587 I perplexity: tokenization took 8.097 ms
0.00.669.592 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.563 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.805.724 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.805.748 I llama_perf_context_print:        load time =     652.70 ms
0.00.805.749 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   950.00 tokens per second)
0.00.805.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.750 I llama_perf_context_print:       total time =     144.30 ms /   129 tokens
0.00.806.160 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.077s
sys	0m0.121s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.012.065 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.764 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.767 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.646 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.690 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.691 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.691 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.692 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.692 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.693 I llama_model_loader: - type  f32:  194 tensors
0.00.027.693 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.693 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.694 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.694 I print_info: file format = GGUF V3 (latest)
0.00.027.695 I print_info: file type   = Q2_K - Medium
0.00.027.700 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.047.954 I load: special tokens cache size = 25
0.00.054.216 I load: token to piece cache size = 0.2984 MB
0.00.054.236 I print_info: arch             = gptneox
0.00.054.236 I print_info: n_vocab (hp)     = 50304
0.00.054.236 I print_info: vocab_only       = 0
0.00.054.237 I print_info: n_ctx_train      = 2048
0.00.054.237 I print_info: n_embd           = 2048
0.00.054.237 I print_info: n_layer          = 24
0.00.054.241 I print_info: n_head           = 16
0.00.054.242 I print_info: n_head_kv        = 16
0.00.054.242 I print_info: n_rot            = 32
0.00.054.242 I print_info: n_swa            = 0
0.00.054.242 I print_info: n_embd_head_k    = 128
0.00.054.242 I print_info: n_embd_head_v    = 128
0.00.054.243 I print_info: n_gqa            = 1
0.00.054.244 I print_info: n_embd_k_gqa     = 2048
0.00.054.244 I print_info: n_embd_v_gqa     = 2048
0.00.054.245 I print_info: f_norm_eps       = 1.0e-05
0.00.054.245 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.245 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.245 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.246 I print_info: f_logit_scale    = 0.0e+00
0.00.054.246 I print_info: n_ff             = 8192
0.00.054.246 I print_info: n_expert         = 0
0.00.054.246 I print_info: n_expert_used    = 0
0.00.054.246 I print_info: causal attn      = 1
0.00.054.247 I print_info: pooling type     = 0
0.00.054.247 I print_info: rope type        = 2
0.00.054.247 I print_info: rope scaling     = linear
0.00.054.247 I print_info: freq_base_train  = 10000.0
0.00.054.248 I print_info: freq_scale_train = 1
0.00.054.248 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.248 I print_info: rope_finetuned   = unknown
0.00.054.248 I print_info: ssm_d_conv       = 0
0.00.054.248 I print_info: ssm_d_inner      = 0
0.00.054.248 I print_info: ssm_d_state      = 0
0.00.054.248 I print_info: ssm_dt_rank      = 0
0.00.054.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.249 I print_info: model type       = 1.4B
0.00.054.249 I print_info: model params     = 1.41 B
0.00.054.249 I print_info: general.name     = 1.4B
0.00.054.252 I print_info: vocab type       = BPE
0.00.054.252 I print_info: n_vocab          = 50304
0.00.054.252 I print_info: n_merges         = 50009
0.00.054.252 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.252 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.255 I print_info: LF token         = 128 ''
0.00.054.255 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.256 I print_info: max token length = 1024
0.00.056.278 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.279 I load_tensors: offloading output layer to GPU
0.00.056.279 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.290 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.056.291 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.056.603 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.603 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.603 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.604 I llama_new_context_with_model: n_batch       = 2048
0.00.056.604 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.604 I llama_new_context_with_model: flash_attn    = 0
0.00.056.604 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.605 I llama_new_context_with_model: freq_scale    = 1
0.00.056.605 I ggml_metal_init: allocating
0.00.056.608 I ggml_metal_init: found device: Apple M4
0.00.056.609 I ggml_metal_init: picking default device: Apple M4
0.00.057.320 I ggml_metal_init: using embedded metal library
0.00.059.836 I ggml_metal_init: GPU name:   Apple M4
0.00.059.838 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.839 I ggml_metal_init: simdgroup reduction   = true
0.00.059.839 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.840 I ggml_metal_init: has bfloat            = true
0.00.059.840 I ggml_metal_init: use bfloat            = true
0.00.059.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.348 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.064 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.075 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.106 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.123 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.124 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.125 I llama_new_context_with_model: graph nodes  = 967
0.00.091.125 I llama_new_context_with_model: graph splits = 2
0.00.091.128 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.274 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.275 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.431.008 I main: llama threadpool init, n_threads = 4
0.00.431.058 I 
0.00.431.083 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.431.084 I 
0.00.431.305 I sampler seed: 1234
0.00.431.309 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.431.349 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.431.352 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.431.352 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.098.990 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50678.09 tokens per second)
0.01.098.990 I llama_perf_context_print:        load time =     418.94 ms
0.01.098.991 I llama_perf_context_print: prompt eval time =      35.94 ms /     7 tokens (    5.13 ms per token,   194.75 tokens per second)
0.01.098.993 I llama_perf_context_print:        eval time =     629.37 ms /    63 runs   (    9.99 ms per token,   100.10 tokens per second)
0.01.098.994 I llama_perf_context_print:       total time =     667.99 ms /    70 tokens
0.01.099.295 I ggml_metal_free: deallocating

real	0m1.118s
user	0m0.110s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.870 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.721 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.723 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.724 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.724 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.725 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.725 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.728 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.728 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.729 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.729 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.729 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.730 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.557 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.313 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.313 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.314 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.314 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.315 I llama_model_loader: - type  f32:  194 tensors
0.00.025.315 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.315 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.316 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.316 I print_info: file format = GGUF V3 (latest)
0.00.025.317 I print_info: file type   = Q2_K - Medium
0.00.025.319 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.799 I load: special tokens cache size = 25
0.00.050.888 I load: token to piece cache size = 0.2984 MB
0.00.050.903 I print_info: arch             = gptneox
0.00.050.904 I print_info: n_vocab (hp)     = 50304
0.00.050.904 I print_info: vocab_only       = 0
0.00.050.904 I print_info: n_ctx_train      = 2048
0.00.050.904 I print_info: n_embd           = 2048
0.00.050.904 I print_info: n_layer          = 24
0.00.050.908 I print_info: n_head           = 16
0.00.050.909 I print_info: n_head_kv        = 16
0.00.050.909 I print_info: n_rot            = 32
0.00.050.909 I print_info: n_swa            = 0
0.00.050.909 I print_info: n_embd_head_k    = 128
0.00.050.909 I print_info: n_embd_head_v    = 128
0.00.050.910 I print_info: n_gqa            = 1
0.00.050.911 I print_info: n_embd_k_gqa     = 2048
0.00.050.912 I print_info: n_embd_v_gqa     = 2048
0.00.050.912 I print_info: f_norm_eps       = 1.0e-05
0.00.050.913 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.913 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.913 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.913 I print_info: f_logit_scale    = 0.0e+00
0.00.050.914 I print_info: n_ff             = 8192
0.00.050.914 I print_info: n_expert         = 0
0.00.050.914 I print_info: n_expert_used    = 0
0.00.050.914 I print_info: causal attn      = 1
0.00.050.914 I print_info: pooling type     = 0
0.00.050.914 I print_info: rope type        = 2
0.00.050.915 I print_info: rope scaling     = linear
0.00.050.915 I print_info: freq_base_train  = 10000.0
0.00.050.915 I print_info: freq_scale_train = 1
0.00.050.916 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.916 I print_info: rope_finetuned   = unknown
0.00.050.916 I print_info: ssm_d_conv       = 0
0.00.050.916 I print_info: ssm_d_inner      = 0
0.00.050.916 I print_info: ssm_d_state      = 0
0.00.050.916 I print_info: ssm_dt_rank      = 0
0.00.050.916 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.918 I print_info: model type       = 1.4B
0.00.050.918 I print_info: model params     = 1.41 B
0.00.050.918 I print_info: general.name     = 1.4B
0.00.050.919 I print_info: vocab type       = BPE
0.00.050.919 I print_info: n_vocab          = 50304
0.00.050.919 I print_info: n_merges         = 50009
0.00.050.919 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.919 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.919 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.920 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.920 I print_info: LF token         = 128 ''
0.00.050.920 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.920 I print_info: max token length = 1024
0.00.052.836 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.836 I load_tensors: offloading output layer to GPU
0.00.052.837 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.848 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.849 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.127 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.128 I llama_new_context_with_model: n_ctx         = 128
0.00.053.128 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.128 I llama_new_context_with_model: n_batch       = 128
0.00.053.128 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.129 I llama_new_context_with_model: flash_attn    = 0
0.00.053.129 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.129 I llama_new_context_with_model: freq_scale    = 1
0.00.053.130 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.130 I ggml_metal_init: allocating
0.00.053.133 I ggml_metal_init: found device: Apple M4
0.00.053.135 I ggml_metal_init: picking default device: Apple M4
0.00.053.696 I ggml_metal_init: using embedded metal library
0.00.056.051 I ggml_metal_init: GPU name:   Apple M4
0.00.056.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.054 I ggml_metal_init: simdgroup reduction   = true
0.00.056.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.054 I ggml_metal_init: has bfloat            = true
0.00.056.054 I ggml_metal_init: use bfloat            = true
0.00.056.054 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.897 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.188 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.190 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.215 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.180 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.181 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.181 I llama_new_context_with_model: graph nodes  = 967
0.00.068.181 I llama_new_context_with_model: graph splits = 2
0.00.068.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.372.801 I 
0.00.372.847 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.372.876 I perplexity: tokenizing the input ..
0.00.380.393 I perplexity: tokenization took 7.516 ms
0.00.380.396 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.512.714 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.513.898 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.513.922 I llama_perf_context_print:        load time =     362.92 ms
0.00.513.923 I llama_perf_context_print: prompt eval time =     132.09 ms /   128 tokens (    1.03 ms per token,   969.04 tokens per second)
0.00.513.924 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.513.925 I llama_perf_context_print:       total time =     141.12 ms /   129 tokens
0.00.514.444 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.078s
sys	0m0.063s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.741 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.963 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.970 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.971 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.971 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.971 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.975 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.874 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.703 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.703 I llama_model_loader: - type  f32:  194 tensors
0.00.025.703 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.704 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.704 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.704 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.705 I print_info: file format = GGUF V3 (latest)
0.00.025.709 I print_info: file type   = Q3_K - Medium
0.00.025.710 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.460 I load: special tokens cache size = 25
0.00.051.417 I load: token to piece cache size = 0.2984 MB
0.00.051.431 I print_info: arch             = gptneox
0.00.051.432 I print_info: n_vocab (hp)     = 50304
0.00.051.433 I print_info: vocab_only       = 0
0.00.051.433 I print_info: n_ctx_train      = 2048
0.00.051.433 I print_info: n_embd           = 2048
0.00.051.433 I print_info: n_layer          = 24
0.00.051.436 I print_info: n_head           = 16
0.00.051.437 I print_info: n_head_kv        = 16
0.00.051.437 I print_info: n_rot            = 32
0.00.051.437 I print_info: n_swa            = 0
0.00.051.438 I print_info: n_embd_head_k    = 128
0.00.051.438 I print_info: n_embd_head_v    = 128
0.00.051.438 I print_info: n_gqa            = 1
0.00.051.439 I print_info: n_embd_k_gqa     = 2048
0.00.051.440 I print_info: n_embd_v_gqa     = 2048
0.00.051.440 I print_info: f_norm_eps       = 1.0e-05
0.00.051.441 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.441 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.441 I print_info: f_logit_scale    = 0.0e+00
0.00.051.443 I print_info: n_ff             = 8192
0.00.051.443 I print_info: n_expert         = 0
0.00.051.443 I print_info: n_expert_used    = 0
0.00.051.443 I print_info: causal attn      = 1
0.00.051.443 I print_info: pooling type     = 0
0.00.051.443 I print_info: rope type        = 2
0.00.051.443 I print_info: rope scaling     = linear
0.00.051.445 I print_info: freq_base_train  = 10000.0
0.00.051.445 I print_info: freq_scale_train = 1
0.00.051.445 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.446 I print_info: rope_finetuned   = unknown
0.00.051.446 I print_info: ssm_d_conv       = 0
0.00.051.446 I print_info: ssm_d_inner      = 0
0.00.051.446 I print_info: ssm_d_state      = 0
0.00.051.446 I print_info: ssm_dt_rank      = 0
0.00.051.446 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.446 I print_info: model type       = 1.4B
0.00.051.447 I print_info: model params     = 1.41 B
0.00.051.447 I print_info: general.name     = 1.4B
0.00.051.447 I print_info: vocab type       = BPE
0.00.051.447 I print_info: n_vocab          = 50304
0.00.051.451 I print_info: n_merges         = 50009
0.00.051.451 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.451 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.451 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.451 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.452 I print_info: LF token         = 128 ''
0.00.051.452 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.452 I print_info: max token length = 1024
0.00.053.457 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.458 I load_tensors: offloading output layer to GPU
0.00.053.458 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.468 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.469 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.779 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.780 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.780 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.780 I llama_new_context_with_model: n_batch       = 2048
0.00.053.780 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.780 I llama_new_context_with_model: flash_attn    = 0
0.00.053.781 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.781 I llama_new_context_with_model: freq_scale    = 1
0.00.053.782 I ggml_metal_init: allocating
0.00.053.785 I ggml_metal_init: found device: Apple M4
0.00.053.787 I ggml_metal_init: picking default device: Apple M4
0.00.054.380 I ggml_metal_init: using embedded metal library
0.00.056.800 I ggml_metal_init: GPU name:   Apple M4
0.00.056.801 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.802 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.802 I ggml_metal_init: simdgroup reduction   = true
0.00.056.803 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.803 I ggml_metal_init: has bfloat            = true
0.00.056.803 I ggml_metal_init: use bfloat            = true
0.00.056.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.362 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.978 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.985 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.019 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.043 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.045 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.046 I llama_new_context_with_model: graph nodes  = 967
0.00.088.046 I llama_new_context_with_model: graph splits = 2
0.00.088.049 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.572 I main: llama threadpool init, n_threads = 4
0.00.538.613 I 
0.00.538.632 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.633 I 
0.00.538.860 I sampler seed: 1234
0.00.538.864 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.538.890 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.538.891 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.538.891 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.288.219 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.288.220 I llama_perf_context_print:        load time =     529.83 ms
0.01.288.221 I llama_perf_context_print: prompt eval time =      40.53 ms /     7 tokens (    5.79 ms per token,   172.69 tokens per second)
0.01.288.222 I llama_perf_context_print:        eval time =     705.77 ms /    63 runs   (   11.20 ms per token,    89.26 tokens per second)
0.01.288.222 I llama_perf_context_print:       total time =     749.65 ms /    70 tokens
0.01.288.424 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.649 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.443 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.448 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.450 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.450 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.451 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.451 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.451 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.452 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.455 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.455 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.456 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.456 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.457 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.460 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.462 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.049 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.049 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.049 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.050 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.050 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.051 I llama_model_loader: - type  f32:  194 tensors
0.00.024.051 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.051 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.051 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.051 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.052 I print_info: file format = GGUF V3 (latest)
0.00.024.053 I print_info: file type   = Q3_K - Medium
0.00.024.053 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.651 I load: special tokens cache size = 25
0.00.048.574 I load: token to piece cache size = 0.2984 MB
0.00.048.589 I print_info: arch             = gptneox
0.00.048.590 I print_info: n_vocab (hp)     = 50304
0.00.048.590 I print_info: vocab_only       = 0
0.00.048.591 I print_info: n_ctx_train      = 2048
0.00.048.591 I print_info: n_embd           = 2048
0.00.048.591 I print_info: n_layer          = 24
0.00.048.594 I print_info: n_head           = 16
0.00.048.595 I print_info: n_head_kv        = 16
0.00.048.595 I print_info: n_rot            = 32
0.00.048.595 I print_info: n_swa            = 0
0.00.048.595 I print_info: n_embd_head_k    = 128
0.00.048.595 I print_info: n_embd_head_v    = 128
0.00.048.596 I print_info: n_gqa            = 1
0.00.048.597 I print_info: n_embd_k_gqa     = 2048
0.00.048.598 I print_info: n_embd_v_gqa     = 2048
0.00.048.598 I print_info: f_norm_eps       = 1.0e-05
0.00.048.599 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.599 I print_info: f_logit_scale    = 0.0e+00
0.00.048.600 I print_info: n_ff             = 8192
0.00.048.600 I print_info: n_expert         = 0
0.00.048.600 I print_info: n_expert_used    = 0
0.00.048.600 I print_info: causal attn      = 1
0.00.048.600 I print_info: pooling type     = 0
0.00.048.601 I print_info: rope type        = 2
0.00.048.601 I print_info: rope scaling     = linear
0.00.048.601 I print_info: freq_base_train  = 10000.0
0.00.048.601 I print_info: freq_scale_train = 1
0.00.048.601 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.602 I print_info: rope_finetuned   = unknown
0.00.048.602 I print_info: ssm_d_conv       = 0
0.00.048.602 I print_info: ssm_d_inner      = 0
0.00.048.602 I print_info: ssm_d_state      = 0
0.00.048.604 I print_info: ssm_dt_rank      = 0
0.00.048.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.604 I print_info: model type       = 1.4B
0.00.048.604 I print_info: model params     = 1.41 B
0.00.048.604 I print_info: general.name     = 1.4B
0.00.048.605 I print_info: vocab type       = BPE
0.00.048.605 I print_info: n_vocab          = 50304
0.00.048.605 I print_info: n_merges         = 50009
0.00.048.605 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.605 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.606 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.606 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.606 I print_info: LF token         = 128 ''
0.00.048.606 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.607 I print_info: max token length = 1024
0.00.050.530 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.530 I load_tensors: offloading output layer to GPU
0.00.050.531 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.541 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.542 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.825 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.826 I llama_new_context_with_model: n_ctx         = 128
0.00.050.826 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.826 I llama_new_context_with_model: n_batch       = 128
0.00.050.826 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.826 I llama_new_context_with_model: flash_attn    = 0
0.00.050.827 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.827 I llama_new_context_with_model: freq_scale    = 1
0.00.050.827 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.828 I ggml_metal_init: allocating
0.00.050.831 I ggml_metal_init: found device: Apple M4
0.00.050.832 I ggml_metal_init: picking default device: Apple M4
0.00.051.435 I ggml_metal_init: using embedded metal library
0.00.053.765 I ggml_metal_init: GPU name:   Apple M4
0.00.053.767 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.767 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.767 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.767 I ggml_metal_init: simdgroup reduction   = true
0.00.053.768 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.768 I ggml_metal_init: has bfloat            = true
0.00.053.768 I ggml_metal_init: use bfloat            = true
0.00.053.768 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.769 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.327 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.562 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.564 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.600 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.558 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.559 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.560 I llama_new_context_with_model: graph nodes  = 967
0.00.065.560 I llama_new_context_with_model: graph splits = 2
0.00.065.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.561 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.367 I 
0.00.478.421 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.459 I perplexity: tokenizing the input ..
0.00.486.755 I perplexity: tokenization took 8.294 ms
0.00.486.761 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.618.989 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.620.144 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.620.172 I llama_perf_context_print:        load time =     469.71 ms
0.00.620.173 I llama_perf_context_print: prompt eval time =     132.00 ms /   128 tokens (    1.03 ms per token,   969.73 tokens per second)
0.00.620.174 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.620.175 I llama_perf_context_print:       total time =     141.81 ms /   129 tokens
0.00.620.746 I ggml_metal_free: deallocating

real	0m0.634s
user	0m0.077s
sys	0m0.088s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.760 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.204 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.209 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.211 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.212 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.215 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.215 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.216 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.216 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.219 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.926 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.926 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.926 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.927 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.927 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.928 I llama_model_loader: - type  f32:  194 tensors
0.00.024.928 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.928 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.928 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.929 I print_info: file format = GGUF V3 (latest)
0.00.024.929 I print_info: file type   = Q4_K - Medium
0.00.024.930 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.590 I load: special tokens cache size = 25
0.00.050.697 I load: token to piece cache size = 0.2984 MB
0.00.050.713 I print_info: arch             = gptneox
0.00.050.714 I print_info: n_vocab (hp)     = 50304
0.00.050.714 I print_info: vocab_only       = 0
0.00.050.714 I print_info: n_ctx_train      = 2048
0.00.050.714 I print_info: n_embd           = 2048
0.00.050.715 I print_info: n_layer          = 24
0.00.050.717 I print_info: n_head           = 16
0.00.050.718 I print_info: n_head_kv        = 16
0.00.050.720 I print_info: n_rot            = 32
0.00.050.720 I print_info: n_swa            = 0
0.00.050.720 I print_info: n_embd_head_k    = 128
0.00.050.720 I print_info: n_embd_head_v    = 128
0.00.050.721 I print_info: n_gqa            = 1
0.00.050.722 I print_info: n_embd_k_gqa     = 2048
0.00.050.723 I print_info: n_embd_v_gqa     = 2048
0.00.050.723 I print_info: f_norm_eps       = 1.0e-05
0.00.050.725 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.725 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.725 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.725 I print_info: f_logit_scale    = 0.0e+00
0.00.050.726 I print_info: n_ff             = 8192
0.00.050.726 I print_info: n_expert         = 0
0.00.050.726 I print_info: n_expert_used    = 0
0.00.050.726 I print_info: causal attn      = 1
0.00.050.726 I print_info: pooling type     = 0
0.00.050.726 I print_info: rope type        = 2
0.00.050.731 I print_info: rope scaling     = linear
0.00.050.732 I print_info: freq_base_train  = 10000.0
0.00.050.732 I print_info: freq_scale_train = 1
0.00.050.732 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.732 I print_info: rope_finetuned   = unknown
0.00.050.732 I print_info: ssm_d_conv       = 0
0.00.050.732 I print_info: ssm_d_inner      = 0
0.00.050.734 I print_info: ssm_d_state      = 0
0.00.050.734 I print_info: ssm_dt_rank      = 0
0.00.050.734 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.734 I print_info: model type       = 1.4B
0.00.050.734 I print_info: model params     = 1.41 B
0.00.050.735 I print_info: general.name     = 1.4B
0.00.050.735 I print_info: vocab type       = BPE
0.00.050.735 I print_info: n_vocab          = 50304
0.00.050.735 I print_info: n_merges         = 50009
0.00.050.736 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.736 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.736 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.736 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.739 I print_info: LF token         = 128 ''
0.00.050.739 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.739 I print_info: max token length = 1024
0.00.052.780 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.780 I load_tensors: offloading output layer to GPU
0.00.052.780 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.791 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.792 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.185 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.186 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.186 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.186 I llama_new_context_with_model: n_batch       = 2048
0.00.053.187 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.187 I llama_new_context_with_model: flash_attn    = 0
0.00.053.187 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.187 I llama_new_context_with_model: freq_scale    = 1
0.00.053.188 I ggml_metal_init: allocating
0.00.053.191 I ggml_metal_init: found device: Apple M4
0.00.053.192 I ggml_metal_init: picking default device: Apple M4
0.00.053.793 I ggml_metal_init: using embedded metal library
0.00.056.203 I ggml_metal_init: GPU name:   Apple M4
0.00.056.204 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.205 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.205 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.205 I ggml_metal_init: simdgroup reduction   = true
0.00.056.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.206 I ggml_metal_init: has bfloat            = true
0.00.056.206 I ggml_metal_init: use bfloat            = true
0.00.056.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.207 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.157 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.673 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.682 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.715 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.738 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.739 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.740 I llama_new_context_with_model: graph nodes  = 967
0.00.086.740 I llama_new_context_with_model: graph splits = 2
0.00.086.745 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.873 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.725.596 I main: llama threadpool init, n_threads = 4
0.00.725.634 I 
0.00.725.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.725.656 I 
0.00.725.888 I sampler seed: 1234
0.00.725.892 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.725.904 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.725.904 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.725.904 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.484.817 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.484.817 I llama_perf_context_print:        load time =     716.83 ms
0.01.484.818 I llama_perf_context_print: prompt eval time =      51.09 ms /     7 tokens (    7.30 ms per token,   137.02 tokens per second)
0.01.484.818 I llama_perf_context_print:        eval time =     704.76 ms /    63 runs   (   11.19 ms per token,    89.39 tokens per second)
0.01.484.819 I llama_perf_context_print:       total time =     759.22 ms /    70 tokens
0.01.485.021 I ggml_metal_free: deallocating

real	0m1.503s
user	0m0.112s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.068 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.931 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.937 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.938 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.941 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.941 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.941 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.942 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.944 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.944 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.788 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.798 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.564 I llama_model_loader: - type  f32:  194 tensors
0.00.024.564 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.564 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.565 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.565 I print_info: file format = GGUF V3 (latest)
0.00.024.566 I print_info: file type   = Q4_K - Medium
0.00.024.568 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.857 I load: special tokens cache size = 25
0.00.049.864 I load: token to piece cache size = 0.2984 MB
0.00.049.878 I print_info: arch             = gptneox
0.00.049.879 I print_info: n_vocab (hp)     = 50304
0.00.049.879 I print_info: vocab_only       = 0
0.00.049.879 I print_info: n_ctx_train      = 2048
0.00.049.880 I print_info: n_embd           = 2048
0.00.049.880 I print_info: n_layer          = 24
0.00.049.883 I print_info: n_head           = 16
0.00.049.883 I print_info: n_head_kv        = 16
0.00.049.884 I print_info: n_rot            = 32
0.00.049.884 I print_info: n_swa            = 0
0.00.049.884 I print_info: n_embd_head_k    = 128
0.00.049.884 I print_info: n_embd_head_v    = 128
0.00.049.885 I print_info: n_gqa            = 1
0.00.049.886 I print_info: n_embd_k_gqa     = 2048
0.00.049.887 I print_info: n_embd_v_gqa     = 2048
0.00.049.887 I print_info: f_norm_eps       = 1.0e-05
0.00.049.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.888 I print_info: f_logit_scale    = 0.0e+00
0.00.049.889 I print_info: n_ff             = 8192
0.00.049.891 I print_info: n_expert         = 0
0.00.049.891 I print_info: n_expert_used    = 0
0.00.049.891 I print_info: causal attn      = 1
0.00.049.891 I print_info: pooling type     = 0
0.00.049.891 I print_info: rope type        = 2
0.00.049.892 I print_info: rope scaling     = linear
0.00.049.893 I print_info: freq_base_train  = 10000.0
0.00.049.893 I print_info: freq_scale_train = 1
0.00.049.893 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.893 I print_info: rope_finetuned   = unknown
0.00.049.893 I print_info: ssm_d_conv       = 0
0.00.049.893 I print_info: ssm_d_inner      = 0
0.00.049.894 I print_info: ssm_d_state      = 0
0.00.049.894 I print_info: ssm_dt_rank      = 0
0.00.049.894 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.894 I print_info: model type       = 1.4B
0.00.049.894 I print_info: model params     = 1.41 B
0.00.049.894 I print_info: general.name     = 1.4B
0.00.049.895 I print_info: vocab type       = BPE
0.00.049.895 I print_info: n_vocab          = 50304
0.00.049.895 I print_info: n_merges         = 50009
0.00.049.895 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.895 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.895 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.896 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.896 I print_info: LF token         = 128 ''
0.00.049.896 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.896 I print_info: max token length = 1024
0.00.051.948 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.948 I load_tensors: offloading output layer to GPU
0.00.051.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.959 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.960 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.255 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.256 I llama_new_context_with_model: n_ctx         = 128
0.00.052.256 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.257 I llama_new_context_with_model: n_batch       = 128
0.00.052.257 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.257 I llama_new_context_with_model: flash_attn    = 0
0.00.052.257 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.257 I llama_new_context_with_model: freq_scale    = 1
0.00.052.258 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.258 I ggml_metal_init: allocating
0.00.052.261 I ggml_metal_init: found device: Apple M4
0.00.052.263 I ggml_metal_init: picking default device: Apple M4
0.00.052.830 I ggml_metal_init: using embedded metal library
0.00.055.194 I ggml_metal_init: GPU name:   Apple M4
0.00.055.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.195 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.196 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.196 I ggml_metal_init: simdgroup reduction   = true
0.00.055.196 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.196 I ggml_metal_init: has bfloat            = true
0.00.055.196 I ggml_metal_init: use bfloat            = true
0.00.055.197 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.198 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.990 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.294 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.299 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.325 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.290 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.291 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.291 I llama_new_context_with_model: graph nodes  = 967
0.00.067.292 I llama_new_context_with_model: graph splits = 2
0.00.067.293 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.293 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.092 I 
0.00.543.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.172 I perplexity: tokenizing the input ..
0.00.550.844 I perplexity: tokenization took 7.67 ms
0.00.550.848 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.685.218 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.686.391 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.686.423 I llama_perf_context_print:        load time =     534.01 ms
0.00.686.424 I llama_perf_context_print: prompt eval time =     134.14 ms /   128 tokens (    1.05 ms per token,   954.22 tokens per second)
0.00.686.424 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.686.425 I llama_perf_context_print:       total time =     143.34 ms /   129 tokens
0.00.686.939 I ggml_metal_free: deallocating

real	0m0.701s
user	0m0.078s
sys	0m0.092s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.154 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.708 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.713 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.716 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.717 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.720 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.720 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.721 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.517 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.560 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.322 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.322 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.323 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.323 I llama_model_loader: - type  f32:  194 tensors
0.00.024.323 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.324 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.324 I print_info: file format = GGUF V3 (latest)
0.00.024.325 I print_info: file type   = Q5_K - Medium
0.00.024.325 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.122 I load: special tokens cache size = 25
0.00.049.114 I load: token to piece cache size = 0.2984 MB
0.00.049.131 I print_info: arch             = gptneox
0.00.049.132 I print_info: n_vocab (hp)     = 50304
0.00.049.132 I print_info: vocab_only       = 0
0.00.049.132 I print_info: n_ctx_train      = 2048
0.00.049.132 I print_info: n_embd           = 2048
0.00.049.132 I print_info: n_layer          = 24
0.00.049.135 I print_info: n_head           = 16
0.00.049.136 I print_info: n_head_kv        = 16
0.00.049.136 I print_info: n_rot            = 32
0.00.049.137 I print_info: n_swa            = 0
0.00.049.139 I print_info: n_embd_head_k    = 128
0.00.049.140 I print_info: n_embd_head_v    = 128
0.00.049.140 I print_info: n_gqa            = 1
0.00.049.142 I print_info: n_embd_k_gqa     = 2048
0.00.049.143 I print_info: n_embd_v_gqa     = 2048
0.00.049.144 I print_info: f_norm_eps       = 1.0e-05
0.00.049.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.144 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.144 I print_info: f_logit_scale    = 0.0e+00
0.00.049.145 I print_info: n_ff             = 8192
0.00.049.145 I print_info: n_expert         = 0
0.00.049.145 I print_info: n_expert_used    = 0
0.00.049.146 I print_info: causal attn      = 1
0.00.049.146 I print_info: pooling type     = 0
0.00.049.146 I print_info: rope type        = 2
0.00.049.146 I print_info: rope scaling     = linear
0.00.049.147 I print_info: freq_base_train  = 10000.0
0.00.049.148 I print_info: freq_scale_train = 1
0.00.049.148 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.148 I print_info: rope_finetuned   = unknown
0.00.049.148 I print_info: ssm_d_conv       = 0
0.00.049.148 I print_info: ssm_d_inner      = 0
0.00.049.149 I print_info: ssm_d_state      = 0
0.00.049.149 I print_info: ssm_dt_rank      = 0
0.00.049.149 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.149 I print_info: model type       = 1.4B
0.00.049.150 I print_info: model params     = 1.41 B
0.00.049.150 I print_info: general.name     = 1.4B
0.00.049.150 I print_info: vocab type       = BPE
0.00.049.150 I print_info: n_vocab          = 50304
0.00.049.152 I print_info: n_merges         = 50009
0.00.049.152 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.152 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.152 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.153 I print_info: LF token         = 128 ''
0.00.049.153 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.154 I print_info: max token length = 1024
0.00.051.145 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.145 I load_tensors: offloading output layer to GPU
0.00.051.146 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.156 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.157 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.483 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.484 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.484 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.484 I llama_new_context_with_model: n_batch       = 2048
0.00.051.485 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.485 I llama_new_context_with_model: flash_attn    = 0
0.00.051.485 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.485 I llama_new_context_with_model: freq_scale    = 1
0.00.051.486 I ggml_metal_init: allocating
0.00.051.489 I ggml_metal_init: found device: Apple M4
0.00.051.490 I ggml_metal_init: picking default device: Apple M4
0.00.052.062 I ggml_metal_init: using embedded metal library
0.00.054.397 I ggml_metal_init: GPU name:   Apple M4
0.00.054.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.399 I ggml_metal_init: simdgroup reduction   = true
0.00.054.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.400 I ggml_metal_init: has bfloat            = true
0.00.054.400 I ggml_metal_init: use bfloat            = true
0.00.054.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.035 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.110 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.119 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.152 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.175 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.176 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.177 I llama_new_context_with_model: graph nodes  = 967
0.00.085.177 I llama_new_context_with_model: graph splits = 2
0.00.085.180 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.321 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.414 I main: llama threadpool init, n_threads = 4
0.00.694.492 I 
0.00.694.513 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.513 I 
0.00.694.745 I sampler seed: 1234
0.00.694.749 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.760 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.760 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.760 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.538.989 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61792.86 tokens per second)
0.01.538.990 I llama_perf_context_print:        load time =     685.25 ms
0.01.538.990 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.80 tokens per second)
0.01.538.991 I llama_perf_context_print:        eval time =     789.78 ms /    63 runs   (   12.54 ms per token,    79.77 tokens per second)
0.01.538.992 I llama_perf_context_print:       total time =     844.58 ms /    70 tokens
0.01.539.204 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.823 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.247 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.247 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.248 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.248 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.248 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.249 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.250 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.250 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.250 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.251 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.253 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.736 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.737 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.738 I llama_model_loader: - type  f32:  194 tensors
0.00.024.738 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.738 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.739 I print_info: file format = GGUF V3 (latest)
0.00.024.739 I print_info: file type   = Q5_K - Medium
0.00.024.740 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.189 I load: special tokens cache size = 25
0.00.049.283 I load: token to piece cache size = 0.2984 MB
0.00.049.298 I print_info: arch             = gptneox
0.00.049.299 I print_info: n_vocab (hp)     = 50304
0.00.049.299 I print_info: vocab_only       = 0
0.00.049.300 I print_info: n_ctx_train      = 2048
0.00.049.300 I print_info: n_embd           = 2048
0.00.049.300 I print_info: n_layer          = 24
0.00.049.303 I print_info: n_head           = 16
0.00.049.303 I print_info: n_head_kv        = 16
0.00.049.304 I print_info: n_rot            = 32
0.00.049.304 I print_info: n_swa            = 0
0.00.049.305 I print_info: n_embd_head_k    = 128
0.00.049.313 I print_info: n_embd_head_v    = 128
0.00.049.319 I print_info: n_gqa            = 1
0.00.049.320 I print_info: n_embd_k_gqa     = 2048
0.00.049.321 I print_info: n_embd_v_gqa     = 2048
0.00.049.321 I print_info: f_norm_eps       = 1.0e-05
0.00.049.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.323 I print_info: f_logit_scale    = 0.0e+00
0.00.049.323 I print_info: n_ff             = 8192
0.00.049.324 I print_info: n_expert         = 0
0.00.049.324 I print_info: n_expert_used    = 0
0.00.049.324 I print_info: causal attn      = 1
0.00.049.325 I print_info: pooling type     = 0
0.00.049.325 I print_info: rope type        = 2
0.00.049.325 I print_info: rope scaling     = linear
0.00.049.325 I print_info: freq_base_train  = 10000.0
0.00.049.325 I print_info: freq_scale_train = 1
0.00.049.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.327 I print_info: rope_finetuned   = unknown
0.00.049.327 I print_info: ssm_d_conv       = 0
0.00.049.327 I print_info: ssm_d_inner      = 0
0.00.049.327 I print_info: ssm_d_state      = 0
0.00.049.327 I print_info: ssm_dt_rank      = 0
0.00.049.327 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.327 I print_info: model type       = 1.4B
0.00.049.328 I print_info: model params     = 1.41 B
0.00.049.329 I print_info: general.name     = 1.4B
0.00.049.329 I print_info: vocab type       = BPE
0.00.049.329 I print_info: n_vocab          = 50304
0.00.049.329 I print_info: n_merges         = 50009
0.00.049.330 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.330 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.330 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.330 I print_info: LF token         = 128 ''
0.00.049.331 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.331 I print_info: max token length = 1024
0.00.051.343 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.343 I load_tensors: offloading output layer to GPU
0.00.051.344 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.354 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.355 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.685 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.686 I llama_new_context_with_model: n_ctx         = 128
0.00.051.686 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.686 I llama_new_context_with_model: n_batch       = 128
0.00.051.686 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.687 I llama_new_context_with_model: flash_attn    = 0
0.00.051.687 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.687 I llama_new_context_with_model: freq_scale    = 1
0.00.051.687 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.688 I ggml_metal_init: allocating
0.00.051.691 I ggml_metal_init: found device: Apple M4
0.00.051.693 I ggml_metal_init: picking default device: Apple M4
0.00.052.223 I ggml_metal_init: using embedded metal library
0.00.054.549 I ggml_metal_init: GPU name:   Apple M4
0.00.054.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.551 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.551 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.552 I ggml_metal_init: simdgroup reduction   = true
0.00.054.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.552 I ggml_metal_init: has bfloat            = true
0.00.054.552 I ggml_metal_init: use bfloat            = true
0.00.054.552 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.553 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.141 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.446 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.448 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.471 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.383 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.384 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.384 I llama_new_context_with_model: graph nodes  = 967
0.00.066.384 I llama_new_context_with_model: graph splits = 2
0.00.066.385 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.385 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.080 I 
0.00.642.110 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.121 I perplexity: tokenizing the input ..
0.00.650.018 I perplexity: tokenization took 7.896 ms
0.00.650.027 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.133 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.791.325 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.791.349 I llama_perf_context_print:        load time =     632.25 ms
0.00.791.349 I llama_perf_context_print: prompt eval time =     139.88 ms /   128 tokens (    1.09 ms per token,   915.08 tokens per second)
0.00.791.350 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.351 I llama_perf_context_print:       total time =     149.27 ms /   129 tokens
0.00.791.832 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.077s
sys	0m0.114s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.760 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.874 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.879 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.881 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.882 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.885 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.886 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.886 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.886 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.887 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.887 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.889 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.889 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.889 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.738 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.616 I llama_model_loader: - type  f32:  194 tensors
0.00.025.616 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.616 I print_info: file format = GGUF V3 (latest)
0.00.025.617 I print_info: file type   = Q6_K
0.00.025.617 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.102 I load: special tokens cache size = 25
0.00.051.156 I load: token to piece cache size = 0.2984 MB
0.00.051.170 I print_info: arch             = gptneox
0.00.051.171 I print_info: n_vocab (hp)     = 50304
0.00.051.172 I print_info: vocab_only       = 0
0.00.051.172 I print_info: n_ctx_train      = 2048
0.00.051.172 I print_info: n_embd           = 2048
0.00.051.172 I print_info: n_layer          = 24
0.00.051.175 I print_info: n_head           = 16
0.00.051.176 I print_info: n_head_kv        = 16
0.00.051.176 I print_info: n_rot            = 32
0.00.051.176 I print_info: n_swa            = 0
0.00.051.177 I print_info: n_embd_head_k    = 128
0.00.051.179 I print_info: n_embd_head_v    = 128
0.00.051.179 I print_info: n_gqa            = 1
0.00.051.180 I print_info: n_embd_k_gqa     = 2048
0.00.051.181 I print_info: n_embd_v_gqa     = 2048
0.00.051.182 I print_info: f_norm_eps       = 1.0e-05
0.00.051.183 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.183 I print_info: f_logit_scale    = 0.0e+00
0.00.051.184 I print_info: n_ff             = 8192
0.00.051.184 I print_info: n_expert         = 0
0.00.051.184 I print_info: n_expert_used    = 0
0.00.051.184 I print_info: causal attn      = 1
0.00.051.185 I print_info: pooling type     = 0
0.00.051.185 I print_info: rope type        = 2
0.00.051.185 I print_info: rope scaling     = linear
0.00.051.185 I print_info: freq_base_train  = 10000.0
0.00.051.185 I print_info: freq_scale_train = 1
0.00.051.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.186 I print_info: rope_finetuned   = unknown
0.00.051.186 I print_info: ssm_d_conv       = 0
0.00.051.190 I print_info: ssm_d_inner      = 0
0.00.051.190 I print_info: ssm_d_state      = 0
0.00.051.190 I print_info: ssm_dt_rank      = 0
0.00.051.190 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.190 I print_info: model type       = 1.4B
0.00.051.192 I print_info: model params     = 1.41 B
0.00.051.192 I print_info: general.name     = 1.4B
0.00.051.192 I print_info: vocab type       = BPE
0.00.051.193 I print_info: n_vocab          = 50304
0.00.051.193 I print_info: n_merges         = 50009
0.00.051.193 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.193 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.193 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.193 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.194 I print_info: LF token         = 128 ''
0.00.051.194 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.194 I print_info: max token length = 1024
0.00.053.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.252 I load_tensors: offloading output layer to GPU
0.00.053.252 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.262 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.264 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.559 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.559 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.560 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.560 I llama_new_context_with_model: n_batch       = 2048
0.00.053.560 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.560 I llama_new_context_with_model: flash_attn    = 0
0.00.053.560 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.561 I llama_new_context_with_model: freq_scale    = 1
0.00.053.561 I ggml_metal_init: allocating
0.00.053.564 I ggml_metal_init: found device: Apple M4
0.00.053.566 I ggml_metal_init: picking default device: Apple M4
0.00.054.160 I ggml_metal_init: using embedded metal library
0.00.056.559 I ggml_metal_init: GPU name:   Apple M4
0.00.056.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.562 I ggml_metal_init: simdgroup reduction   = true
0.00.056.562 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.562 I ggml_metal_init: has bfloat            = true
0.00.056.562 I ggml_metal_init: use bfloat            = true
0.00.056.563 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.509 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.320 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.326 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.355 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.442 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.443 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.443 I llama_new_context_with_model: graph nodes  = 967
0.00.087.444 I llama_new_context_with_model: graph splits = 2
0.00.087.446 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.125 I main: llama threadpool init, n_threads = 4
0.00.739.167 I 
0.00.739.185 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.186 I 
0.00.739.426 I sampler seed: 1234
0.00.739.431 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.442 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.442 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.442 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.620.583 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.01.620.583 I llama_perf_context_print:        load time =     730.36 ms
0.01.620.584 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.64 tokens per second)
0.01.620.585 I llama_perf_context_print:        eval time =     823.65 ms /    63 runs   (   13.07 ms per token,    76.49 tokens per second)
0.01.620.586 I llama_perf_context_print:       total time =     881.46 ms /    70 tokens
0.01.620.816 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.111s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4473 (4e8bf7c8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.634 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.587 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.587 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.588 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.589 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.590 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.591 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.591 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.594 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.064 I llama_model_loader: - type  f32:  194 tensors
0.00.024.065 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.065 I print_info: file format = GGUF V3 (latest)
0.00.024.065 I print_info: file type   = Q6_K
0.00.024.066 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.563 I load: special tokens cache size = 25
0.00.048.572 I load: token to piece cache size = 0.2984 MB
0.00.048.586 I print_info: arch             = gptneox
0.00.048.587 I print_info: n_vocab (hp)     = 50304
0.00.048.587 I print_info: vocab_only       = 0
0.00.048.588 I print_info: n_ctx_train      = 2048
0.00.048.588 I print_info: n_embd           = 2048
0.00.048.588 I print_info: n_layer          = 24
0.00.048.591 I print_info: n_head           = 16
0.00.048.591 I print_info: n_head_kv        = 16
0.00.048.591 I print_info: n_rot            = 32
0.00.048.592 I print_info: n_swa            = 0
0.00.048.592 I print_info: n_embd_head_k    = 128
0.00.048.592 I print_info: n_embd_head_v    = 128
0.00.048.593 I print_info: n_gqa            = 1
0.00.048.593 I print_info: n_embd_k_gqa     = 2048
0.00.048.594 I print_info: n_embd_v_gqa     = 2048
0.00.048.595 I print_info: f_norm_eps       = 1.0e-05
0.00.048.595 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.595 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.596 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.598 I print_info: f_logit_scale    = 0.0e+00
0.00.048.598 I print_info: n_ff             = 8192
0.00.048.598 I print_info: n_expert         = 0
0.00.048.598 I print_info: n_expert_used    = 0
0.00.048.598 I print_info: causal attn      = 1
0.00.048.599 I print_info: pooling type     = 0
0.00.048.599 I print_info: rope type        = 2
0.00.048.599 I print_info: rope scaling     = linear
0.00.048.599 I print_info: freq_base_train  = 10000.0
0.00.048.600 I print_info: freq_scale_train = 1
0.00.048.600 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.600 I print_info: rope_finetuned   = unknown
0.00.048.600 I print_info: ssm_d_conv       = 0
0.00.048.600 I print_info: ssm_d_inner      = 0
0.00.048.600 I print_info: ssm_d_state      = 0
0.00.048.601 I print_info: ssm_dt_rank      = 0
0.00.048.601 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.609 I print_info: model type       = 1.4B
0.00.048.610 I print_info: model params     = 1.41 B
0.00.048.610 I print_info: general.name     = 1.4B
0.00.048.611 I print_info: vocab type       = BPE
0.00.048.611 I print_info: n_vocab          = 50304
0.00.048.611 I print_info: n_merges         = 50009
0.00.048.611 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.611 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.612 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.612 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.612 I print_info: LF token         = 128 ''
0.00.048.612 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.612 I print_info: max token length = 1024
0.00.050.591 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.591 I load_tensors: offloading output layer to GPU
0.00.050.591 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.602 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.603 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.872 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.873 I llama_new_context_with_model: n_ctx         = 128
0.00.050.873 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.873 I llama_new_context_with_model: n_batch       = 128
0.00.050.873 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.873 I llama_new_context_with_model: flash_attn    = 0
0.00.050.874 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.874 I llama_new_context_with_model: freq_scale    = 1
0.00.050.874 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.875 I ggml_metal_init: allocating
0.00.050.878 I ggml_metal_init: found device: Apple M4
0.00.050.880 I ggml_metal_init: picking default device: Apple M4
0.00.051.438 I ggml_metal_init: using embedded metal library
0.00.053.770 I ggml_metal_init: GPU name:   Apple M4
0.00.053.771 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.772 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.772 I ggml_metal_init: simdgroup reduction   = true
0.00.053.772 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.772 I ggml_metal_init: has bfloat            = true
0.00.053.772 I ggml_metal_init: use bfloat            = true
0.00.053.773 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.226 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.524 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.529 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.555 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.462 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.463 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.463 I llama_new_context_with_model: graph nodes  = 967
0.00.065.463 I llama_new_context_with_model: graph splits = 2
0.00.065.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.754 I 
0.00.366.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.366.800 I perplexity: tokenizing the input ..
0.00.374.420 I perplexity: tokenization took 7.617 ms
0.00.374.423 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.514.487 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.515.659 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.515.678 I llama_perf_context_print:        load time =     358.11 ms
0.00.515.679 I llama_perf_context_print: prompt eval time =     139.84 ms /   128 tokens (    1.09 ms per token,   915.34 tokens per second)
0.00.515.680 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.515.680 I llama_perf_context_print:       total time =     148.93 ms /   129 tokens
0.00.516.026 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.076s
sys	0m0.078s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4473 (4e8bf7c8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: n_vocab (hp)     = 50304
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f60a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f60aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f60b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f60b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f60bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f60c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f60c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f60ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f60d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f60d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f60dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f60e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f60eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f60f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f60fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f610380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f610aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f6111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f6118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f6120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f6127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f612ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f613610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f613eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f6145d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f614890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f614ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f615b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f616050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f616310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f6167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f616a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f617300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f617840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f617fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f618440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f6188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f618d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f619220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f6196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f619b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f61a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f61a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f61a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f61ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f61b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f61bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f61c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f61c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f61ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f61d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f61daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f61e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f61e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f61ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f61f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f61f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f61fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f6202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f6205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f620a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f620ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f621390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f621830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f621cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f622170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f622610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f622ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f622f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f6233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f623890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f623d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f624280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f6247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f624d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f625270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f6257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f625d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f626260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f6267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f626d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f627250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f6277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f627cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f628240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f628790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f628ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f629230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f629780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f629cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f62a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f62a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f62acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f62b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f62b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f62bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f61b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f62c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f62c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f62ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f62d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f62d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f62de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f62e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f62e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f62ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f62f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f62f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f62fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f630340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f630890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f630de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f631280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f631720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f631bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f632060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f632500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f6329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f632e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f6332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f633780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f633c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f6340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f634560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f634a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f634ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f635340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f6357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f635c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f636120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f6365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f636a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f636f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f6373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f637840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f637ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f638180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f638620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f638ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f638f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f639400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f6398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f639d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f63a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f63a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f63ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f63afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f63b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f63b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f63bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f63c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f63c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f63cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f63d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f63d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f63d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f63de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f63e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f63e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f63ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f63f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f63f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f63f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f63fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f640300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f6407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f640c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f6410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f641580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f641a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f641ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f642360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f642800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f642ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f643140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f6435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f643a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f643f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f6443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f644860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f644d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f6451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f645640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f645ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f645f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f646420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f6468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f646d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f647200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f6476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f647b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f647fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f648530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f648fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f649520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f6497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f649df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f64a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f64aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f64b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f64b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f64b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f64bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f64c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f64cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f64d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f64d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f64db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f64e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f64e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f64eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f64f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f64f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f64fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f6502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f650830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f650d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f6512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f651820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f651d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f6522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f652810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f652d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f6532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f653800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f653d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f6542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f6547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f654d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f655290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f6557e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f655d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f656280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f6567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f656d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f657270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f6577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f657d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f658260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f6587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f658d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f659250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f6597a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f659cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f65a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f65a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f65ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f65b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f65b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f65bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f65c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f65c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f65ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f65d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f65d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f65dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f65e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f65e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f65eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f65f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f65f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f65fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f6601e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f660730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f660c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f661120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f6615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f661a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f661f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f6623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f662840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f662ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f663180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f663620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f663ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f663f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f664400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f6648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f664d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f6651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f665730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f665e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f666570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f666c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f6673b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f667670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f667e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f668120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f668730 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.116.579 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f6683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f64a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f649aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f64a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f61d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f61d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f61f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f64c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f614b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f61b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f61bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f61c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f61aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f61cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f613b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f6099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f61e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f61fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f62c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f667930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f616d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f616ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f64c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f64acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f615160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f615420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f6156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f668b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f668e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f669110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f6693d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f669690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f669950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f669c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f669ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f66a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f66a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f66a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f66a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f66ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f66af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f66b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f66b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f66b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f66ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f66bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f66bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f66c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f66c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f66c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f66cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f66cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f66d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f66d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f66d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f66d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f66db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f66de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f66e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f66e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f66e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f66e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f66ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f66ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f66f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f66f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f66f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f66f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f66fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f66ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f6701d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f670490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f670750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f670a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f670cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f670f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f671250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f671510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f6717d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f671a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f671d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f672010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f6722d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f672590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f672850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f672b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f672dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f673090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f673350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f673610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f6738d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f673b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f673e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f674110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f6743d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f674690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f674950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f674c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f674ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f675190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f675450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f675710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f6759d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f675c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f675f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f676210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f6764d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f676790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f676a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f676d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f676fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f677290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f677550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f677810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f677ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f677d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f678050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f678310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f6785d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f678890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f678b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f678e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f6790d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f679390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f679650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f679910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f679bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f679e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f67a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f67a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f67a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f67a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f67ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f67af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f67b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f67b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f67b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f67ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f67bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f67bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f67c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f67c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f67c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f67ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f67cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f67d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f67d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f67d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f67d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f67db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f67ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f67e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f67e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f67e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f67e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f67eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f67ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f67f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f67f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f67f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f67f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f67fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f67fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f680190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f680450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f680710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f6809d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f680c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f680f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f681210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f6814d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f681790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f681a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f681d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f681fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f682290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f682550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f682810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f682ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f682d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f683050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f683310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f6835d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f683890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f683b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f683e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f6840d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f684390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f684650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f684910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f684bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f684e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f685150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f685410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f6856d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f685990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f685c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f685f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f6861d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f686490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f686750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f686a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f686cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f686f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f687250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f687510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f6877d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f687a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f687d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f688010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f6885e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f6888a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f688b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f688e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f6890e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f6893a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f689660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f689920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f689be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f689ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f68a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f68a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f68a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f68a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f68ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f68af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f68b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f68b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f68b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f68ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f68bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f68bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f68c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f68c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f68c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f68caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f68cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f68d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f68d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f68d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f68d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f68ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f68e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f68e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f68eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f68f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f68f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f68fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f6902e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f690830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f690d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f6912d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f691820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f691d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f6922c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f692810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f692d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f6932b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f693800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f693d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f6942a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f6947f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f694d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f695290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f6957e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f695d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f696280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f696540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f696800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f696d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f697200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f697700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f697c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f698100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f698600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f698b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f699000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f699500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f699a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f699f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f69a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f69a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f69ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f69b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f69bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f69c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f69cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f69d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f69d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f69dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f69e0f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10cc08190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10cc06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10cc087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10cc08c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10cc09090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10cc09640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10cc09bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10cc0a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10cc0a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10cc0ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10cc0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10cc0b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10cc0c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10cc0c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10cc0d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10cc0d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10cc0df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10cc0e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10cc0edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10cc0f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10cc0fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10cc103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10cc10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10cc11200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10cc11920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10cc11be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10cc121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10cc12800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10cc12e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10cc13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10cc13aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10cc13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10cc145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10cc14b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10cc14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10cc15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10cc15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10cc15bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10cc16070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10cc16510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10cc169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10cc16e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10cc172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10cc17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10cc17a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10cc18060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10cc18670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10cc18c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10cc19290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10cc198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10cc19eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10cc1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10cc1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10cc1b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10cc1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10cc1bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10cc1c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10cc1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10cc1cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10cc1d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10cc1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10cc1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10cc1e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10cc1e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10cc1e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10cc1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10cc1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10cc1f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10cc1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10cc20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10cc205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10cc20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10cc20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10cc21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10cc21990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10cc21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10cc22430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10cc22980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10cc22ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10cc23420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10cc23970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10cc23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10cc24410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10cc24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10cc24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10cc25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10cc25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10cc25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10cc263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10cc26940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10cc26e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10cc273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10cc27930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10cc27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10cc283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10cc28920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10cc28e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10cc293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10cc29910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10cc29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10cc2a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10cc2a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10cc2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10cc2b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10cc2b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10cc2be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10cc2c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10cc2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10cc2ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10cc2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10cc2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10cc2de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10cc2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10cc2e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10cc2ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10cc2f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10cc2f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10cc2fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10cc2ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10cc303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10cc30870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10cc30d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10cc311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10cc31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10cc31af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10cc31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10cc32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10cc328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10cc32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10cc33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10cc336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10cc33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10cc33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10cc34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10cc34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10cc34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10cc35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10cc35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10cc35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10cc36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10cc364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10cc36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10cc36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10cc372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10cc37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10cc37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10cc380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10cc38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10cc389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10cc38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10cc39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10cc397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10cc39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10cc3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10cc3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10cc3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10cc3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10cc3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10cc3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10cc3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10cc3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10cc3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10cc3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10cc3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10cc3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10cc3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10cc3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10cc3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10cc3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10cc3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10cc3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10cc3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10cc3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10cc3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10cc40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10cc406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10cc40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10cc41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10cc414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10cc41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10cc41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10cc42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10cc42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10cc42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10cc43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10cc43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10cc439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10cc43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10cc442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10cc44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10cc44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10cc450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10cc45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10cc45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10cc46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10cc46560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10cc46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10cc46d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10cc47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10cc47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10cc47fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10cc48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10cc48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10cc48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10cc49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10cc49b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10cc4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10cc4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10cc4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10cc4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10cc4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10cc4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10cc4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10cc4c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10cc4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10cc4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10cc4d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10cc4ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10cc4e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10cc4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10cc4edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10cc4f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10cc4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10cc4fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10cc502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10cc50840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10cc50d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10cc512e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10cc51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10cc51d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10cc522d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10cc52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10cc52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10cc532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10cc53810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10cc53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10cc542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10cc54800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10cc54d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10cc552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10cc557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10cc55d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10cc56290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10cc567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10cc56d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10cc57280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10cc577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10cc57d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10cc58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10cc587c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10cc58d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10cc59260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10cc597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10cc59d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10cc5a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10cc5a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10cc5acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10cc5b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10cc5b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10cc5bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10cc5c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10cc5c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10cc5ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10cc5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10cc5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10cc5dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10cc5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10cc5e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10cc5eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10cc5eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10cc5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10cc5f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10cc5fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10cc60270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10cc60710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10cc60bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10cc61050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10cc614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10cc61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10cc61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10cc622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10cc62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10cc62cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10cc633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10cc63b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10cc64220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10cc64940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10cc64c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10cc653f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10cc656b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10cc65cc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.824s
user	0m0.273s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4473 (4e8bf7c8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: n_vocab (hp)     = 50304
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a70afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a70b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a70bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a70c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a70c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a70cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a70d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a70d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a70dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a70e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a70e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a70eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a70f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a710070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a710880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a710fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a7116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a711de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a712500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a712cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a7133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a713b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a714230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a714ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a7151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a7154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a715ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a716730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a716c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a716f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a7173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a717690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a717f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a718460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a718720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a718bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a719060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a719500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a7199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a719e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a71a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a71a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a71ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a71b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a71b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a71b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a71bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a71c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a71ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a71d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a71daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a71e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a71e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a71ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a71f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a71f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a71fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a720110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a720720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a720f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a7211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a721670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a721b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a721fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a722450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a7228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a722d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a723230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a7236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a723b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a724010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a7244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a724950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a724ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a7253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a725940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a725e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a7263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a726e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a7273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a727920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a727e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a7283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a728910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a728e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a7293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a729900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a729e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a72a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a72a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a72ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a72b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a72b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a72be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a72c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a72c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a71c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a72cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a72d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a72da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a72df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a72e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a72ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a72ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a72f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a72fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a72ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a7304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a730a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a730f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a7314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a731a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a731ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a732340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a7327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a732c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a733120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a7335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a733a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a733f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a7343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a734840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a734ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a735180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a735620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a735ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a735f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a736400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a7368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a736d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a7371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a737680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a737b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a737fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a738460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a738900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a738da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a739240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a7396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a739b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a73a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a73a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a73a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a73ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a73b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a73b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a73bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a73c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a73c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a73c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a73ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a73d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a73d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a73dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a73e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a73e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a73ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a73eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a73f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a73f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a73fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a740140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a7405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a740a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a740f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a7413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a741860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a741d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a7421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a742640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a742ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a742f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a743420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a7438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a743d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a744200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a7446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a744b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a744fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a745480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a745920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a745dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a746260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a746700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a746ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a747040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a7474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a747980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a747e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a7482c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a748760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a748c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a749150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a7496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a749bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a74a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a74a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a74aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a74b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a74b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a74be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a74c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a74c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a74cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a74d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a74d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a74de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a74e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a74e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a74ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a74f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a74f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a74ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a750460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a7509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a750f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a751450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a7519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a751ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a752440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a752990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a752ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a753980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a753ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a754420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a754970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a754ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a755410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a755960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a755eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a756400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a756950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a756ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a7573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a757940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a757e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a7583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a758930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a758e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a7593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a759920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a759e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a75a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a75a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a75ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a75b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a75b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a75be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a75c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a75c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a75ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a75d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a75d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a75de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a75e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a75e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a75ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a75f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a75f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a75fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a760360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a7608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a760e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a761350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a7618a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a761d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a7621e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a762680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a762b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a762fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a763460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a763900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a763da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a764240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a7646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a764b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a765020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a7654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a765960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a765e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a766350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a766a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a767190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a7678b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a767fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a768290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a768a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a768d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a769350 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.319 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a769000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a74acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a74a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a74b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a71e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a71ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a7203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a74ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a715770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a71c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a71cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a71d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a71b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a71d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a714770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a70a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a71efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a7209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a72d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a768550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a717950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a717c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a74d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a74b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a715d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a716040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a716300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a7697b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a769a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a769d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a769ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a76a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a76a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a76a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a76aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a76adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a76b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a76b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a76b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a76b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a76bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a76be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a76c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a76c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a76c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a76c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a76cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a76ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a76d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a76d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a76d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a76d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a76dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a76df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a76e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a76e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a76e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a76ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a76ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a76efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a76f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a76f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a76f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a76fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a76fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a770030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a7702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a7705b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a770870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a770b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a770df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a7710b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a771370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a771630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a7718f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a771bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a771e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a772130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a7723f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a7726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a772970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a772c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a772ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a7731b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a773470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a773730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a7739f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a773cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a773f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a774230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a7744f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a7747b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a774a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a774d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a774ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a7752b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a775570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a775830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a775af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a775db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a776070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a776330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a7765f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a7768b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a776b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a776e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a7770f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a7773b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a777670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a777930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a777bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a777eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a778170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a778430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a7786f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a7789b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a778c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a778f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a7791f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a7794b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a779770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a779a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a779cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a779fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a77a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a77a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a77a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a77aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a77ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a77b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a77b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a77b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a77b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a77bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a77bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a77c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a77c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a77c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a77c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a77cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a77ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a77d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a77d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a77d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a77d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a77dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a77def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a77e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a77e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a77e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a77e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a77ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a77ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a77f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a77f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a77f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a77fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a77fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a77fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a7802b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a780570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a780830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a780af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a780db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a781070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a781330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a7815f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a7818b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a781b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a781e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a7820f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a7823b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a782670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a782930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a782bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a782eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a783170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a783430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a7836f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a7839b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a783c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a783f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a7841f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a7844b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a784770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a784a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a784cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a784fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a785270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a785530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a7857f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a785ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a785d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a786030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a7862f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a7865b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a786870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a786b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a786df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a7870b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a787370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a787630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a7878f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a787bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a787e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a788130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a7883f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a7886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a788970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a788c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a789200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a7894c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a789780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a789a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a789d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a789fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a78a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a78a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a78a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a78aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a78ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a78b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a78b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a78b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a78b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a78bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a78be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a78c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a78c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a78c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a78c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a78cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a78ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a78d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a78d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a78d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a78d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a78dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a78df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a78e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a78e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a78e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a78ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a78ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a78f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a78f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a78fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a790200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a790750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a790ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a7911f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a791740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a791c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a7921e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a792730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a792c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a7931d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a793720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a793c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a7941c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a794710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a794c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a7951b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a795700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a795c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a7961a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a7966f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a796b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a797030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a7974d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a797970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a797e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a7982b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a798750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a798bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a799090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a799530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a7999d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a799e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a79a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a79a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a79ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a79b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a79b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a79bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a79c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a79ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a79d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a79d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a79db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a79e1a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b8046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b8058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b8065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b807a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b808530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b808ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b8094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b809c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b80a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b80aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b80b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b80b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b80c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b80c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b80cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b80d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b80dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b80dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b80e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b80e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b80eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b80efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b80f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b80f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b80fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b810080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b8104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b810960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b810dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b811240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b8116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b811b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b811f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b812400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b812870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b812ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b813150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b8135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b813a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b813ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b814310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b814780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b814bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b815060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b8154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b815db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b816220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b816790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b816c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b817100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b817570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b8179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b817e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b8182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b818730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b818ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b819010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b819480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b8198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b819d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b81a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b81a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b81aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b81af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b81b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b81b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b81bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b81c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b81c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b81c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b81ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b81d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b81d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b81db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b81dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b81e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b81e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b81ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b81f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b81f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b81fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b81ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b820370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b8207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b820c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b8210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b821530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b8219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b821e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b822280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b8226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b822b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b822fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b823440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b823cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b823f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b824400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b824870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b824ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b825150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b8255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b825a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b825ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b826310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b826780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b826bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b827060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b8274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b827940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b827db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b828220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b828690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b828b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b828f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b8293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b829850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b829cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b82a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b82a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b82aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b82ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b82b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b82b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b82bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b82c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b82c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b82c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b82cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b82d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b82d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b82dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b82df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b82e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b82e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b82eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b82f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b82f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b82f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b82fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b8302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b830740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b830bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b831020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b831490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b831900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b831d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b8321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b832650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b832ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b832f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b8333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b833810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b833c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b8340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b834560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b8349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b834e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b8352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b835720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b835b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b836000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b836470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b8368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b836d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b8371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b837630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b837aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b837f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b838380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b8387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b838c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b8390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b839540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b8399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b839e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b83a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b83a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b83ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b83afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b83b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b83b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b83bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b83c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b83c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b83ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b83cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b83d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b83d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b83dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b83e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b83e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b83e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b83ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b83f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b83f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b83fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b83ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b840430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b8408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b840d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b841180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b841d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b841fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b842280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b8426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b842b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b842fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b843440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b8438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b843d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b844190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b844600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b844a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b844ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b845350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b8457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b845c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b8460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b846510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b846980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b846df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b847260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b8476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b847b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b847fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b848420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b848890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b848d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b849170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b8495e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b849a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b849ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b84a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b84a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b84ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b84b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b84b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b84b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b84bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b84c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b84c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b84cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b84cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b84d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b84d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b84dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b84e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b84e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b84ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b84eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b84f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b84f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b84fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b850060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b8504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b850940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b850db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b851220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b851690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b851b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b851f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b8523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b852850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b852cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b853130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b8535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b853a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b853e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b8542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b854760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b854bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b855040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b8554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b855920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b856390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b856ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b8571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b8578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b857bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b858020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b858620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b858c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.916s
user	0m0.242s
sys	0m0.134s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.12 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.51 real         0.14 user         0.04 sys
```
