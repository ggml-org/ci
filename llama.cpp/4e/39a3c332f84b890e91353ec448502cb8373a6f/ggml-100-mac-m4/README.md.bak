### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.40 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.29 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.90 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.32 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.24 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.37 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.01 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.74 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.86 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.05 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 166.56 sec*proc (29 tests)

Total Test time (real) = 166.57 sec

real	2m46.581s
user	4m41.059s
sys	0m5.704s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.85 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.37 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.38 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.05 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.36 sec*proc (29 tests)

Total Test time (real) =  48.37 sec

real	0m48.383s
user	0m54.251s
sys	0m5.252s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.228 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.539 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.366 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.374 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.376 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.376 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.377 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.378 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.379 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.379 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.380 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.380 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.383 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.383 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.384 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.384 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.385 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.385 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.386 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.761 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.901 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.902 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.903 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.903 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.904 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.904 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.029.905 I llama_model_loader: - type  f32:  124 tensors
0.00.029.905 I llama_model_loader: - type  f16:   73 tensors
0.00.029.906 I print_info: file format = GGUF V3 (latest)
0.00.029.907 I print_info: file type   = F16
0.00.029.908 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.033.869 I load: special tokens cache size = 5
0.00.035.909 I load: token to piece cache size = 0.2032 MB
0.00.035.934 I print_info: arch             = bert
0.00.035.935 I print_info: vocab_only       = 0
0.00.035.936 I print_info: n_ctx_train      = 512
0.00.035.936 I print_info: n_embd           = 384
0.00.035.936 I print_info: n_layer          = 12
0.00.035.939 I print_info: n_head           = 12
0.00.035.939 I print_info: n_head_kv        = 12
0.00.035.940 I print_info: n_rot            = 32
0.00.035.940 I print_info: n_swa            = 0
0.00.035.940 I print_info: n_embd_head_k    = 32
0.00.035.940 I print_info: n_embd_head_v    = 32
0.00.035.941 I print_info: n_gqa            = 1
0.00.035.942 I print_info: n_embd_k_gqa     = 384
0.00.035.942 I print_info: n_embd_v_gqa     = 384
0.00.035.943 I print_info: f_norm_eps       = 1.0e-12
0.00.035.944 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.035.944 I print_info: f_clamp_kqv      = 0.0e+00
0.00.035.944 I print_info: f_max_alibi_bias = 0.0e+00
0.00.035.945 I print_info: f_logit_scale    = 0.0e+00
0.00.035.945 I print_info: n_ff             = 1536
0.00.035.946 I print_info: n_expert         = 0
0.00.035.946 I print_info: n_expert_used    = 0
0.00.035.946 I print_info: causal attn      = 0
0.00.035.946 I print_info: pooling type     = 2
0.00.035.946 I print_info: rope type        = 2
0.00.035.948 I print_info: rope scaling     = linear
0.00.035.949 I print_info: freq_base_train  = 10000.0
0.00.035.949 I print_info: freq_scale_train = 1
0.00.035.950 I print_info: n_ctx_orig_yarn  = 512
0.00.035.950 I print_info: rope_finetuned   = unknown
0.00.035.950 I print_info: ssm_d_conv       = 0
0.00.035.950 I print_info: ssm_d_inner      = 0
0.00.035.950 I print_info: ssm_d_state      = 0
0.00.035.950 I print_info: ssm_dt_rank      = 0
0.00.035.951 I print_info: ssm_dt_b_c_rms   = 0
0.00.035.951 I print_info: model type       = 33M
0.00.035.952 I print_info: model params     = 33.21 M
0.00.035.952 I print_info: general.name     = Bge Small
0.00.035.953 I print_info: vocab type       = WPM
0.00.035.954 I print_info: n_vocab          = 30522
0.00.035.954 I print_info: n_merges         = 0
0.00.035.954 I print_info: BOS token        = 101 '[CLS]'
0.00.035.955 I print_info: UNK token        = 100 '[UNK]'
0.00.035.955 I print_info: SEP token        = 102 '[SEP]'
0.00.035.955 I print_info: PAD token        = 0 '[PAD]'
0.00.035.955 I print_info: MASK token       = 103 '[MASK]'
0.00.035.956 I print_info: LF token         = 0 '[PAD]'
0.00.035.956 I print_info: max token length = 21
0.00.035.956 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.038.783 I load_tensors: offloading 12 repeating layers to GPU
0.00.038.784 I load_tensors: offloading output layer to GPU
0.00.038.784 I load_tensors: offloaded 13/13 layers to GPU
0.00.038.808 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.809 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.039.081 I llama_init_from_model: n_seq_max     = 1
0.00.039.082 I llama_init_from_model: n_ctx         = 512
0.00.039.082 I llama_init_from_model: n_ctx_per_seq = 512
0.00.039.082 I llama_init_from_model: n_batch       = 2048
0.00.039.083 I llama_init_from_model: n_ubatch      = 2048
0.00.039.083 I llama_init_from_model: flash_attn    = 0
0.00.039.083 I llama_init_from_model: freq_base     = 10000.0
0.00.039.084 I llama_init_from_model: freq_scale    = 1
0.00.039.084 I ggml_metal_init: allocating
0.00.039.089 I ggml_metal_init: found device: Apple M4
0.00.039.093 I ggml_metal_init: picking default device: Apple M4
0.00.039.699 I ggml_metal_init: using embedded metal library
0.00.043.644 I ggml_metal_init: GPU name:   Apple M4
0.00.043.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.043.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.043.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.043.648 I ggml_metal_init: simdgroup reduction   = true
0.00.043.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.043.649 I ggml_metal_init: has residency sets    = true
0.00.043.649 I ggml_metal_init: has bfloat            = true
0.00.043.649 I ggml_metal_init: use bfloat            = true
0.00.043.649 I ggml_metal_init: hasUnifiedMemory      = true
0.00.043.650 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.055.539 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.056.207 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.056.209 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.211 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.057.360 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.057.361 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.057.361 I llama_init_from_model: graph nodes  = 429
0.00.057.362 I llama_init_from_model: graph splits = 2
0.00.057.363 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.057.363 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.063.046 I 
0.00.063.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.063.687 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.759 I llama_perf_context_print:        load time =      43.50 ms
0.00.068.760 I llama_perf_context_print: prompt eval time =       4.94 ms /     9 tokens (    0.55 ms per token,  1823.34 tokens per second)
0.00.068.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.761 I llama_perf_context_print:       total time =       5.71 ms /    10 tokens
0.00.068.924 I ggml_metal_free: deallocating

real	0m0.279s
user	0m0.048s
sys	0m0.037s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.164 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.866 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.870 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.872 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.872 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.872 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.873 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.873 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.874 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.874 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.875 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.876 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.876 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.878 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.878 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.879 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.879 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.879 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.879 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.318 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.953 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.954 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.955 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.955 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.955 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.955 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.956 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.956 I llama_model_loader: - type  f32:  124 tensors
0.00.014.957 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.957 I print_info: file format = GGUF V3 (latest)
0.00.014.958 I print_info: file type   = Q8_0
0.00.014.958 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.493 I load: special tokens cache size = 5
0.00.018.847 I load: token to piece cache size = 0.2032 MB
0.00.018.857 I print_info: arch             = bert
0.00.018.858 I print_info: vocab_only       = 0
0.00.018.858 I print_info: n_ctx_train      = 512
0.00.018.858 I print_info: n_embd           = 384
0.00.018.858 I print_info: n_layer          = 12
0.00.018.862 I print_info: n_head           = 12
0.00.018.862 I print_info: n_head_kv        = 12
0.00.018.862 I print_info: n_rot            = 32
0.00.018.863 I print_info: n_swa            = 0
0.00.018.863 I print_info: n_embd_head_k    = 32
0.00.018.863 I print_info: n_embd_head_v    = 32
0.00.018.863 I print_info: n_gqa            = 1
0.00.018.864 I print_info: n_embd_k_gqa     = 384
0.00.018.864 I print_info: n_embd_v_gqa     = 384
0.00.018.865 I print_info: f_norm_eps       = 1.0e-12
0.00.018.865 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.865 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.865 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.865 I print_info: f_logit_scale    = 0.0e+00
0.00.018.866 I print_info: n_ff             = 1536
0.00.018.866 I print_info: n_expert         = 0
0.00.018.866 I print_info: n_expert_used    = 0
0.00.018.867 I print_info: causal attn      = 0
0.00.018.867 I print_info: pooling type     = 2
0.00.018.867 I print_info: rope type        = 2
0.00.018.867 I print_info: rope scaling     = linear
0.00.018.867 I print_info: freq_base_train  = 10000.0
0.00.018.868 I print_info: freq_scale_train = 1
0.00.018.868 I print_info: n_ctx_orig_yarn  = 512
0.00.018.868 I print_info: rope_finetuned   = unknown
0.00.018.868 I print_info: ssm_d_conv       = 0
0.00.018.868 I print_info: ssm_d_inner      = 0
0.00.018.868 I print_info: ssm_d_state      = 0
0.00.018.868 I print_info: ssm_dt_rank      = 0
0.00.018.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.869 I print_info: model type       = 33M
0.00.018.869 I print_info: model params     = 33.21 M
0.00.018.869 I print_info: general.name     = Bge Small
0.00.018.871 I print_info: vocab type       = WPM
0.00.018.871 I print_info: n_vocab          = 30522
0.00.018.871 I print_info: n_merges         = 0
0.00.018.871 I print_info: BOS token        = 101 '[CLS]'
0.00.018.872 I print_info: UNK token        = 100 '[UNK]'
0.00.018.874 I print_info: SEP token        = 102 '[SEP]'
0.00.018.874 I print_info: PAD token        = 0 '[PAD]'
0.00.018.874 I print_info: MASK token       = 103 '[MASK]'
0.00.018.874 I print_info: LF token         = 0 '[PAD]'
0.00.018.875 I print_info: max token length = 21
0.00.018.875 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.700 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.701 I load_tensors: offloading output layer to GPU
0.00.020.701 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.708 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.708 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.961 I llama_init_from_model: n_seq_max     = 1
0.00.020.962 I llama_init_from_model: n_ctx         = 512
0.00.020.962 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.962 I llama_init_from_model: n_batch       = 2048
0.00.020.963 I llama_init_from_model: n_ubatch      = 2048
0.00.020.963 I llama_init_from_model: flash_attn    = 0
0.00.020.963 I llama_init_from_model: freq_base     = 10000.0
0.00.020.963 I llama_init_from_model: freq_scale    = 1
0.00.020.964 I ggml_metal_init: allocating
0.00.020.969 I ggml_metal_init: found device: Apple M4
0.00.020.972 I ggml_metal_init: picking default device: Apple M4
0.00.021.406 I ggml_metal_init: using embedded metal library
0.00.024.011 I ggml_metal_init: GPU name:   Apple M4
0.00.024.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.014 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.014 I ggml_metal_init: simdgroup reduction   = true
0.00.024.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.015 I ggml_metal_init: has residency sets    = true
0.00.024.015 I ggml_metal_init: has bfloat            = true
0.00.024.015 I ggml_metal_init: use bfloat            = true
0.00.024.015 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.558 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.158 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.160 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.162 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.153 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.154 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.154 I llama_init_from_model: graph nodes  = 429
0.00.036.155 I llama_init_from_model: graph splits = 2
0.00.036.156 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.157 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.207 I 
0.00.040.230 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.808 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.264 I llama_perf_context_print:        load time =      31.04 ms
0.00.044.265 I llama_perf_context_print: prompt eval time =       3.34 ms /     9 tokens (    0.37 ms per token,  2697.84 tokens per second)
0.00.044.266 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.266 I llama_perf_context_print:       total time =       4.06 ms /    10 tokens
0.00.044.491 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.257 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.593 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.506 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.511 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.513 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.522 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.523 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.523 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.525 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.525 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.526 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.526 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.527 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.531 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.531 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.532 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.648 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.723 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.294 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.296 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.297 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.297 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.297 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.298 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.298 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.298 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.299 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.299 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.300 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.300 I llama_model_loader: - type  f32:   40 tensors
0.00.049.300 I llama_model_loader: - type  f16:   30 tensors
0.00.049.305 I print_info: file format = GGUF V3 (latest)
0.00.049.306 I print_info: file type   = F16
0.00.049.309 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.616 W load: empty token at index 5
0.00.058.822 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.369 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.060.405 I load: special tokens cache size = 5
0.00.325.636 I load: token to piece cache size = 1.5060 MB
0.00.325.666 I print_info: arch             = jina-bert-v2
0.00.325.667 I print_info: vocab_only       = 0
0.00.325.667 I print_info: n_ctx_train      = 8192
0.00.325.667 I print_info: n_embd           = 384
0.00.325.668 I print_info: n_layer          = 4
0.00.325.673 I print_info: n_head           = 12
0.00.325.674 I print_info: n_head_kv        = 12
0.00.325.674 I print_info: n_rot            = 32
0.00.325.674 I print_info: n_swa            = 0
0.00.325.674 I print_info: n_embd_head_k    = 32
0.00.325.675 I print_info: n_embd_head_v    = 32
0.00.325.677 I print_info: n_gqa            = 1
0.00.325.677 I print_info: n_embd_k_gqa     = 384
0.00.325.678 I print_info: n_embd_v_gqa     = 384
0.00.325.679 I print_info: f_norm_eps       = 1.0e-12
0.00.325.679 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.325.680 I print_info: f_clamp_kqv      = 0.0e+00
0.00.325.680 I print_info: f_max_alibi_bias = 8.0e+00
0.00.325.680 I print_info: f_logit_scale    = 0.0e+00
0.00.325.681 I print_info: n_ff             = 1536
0.00.325.682 I print_info: n_expert         = 0
0.00.325.682 I print_info: n_expert_used    = 0
0.00.325.682 I print_info: causal attn      = 0
0.00.325.682 I print_info: pooling type     = -1
0.00.325.683 I print_info: rope type        = -1
0.00.325.683 I print_info: rope scaling     = linear
0.00.325.683 I print_info: freq_base_train  = 10000.0
0.00.325.684 I print_info: freq_scale_train = 1
0.00.325.684 I print_info: n_ctx_orig_yarn  = 8192
0.00.325.684 I print_info: rope_finetuned   = unknown
0.00.325.684 I print_info: ssm_d_conv       = 0
0.00.325.684 I print_info: ssm_d_inner      = 0
0.00.325.684 I print_info: ssm_d_state      = 0
0.00.325.686 I print_info: ssm_dt_rank      = 0
0.00.325.686 I print_info: ssm_dt_b_c_rms   = 0
0.00.325.686 I print_info: model type       = 33M
0.00.325.687 I print_info: model params     = 32.90 M
0.00.325.687 I print_info: general.name     = Jina Bert Implementation
0.00.325.688 I print_info: vocab type       = BPE
0.00.325.689 I print_info: n_vocab          = 61056
0.00.325.689 I print_info: n_merges         = 39382
0.00.325.689 I print_info: BOS token        = 0 '<s>'
0.00.325.689 I print_info: EOS token        = 2 '</s>'
0.00.325.690 I print_info: UNK token        = 3 '<unk>'
0.00.325.690 I print_info: SEP token        = 2 '</s>'
0.00.325.690 I print_info: PAD token        = 1 '<pad>'
0.00.325.696 I print_info: MASK token       = 4 '<mask>'
0.00.325.700 I print_info: EOG token        = 2 '</s>'
0.00.325.700 I print_info: max token length = 45
0.00.325.701 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.328.031 I load_tensors: offloading 4 repeating layers to GPU
0.00.328.032 I load_tensors: offloading output layer to GPU
0.00.328.033 I load_tensors: offloaded 5/5 layers to GPU
0.00.328.055 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.328.056 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.328.389 I llama_init_from_model: n_seq_max     = 1
0.00.328.390 I llama_init_from_model: n_ctx         = 8192
0.00.328.390 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.328.390 I llama_init_from_model: n_batch       = 2048
0.00.328.391 I llama_init_from_model: n_ubatch      = 2048
0.00.328.391 I llama_init_from_model: flash_attn    = 0
0.00.328.391 I llama_init_from_model: freq_base     = 10000.0
0.00.328.391 I llama_init_from_model: freq_scale    = 1
0.00.328.392 I ggml_metal_init: allocating
0.00.328.395 I ggml_metal_init: found device: Apple M4
0.00.328.398 I ggml_metal_init: picking default device: Apple M4
0.00.328.984 I ggml_metal_init: using embedded metal library
0.00.331.825 I ggml_metal_init: GPU name:   Apple M4
0.00.331.827 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.331.827 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.331.828 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.331.828 I ggml_metal_init: simdgroup reduction   = true
0.00.331.828 I ggml_metal_init: simdgroup matrix mul. = true
0.00.331.828 I ggml_metal_init: has residency sets    = true
0.00.331.828 I ggml_metal_init: has bfloat            = true
0.00.331.828 I ggml_metal_init: use bfloat            = true
0.00.331.829 I ggml_metal_init: hasUnifiedMemory      = true
0.00.331.831 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.341.634 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.344.630 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.344.632 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.344.634 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.352.456 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.352.458 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.352.458 I llama_init_from_model: graph nodes  = 154
0.00.352.458 I llama_init_from_model: graph splits = 2
0.00.352.460 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.352.460 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.834 I 
0.00.359.864 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.251 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.252 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.259 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.259 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.264 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.264 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.762 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.525 I llama_perf_context_print:        load time =     337.23 ms
0.00.364.526 I llama_perf_context_print: prompt eval time =       3.75 ms /    62 tokens (    0.06 ms per token, 16511.32 tokens per second)
0.00.364.530 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.530 I llama_perf_context_print:       total time =       4.69 ms /    63 tokens
0.00.364.776 I ggml_metal_free: deallocating

real	0m1.153s
user	0m0.333s
sys	0m0.051s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.203 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.394 I main: llama backend init
0.00.000.409 I main: load the model and apply lora adapter, if any
0.00.059.987 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.072.801 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.072.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.072.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.072.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.072.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.072.826 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.072.826 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.072.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.072.830 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.072.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.072.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.072.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.072.834 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.072.835 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.072.840 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.072.840 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.072.841 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.080.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.082.810 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.091.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.091.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.091.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.091.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.091.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.091.592 I llama_model_loader: - type  f32:  194 tensors
0.00.091.593 I llama_model_loader: - type  f16:   98 tensors
0.00.091.594 I print_info: file format = GGUF V3 (latest)
0.00.091.596 I print_info: file type   = all F32 (guessed)
0.00.091.598 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.947 I load: special tokens cache size = 25
0.00.116.270 I load: token to piece cache size = 0.2984 MB
0.00.116.299 I print_info: arch             = gptneox
0.00.116.300 I print_info: vocab_only       = 0
0.00.116.301 I print_info: n_ctx_train      = 2048
0.00.116.301 I print_info: n_embd           = 2048
0.00.116.301 I print_info: n_layer          = 24
0.00.116.308 I print_info: n_head           = 16
0.00.116.309 I print_info: n_head_kv        = 16
0.00.116.309 I print_info: n_rot            = 32
0.00.116.310 I print_info: n_swa            = 0
0.00.116.310 I print_info: n_embd_head_k    = 128
0.00.116.310 I print_info: n_embd_head_v    = 128
0.00.116.311 I print_info: n_gqa            = 1
0.00.116.312 I print_info: n_embd_k_gqa     = 2048
0.00.116.313 I print_info: n_embd_v_gqa     = 2048
0.00.116.313 I print_info: f_norm_eps       = 1.0e-05
0.00.116.314 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.116.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.116.315 I print_info: f_max_alibi_bias = 0.0e+00
0.00.116.315 I print_info: f_logit_scale    = 0.0e+00
0.00.116.316 I print_info: n_ff             = 8192
0.00.116.316 I print_info: n_expert         = 0
0.00.116.316 I print_info: n_expert_used    = 0
0.00.116.317 I print_info: causal attn      = 1
0.00.116.317 I print_info: pooling type     = 0
0.00.116.317 I print_info: rope type        = 2
0.00.116.321 I print_info: rope scaling     = linear
0.00.116.321 I print_info: freq_base_train  = 10000.0
0.00.116.322 I print_info: freq_scale_train = 1
0.00.116.322 I print_info: n_ctx_orig_yarn  = 2048
0.00.116.322 I print_info: rope_finetuned   = unknown
0.00.116.322 I print_info: ssm_d_conv       = 0
0.00.116.322 I print_info: ssm_d_inner      = 0
0.00.116.323 I print_info: ssm_d_state      = 0
0.00.116.323 I print_info: ssm_dt_rank      = 0
0.00.116.323 I print_info: ssm_dt_b_c_rms   = 0
0.00.116.323 I print_info: model type       = 1.4B
0.00.116.324 I print_info: model params     = 1.41 B
0.00.116.324 I print_info: general.name     = 1.4B
0.00.116.325 I print_info: vocab type       = BPE
0.00.116.326 I print_info: n_vocab          = 50304
0.00.116.326 I print_info: n_merges         = 50009
0.00.116.326 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.116.326 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.116.327 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.116.327 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.116.327 I print_info: LF token         = 187 ''
0.00.116.328 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.116.328 I print_info: max token length = 1024
0.00.116.328 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.171.323 I load_tensors: offloading 24 repeating layers to GPU
0.00.171.327 I load_tensors: offloading output layer to GPU
0.00.171.327 I load_tensors: offloaded 25/25 layers to GPU
0.00.171.353 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.171.354 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.171.963 I llama_init_from_model: n_seq_max     = 1
0.00.171.965 I llama_init_from_model: n_ctx         = 2048
0.00.171.965 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.171.965 I llama_init_from_model: n_batch       = 2048
0.00.171.965 I llama_init_from_model: n_ubatch      = 512
0.00.171.965 I llama_init_from_model: flash_attn    = 0
0.00.171.966 I llama_init_from_model: freq_base     = 10000.0
0.00.171.966 I llama_init_from_model: freq_scale    = 1
0.00.171.967 I ggml_metal_init: allocating
0.00.172.005 I ggml_metal_init: found device: Apple M4
0.00.172.010 I ggml_metal_init: picking default device: Apple M4
0.00.172.611 I ggml_metal_init: using embedded metal library
0.00.191.620 I ggml_metal_init: GPU name:   Apple M4
0.00.191.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.191.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.191.623 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.191.623 I ggml_metal_init: simdgroup reduction   = true
0.00.191.623 I ggml_metal_init: simdgroup matrix mul. = true
0.00.191.624 I ggml_metal_init: has residency sets    = true
0.00.191.624 I ggml_metal_init: has bfloat            = true
0.00.191.624 I ggml_metal_init: use bfloat            = true
0.00.191.624 I ggml_metal_init: hasUnifiedMemory      = true
0.00.191.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.247.177 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.277.280 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.277.287 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.277.308 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.281.120 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.281.123 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.281.123 I llama_init_from_model: graph nodes  = 967
0.00.281.123 I llama_init_from_model: graph splits = 2
0.00.281.130 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.281.259 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.281.260 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.346.041 I main: llama threadpool init, n_threads = 4
0.00.346.104 I 
0.00.346.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.346.133 I 
0.00.346.320 I sampler seed: 1234
0.00.346.325 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.346.359 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.346.361 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.346.361 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.179.659 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.02.179.659 I llama_perf_context_print:        load time =     285.14 ms
0.02.179.660 I llama_perf_context_print: prompt eval time =      43.81 ms /     7 tokens (    6.26 ms per token,   159.78 tokens per second)
0.02.179.661 I llama_perf_context_print:        eval time =    1786.65 ms /    63 runs   (   28.36 ms per token,    35.26 tokens per second)
0.02.179.661 I llama_perf_context_print:       total time =    1834.52 ms /    70 tokens
0.02.179.878 I ggml_metal_free: deallocating

real	0m2.583s
user	0m0.134s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.658 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.070 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.033 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.044 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.046 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.047 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.051 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.052 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.052 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.053 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.054 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.130 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.391 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.391 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.392 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.392 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.393 I llama_model_loader: - type  f32:  194 tensors
0.00.056.393 I llama_model_loader: - type  f16:   98 tensors
0.00.056.394 I print_info: file format = GGUF V3 (latest)
0.00.056.395 I print_info: file type   = all F32 (guessed)
0.00.056.396 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.942 I load: special tokens cache size = 25
0.00.077.346 I load: token to piece cache size = 0.2984 MB
0.00.077.361 I print_info: arch             = gptneox
0.00.077.362 I print_info: vocab_only       = 0
0.00.077.362 I print_info: n_ctx_train      = 2048
0.00.077.362 I print_info: n_embd           = 2048
0.00.077.363 I print_info: n_layer          = 24
0.00.077.366 I print_info: n_head           = 16
0.00.077.367 I print_info: n_head_kv        = 16
0.00.077.367 I print_info: n_rot            = 32
0.00.077.367 I print_info: n_swa            = 0
0.00.077.367 I print_info: n_embd_head_k    = 128
0.00.077.367 I print_info: n_embd_head_v    = 128
0.00.077.368 I print_info: n_gqa            = 1
0.00.077.369 I print_info: n_embd_k_gqa     = 2048
0.00.077.369 I print_info: n_embd_v_gqa     = 2048
0.00.077.370 I print_info: f_norm_eps       = 1.0e-05
0.00.077.370 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.371 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.375 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.375 I print_info: f_logit_scale    = 0.0e+00
0.00.077.376 I print_info: n_ff             = 8192
0.00.077.376 I print_info: n_expert         = 0
0.00.077.376 I print_info: n_expert_used    = 0
0.00.077.376 I print_info: causal attn      = 1
0.00.077.377 I print_info: pooling type     = 0
0.00.077.377 I print_info: rope type        = 2
0.00.077.381 I print_info: rope scaling     = linear
0.00.077.382 I print_info: freq_base_train  = 10000.0
0.00.077.382 I print_info: freq_scale_train = 1
0.00.077.382 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.382 I print_info: rope_finetuned   = unknown
0.00.077.383 I print_info: ssm_d_conv       = 0
0.00.077.383 I print_info: ssm_d_inner      = 0
0.00.077.383 I print_info: ssm_d_state      = 0
0.00.077.387 I print_info: ssm_dt_rank      = 0
0.00.077.387 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.387 I print_info: model type       = 1.4B
0.00.077.388 I print_info: model params     = 1.41 B
0.00.077.388 I print_info: general.name     = 1.4B
0.00.077.388 I print_info: vocab type       = BPE
0.00.077.390 I print_info: n_vocab          = 50304
0.00.077.390 I print_info: n_merges         = 50009
0.00.077.391 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.391 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.391 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.391 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.391 I print_info: LF token         = 187 ''
0.00.077.392 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.392 I print_info: max token length = 1024
0.00.077.392 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.448.053 I load_tensors: offloading 24 repeating layers to GPU
0.01.448.056 I load_tensors: offloading output layer to GPU
0.01.448.056 I load_tensors: offloaded 25/25 layers to GPU
0.01.448.085 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.448.087 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.449.047 I llama_init_from_model: n_seq_max     = 1
0.01.449.048 I llama_init_from_model: n_ctx         = 128
0.01.449.048 I llama_init_from_model: n_ctx_per_seq = 128
0.01.449.048 I llama_init_from_model: n_batch       = 128
0.01.449.048 I llama_init_from_model: n_ubatch      = 128
0.01.449.049 I llama_init_from_model: flash_attn    = 0
0.01.449.049 I llama_init_from_model: freq_base     = 10000.0
0.01.449.049 I llama_init_from_model: freq_scale    = 1
0.01.449.050 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.449.050 I ggml_metal_init: allocating
0.01.449.145 I ggml_metal_init: found device: Apple M4
0.01.449.155 I ggml_metal_init: picking default device: Apple M4
0.01.450.161 I ggml_metal_init: using embedded metal library
0.01.454.031 I ggml_metal_init: GPU name:   Apple M4
0.01.454.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.454.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.454.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.454.035 I ggml_metal_init: simdgroup reduction   = true
0.01.454.035 I ggml_metal_init: simdgroup matrix mul. = true
0.01.454.035 I ggml_metal_init: has residency sets    = true
0.01.454.035 I ggml_metal_init: has bfloat            = true
0.01.454.035 I ggml_metal_init: use bfloat            = true
0.01.454.036 I ggml_metal_init: hasUnifiedMemory      = true
0.01.454.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.465.596 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.467.346 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.467.349 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.467.362 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.469.112 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.469.113 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.469.113 I llama_init_from_model: graph nodes  = 967
0.01.469.114 I llama_init_from_model: graph splits = 2
0.01.469.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.469.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.504.986 I 
0.01.505.031 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.505.036 I perplexity: tokenizing the input ..
0.01.510.467 I perplexity: tokenization took 5.428 ms
0.01.510.471 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.642.156 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.643.506 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.643.539 I llama_perf_context_print:        load time =    1479.90 ms
0.01.643.540 I llama_perf_context_print: prompt eval time =     131.37 ms /   128 tokens (    1.03 ms per token,   974.32 tokens per second)
0.01.643.541 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.643.541 I llama_perf_context_print:       total time =     138.56 ms /   129 tokens
0.01.643.940 I ggml_metal_free: deallocating

real	0m1.858s
user	0m0.098s
sys	0m0.274s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.010.114 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.313 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.318 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.320 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.321 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.322 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.322 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.323 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.323 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.324 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.324 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.324 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.325 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.327 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.101 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.176 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.940 I llama_model_loader: - type  f32:  194 tensors
0.00.029.940 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.941 I print_info: file format = GGUF V3 (latest)
0.00.029.941 I print_info: file type   = Q8_0
0.00.029.943 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.038.094 I load: special tokens cache size = 25
0.00.044.585 I load: token to piece cache size = 0.2984 MB
0.00.044.603 I print_info: arch             = gptneox
0.00.044.604 I print_info: vocab_only       = 0
0.00.044.604 I print_info: n_ctx_train      = 2048
0.00.044.605 I print_info: n_embd           = 2048
0.00.044.605 I print_info: n_layer          = 24
0.00.044.611 I print_info: n_head           = 16
0.00.044.612 I print_info: n_head_kv        = 16
0.00.044.612 I print_info: n_rot            = 32
0.00.044.612 I print_info: n_swa            = 0
0.00.044.612 I print_info: n_embd_head_k    = 128
0.00.044.614 I print_info: n_embd_head_v    = 128
0.00.044.615 I print_info: n_gqa            = 1
0.00.044.616 I print_info: n_embd_k_gqa     = 2048
0.00.044.616 I print_info: n_embd_v_gqa     = 2048
0.00.044.617 I print_info: f_norm_eps       = 1.0e-05
0.00.044.618 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.618 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.618 I print_info: f_logit_scale    = 0.0e+00
0.00.044.619 I print_info: n_ff             = 8192
0.00.044.619 I print_info: n_expert         = 0
0.00.044.619 I print_info: n_expert_used    = 0
0.00.044.619 I print_info: causal attn      = 1
0.00.044.620 I print_info: pooling type     = 0
0.00.044.620 I print_info: rope type        = 2
0.00.044.621 I print_info: rope scaling     = linear
0.00.044.621 I print_info: freq_base_train  = 10000.0
0.00.044.621 I print_info: freq_scale_train = 1
0.00.044.621 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.622 I print_info: rope_finetuned   = unknown
0.00.044.623 I print_info: ssm_d_conv       = 0
0.00.044.623 I print_info: ssm_d_inner      = 0
0.00.044.624 I print_info: ssm_d_state      = 0
0.00.044.624 I print_info: ssm_dt_rank      = 0
0.00.044.624 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.624 I print_info: model type       = 1.4B
0.00.044.624 I print_info: model params     = 1.41 B
0.00.044.624 I print_info: general.name     = 1.4B
0.00.044.625 I print_info: vocab type       = BPE
0.00.044.625 I print_info: n_vocab          = 50304
0.00.044.625 I print_info: n_merges         = 50009
0.00.044.629 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.630 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.631 I print_info: LF token         = 187 ''
0.00.044.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.631 I print_info: max token length = 1024
0.00.044.632 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.063.212 I load_tensors: offloading 24 repeating layers to GPU
0.01.063.218 I load_tensors: offloading output layer to GPU
0.01.063.220 I load_tensors: offloaded 25/25 layers to GPU
0.01.063.245 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.063.250 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.064.255 I llama_init_from_model: n_seq_max     = 1
0.01.064.257 I llama_init_from_model: n_ctx         = 2048
0.01.064.258 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.064.258 I llama_init_from_model: n_batch       = 2048
0.01.064.259 I llama_init_from_model: n_ubatch      = 512
0.01.064.259 I llama_init_from_model: flash_attn    = 0
0.01.064.260 I llama_init_from_model: freq_base     = 10000.0
0.01.064.260 I llama_init_from_model: freq_scale    = 1
0.01.064.261 I ggml_metal_init: allocating
0.01.064.275 I ggml_metal_init: found device: Apple M4
0.01.064.284 I ggml_metal_init: picking default device: Apple M4
0.01.065.617 I ggml_metal_init: using embedded metal library
0.01.071.378 I ggml_metal_init: GPU name:   Apple M4
0.01.071.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.071.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.071.382 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.071.383 I ggml_metal_init: simdgroup reduction   = true
0.01.071.383 I ggml_metal_init: simdgroup matrix mul. = true
0.01.071.383 I ggml_metal_init: has residency sets    = true
0.01.071.384 I ggml_metal_init: has bfloat            = true
0.01.071.384 I ggml_metal_init: use bfloat            = true
0.01.071.385 I ggml_metal_init: hasUnifiedMemory      = true
0.01.071.386 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.087.601 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.155.498 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.155.504 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.155.530 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.160.257 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.160.259 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.160.259 I llama_init_from_model: graph nodes  = 967
0.01.160.259 I llama_init_from_model: graph splits = 2
0.01.160.266 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.160.394 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.160.395 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.216.809 I main: llama threadpool init, n_threads = 4
0.01.216.864 I 
0.01.216.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.216.886 I 
0.01.217.047 I sampler seed: 1234
0.01.217.051 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.217.096 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.217.099 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.217.099 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.303.251 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.02.303.252 I llama_perf_context_print:        load time =    1205.97 ms
0.02.303.253 I llama_perf_context_print: prompt eval time =      49.08 ms /     7 tokens (    7.01 ms per token,   142.63 tokens per second)
0.02.303.254 I llama_perf_context_print:        eval time =    1034.03 ms /    63 runs   (   16.41 ms per token,    60.93 tokens per second)
0.02.303.254 I llama_perf_context_print:       total time =    1087.17 ms /    70 tokens
0.02.303.541 I ggml_metal_free: deallocating

real	0m2.322s
user	0m0.111s
sys	0m0.286s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.236 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.363 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.371 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.372 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.379 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.171 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.020 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.021 I llama_model_loader: - type  f32:  194 tensors
0.00.025.021 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.022 I print_info: file format = GGUF V3 (latest)
0.00.025.022 I print_info: file type   = Q8_0
0.00.025.024 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.476 I load: special tokens cache size = 25
0.00.040.009 I load: token to piece cache size = 0.2984 MB
0.00.040.026 I print_info: arch             = gptneox
0.00.040.027 I print_info: vocab_only       = 0
0.00.040.028 I print_info: n_ctx_train      = 2048
0.00.040.028 I print_info: n_embd           = 2048
0.00.040.028 I print_info: n_layer          = 24
0.00.040.031 I print_info: n_head           = 16
0.00.040.032 I print_info: n_head_kv        = 16
0.00.040.032 I print_info: n_rot            = 32
0.00.040.032 I print_info: n_swa            = 0
0.00.040.032 I print_info: n_embd_head_k    = 128
0.00.040.032 I print_info: n_embd_head_v    = 128
0.00.040.033 I print_info: n_gqa            = 1
0.00.040.034 I print_info: n_embd_k_gqa     = 2048
0.00.040.034 I print_info: n_embd_v_gqa     = 2048
0.00.040.035 I print_info: f_norm_eps       = 1.0e-05
0.00.040.035 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.035 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.035 I print_info: f_logit_scale    = 0.0e+00
0.00.040.036 I print_info: n_ff             = 8192
0.00.040.036 I print_info: n_expert         = 0
0.00.040.036 I print_info: n_expert_used    = 0
0.00.040.036 I print_info: causal attn      = 1
0.00.040.037 I print_info: pooling type     = 0
0.00.040.037 I print_info: rope type        = 2
0.00.040.037 I print_info: rope scaling     = linear
0.00.040.037 I print_info: freq_base_train  = 10000.0
0.00.040.038 I print_info: freq_scale_train = 1
0.00.040.038 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.038 I print_info: rope_finetuned   = unknown
0.00.040.038 I print_info: ssm_d_conv       = 0
0.00.040.038 I print_info: ssm_d_inner      = 0
0.00.040.038 I print_info: ssm_d_state      = 0
0.00.040.039 I print_info: ssm_dt_rank      = 0
0.00.040.039 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.039 I print_info: model type       = 1.4B
0.00.040.039 I print_info: model params     = 1.41 B
0.00.040.039 I print_info: general.name     = 1.4B
0.00.040.040 I print_info: vocab type       = BPE
0.00.040.040 I print_info: n_vocab          = 50304
0.00.040.040 I print_info: n_merges         = 50009
0.00.040.040 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.040 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.041 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.041 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.041 I print_info: LF token         = 187 ''
0.00.040.042 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: max token length = 1024
0.00.040.043 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.776.764 I load_tensors: offloading 24 repeating layers to GPU
0.00.776.771 I load_tensors: offloading output layer to GPU
0.00.776.771 I load_tensors: offloaded 25/25 layers to GPU
0.00.776.799 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.776.801 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.778.195 I llama_init_from_model: n_seq_max     = 1
0.00.778.196 I llama_init_from_model: n_ctx         = 128
0.00.778.197 I llama_init_from_model: n_ctx_per_seq = 128
0.00.778.197 I llama_init_from_model: n_batch       = 128
0.00.778.197 I llama_init_from_model: n_ubatch      = 128
0.00.778.198 I llama_init_from_model: flash_attn    = 0
0.00.778.199 I llama_init_from_model: freq_base     = 10000.0
0.00.778.199 I llama_init_from_model: freq_scale    = 1
0.00.778.200 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.778.201 I ggml_metal_init: allocating
0.00.778.304 I ggml_metal_init: found device: Apple M4
0.00.778.315 I ggml_metal_init: picking default device: Apple M4
0.00.779.473 I ggml_metal_init: using embedded metal library
0.00.784.785 I ggml_metal_init: GPU name:   Apple M4
0.00.784.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.784.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.784.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.784.791 I ggml_metal_init: simdgroup reduction   = true
0.00.784.792 I ggml_metal_init: simdgroup matrix mul. = true
0.00.784.792 I ggml_metal_init: has residency sets    = true
0.00.784.792 I ggml_metal_init: has bfloat            = true
0.00.784.792 I ggml_metal_init: use bfloat            = true
0.00.784.793 I ggml_metal_init: hasUnifiedMemory      = true
0.00.784.795 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.800.383 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.803.821 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.803.825 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.803.849 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.806.854 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.806.856 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.806.856 I llama_init_from_model: graph nodes  = 967
0.00.806.857 I llama_init_from_model: graph splits = 2
0.00.806.859 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.806.860 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.457 I 
0.00.836.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.836.557 I perplexity: tokenizing the input ..
0.00.844.032 I perplexity: tokenization took 7.472 ms
0.00.844.041 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.983.415 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.984.764 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.984.785 I llama_perf_context_print:        load time =     827.21 ms
0.00.984.786 I llama_perf_context_print: prompt eval time =     138.51 ms /   128 tokens (    1.08 ms per token,   924.15 tokens per second)
0.00.984.787 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.984.787 I llama_perf_context_print:       total time =     148.33 ms /   129 tokens
0.00.985.170 I ggml_metal_free: deallocating

real	0m1.000s
user	0m0.078s
sys	0m0.165s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.014.134 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.913 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.024.919 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.921 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.922 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.922 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.923 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.923 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.924 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.925 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.925 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.926 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.926 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.926 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.928 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.929 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.929 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.791 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.784 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.785 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.785 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.786 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.786 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.786 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.787 I llama_model_loader: - type  f32:  194 tensors
0.00.033.787 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.787 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.788 I print_info: file format = GGUF V3 (latest)
0.00.033.789 I print_info: file type   = Q4_0
0.00.033.790 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.042.695 I load: special tokens cache size = 25
0.00.049.266 I load: token to piece cache size = 0.2984 MB
0.00.049.280 I print_info: arch             = gptneox
0.00.049.281 I print_info: vocab_only       = 0
0.00.049.281 I print_info: n_ctx_train      = 2048
0.00.049.281 I print_info: n_embd           = 2048
0.00.049.281 I print_info: n_layer          = 24
0.00.049.285 I print_info: n_head           = 16
0.00.049.286 I print_info: n_head_kv        = 16
0.00.049.286 I print_info: n_rot            = 32
0.00.049.286 I print_info: n_swa            = 0
0.00.049.287 I print_info: n_embd_head_k    = 128
0.00.049.287 I print_info: n_embd_head_v    = 128
0.00.049.288 I print_info: n_gqa            = 1
0.00.049.289 I print_info: n_embd_k_gqa     = 2048
0.00.049.289 I print_info: n_embd_v_gqa     = 2048
0.00.049.290 I print_info: f_norm_eps       = 1.0e-05
0.00.049.291 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.291 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.291 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.291 I print_info: f_logit_scale    = 0.0e+00
0.00.049.292 I print_info: n_ff             = 8192
0.00.049.292 I print_info: n_expert         = 0
0.00.049.292 I print_info: n_expert_used    = 0
0.00.049.292 I print_info: causal attn      = 1
0.00.049.292 I print_info: pooling type     = 0
0.00.049.293 I print_info: rope type        = 2
0.00.049.293 I print_info: rope scaling     = linear
0.00.049.294 I print_info: freq_base_train  = 10000.0
0.00.049.294 I print_info: freq_scale_train = 1
0.00.049.294 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.294 I print_info: rope_finetuned   = unknown
0.00.049.295 I print_info: ssm_d_conv       = 0
0.00.049.295 I print_info: ssm_d_inner      = 0
0.00.049.295 I print_info: ssm_d_state      = 0
0.00.049.295 I print_info: ssm_dt_rank      = 0
0.00.049.295 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.295 I print_info: model type       = 1.4B
0.00.049.297 I print_info: model params     = 1.41 B
0.00.049.297 I print_info: general.name     = 1.4B
0.00.049.298 I print_info: vocab type       = BPE
0.00.049.298 I print_info: n_vocab          = 50304
0.00.049.298 I print_info: n_merges         = 50009
0.00.049.299 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.299 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.299 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.300 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.300 I print_info: LF token         = 187 ''
0.00.049.301 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.302 I print_info: max token length = 1024
0.00.049.302 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.635.451 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.465 I load_tensors: offloading output layer to GPU
0.00.635.466 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.495 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.635.496 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.636.800 I llama_init_from_model: n_seq_max     = 1
0.00.636.806 I llama_init_from_model: n_ctx         = 2048
0.00.636.806 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.636.807 I llama_init_from_model: n_batch       = 2048
0.00.636.807 I llama_init_from_model: n_ubatch      = 512
0.00.636.808 I llama_init_from_model: flash_attn    = 0
0.00.636.809 I llama_init_from_model: freq_base     = 10000.0
0.00.636.810 I llama_init_from_model: freq_scale    = 1
0.00.636.812 I ggml_metal_init: allocating
0.00.636.898 I ggml_metal_init: found device: Apple M4
0.00.636.911 I ggml_metal_init: picking default device: Apple M4
0.00.638.413 I ggml_metal_init: using embedded metal library
0.00.644.566 I ggml_metal_init: GPU name:   Apple M4
0.00.644.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.579 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.579 I ggml_metal_init: simdgroup reduction   = true
0.00.644.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.580 I ggml_metal_init: has residency sets    = true
0.00.644.580 I ggml_metal_init: has bfloat            = true
0.00.644.580 I ggml_metal_init: use bfloat            = true
0.00.644.585 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.590 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.299 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.356 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.711.365 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.711.410 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.716.660 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.716.662 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.716.663 I llama_init_from_model: graph nodes  = 967
0.00.716.663 I llama_init_from_model: graph splits = 2
0.00.716.670 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.716.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.721 I main: llama threadpool init, n_threads = 4
0.00.773.772 I 
0.00.773.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.793 I 
0.00.773.977 I sampler seed: 1234
0.00.773.982 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.997 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.997 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.997 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.457.393 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.457.394 I llama_perf_context_print:        load time =     758.86 ms
0.01.457.395 I llama_perf_context_print: prompt eval time =      49.00 ms /     7 tokens (    7.00 ms per token,   142.85 tokens per second)
0.01.457.397 I llama_perf_context_print:        eval time =     631.55 ms /    63 runs   (   10.02 ms per token,    99.76 tokens per second)
0.01.457.397 I llama_perf_context_print:       total time =     684.40 ms /    70 tokens
0.01.457.638 I ggml_metal_free: deallocating

real	0m1.481s
user	0m0.115s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.618 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.810 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.816 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.820 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.821 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.821 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.823 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.824 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.825 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.634 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.646 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.456 I llama_model_loader: - type  f32:  194 tensors
0.00.025.457 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.458 I print_info: file format = GGUF V3 (latest)
0.00.025.458 I print_info: file type   = Q4_0
0.00.025.459 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.621 I load: special tokens cache size = 25
0.00.040.088 I load: token to piece cache size = 0.2984 MB
0.00.040.105 I print_info: arch             = gptneox
0.00.040.106 I print_info: vocab_only       = 0
0.00.040.106 I print_info: n_ctx_train      = 2048
0.00.040.107 I print_info: n_embd           = 2048
0.00.040.107 I print_info: n_layer          = 24
0.00.040.111 I print_info: n_head           = 16
0.00.040.112 I print_info: n_head_kv        = 16
0.00.040.112 I print_info: n_rot            = 32
0.00.040.112 I print_info: n_swa            = 0
0.00.040.112 I print_info: n_embd_head_k    = 128
0.00.040.112 I print_info: n_embd_head_v    = 128
0.00.040.113 I print_info: n_gqa            = 1
0.00.040.113 I print_info: n_embd_k_gqa     = 2048
0.00.040.114 I print_info: n_embd_v_gqa     = 2048
0.00.040.120 I print_info: f_norm_eps       = 1.0e-05
0.00.040.120 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.120 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.120 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.120 I print_info: f_logit_scale    = 0.0e+00
0.00.040.121 I print_info: n_ff             = 8192
0.00.040.122 I print_info: n_expert         = 0
0.00.040.126 I print_info: n_expert_used    = 0
0.00.040.126 I print_info: causal attn      = 1
0.00.040.126 I print_info: pooling type     = 0
0.00.040.126 I print_info: rope type        = 2
0.00.040.126 I print_info: rope scaling     = linear
0.00.040.127 I print_info: freq_base_train  = 10000.0
0.00.040.127 I print_info: freq_scale_train = 1
0.00.040.127 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.127 I print_info: rope_finetuned   = unknown
0.00.040.127 I print_info: ssm_d_conv       = 0
0.00.040.127 I print_info: ssm_d_inner      = 0
0.00.040.127 I print_info: ssm_d_state      = 0
0.00.040.127 I print_info: ssm_dt_rank      = 0
0.00.040.128 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.128 I print_info: model type       = 1.4B
0.00.040.128 I print_info: model params     = 1.41 B
0.00.040.128 I print_info: general.name     = 1.4B
0.00.040.129 I print_info: vocab type       = BPE
0.00.040.129 I print_info: n_vocab          = 50304
0.00.040.129 I print_info: n_merges         = 50009
0.00.040.130 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.130 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.131 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.131 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.131 I print_info: LF token         = 187 ''
0.00.040.131 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.131 I print_info: max token length = 1024
0.00.040.132 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.574.765 I load_tensors: offloading 24 repeating layers to GPU
0.00.574.777 I load_tensors: offloading output layer to GPU
0.00.574.778 I load_tensors: offloaded 25/25 layers to GPU
0.00.574.812 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.574.813 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.576.368 I llama_init_from_model: n_seq_max     = 1
0.00.576.374 I llama_init_from_model: n_ctx         = 128
0.00.576.375 I llama_init_from_model: n_ctx_per_seq = 128
0.00.576.376 I llama_init_from_model: n_batch       = 128
0.00.576.376 I llama_init_from_model: n_ubatch      = 128
0.00.576.376 I llama_init_from_model: flash_attn    = 0
0.00.576.377 I llama_init_from_model: freq_base     = 10000.0
0.00.576.378 I llama_init_from_model: freq_scale    = 1
0.00.576.378 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.576.381 I ggml_metal_init: allocating
0.00.576.449 I ggml_metal_init: found device: Apple M4
0.00.576.462 I ggml_metal_init: picking default device: Apple M4
0.00.578.375 I ggml_metal_init: using embedded metal library
0.00.584.665 I ggml_metal_init: GPU name:   Apple M4
0.00.584.673 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.584.674 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.584.675 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.584.676 I ggml_metal_init: simdgroup reduction   = true
0.00.584.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.584.676 I ggml_metal_init: has residency sets    = true
0.00.584.676 I ggml_metal_init: has bfloat            = true
0.00.584.677 I ggml_metal_init: use bfloat            = true
0.00.584.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.584.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.604.387 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.091 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.608.096 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.608.125 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.611.636 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.611.638 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.611.639 I llama_init_from_model: graph nodes  = 967
0.00.611.639 I llama_init_from_model: graph splits = 2
0.00.611.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.611.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.284 I 
0.00.639.378 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.387 I perplexity: tokenizing the input ..
0.00.646.728 I perplexity: tokenization took 7.34 ms
0.00.646.741 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.445 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.778.855 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.778.880 I llama_perf_context_print:        load time =     629.65 ms
0.00.778.881 I llama_perf_context_print: prompt eval time =     129.65 ms /   128 tokens (    1.01 ms per token,   987.28 tokens per second)
0.00.778.882 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.882 I llama_perf_context_print:       total time =     139.60 ms /   129 tokens
0.00.779.263 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.080s
sys	0m0.129s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.990 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.308 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.313 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.319 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.320 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.320 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.321 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.321 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.322 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.322 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.323 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.323 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.323 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.324 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.325 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.326 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.326 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.141 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.943 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.943 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.944 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.944 I llama_model_loader: - type  f32:  194 tensors
0.00.024.945 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.945 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.945 I print_info: file format = GGUF V3 (latest)
0.00.024.946 I print_info: file type   = Q4_1
0.00.024.947 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.103 I load: special tokens cache size = 25
0.00.039.280 I load: token to piece cache size = 0.2984 MB
0.00.039.294 I print_info: arch             = gptneox
0.00.039.295 I print_info: vocab_only       = 0
0.00.039.295 I print_info: n_ctx_train      = 2048
0.00.039.295 I print_info: n_embd           = 2048
0.00.039.296 I print_info: n_layer          = 24
0.00.039.299 I print_info: n_head           = 16
0.00.039.299 I print_info: n_head_kv        = 16
0.00.039.300 I print_info: n_rot            = 32
0.00.039.300 I print_info: n_swa            = 0
0.00.039.300 I print_info: n_embd_head_k    = 128
0.00.039.300 I print_info: n_embd_head_v    = 128
0.00.039.301 I print_info: n_gqa            = 1
0.00.039.302 I print_info: n_embd_k_gqa     = 2048
0.00.039.302 I print_info: n_embd_v_gqa     = 2048
0.00.039.303 I print_info: f_norm_eps       = 1.0e-05
0.00.039.303 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.304 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.304 I print_info: f_logit_scale    = 0.0e+00
0.00.039.304 I print_info: n_ff             = 8192
0.00.039.305 I print_info: n_expert         = 0
0.00.039.305 I print_info: n_expert_used    = 0
0.00.039.305 I print_info: causal attn      = 1
0.00.039.305 I print_info: pooling type     = 0
0.00.039.305 I print_info: rope type        = 2
0.00.039.310 I print_info: rope scaling     = linear
0.00.039.312 I print_info: freq_base_train  = 10000.0
0.00.039.312 I print_info: freq_scale_train = 1
0.00.039.313 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.313 I print_info: rope_finetuned   = unknown
0.00.039.313 I print_info: ssm_d_conv       = 0
0.00.039.313 I print_info: ssm_d_inner      = 0
0.00.039.313 I print_info: ssm_d_state      = 0
0.00.039.313 I print_info: ssm_dt_rank      = 0
0.00.039.314 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.314 I print_info: model type       = 1.4B
0.00.039.314 I print_info: model params     = 1.41 B
0.00.039.314 I print_info: general.name     = 1.4B
0.00.039.315 I print_info: vocab type       = BPE
0.00.039.315 I print_info: n_vocab          = 50304
0.00.039.316 I print_info: n_merges         = 50009
0.00.039.316 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.316 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.317 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.317 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.317 I print_info: LF token         = 187 ''
0.00.039.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.317 I print_info: max token length = 1024
0.00.039.318 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.711.514 I load_tensors: offloading 24 repeating layers to GPU
0.00.711.526 I load_tensors: offloading output layer to GPU
0.00.711.527 I load_tensors: offloaded 25/25 layers to GPU
0.00.711.567 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.711.569 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.713.347 I llama_init_from_model: n_seq_max     = 1
0.00.713.350 I llama_init_from_model: n_ctx         = 2048
0.00.713.350 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.713.351 I llama_init_from_model: n_batch       = 2048
0.00.713.351 I llama_init_from_model: n_ubatch      = 512
0.00.713.352 I llama_init_from_model: flash_attn    = 0
0.00.713.354 I llama_init_from_model: freq_base     = 10000.0
0.00.713.355 I llama_init_from_model: freq_scale    = 1
0.00.713.357 I ggml_metal_init: allocating
0.00.713.468 I ggml_metal_init: found device: Apple M4
0.00.713.482 I ggml_metal_init: picking default device: Apple M4
0.00.715.178 I ggml_metal_init: using embedded metal library
0.00.721.728 I ggml_metal_init: GPU name:   Apple M4
0.00.721.732 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.721.733 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.721.734 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.721.735 I ggml_metal_init: simdgroup reduction   = true
0.00.721.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.721.736 I ggml_metal_init: has residency sets    = true
0.00.721.736 I ggml_metal_init: has bfloat            = true
0.00.721.736 I ggml_metal_init: use bfloat            = true
0.00.721.737 I ggml_metal_init: hasUnifiedMemory      = true
0.00.721.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.740.082 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.813.088 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.813.094 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.813.127 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.817.329 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.817.331 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.817.331 I llama_init_from_model: graph nodes  = 967
0.00.817.331 I llama_init_from_model: graph splits = 2
0.00.817.337 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.817.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.817.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.875.104 I main: llama threadpool init, n_threads = 4
0.00.875.148 I 
0.00.875.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.875.170 I 
0.00.875.340 I sampler seed: 1234
0.00.875.345 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.875.361 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.875.362 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.875.362 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.615.393 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.01.615.393 I llama_perf_context_print:        load time =     865.34 ms
0.01.615.394 I llama_perf_context_print: prompt eval time =      49.15 ms /     7 tokens (    7.02 ms per token,   142.42 tokens per second)
0.01.615.396 I llama_perf_context_print:        eval time =     688.15 ms /    63 runs   (   10.92 ms per token,    91.55 tokens per second)
0.01.615.396 I llama_perf_context_print:       total time =     741.06 ms /    70 tokens
0.01.615.645 I ggml_metal_free: deallocating

real	0m1.632s
user	0m0.111s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.029 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.008 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.020 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.816 I llama_model_loader: - type  f32:  194 tensors
0.00.024.816 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.817 I print_info: file format = GGUF V3 (latest)
0.00.024.817 I print_info: file type   = Q4_1
0.00.024.819 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.130 I load: special tokens cache size = 25
0.00.039.542 I load: token to piece cache size = 0.2984 MB
0.00.039.554 I print_info: arch             = gptneox
0.00.039.555 I print_info: vocab_only       = 0
0.00.039.555 I print_info: n_ctx_train      = 2048
0.00.039.556 I print_info: n_embd           = 2048
0.00.039.556 I print_info: n_layer          = 24
0.00.039.560 I print_info: n_head           = 16
0.00.039.560 I print_info: n_head_kv        = 16
0.00.039.561 I print_info: n_rot            = 32
0.00.039.561 I print_info: n_swa            = 0
0.00.039.561 I print_info: n_embd_head_k    = 128
0.00.039.561 I print_info: n_embd_head_v    = 128
0.00.039.564 I print_info: n_gqa            = 1
0.00.039.565 I print_info: n_embd_k_gqa     = 2048
0.00.039.566 I print_info: n_embd_v_gqa     = 2048
0.00.039.566 I print_info: f_norm_eps       = 1.0e-05
0.00.039.567 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.567 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.567 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.567 I print_info: f_logit_scale    = 0.0e+00
0.00.039.568 I print_info: n_ff             = 8192
0.00.039.568 I print_info: n_expert         = 0
0.00.039.568 I print_info: n_expert_used    = 0
0.00.039.569 I print_info: causal attn      = 1
0.00.039.569 I print_info: pooling type     = 0
0.00.039.569 I print_info: rope type        = 2
0.00.039.570 I print_info: rope scaling     = linear
0.00.039.572 I print_info: freq_base_train  = 10000.0
0.00.039.572 I print_info: freq_scale_train = 1
0.00.039.572 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.573 I print_info: rope_finetuned   = unknown
0.00.039.573 I print_info: ssm_d_conv       = 0
0.00.039.573 I print_info: ssm_d_inner      = 0
0.00.039.573 I print_info: ssm_d_state      = 0
0.00.039.573 I print_info: ssm_dt_rank      = 0
0.00.039.573 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.573 I print_info: model type       = 1.4B
0.00.039.574 I print_info: model params     = 1.41 B
0.00.039.574 I print_info: general.name     = 1.4B
0.00.039.575 I print_info: vocab type       = BPE
0.00.039.579 I print_info: n_vocab          = 50304
0.00.039.579 I print_info: n_merges         = 50009
0.00.039.579 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.579 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: LF token         = 187 ''
0.00.039.580 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: max token length = 1024
0.00.039.581 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.682.035 I load_tensors: offloading 24 repeating layers to GPU
0.00.682.048 I load_tensors: offloading output layer to GPU
0.00.682.049 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.084 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.682.086 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.683.687 I llama_init_from_model: n_seq_max     = 1
0.00.683.694 I llama_init_from_model: n_ctx         = 128
0.00.683.695 I llama_init_from_model: n_ctx_per_seq = 128
0.00.683.696 I llama_init_from_model: n_batch       = 128
0.00.683.696 I llama_init_from_model: n_ubatch      = 128
0.00.683.696 I llama_init_from_model: flash_attn    = 0
0.00.683.699 I llama_init_from_model: freq_base     = 10000.0
0.00.683.699 I llama_init_from_model: freq_scale    = 1
0.00.683.700 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.683.702 I ggml_metal_init: allocating
0.00.683.795 I ggml_metal_init: found device: Apple M4
0.00.683.810 I ggml_metal_init: picking default device: Apple M4
0.00.685.391 I ggml_metal_init: using embedded metal library
0.00.690.649 I ggml_metal_init: GPU name:   Apple M4
0.00.690.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.690.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.690.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.690.656 I ggml_metal_init: simdgroup reduction   = true
0.00.690.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.690.656 I ggml_metal_init: has residency sets    = true
0.00.690.656 I ggml_metal_init: has bfloat            = true
0.00.690.656 I ggml_metal_init: use bfloat            = true
0.00.690.657 I ggml_metal_init: hasUnifiedMemory      = true
0.00.690.661 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.704.255 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.706.206 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.706.211 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.706.226 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.708.201 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.708.202 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.708.203 I llama_init_from_model: graph nodes  = 967
0.00.708.203 I llama_init_from_model: graph splits = 2
0.00.708.205 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.708.205 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.421 I 
0.00.729.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.462 I perplexity: tokenizing the input ..
0.00.733.464 I perplexity: tokenization took 4 ms
0.00.733.470 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.601 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.860.047 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.860.083 I llama_perf_context_print:        load time =     720.39 ms
0.00.860.084 I llama_perf_context_print: prompt eval time =     121.90 ms /   128 tokens (    0.95 ms per token,  1050.08 tokens per second)
0.00.860.085 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.860.087 I llama_perf_context_print:       total time =     130.66 ms /   129 tokens
0.00.860.855 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.088s
sys	0m0.107s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.191 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.812 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.813 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.814 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.815 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.821 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.822 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.552 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.255 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.256 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.257 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.257 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.257 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.258 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.258 I llama_model_loader: - type  f32:  194 tensors
0.00.026.258 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.259 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.259 I print_info: file format = GGUF V3 (latest)
0.00.026.260 I print_info: file type   = Q5_0
0.00.026.261 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.099 I load: special tokens cache size = 25
0.00.040.118 I load: token to piece cache size = 0.2984 MB
0.00.040.131 I print_info: arch             = gptneox
0.00.040.133 I print_info: vocab_only       = 0
0.00.040.133 I print_info: n_ctx_train      = 2048
0.00.040.133 I print_info: n_embd           = 2048
0.00.040.133 I print_info: n_layer          = 24
0.00.040.136 I print_info: n_head           = 16
0.00.040.137 I print_info: n_head_kv        = 16
0.00.040.137 I print_info: n_rot            = 32
0.00.040.137 I print_info: n_swa            = 0
0.00.040.137 I print_info: n_embd_head_k    = 128
0.00.040.137 I print_info: n_embd_head_v    = 128
0.00.040.138 I print_info: n_gqa            = 1
0.00.040.139 I print_info: n_embd_k_gqa     = 2048
0.00.040.139 I print_info: n_embd_v_gqa     = 2048
0.00.040.140 I print_info: f_norm_eps       = 1.0e-05
0.00.040.140 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.141 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.141 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.141 I print_info: f_logit_scale    = 0.0e+00
0.00.040.148 I print_info: n_ff             = 8192
0.00.040.150 I print_info: n_expert         = 0
0.00.040.150 I print_info: n_expert_used    = 0
0.00.040.150 I print_info: causal attn      = 1
0.00.040.151 I print_info: pooling type     = 0
0.00.040.152 I print_info: rope type        = 2
0.00.040.153 I print_info: rope scaling     = linear
0.00.040.154 I print_info: freq_base_train  = 10000.0
0.00.040.154 I print_info: freq_scale_train = 1
0.00.040.154 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.156 I print_info: rope_finetuned   = unknown
0.00.040.156 I print_info: ssm_d_conv       = 0
0.00.040.156 I print_info: ssm_d_inner      = 0
0.00.040.156 I print_info: ssm_d_state      = 0
0.00.040.156 I print_info: ssm_dt_rank      = 0
0.00.040.156 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.156 I print_info: model type       = 1.4B
0.00.040.157 I print_info: model params     = 1.41 B
0.00.040.157 I print_info: general.name     = 1.4B
0.00.040.158 I print_info: vocab type       = BPE
0.00.040.158 I print_info: n_vocab          = 50304
0.00.040.158 I print_info: n_merges         = 50009
0.00.040.158 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.159 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.159 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: LF token         = 187 ''
0.00.040.160 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.160 I print_info: max token length = 1024
0.00.040.161 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.631.773 I load_tensors: offloading 24 repeating layers to GPU
0.00.631.788 I load_tensors: offloading output layer to GPU
0.00.631.789 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.823 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.631.825 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.633.425 I llama_init_from_model: n_seq_max     = 1
0.00.633.428 I llama_init_from_model: n_ctx         = 2048
0.00.633.429 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.633.430 I llama_init_from_model: n_batch       = 2048
0.00.633.430 I llama_init_from_model: n_ubatch      = 512
0.00.633.430 I llama_init_from_model: flash_attn    = 0
0.00.633.433 I llama_init_from_model: freq_base     = 10000.0
0.00.633.434 I llama_init_from_model: freq_scale    = 1
0.00.633.436 I ggml_metal_init: allocating
0.00.633.511 I ggml_metal_init: found device: Apple M4
0.00.633.525 I ggml_metal_init: picking default device: Apple M4
0.00.635.127 I ggml_metal_init: using embedded metal library
0.00.641.949 I ggml_metal_init: GPU name:   Apple M4
0.00.641.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.956 I ggml_metal_init: simdgroup reduction   = true
0.00.641.956 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.956 I ggml_metal_init: has residency sets    = true
0.00.641.956 I ggml_metal_init: has bfloat            = true
0.00.641.957 I ggml_metal_init: use bfloat            = true
0.00.641.958 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.959 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.207 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.406 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.720.414 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.720.443 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.724.687 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.724.689 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.724.690 I llama_init_from_model: graph nodes  = 967
0.00.724.690 I llama_init_from_model: graph splits = 2
0.00.724.695 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.979 I main: llama threadpool init, n_threads = 4
0.00.781.028 I 
0.00.781.048 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.049 I 
0.00.781.197 I sampler seed: 1234
0.00.781.201 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.246 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.249 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.250 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.569.102 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49964.81 tokens per second)
0.01.569.103 I llama_perf_context_print:        load time =     770.07 ms
0.01.569.106 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.38 tokens per second)
0.01.569.106 I llama_perf_context_print:        eval time =     741.75 ms /    63 runs   (   11.77 ms per token,    84.93 tokens per second)
0.01.569.107 I llama_perf_context_print:       total time =     788.84 ms /    70 tokens
0.01.569.392 I ggml_metal_free: deallocating

real	0m1.592s
user	0m0.110s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.431 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.006 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.012 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.019 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.019 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.020 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.020 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.021 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.021 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.022 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.022 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.023 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.024 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.026 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.026 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.027 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.921 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.798 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.799 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.801 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.801 I llama_model_loader: - type  f32:  194 tensors
0.00.025.802 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.802 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.803 I print_info: file format = GGUF V3 (latest)
0.00.025.803 I print_info: file type   = Q5_0
0.00.025.804 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.120 I load: special tokens cache size = 25
0.00.040.613 I load: token to piece cache size = 0.2984 MB
0.00.040.630 I print_info: arch             = gptneox
0.00.040.631 I print_info: vocab_only       = 0
0.00.040.631 I print_info: n_ctx_train      = 2048
0.00.040.631 I print_info: n_embd           = 2048
0.00.040.632 I print_info: n_layer          = 24
0.00.040.635 I print_info: n_head           = 16
0.00.040.635 I print_info: n_head_kv        = 16
0.00.040.636 I print_info: n_rot            = 32
0.00.040.636 I print_info: n_swa            = 0
0.00.040.636 I print_info: n_embd_head_k    = 128
0.00.040.636 I print_info: n_embd_head_v    = 128
0.00.040.637 I print_info: n_gqa            = 1
0.00.040.637 I print_info: n_embd_k_gqa     = 2048
0.00.040.638 I print_info: n_embd_v_gqa     = 2048
0.00.040.638 I print_info: f_norm_eps       = 1.0e-05
0.00.040.639 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.639 I print_info: f_logit_scale    = 0.0e+00
0.00.040.640 I print_info: n_ff             = 8192
0.00.040.640 I print_info: n_expert         = 0
0.00.040.640 I print_info: n_expert_used    = 0
0.00.040.640 I print_info: causal attn      = 1
0.00.040.640 I print_info: pooling type     = 0
0.00.040.640 I print_info: rope type        = 2
0.00.040.642 I print_info: rope scaling     = linear
0.00.040.643 I print_info: freq_base_train  = 10000.0
0.00.040.643 I print_info: freq_scale_train = 1
0.00.040.643 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.644 I print_info: rope_finetuned   = unknown
0.00.040.644 I print_info: ssm_d_conv       = 0
0.00.040.644 I print_info: ssm_d_inner      = 0
0.00.040.644 I print_info: ssm_d_state      = 0
0.00.040.644 I print_info: ssm_dt_rank      = 0
0.00.040.644 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.644 I print_info: model type       = 1.4B
0.00.040.645 I print_info: model params     = 1.41 B
0.00.040.647 I print_info: general.name     = 1.4B
0.00.040.647 I print_info: vocab type       = BPE
0.00.040.647 I print_info: n_vocab          = 50304
0.00.040.647 I print_info: n_merges         = 50009
0.00.040.648 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.648 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.648 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.648 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.648 I print_info: LF token         = 187 ''
0.00.040.648 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.649 I print_info: max token length = 1024
0.00.040.649 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.614 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.618 I load_tensors: offloading output layer to GPU
0.00.619.619 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.637 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.619.638 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.620.529 I llama_init_from_model: n_seq_max     = 1
0.00.620.535 I llama_init_from_model: n_ctx         = 128
0.00.620.535 I llama_init_from_model: n_ctx_per_seq = 128
0.00.620.535 I llama_init_from_model: n_batch       = 128
0.00.620.535 I llama_init_from_model: n_ubatch      = 128
0.00.620.536 I llama_init_from_model: flash_attn    = 0
0.00.620.537 I llama_init_from_model: freq_base     = 10000.0
0.00.620.537 I llama_init_from_model: freq_scale    = 1
0.00.620.538 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.620.539 I ggml_metal_init: allocating
0.00.620.565 I ggml_metal_init: found device: Apple M4
0.00.620.575 I ggml_metal_init: picking default device: Apple M4
0.00.621.543 I ggml_metal_init: using embedded metal library
0.00.625.962 I ggml_metal_init: GPU name:   Apple M4
0.00.625.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.970 I ggml_metal_init: simdgroup reduction   = true
0.00.625.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.971 I ggml_metal_init: has residency sets    = true
0.00.625.971 I ggml_metal_init: has bfloat            = true
0.00.625.971 I ggml_metal_init: use bfloat            = true
0.00.625.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.813 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.614 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.618 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.638 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.644.278 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.644.279 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.644.279 I llama_init_from_model: graph nodes  = 967
0.00.644.280 I llama_init_from_model: graph splits = 2
0.00.644.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.911 I 
0.00.672.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.963 I perplexity: tokenizing the input ..
0.00.676.986 I perplexity: tokenization took 4.021 ms
0.00.676.989 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.195 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.826.428 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.826.455 I llama_perf_context_print:        load time =     663.48 ms
0.00.826.456 I llama_perf_context_print: prompt eval time =     147.95 ms /   128 tokens (    1.16 ms per token,   865.14 tokens per second)
0.00.826.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.457 I llama_perf_context_print:       total time =     153.54 ms /   129 tokens
0.00.826.817 I ggml_metal_free: deallocating

real	0m0.846s
user	0m0.072s
sys	0m0.109s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.201 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.719 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.729 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.731 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.732 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.732 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.734 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.734 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.735 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.735 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.736 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.736 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.739 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.740 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.740 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.742 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.742 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.743 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.336 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.337 I llama_model_loader: - type  f32:  194 tensors
0.00.026.337 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.338 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.338 I print_info: file format = GGUF V3 (latest)
0.00.026.339 I print_info: file type   = Q5_1
0.00.026.340 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.570 I load: special tokens cache size = 25
0.00.040.909 I load: token to piece cache size = 0.2984 MB
0.00.040.918 I print_info: arch             = gptneox
0.00.040.920 I print_info: vocab_only       = 0
0.00.040.920 I print_info: n_ctx_train      = 2048
0.00.040.920 I print_info: n_embd           = 2048
0.00.040.920 I print_info: n_layer          = 24
0.00.040.923 I print_info: n_head           = 16
0.00.040.924 I print_info: n_head_kv        = 16
0.00.040.924 I print_info: n_rot            = 32
0.00.040.924 I print_info: n_swa            = 0
0.00.040.924 I print_info: n_embd_head_k    = 128
0.00.040.925 I print_info: n_embd_head_v    = 128
0.00.040.925 I print_info: n_gqa            = 1
0.00.040.926 I print_info: n_embd_k_gqa     = 2048
0.00.040.927 I print_info: n_embd_v_gqa     = 2048
0.00.040.927 I print_info: f_norm_eps       = 1.0e-05
0.00.040.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.932 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.932 I print_info: f_logit_scale    = 0.0e+00
0.00.040.933 I print_info: n_ff             = 8192
0.00.040.933 I print_info: n_expert         = 0
0.00.040.935 I print_info: n_expert_used    = 0
0.00.040.935 I print_info: causal attn      = 1
0.00.040.935 I print_info: pooling type     = 0
0.00.040.935 I print_info: rope type        = 2
0.00.040.935 I print_info: rope scaling     = linear
0.00.040.936 I print_info: freq_base_train  = 10000.0
0.00.040.936 I print_info: freq_scale_train = 1
0.00.040.937 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.937 I print_info: rope_finetuned   = unknown
0.00.040.937 I print_info: ssm_d_conv       = 0
0.00.040.937 I print_info: ssm_d_inner      = 0
0.00.040.937 I print_info: ssm_d_state      = 0
0.00.040.938 I print_info: ssm_dt_rank      = 0
0.00.040.938 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.938 I print_info: model type       = 1.4B
0.00.040.938 I print_info: model params     = 1.41 B
0.00.040.938 I print_info: general.name     = 1.4B
0.00.040.939 I print_info: vocab type       = BPE
0.00.040.939 I print_info: n_vocab          = 50304
0.00.040.939 I print_info: n_merges         = 50009
0.00.040.940 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.940 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.940 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.940 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.940 I print_info: LF token         = 187 ''
0.00.040.942 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.942 I print_info: max token length = 1024
0.00.040.942 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.655.516 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.526 I load_tensors: offloading output layer to GPU
0.00.655.527 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.561 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.655.565 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.657.362 I llama_init_from_model: n_seq_max     = 1
0.00.657.366 I llama_init_from_model: n_ctx         = 2048
0.00.657.366 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.657.367 I llama_init_from_model: n_batch       = 2048
0.00.657.367 I llama_init_from_model: n_ubatch      = 512
0.00.657.368 I llama_init_from_model: flash_attn    = 0
0.00.657.370 I llama_init_from_model: freq_base     = 10000.0
0.00.657.370 I llama_init_from_model: freq_scale    = 1
0.00.657.376 I ggml_metal_init: allocating
0.00.657.441 I ggml_metal_init: found device: Apple M4
0.00.657.454 I ggml_metal_init: picking default device: Apple M4
0.00.659.257 I ggml_metal_init: using embedded metal library
0.00.666.023 I ggml_metal_init: GPU name:   Apple M4
0.00.666.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.027 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.028 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.028 I ggml_metal_init: simdgroup reduction   = true
0.00.666.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.029 I ggml_metal_init: has residency sets    = true
0.00.666.029 I ggml_metal_init: has bfloat            = true
0.00.666.029 I ggml_metal_init: use bfloat            = true
0.00.666.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.583 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.756.067 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.756.074 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.756.097 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.761.883 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.761.886 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.761.887 I llama_init_from_model: graph nodes  = 967
0.00.761.887 I llama_init_from_model: graph splits = 2
0.00.761.893 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.762.026 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.762.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.511 I main: llama threadpool init, n_threads = 4
0.00.821.560 I 
0.00.821.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.581 I 
0.00.821.742 I sampler seed: 1234
0.00.821.747 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.762 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.764 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.764 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.660.035 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.660.036 I llama_perf_context_print:        load time =     811.58 ms
0.01.660.037 I llama_perf_context_print: prompt eval time =      51.98 ms /     7 tokens (    7.43 ms per token,   134.66 tokens per second)
0.01.660.037 I llama_perf_context_print:        eval time =     783.37 ms /    63 runs   (   12.43 ms per token,    80.42 tokens per second)
0.01.660.038 I llama_perf_context_print:       total time =     839.25 ms /    70 tokens
0.01.660.310 I ggml_metal_free: deallocating

real	0m1.677s
user	0m0.110s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.081 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.467 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.472 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.479 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.483 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.484 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.484 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.486 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.486 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.486 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.281 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.018 I llama_model_loader: - type  f32:  194 tensors
0.00.026.018 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.018 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.019 I print_info: file format = GGUF V3 (latest)
0.00.026.019 I print_info: file type   = Q5_1
0.00.026.020 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.285 I load: special tokens cache size = 25
0.00.040.674 I load: token to piece cache size = 0.2984 MB
0.00.040.689 I print_info: arch             = gptneox
0.00.040.690 I print_info: vocab_only       = 0
0.00.040.690 I print_info: n_ctx_train      = 2048
0.00.040.690 I print_info: n_embd           = 2048
0.00.040.690 I print_info: n_layer          = 24
0.00.040.693 I print_info: n_head           = 16
0.00.040.694 I print_info: n_head_kv        = 16
0.00.040.694 I print_info: n_rot            = 32
0.00.040.695 I print_info: n_swa            = 0
0.00.040.695 I print_info: n_embd_head_k    = 128
0.00.040.695 I print_info: n_embd_head_v    = 128
0.00.040.696 I print_info: n_gqa            = 1
0.00.040.697 I print_info: n_embd_k_gqa     = 2048
0.00.040.697 I print_info: n_embd_v_gqa     = 2048
0.00.040.698 I print_info: f_norm_eps       = 1.0e-05
0.00.040.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.699 I print_info: f_logit_scale    = 0.0e+00
0.00.040.700 I print_info: n_ff             = 8192
0.00.040.700 I print_info: n_expert         = 0
0.00.040.700 I print_info: n_expert_used    = 0
0.00.040.700 I print_info: causal attn      = 1
0.00.040.700 I print_info: pooling type     = 0
0.00.040.700 I print_info: rope type        = 2
0.00.040.701 I print_info: rope scaling     = linear
0.00.040.701 I print_info: freq_base_train  = 10000.0
0.00.040.701 I print_info: freq_scale_train = 1
0.00.040.701 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.702 I print_info: rope_finetuned   = unknown
0.00.040.702 I print_info: ssm_d_conv       = 0
0.00.040.702 I print_info: ssm_d_inner      = 0
0.00.040.702 I print_info: ssm_d_state      = 0
0.00.040.702 I print_info: ssm_dt_rank      = 0
0.00.040.702 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.702 I print_info: model type       = 1.4B
0.00.040.703 I print_info: model params     = 1.41 B
0.00.040.705 I print_info: general.name     = 1.4B
0.00.040.705 I print_info: vocab type       = BPE
0.00.040.705 I print_info: n_vocab          = 50304
0.00.040.705 I print_info: n_merges         = 50009
0.00.040.706 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.706 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.706 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.706 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.706 I print_info: LF token         = 187 ''
0.00.040.706 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.706 I print_info: max token length = 1024
0.00.040.707 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.631 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.638 I load_tensors: offloading output layer to GPU
0.00.660.639 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.671 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.660.674 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.661.984 I llama_init_from_model: n_seq_max     = 1
0.00.661.991 I llama_init_from_model: n_ctx         = 128
0.00.661.992 I llama_init_from_model: n_ctx_per_seq = 128
0.00.661.993 I llama_init_from_model: n_batch       = 128
0.00.661.993 I llama_init_from_model: n_ubatch      = 128
0.00.661.993 I llama_init_from_model: flash_attn    = 0
0.00.661.995 I llama_init_from_model: freq_base     = 10000.0
0.00.661.996 I llama_init_from_model: freq_scale    = 1
0.00.661.997 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.661.999 I ggml_metal_init: allocating
0.00.662.080 I ggml_metal_init: found device: Apple M4
0.00.662.093 I ggml_metal_init: picking default device: Apple M4
0.00.663.522 I ggml_metal_init: using embedded metal library
0.00.669.534 I ggml_metal_init: GPU name:   Apple M4
0.00.669.543 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.669.544 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.669.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.669.545 I ggml_metal_init: simdgroup reduction   = true
0.00.669.545 I ggml_metal_init: simdgroup matrix mul. = true
0.00.669.545 I ggml_metal_init: has residency sets    = true
0.00.669.545 I ggml_metal_init: has bfloat            = true
0.00.669.546 I ggml_metal_init: use bfloat            = true
0.00.669.547 I ggml_metal_init: hasUnifiedMemory      = true
0.00.669.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.686.856 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.690.404 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.690.408 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.690.445 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.693.654 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.693.656 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.693.657 I llama_init_from_model: graph nodes  = 967
0.00.693.658 I llama_init_from_model: graph splits = 2
0.00.693.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.693.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.813 I 
0.00.733.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.733.856 I perplexity: tokenizing the input ..
0.00.737.933 I perplexity: tokenization took 4.075 ms
0.00.737.940 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.847 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.873.245 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.873.270 I llama_perf_context_print:        load time =     723.73 ms
0.00.873.271 I llama_perf_context_print: prompt eval time =     133.67 ms /   128 tokens (    1.04 ms per token,   957.59 tokens per second)
0.00.873.272 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.873.272 I llama_perf_context_print:       total time =     139.46 ms /   129 tokens
0.00.873.633 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.074s
sys	0m0.139s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.515 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.262 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.269 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.270 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.273 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.274 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.275 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.072 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.901 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.902 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.903 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.904 I llama_model_loader: - type  f32:  194 tensors
0.00.025.904 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.904 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.904 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.905 I print_info: file format = GGUF V3 (latest)
0.00.025.905 I print_info: file type   = Q2_K - Medium
0.00.025.906 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.034 I load: special tokens cache size = 25
0.00.040.547 I load: token to piece cache size = 0.2984 MB
0.00.040.556 I print_info: arch             = gptneox
0.00.040.556 I print_info: vocab_only       = 0
0.00.040.557 I print_info: n_ctx_train      = 2048
0.00.040.557 I print_info: n_embd           = 2048
0.00.040.557 I print_info: n_layer          = 24
0.00.040.560 I print_info: n_head           = 16
0.00.040.561 I print_info: n_head_kv        = 16
0.00.040.561 I print_info: n_rot            = 32
0.00.040.561 I print_info: n_swa            = 0
0.00.040.562 I print_info: n_embd_head_k    = 128
0.00.040.562 I print_info: n_embd_head_v    = 128
0.00.040.562 I print_info: n_gqa            = 1
0.00.040.563 I print_info: n_embd_k_gqa     = 2048
0.00.040.564 I print_info: n_embd_v_gqa     = 2048
0.00.040.564 I print_info: f_norm_eps       = 1.0e-05
0.00.040.565 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.565 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.565 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.565 I print_info: f_logit_scale    = 0.0e+00
0.00.040.566 I print_info: n_ff             = 8192
0.00.040.566 I print_info: n_expert         = 0
0.00.040.566 I print_info: n_expert_used    = 0
0.00.040.566 I print_info: causal attn      = 1
0.00.040.567 I print_info: pooling type     = 0
0.00.040.567 I print_info: rope type        = 2
0.00.040.567 I print_info: rope scaling     = linear
0.00.040.567 I print_info: freq_base_train  = 10000.0
0.00.040.568 I print_info: freq_scale_train = 1
0.00.040.568 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.568 I print_info: rope_finetuned   = unknown
0.00.040.568 I print_info: ssm_d_conv       = 0
0.00.040.568 I print_info: ssm_d_inner      = 0
0.00.040.568 I print_info: ssm_d_state      = 0
0.00.040.569 I print_info: ssm_dt_rank      = 0
0.00.040.569 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.569 I print_info: model type       = 1.4B
0.00.040.569 I print_info: model params     = 1.41 B
0.00.040.569 I print_info: general.name     = 1.4B
0.00.040.570 I print_info: vocab type       = BPE
0.00.040.570 I print_info: n_vocab          = 50304
0.00.040.570 I print_info: n_merges         = 50009
0.00.040.571 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.571 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.571 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.571 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.572 I print_info: LF token         = 187 ''
0.00.040.572 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.572 I print_info: max token length = 1024
0.00.040.573 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.404.302 I load_tensors: offloading 24 repeating layers to GPU
0.00.404.311 I load_tensors: offloading output layer to GPU
0.00.404.311 I load_tensors: offloaded 25/25 layers to GPU
0.00.404.344 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.404.345 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.406.077 I llama_init_from_model: n_seq_max     = 1
0.00.406.081 I llama_init_from_model: n_ctx         = 2048
0.00.406.082 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.406.082 I llama_init_from_model: n_batch       = 2048
0.00.406.082 I llama_init_from_model: n_ubatch      = 512
0.00.406.083 I llama_init_from_model: flash_attn    = 0
0.00.406.085 I llama_init_from_model: freq_base     = 10000.0
0.00.406.086 I llama_init_from_model: freq_scale    = 1
0.00.406.088 I ggml_metal_init: allocating
0.00.406.165 I ggml_metal_init: found device: Apple M4
0.00.406.177 I ggml_metal_init: picking default device: Apple M4
0.00.408.056 I ggml_metal_init: using embedded metal library
0.00.414.044 I ggml_metal_init: GPU name:   Apple M4
0.00.414.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.414.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.414.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.414.055 I ggml_metal_init: simdgroup reduction   = true
0.00.414.055 I ggml_metal_init: simdgroup matrix mul. = true
0.00.414.055 I ggml_metal_init: has residency sets    = true
0.00.414.056 I ggml_metal_init: has bfloat            = true
0.00.414.056 I ggml_metal_init: use bfloat            = true
0.00.414.061 I ggml_metal_init: hasUnifiedMemory      = true
0.00.414.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.436.870 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.493.633 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.493.642 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.493.664 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.498.034 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.498.036 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.498.036 I llama_init_from_model: graph nodes  = 967
0.00.498.037 I llama_init_from_model: graph splits = 2
0.00.498.043 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.498.158 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.498.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.892 I main: llama threadpool init, n_threads = 4
0.00.558.942 I 
0.00.558.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.558.963 I 
0.00.559.135 I sampler seed: 1234
0.00.559.140 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.559.155 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.559.157 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.559.157 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.243.292 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53263.32 tokens per second)
0.01.243.292 I llama_perf_context_print:        load time =     547.66 ms
0.01.243.293 I llama_perf_context_print: prompt eval time =      44.15 ms /     7 tokens (    6.31 ms per token,   158.55 tokens per second)
0.01.243.294 I llama_perf_context_print:        eval time =     637.15 ms /    63 runs   (   10.11 ms per token,    98.88 tokens per second)
0.01.243.294 I llama_perf_context_print:       total time =     685.11 ms /    70 tokens
0.01.243.502 I ggml_metal_free: deallocating

real	0m1.262s
user	0m0.112s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.075 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.240 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.249 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.250 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.250 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.250 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.251 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.252 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.252 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.252 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.254 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.254 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.260 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.260 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.159 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.263 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.140 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.140 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.147 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.149 I llama_model_loader: - type  f32:  194 tensors
0.00.025.149 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.149 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.150 I print_info: file format = GGUF V3 (latest)
0.00.025.151 I print_info: file type   = Q2_K - Medium
0.00.025.154 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.245 I load: special tokens cache size = 25
0.00.039.647 I load: token to piece cache size = 0.2984 MB
0.00.039.664 I print_info: arch             = gptneox
0.00.039.665 I print_info: vocab_only       = 0
0.00.039.665 I print_info: n_ctx_train      = 2048
0.00.039.665 I print_info: n_embd           = 2048
0.00.039.665 I print_info: n_layer          = 24
0.00.039.670 I print_info: n_head           = 16
0.00.039.670 I print_info: n_head_kv        = 16
0.00.039.671 I print_info: n_rot            = 32
0.00.039.671 I print_info: n_swa            = 0
0.00.039.671 I print_info: n_embd_head_k    = 128
0.00.039.671 I print_info: n_embd_head_v    = 128
0.00.039.672 I print_info: n_gqa            = 1
0.00.039.672 I print_info: n_embd_k_gqa     = 2048
0.00.039.673 I print_info: n_embd_v_gqa     = 2048
0.00.039.675 I print_info: f_norm_eps       = 1.0e-05
0.00.039.676 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.702 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.703 I print_info: f_logit_scale    = 0.0e+00
0.00.039.709 I print_info: n_ff             = 8192
0.00.039.710 I print_info: n_expert         = 0
0.00.039.710 I print_info: n_expert_used    = 0
0.00.039.710 I print_info: causal attn      = 1
0.00.039.711 I print_info: pooling type     = 0
0.00.039.711 I print_info: rope type        = 2
0.00.039.712 I print_info: rope scaling     = linear
0.00.039.714 I print_info: freq_base_train  = 10000.0
0.00.039.714 I print_info: freq_scale_train = 1
0.00.039.714 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.716 I print_info: rope_finetuned   = unknown
0.00.039.716 I print_info: ssm_d_conv       = 0
0.00.039.716 I print_info: ssm_d_inner      = 0
0.00.039.716 I print_info: ssm_d_state      = 0
0.00.039.716 I print_info: ssm_dt_rank      = 0
0.00.039.716 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.720 I print_info: model type       = 1.4B
0.00.039.724 I print_info: model params     = 1.41 B
0.00.039.724 I print_info: general.name     = 1.4B
0.00.039.725 I print_info: vocab type       = BPE
0.00.039.725 I print_info: n_vocab          = 50304
0.00.039.725 I print_info: n_merges         = 50009
0.00.039.725 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.725 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.726 I print_info: LF token         = 187 ''
0.00.039.728 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: max token length = 1024
0.00.039.729 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.366.195 I load_tensors: offloading 24 repeating layers to GPU
0.00.366.210 I load_tensors: offloading output layer to GPU
0.00.366.210 I load_tensors: offloaded 25/25 layers to GPU
0.00.366.245 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.366.246 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.367.540 I llama_init_from_model: n_seq_max     = 1
0.00.367.546 I llama_init_from_model: n_ctx         = 128
0.00.367.547 I llama_init_from_model: n_ctx_per_seq = 128
0.00.367.547 I llama_init_from_model: n_batch       = 128
0.00.367.548 I llama_init_from_model: n_ubatch      = 128
0.00.367.548 I llama_init_from_model: flash_attn    = 0
0.00.367.550 I llama_init_from_model: freq_base     = 10000.0
0.00.367.550 I llama_init_from_model: freq_scale    = 1
0.00.367.551 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.367.553 I ggml_metal_init: allocating
0.00.367.627 I ggml_metal_init: found device: Apple M4
0.00.367.641 I ggml_metal_init: picking default device: Apple M4
0.00.369.239 I ggml_metal_init: using embedded metal library
0.00.374.041 I ggml_metal_init: GPU name:   Apple M4
0.00.374.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.374.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.374.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.374.054 I ggml_metal_init: simdgroup reduction   = true
0.00.374.055 I ggml_metal_init: simdgroup matrix mul. = true
0.00.374.055 I ggml_metal_init: has residency sets    = true
0.00.374.055 I ggml_metal_init: has bfloat            = true
0.00.374.056 I ggml_metal_init: use bfloat            = true
0.00.374.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.374.061 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.445 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.397.003 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.397.007 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.397.040 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.400.197 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.400.199 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.400.199 I llama_init_from_model: graph nodes  = 967
0.00.400.200 I llama_init_from_model: graph splits = 2
0.00.400.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.400.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.428.443 I 
0.00.428.481 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.428.485 I perplexity: tokenizing the input ..
0.00.432.726 I perplexity: tokenization took 4.239 ms
0.00.432.730 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.563.891 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.565.409 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.565.428 I llama_perf_context_print:        load time =     419.36 ms
0.00.565.429 I llama_perf_context_print: prompt eval time =     130.93 ms /   128 tokens (    1.02 ms per token,   977.62 tokens per second)
0.00.565.434 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.565.434 I llama_perf_context_print:       total time =     136.99 ms /   129 tokens
0.00.565.784 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.075s
sys	0m0.077s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.934 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.524 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.527 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.529 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.529 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.529 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.530 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.535 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.535 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.247 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.009 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.010 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.011 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.012 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.012 I llama_model_loader: - type  f32:  194 tensors
0.00.024.012 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.013 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.013 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.013 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.014 I print_info: file format = GGUF V3 (latest)
0.00.024.014 I print_info: file type   = Q3_K - Medium
0.00.024.015 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.818 I load: special tokens cache size = 25
0.00.038.174 I load: token to piece cache size = 0.2984 MB
0.00.038.188 I print_info: arch             = gptneox
0.00.038.189 I print_info: vocab_only       = 0
0.00.038.189 I print_info: n_ctx_train      = 2048
0.00.038.189 I print_info: n_embd           = 2048
0.00.038.190 I print_info: n_layer          = 24
0.00.038.193 I print_info: n_head           = 16
0.00.038.194 I print_info: n_head_kv        = 16
0.00.038.194 I print_info: n_rot            = 32
0.00.038.194 I print_info: n_swa            = 0
0.00.038.194 I print_info: n_embd_head_k    = 128
0.00.038.194 I print_info: n_embd_head_v    = 128
0.00.038.195 I print_info: n_gqa            = 1
0.00.038.196 I print_info: n_embd_k_gqa     = 2048
0.00.038.197 I print_info: n_embd_v_gqa     = 2048
0.00.038.201 I print_info: f_norm_eps       = 1.0e-05
0.00.038.202 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.202 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.202 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.202 I print_info: f_logit_scale    = 0.0e+00
0.00.038.204 I print_info: n_ff             = 8192
0.00.038.204 I print_info: n_expert         = 0
0.00.038.205 I print_info: n_expert_used    = 0
0.00.038.205 I print_info: causal attn      = 1
0.00.038.205 I print_info: pooling type     = 0
0.00.038.206 I print_info: rope type        = 2
0.00.038.207 I print_info: rope scaling     = linear
0.00.038.209 I print_info: freq_base_train  = 10000.0
0.00.038.210 I print_info: freq_scale_train = 1
0.00.038.210 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.210 I print_info: rope_finetuned   = unknown
0.00.038.210 I print_info: ssm_d_conv       = 0
0.00.038.210 I print_info: ssm_d_inner      = 0
0.00.038.210 I print_info: ssm_d_state      = 0
0.00.038.210 I print_info: ssm_dt_rank      = 0
0.00.038.211 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.211 I print_info: model type       = 1.4B
0.00.038.211 I print_info: model params     = 1.41 B
0.00.038.211 I print_info: general.name     = 1.4B
0.00.038.212 I print_info: vocab type       = BPE
0.00.038.212 I print_info: n_vocab          = 50304
0.00.038.212 I print_info: n_merges         = 50009
0.00.038.212 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.213 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.213 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.213 I print_info: LF token         = 187 ''
0.00.038.214 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.214 I print_info: max token length = 1024
0.00.038.214 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.526 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.537 I load_tensors: offloading output layer to GPU
0.00.442.537 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.571 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.579 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.383 I llama_init_from_model: n_seq_max     = 1
0.00.444.386 I llama_init_from_model: n_ctx         = 2048
0.00.444.387 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.388 I llama_init_from_model: n_batch       = 2048
0.00.444.388 I llama_init_from_model: n_ubatch      = 512
0.00.444.388 I llama_init_from_model: flash_attn    = 0
0.00.444.391 I llama_init_from_model: freq_base     = 10000.0
0.00.444.391 I llama_init_from_model: freq_scale    = 1
0.00.444.394 I ggml_metal_init: allocating
0.00.444.492 I ggml_metal_init: found device: Apple M4
0.00.444.511 I ggml_metal_init: picking default device: Apple M4
0.00.446.141 I ggml_metal_init: using embedded metal library
0.00.451.975 I ggml_metal_init: GPU name:   Apple M4
0.00.451.981 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.981 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.983 I ggml_metal_init: simdgroup reduction   = true
0.00.451.983 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.984 I ggml_metal_init: has residency sets    = true
0.00.451.984 I ggml_metal_init: has bfloat            = true
0.00.451.984 I ggml_metal_init: use bfloat            = true
0.00.451.985 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.110 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.531.032 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.531.040 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.531.072 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.535.673 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.535.675 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.535.675 I llama_init_from_model: graph nodes  = 967
0.00.535.675 I llama_init_from_model: graph splits = 2
0.00.535.680 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.535.809 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.535.809 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.593.968 I main: llama threadpool init, n_threads = 4
0.00.594.019 I 
0.00.594.040 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.042 I 
0.00.594.219 I sampler seed: 1234
0.00.594.223 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.594.248 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.594.250 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.594.250 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.346.578 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.346.578 I llama_perf_context_print:        load time =     584.31 ms
0.01.346.579 I llama_perf_context_print: prompt eval time =      49.81 ms /     7 tokens (    7.12 ms per token,   140.53 tokens per second)
0.01.346.580 I llama_perf_context_print:        eval time =     699.62 ms /    63 runs   (   11.11 ms per token,    90.05 tokens per second)
0.01.346.580 I llama_perf_context_print:       total time =     753.33 ms /    70 tokens
0.01.346.793 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.110s
sys	0m0.189s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.897 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.999 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.012 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.013 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.016 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.016 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.017 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.018 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.018 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.978 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.824 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.824 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.825 I llama_model_loader: - type  f32:  194 tensors
0.00.024.825 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.825 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.825 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.825 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.826 I print_info: file format = GGUF V3 (latest)
0.00.024.827 I print_info: file type   = Q3_K - Medium
0.00.024.827 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.326 I load: special tokens cache size = 25
0.00.039.896 I load: token to piece cache size = 0.2984 MB
0.00.039.909 I print_info: arch             = gptneox
0.00.039.910 I print_info: vocab_only       = 0
0.00.039.910 I print_info: n_ctx_train      = 2048
0.00.039.910 I print_info: n_embd           = 2048
0.00.039.910 I print_info: n_layer          = 24
0.00.039.914 I print_info: n_head           = 16
0.00.039.915 I print_info: n_head_kv        = 16
0.00.039.915 I print_info: n_rot            = 32
0.00.039.915 I print_info: n_swa            = 0
0.00.039.916 I print_info: n_embd_head_k    = 128
0.00.039.916 I print_info: n_embd_head_v    = 128
0.00.039.916 I print_info: n_gqa            = 1
0.00.039.917 I print_info: n_embd_k_gqa     = 2048
0.00.039.917 I print_info: n_embd_v_gqa     = 2048
0.00.039.918 I print_info: f_norm_eps       = 1.0e-05
0.00.039.921 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.922 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.922 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.922 I print_info: f_logit_scale    = 0.0e+00
0.00.039.922 I print_info: n_ff             = 8192
0.00.039.922 I print_info: n_expert         = 0
0.00.039.923 I print_info: n_expert_used    = 0
0.00.039.923 I print_info: causal attn      = 1
0.00.039.924 I print_info: pooling type     = 0
0.00.039.924 I print_info: rope type        = 2
0.00.039.925 I print_info: rope scaling     = linear
0.00.039.925 I print_info: freq_base_train  = 10000.0
0.00.039.925 I print_info: freq_scale_train = 1
0.00.039.925 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.926 I print_info: rope_finetuned   = unknown
0.00.039.926 I print_info: ssm_d_conv       = 0
0.00.039.926 I print_info: ssm_d_inner      = 0
0.00.039.926 I print_info: ssm_d_state      = 0
0.00.039.926 I print_info: ssm_dt_rank      = 0
0.00.039.926 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.926 I print_info: model type       = 1.4B
0.00.039.927 I print_info: model params     = 1.41 B
0.00.039.927 I print_info: general.name     = 1.4B
0.00.039.928 I print_info: vocab type       = BPE
0.00.039.928 I print_info: n_vocab          = 50304
0.00.039.928 I print_info: n_merges         = 50009
0.00.039.928 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.929 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.929 I print_info: LF token         = 187 ''
0.00.039.929 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.929 I print_info: max token length = 1024
0.00.039.930 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.456.866 I load_tensors: offloading 24 repeating layers to GPU
0.00.456.881 I load_tensors: offloading output layer to GPU
0.00.456.882 I load_tensors: offloaded 25/25 layers to GPU
0.00.456.917 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.456.925 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.458.661 I llama_init_from_model: n_seq_max     = 1
0.00.458.664 I llama_init_from_model: n_ctx         = 128
0.00.458.665 I llama_init_from_model: n_ctx_per_seq = 128
0.00.458.665 I llama_init_from_model: n_batch       = 128
0.00.458.665 I llama_init_from_model: n_ubatch      = 128
0.00.458.666 I llama_init_from_model: flash_attn    = 0
0.00.458.668 I llama_init_from_model: freq_base     = 10000.0
0.00.458.668 I llama_init_from_model: freq_scale    = 1
0.00.458.669 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.458.672 I ggml_metal_init: allocating
0.00.458.751 I ggml_metal_init: found device: Apple M4
0.00.458.765 I ggml_metal_init: picking default device: Apple M4
0.00.460.345 I ggml_metal_init: using embedded metal library
0.00.466.031 I ggml_metal_init: GPU name:   Apple M4
0.00.466.040 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.466.041 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.466.042 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.466.043 I ggml_metal_init: simdgroup reduction   = true
0.00.466.043 I ggml_metal_init: simdgroup matrix mul. = true
0.00.466.044 I ggml_metal_init: has residency sets    = true
0.00.466.044 I ggml_metal_init: has bfloat            = true
0.00.466.044 I ggml_metal_init: use bfloat            = true
0.00.466.045 I ggml_metal_init: hasUnifiedMemory      = true
0.00.466.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.485.831 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.489.525 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.489.532 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.489.575 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.492.865 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.492.867 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.492.867 I llama_init_from_model: graph nodes  = 967
0.00.492.868 I llama_init_from_model: graph splits = 2
0.00.492.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.492.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.525.321 I 
0.00.525.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.525.415 I perplexity: tokenizing the input ..
0.00.532.522 I perplexity: tokenization took 7.103 ms
0.00.532.537 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.672.710 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.674.056 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.674.078 I llama_perf_context_print:        load time =     516.42 ms
0.00.674.082 I llama_perf_context_print: prompt eval time =     139.26 ms /   128 tokens (    1.09 ms per token,   919.12 tokens per second)
0.00.674.083 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.674.084 I llama_perf_context_print:       total time =     148.76 ms /   129 tokens
0.00.674.468 I ggml_metal_free: deallocating

real	0m0.688s
user	0m0.081s
sys	0m0.126s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.008.985 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.596 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.601 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.603 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.603 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.604 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.604 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.606 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.606 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.606 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.607 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.607 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.608 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.609 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.609 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.610 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.315 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.003 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.005 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.006 I llama_model_loader: - type  f32:  194 tensors
0.00.024.006 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.007 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.007 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.007 I print_info: file format = GGUF V3 (latest)
0.00.024.008 I print_info: file type   = Q4_K - Medium
0.00.024.009 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.031.840 I load: special tokens cache size = 25
0.00.038.035 I load: token to piece cache size = 0.2984 MB
0.00.038.049 I print_info: arch             = gptneox
0.00.038.050 I print_info: vocab_only       = 0
0.00.038.050 I print_info: n_ctx_train      = 2048
0.00.038.051 I print_info: n_embd           = 2048
0.00.038.051 I print_info: n_layer          = 24
0.00.038.055 I print_info: n_head           = 16
0.00.038.056 I print_info: n_head_kv        = 16
0.00.038.056 I print_info: n_rot            = 32
0.00.038.056 I print_info: n_swa            = 0
0.00.038.057 I print_info: n_embd_head_k    = 128
0.00.038.057 I print_info: n_embd_head_v    = 128
0.00.038.057 I print_info: n_gqa            = 1
0.00.038.058 I print_info: n_embd_k_gqa     = 2048
0.00.038.059 I print_info: n_embd_v_gqa     = 2048
0.00.038.059 I print_info: f_norm_eps       = 1.0e-05
0.00.038.060 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.061 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.061 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.061 I print_info: f_logit_scale    = 0.0e+00
0.00.038.062 I print_info: n_ff             = 8192
0.00.038.063 I print_info: n_expert         = 0
0.00.038.064 I print_info: n_expert_used    = 0
0.00.038.064 I print_info: causal attn      = 1
0.00.038.064 I print_info: pooling type     = 0
0.00.038.064 I print_info: rope type        = 2
0.00.038.064 I print_info: rope scaling     = linear
0.00.038.064 I print_info: freq_base_train  = 10000.0
0.00.038.065 I print_info: freq_scale_train = 1
0.00.038.065 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.065 I print_info: rope_finetuned   = unknown
0.00.038.065 I print_info: ssm_d_conv       = 0
0.00.038.065 I print_info: ssm_d_inner      = 0
0.00.038.065 I print_info: ssm_d_state      = 0
0.00.038.065 I print_info: ssm_dt_rank      = 0
0.00.038.066 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.066 I print_info: model type       = 1.4B
0.00.038.069 I print_info: model params     = 1.41 B
0.00.038.069 I print_info: general.name     = 1.4B
0.00.038.070 I print_info: vocab type       = BPE
0.00.038.070 I print_info: n_vocab          = 50304
0.00.038.070 I print_info: n_merges         = 50009
0.00.038.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.071 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.071 I print_info: LF token         = 187 ''
0.00.038.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.072 I print_info: max token length = 1024
0.00.038.072 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.516.482 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.495 I load_tensors: offloading output layer to GPU
0.00.516.495 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.532 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.533 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.518.245 I llama_init_from_model: n_seq_max     = 1
0.00.518.250 I llama_init_from_model: n_ctx         = 2048
0.00.518.250 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.518.251 I llama_init_from_model: n_batch       = 2048
0.00.518.251 I llama_init_from_model: n_ubatch      = 512
0.00.518.251 I llama_init_from_model: flash_attn    = 0
0.00.518.253 I llama_init_from_model: freq_base     = 10000.0
0.00.518.254 I llama_init_from_model: freq_scale    = 1
0.00.518.256 I ggml_metal_init: allocating
0.00.518.374 I ggml_metal_init: found device: Apple M4
0.00.518.388 I ggml_metal_init: picking default device: Apple M4
0.00.520.020 I ggml_metal_init: using embedded metal library
0.00.526.821 I ggml_metal_init: GPU name:   Apple M4
0.00.526.826 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.827 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.828 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.828 I ggml_metal_init: simdgroup reduction   = true
0.00.526.828 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.829 I ggml_metal_init: has residency sets    = true
0.00.526.829 I ggml_metal_init: has bfloat            = true
0.00.526.829 I ggml_metal_init: use bfloat            = true
0.00.526.830 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.831 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.221 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.603.214 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.603.222 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.603.251 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.608.915 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.608.917 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.608.917 I llama_init_from_model: graph nodes  = 967
0.00.608.918 I llama_init_from_model: graph splits = 2
0.00.608.923 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.609.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.609.040 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.969 I main: llama threadpool init, n_threads = 4
0.00.667.020 I 
0.00.667.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.041 I 
0.00.667.216 I sampler seed: 1234
0.00.667.221 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.667.236 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.667.238 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.667.238 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.435.340 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51042.42 tokens per second)
0.01.435.341 I llama_perf_context_print:        load time =     657.24 ms
0.01.435.341 I llama_perf_context_print: prompt eval time =      57.61 ms /     7 tokens (    8.23 ms per token,   121.50 tokens per second)
0.01.435.342 I llama_perf_context_print:        eval time =     707.62 ms /    63 runs   (   11.23 ms per token,    89.03 tokens per second)
0.01.435.342 I llama_perf_context_print:       total time =     769.11 ms /    70 tokens
0.01.435.597 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.120 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.982 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.991 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.991 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.994 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.996 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.997 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.620 I llama_model_loader: - type  f32:  194 tensors
0.00.025.620 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.620 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.620 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.621 I print_info: file format = GGUF V3 (latest)
0.00.025.622 I print_info: file type   = Q4_K - Medium
0.00.025.625 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.147 I load: special tokens cache size = 25
0.00.040.556 I load: token to piece cache size = 0.2984 MB
0.00.040.574 I print_info: arch             = gptneox
0.00.040.574 I print_info: vocab_only       = 0
0.00.040.575 I print_info: n_ctx_train      = 2048
0.00.040.575 I print_info: n_embd           = 2048
0.00.040.575 I print_info: n_layer          = 24
0.00.040.579 I print_info: n_head           = 16
0.00.040.579 I print_info: n_head_kv        = 16
0.00.040.579 I print_info: n_rot            = 32
0.00.040.579 I print_info: n_swa            = 0
0.00.040.580 I print_info: n_embd_head_k    = 128
0.00.040.580 I print_info: n_embd_head_v    = 128
0.00.040.580 I print_info: n_gqa            = 1
0.00.040.581 I print_info: n_embd_k_gqa     = 2048
0.00.040.581 I print_info: n_embd_v_gqa     = 2048
0.00.040.582 I print_info: f_norm_eps       = 1.0e-05
0.00.040.582 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.582 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.583 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.583 I print_info: f_logit_scale    = 0.0e+00
0.00.040.583 I print_info: n_ff             = 8192
0.00.040.583 I print_info: n_expert         = 0
0.00.040.584 I print_info: n_expert_used    = 0
0.00.040.584 I print_info: causal attn      = 1
0.00.040.584 I print_info: pooling type     = 0
0.00.040.584 I print_info: rope type        = 2
0.00.040.584 I print_info: rope scaling     = linear
0.00.040.585 I print_info: freq_base_train  = 10000.0
0.00.040.585 I print_info: freq_scale_train = 1
0.00.040.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.585 I print_info: rope_finetuned   = unknown
0.00.040.585 I print_info: ssm_d_conv       = 0
0.00.040.585 I print_info: ssm_d_inner      = 0
0.00.040.587 I print_info: ssm_d_state      = 0
0.00.040.587 I print_info: ssm_dt_rank      = 0
0.00.040.587 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.587 I print_info: model type       = 1.4B
0.00.040.587 I print_info: model params     = 1.41 B
0.00.040.587 I print_info: general.name     = 1.4B
0.00.040.588 I print_info: vocab type       = BPE
0.00.040.590 I print_info: n_vocab          = 50304
0.00.040.590 I print_info: n_merges         = 50009
0.00.040.590 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.590 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.590 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.590 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.591 I print_info: LF token         = 187 ''
0.00.040.591 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.591 I print_info: max token length = 1024
0.00.040.592 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.514.204 I load_tensors: offloading 24 repeating layers to GPU
0.00.514.221 I load_tensors: offloading output layer to GPU
0.00.514.222 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.260 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.262 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.515.922 I llama_init_from_model: n_seq_max     = 1
0.00.515.925 I llama_init_from_model: n_ctx         = 128
0.00.515.926 I llama_init_from_model: n_ctx_per_seq = 128
0.00.515.926 I llama_init_from_model: n_batch       = 128
0.00.515.927 I llama_init_from_model: n_ubatch      = 128
0.00.515.927 I llama_init_from_model: flash_attn    = 0
0.00.515.929 I llama_init_from_model: freq_base     = 10000.0
0.00.515.930 I llama_init_from_model: freq_scale    = 1
0.00.515.930 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.515.933 I ggml_metal_init: allocating
0.00.516.011 I ggml_metal_init: found device: Apple M4
0.00.516.025 I ggml_metal_init: picking default device: Apple M4
0.00.517.582 I ggml_metal_init: using embedded metal library
0.00.524.483 I ggml_metal_init: GPU name:   Apple M4
0.00.524.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.524.492 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.524.493 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.524.497 I ggml_metal_init: simdgroup reduction   = true
0.00.524.497 I ggml_metal_init: simdgroup matrix mul. = true
0.00.524.498 I ggml_metal_init: has residency sets    = true
0.00.524.498 I ggml_metal_init: has bfloat            = true
0.00.524.498 I ggml_metal_init: use bfloat            = true
0.00.524.500 I ggml_metal_init: hasUnifiedMemory      = true
0.00.524.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.363 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.545.972 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.545.976 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.546.026 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.549.207 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.549.209 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.549.210 I llama_init_from_model: graph nodes  = 967
0.00.549.210 I llama_init_from_model: graph splits = 2
0.00.549.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.549.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.160 I 
0.00.575.252 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.260 I perplexity: tokenizing the input ..
0.00.582.445 I perplexity: tokenization took 7.182 ms
0.00.582.452 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.716.384 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.717.726 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.717.761 I llama_perf_context_print:        load time =     565.03 ms
0.00.717.762 I llama_perf_context_print: prompt eval time =     133.03 ms /   128 tokens (    1.04 ms per token,   962.23 tokens per second)
0.00.717.763 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.717.764 I llama_perf_context_print:       total time =     142.60 ms /   129 tokens
0.00.718.206 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.081s
sys	0m0.121s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.012.173 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.852 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.857 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.859 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.860 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.862 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.863 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.864 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.864 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.864 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.865 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.870 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.680 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.391 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.392 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.393 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.393 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.393 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.394 I llama_model_loader: - type  f32:  194 tensors
0.00.028.394 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.394 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.395 I print_info: file format = GGUF V3 (latest)
0.00.028.395 I print_info: file type   = Q5_K - Medium
0.00.028.396 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.036.639 I load: special tokens cache size = 25
0.00.042.924 I load: token to piece cache size = 0.2984 MB
0.00.042.938 I print_info: arch             = gptneox
0.00.042.940 I print_info: vocab_only       = 0
0.00.042.940 I print_info: n_ctx_train      = 2048
0.00.042.940 I print_info: n_embd           = 2048
0.00.042.940 I print_info: n_layer          = 24
0.00.042.943 I print_info: n_head           = 16
0.00.042.944 I print_info: n_head_kv        = 16
0.00.042.944 I print_info: n_rot            = 32
0.00.042.944 I print_info: n_swa            = 0
0.00.042.944 I print_info: n_embd_head_k    = 128
0.00.042.944 I print_info: n_embd_head_v    = 128
0.00.042.945 I print_info: n_gqa            = 1
0.00.042.946 I print_info: n_embd_k_gqa     = 2048
0.00.042.947 I print_info: n_embd_v_gqa     = 2048
0.00.042.947 I print_info: f_norm_eps       = 1.0e-05
0.00.042.948 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.948 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.948 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.948 I print_info: f_logit_scale    = 0.0e+00
0.00.042.949 I print_info: n_ff             = 8192
0.00.042.951 I print_info: n_expert         = 0
0.00.042.951 I print_info: n_expert_used    = 0
0.00.042.951 I print_info: causal attn      = 1
0.00.042.951 I print_info: pooling type     = 0
0.00.042.953 I print_info: rope type        = 2
0.00.042.953 I print_info: rope scaling     = linear
0.00.042.953 I print_info: freq_base_train  = 10000.0
0.00.042.953 I print_info: freq_scale_train = 1
0.00.042.953 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.954 I print_info: rope_finetuned   = unknown
0.00.042.954 I print_info: ssm_d_conv       = 0
0.00.042.955 I print_info: ssm_d_inner      = 0
0.00.042.958 I print_info: ssm_d_state      = 0
0.00.042.958 I print_info: ssm_dt_rank      = 0
0.00.042.958 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.958 I print_info: model type       = 1.4B
0.00.042.959 I print_info: model params     = 1.41 B
0.00.042.959 I print_info: general.name     = 1.4B
0.00.042.961 I print_info: vocab type       = BPE
0.00.042.961 I print_info: n_vocab          = 50304
0.00.042.961 I print_info: n_merges         = 50009
0.00.042.961 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.961 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.961 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.961 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.962 I print_info: LF token         = 187 ''
0.00.042.962 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.962 I print_info: max token length = 1024
0.00.042.962 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.988 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.004 I load_tensors: offloading output layer to GPU
0.00.591.005 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.038 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.039 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.592.708 I llama_init_from_model: n_seq_max     = 1
0.00.592.711 I llama_init_from_model: n_ctx         = 2048
0.00.592.711 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.592.712 I llama_init_from_model: n_batch       = 2048
0.00.592.712 I llama_init_from_model: n_ubatch      = 512
0.00.592.713 I llama_init_from_model: flash_attn    = 0
0.00.592.715 I llama_init_from_model: freq_base     = 10000.0
0.00.592.716 I llama_init_from_model: freq_scale    = 1
0.00.592.724 I ggml_metal_init: allocating
0.00.592.800 I ggml_metal_init: found device: Apple M4
0.00.592.813 I ggml_metal_init: picking default device: Apple M4
0.00.594.450 I ggml_metal_init: using embedded metal library
0.00.601.292 I ggml_metal_init: GPU name:   Apple M4
0.00.601.296 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.297 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.298 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.299 I ggml_metal_init: simdgroup reduction   = true
0.00.601.299 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.299 I ggml_metal_init: has residency sets    = true
0.00.601.299 I ggml_metal_init: has bfloat            = true
0.00.601.300 I ggml_metal_init: use bfloat            = true
0.00.601.300 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.158 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.435 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.675.442 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.675.464 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.875 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.679.877 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.679.877 I llama_init_from_model: graph nodes  = 967
0.00.679.877 I llama_init_from_model: graph splits = 2
0.00.679.882 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.680.004 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.680.004 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.002 I main: llama threadpool init, n_threads = 4
0.00.747.053 I 
0.00.747.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.075 I 
0.00.747.231 I sampler seed: 1234
0.00.747.236 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.250 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.251 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.252 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.599.892 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54281.35 tokens per second)
0.01.599.894 I llama_perf_context_print:        load time =     734.10 ms
0.01.599.895 I llama_perf_context_print: prompt eval time =      63.95 ms /     7 tokens (    9.14 ms per token,   109.46 tokens per second)
0.01.599.895 I llama_perf_context_print:        eval time =     785.85 ms /    63 runs   (   12.47 ms per token,    80.17 tokens per second)
0.01.599.896 I llama_perf_context_print:       total time =     853.62 ms /    70 tokens
0.01.600.110 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.109s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.978 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.066 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.072 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.076 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.077 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.077 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.078 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.079 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.079 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.080 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.080 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.081 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.081 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.083 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.083 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.084 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.845 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.916 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.718 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.718 I llama_model_loader: - type  f32:  194 tensors
0.00.024.719 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.719 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.719 I print_info: file format = GGUF V3 (latest)
0.00.024.720 I print_info: file type   = Q5_K - Medium
0.00.024.721 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.001 I load: special tokens cache size = 25
0.00.039.370 I load: token to piece cache size = 0.2984 MB
0.00.039.387 I print_info: arch             = gptneox
0.00.039.388 I print_info: vocab_only       = 0
0.00.039.388 I print_info: n_ctx_train      = 2048
0.00.039.388 I print_info: n_embd           = 2048
0.00.039.389 I print_info: n_layer          = 24
0.00.039.393 I print_info: n_head           = 16
0.00.039.393 I print_info: n_head_kv        = 16
0.00.039.393 I print_info: n_rot            = 32
0.00.039.396 I print_info: n_swa            = 0
0.00.039.397 I print_info: n_embd_head_k    = 128
0.00.039.397 I print_info: n_embd_head_v    = 128
0.00.039.397 I print_info: n_gqa            = 1
0.00.039.398 I print_info: n_embd_k_gqa     = 2048
0.00.039.398 I print_info: n_embd_v_gqa     = 2048
0.00.039.399 I print_info: f_norm_eps       = 1.0e-05
0.00.039.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.400 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.400 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.400 I print_info: f_logit_scale    = 0.0e+00
0.00.039.401 I print_info: n_ff             = 8192
0.00.039.401 I print_info: n_expert         = 0
0.00.039.401 I print_info: n_expert_used    = 0
0.00.039.401 I print_info: causal attn      = 1
0.00.039.401 I print_info: pooling type     = 0
0.00.039.401 I print_info: rope type        = 2
0.00.039.402 I print_info: rope scaling     = linear
0.00.039.402 I print_info: freq_base_train  = 10000.0
0.00.039.402 I print_info: freq_scale_train = 1
0.00.039.402 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.402 I print_info: rope_finetuned   = unknown
0.00.039.402 I print_info: ssm_d_conv       = 0
0.00.039.403 I print_info: ssm_d_inner      = 0
0.00.039.403 I print_info: ssm_d_state      = 0
0.00.039.403 I print_info: ssm_dt_rank      = 0
0.00.039.403 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.403 I print_info: model type       = 1.4B
0.00.039.403 I print_info: model params     = 1.41 B
0.00.039.404 I print_info: general.name     = 1.4B
0.00.039.404 I print_info: vocab type       = BPE
0.00.039.404 I print_info: n_vocab          = 50304
0.00.039.404 I print_info: n_merges         = 50009
0.00.039.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.405 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.405 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.405 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.405 I print_info: LF token         = 187 ''
0.00.039.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.411 I print_info: max token length = 1024
0.00.039.412 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.578.842 I load_tensors: offloading 24 repeating layers to GPU
0.00.578.848 I load_tensors: offloading output layer to GPU
0.00.578.849 I load_tensors: offloaded 25/25 layers to GPU
0.00.578.877 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.578.879 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.580.496 I llama_init_from_model: n_seq_max     = 1
0.00.580.499 I llama_init_from_model: n_ctx         = 128
0.00.580.499 I llama_init_from_model: n_ctx_per_seq = 128
0.00.580.500 I llama_init_from_model: n_batch       = 128
0.00.580.500 I llama_init_from_model: n_ubatch      = 128
0.00.580.500 I llama_init_from_model: flash_attn    = 0
0.00.580.501 I llama_init_from_model: freq_base     = 10000.0
0.00.580.502 I llama_init_from_model: freq_scale    = 1
0.00.580.503 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.580.505 I ggml_metal_init: allocating
0.00.580.561 I ggml_metal_init: found device: Apple M4
0.00.580.573 I ggml_metal_init: picking default device: Apple M4
0.00.581.904 I ggml_metal_init: using embedded metal library
0.00.587.982 I ggml_metal_init: GPU name:   Apple M4
0.00.587.986 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.587.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.587.988 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.587.988 I ggml_metal_init: simdgroup reduction   = true
0.00.587.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.587.989 I ggml_metal_init: has residency sets    = true
0.00.587.989 I ggml_metal_init: has bfloat            = true
0.00.587.990 I ggml_metal_init: use bfloat            = true
0.00.587.991 I ggml_metal_init: hasUnifiedMemory      = true
0.00.587.995 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.605.375 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.810 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.608.813 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.608.839 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.115 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.612.117 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.612.117 I llama_init_from_model: graph nodes  = 967
0.00.612.117 I llama_init_from_model: graph splits = 2
0.00.612.120 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.612.120 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.557 I 
0.00.644.646 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.655 I perplexity: tokenizing the input ..
0.00.651.632 I perplexity: tokenization took 6.976 ms
0.00.651.637 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.671 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.789.021 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.789.045 I llama_perf_context_print:        load time =     635.57 ms
0.00.789.045 I llama_perf_context_print: prompt eval time =     135.80 ms /   128 tokens (    1.06 ms per token,   942.53 tokens per second)
0.00.789.046 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.046 I llama_perf_context_print:       total time =     144.49 ms /   129 tokens
0.00.789.422 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.078s
sys	0m0.128s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.128 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.132 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.133 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.140 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.140 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.142 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.142 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.142 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.143 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.143 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.144 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.145 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.145 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.146 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.891 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.893 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.599 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.601 I llama_model_loader: - type  f32:  194 tensors
0.00.025.602 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.602 I print_info: file format = GGUF V3 (latest)
0.00.025.603 I print_info: file type   = Q6_K
0.00.025.604 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.526 I load: special tokens cache size = 25
0.00.039.891 I load: token to piece cache size = 0.2984 MB
0.00.039.905 I print_info: arch             = gptneox
0.00.039.906 I print_info: vocab_only       = 0
0.00.039.906 I print_info: n_ctx_train      = 2048
0.00.039.907 I print_info: n_embd           = 2048
0.00.039.907 I print_info: n_layer          = 24
0.00.039.909 I print_info: n_head           = 16
0.00.039.910 I print_info: n_head_kv        = 16
0.00.039.910 I print_info: n_rot            = 32
0.00.039.910 I print_info: n_swa            = 0
0.00.039.910 I print_info: n_embd_head_k    = 128
0.00.039.911 I print_info: n_embd_head_v    = 128
0.00.039.911 I print_info: n_gqa            = 1
0.00.039.912 I print_info: n_embd_k_gqa     = 2048
0.00.039.913 I print_info: n_embd_v_gqa     = 2048
0.00.039.913 I print_info: f_norm_eps       = 1.0e-05
0.00.039.914 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.916 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.916 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.916 I print_info: f_logit_scale    = 0.0e+00
0.00.039.917 I print_info: n_ff             = 8192
0.00.039.917 I print_info: n_expert         = 0
0.00.039.917 I print_info: n_expert_used    = 0
0.00.039.918 I print_info: causal attn      = 1
0.00.039.918 I print_info: pooling type     = 0
0.00.039.918 I print_info: rope type        = 2
0.00.039.918 I print_info: rope scaling     = linear
0.00.039.919 I print_info: freq_base_train  = 10000.0
0.00.039.922 I print_info: freq_scale_train = 1
0.00.039.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.923 I print_info: rope_finetuned   = unknown
0.00.039.923 I print_info: ssm_d_conv       = 0
0.00.039.923 I print_info: ssm_d_inner      = 0
0.00.039.923 I print_info: ssm_d_state      = 0
0.00.039.923 I print_info: ssm_dt_rank      = 0
0.00.039.923 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.923 I print_info: model type       = 1.4B
0.00.039.924 I print_info: model params     = 1.41 B
0.00.039.924 I print_info: general.name     = 1.4B
0.00.039.924 I print_info: vocab type       = BPE
0.00.039.924 I print_info: n_vocab          = 50304
0.00.039.924 I print_info: n_merges         = 50009
0.00.039.925 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.925 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.925 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.925 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.925 I print_info: LF token         = 187 ''
0.00.039.925 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.926 I print_info: max token length = 1024
0.00.039.926 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.663.926 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.930 I load_tensors: offloading output layer to GPU
0.00.663.931 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.954 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.663.955 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.665.229 I llama_init_from_model: n_seq_max     = 1
0.00.665.231 I llama_init_from_model: n_ctx         = 2048
0.00.665.231 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.665.232 I llama_init_from_model: n_batch       = 2048
0.00.665.232 I llama_init_from_model: n_ubatch      = 512
0.00.665.232 I llama_init_from_model: flash_attn    = 0
0.00.665.233 I llama_init_from_model: freq_base     = 10000.0
0.00.665.234 I llama_init_from_model: freq_scale    = 1
0.00.665.235 I ggml_metal_init: allocating
0.00.665.245 I ggml_metal_init: found device: Apple M4
0.00.665.264 I ggml_metal_init: picking default device: Apple M4
0.00.666.535 I ggml_metal_init: using embedded metal library
0.00.672.897 I ggml_metal_init: GPU name:   Apple M4
0.00.672.901 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.902 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.903 I ggml_metal_init: simdgroup reduction   = true
0.00.672.903 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.904 I ggml_metal_init: has residency sets    = true
0.00.672.904 I ggml_metal_init: has bfloat            = true
0.00.672.904 I ggml_metal_init: use bfloat            = true
0.00.672.905 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.136 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.745.422 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.745.429 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.745.460 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.749.435 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.749.437 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.749.437 I llama_init_from_model: graph nodes  = 967
0.00.749.438 I llama_init_from_model: graph splits = 2
0.00.749.444 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.749.573 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.776 I main: llama threadpool init, n_threads = 4
0.00.818.826 I 
0.00.818.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.846 I 
0.00.819.004 I sampler seed: 1234
0.00.819.009 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.025 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.026 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.027 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.693.204 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.693.204 I llama_perf_context_print:        load time =     809.43 ms
0.01.693.205 I llama_perf_context_print: prompt eval time =      57.46 ms /     7 tokens (    8.21 ms per token,   121.82 tokens per second)
0.01.693.206 I llama_perf_context_print:        eval time =     813.78 ms /    63 runs   (   12.92 ms per token,    77.42 tokens per second)
0.01.693.206 I llama_perf_context_print:       total time =     875.14 ms /    70 tokens
0.01.693.432 I ggml_metal_free: deallocating

real	0m1.711s
user	0m0.109s
sys	0m0.230s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4863 (4e39a3c3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.013 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.616 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.621 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.629 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.631 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.631 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.632 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.632 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.634 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.634 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.635 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.311 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.159 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.159 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.160 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.161 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.161 I llama_model_loader: - type  f32:  194 tensors
0.00.026.162 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.162 I print_info: file format = GGUF V3 (latest)
0.00.026.163 I print_info: file type   = Q6_K
0.00.026.164 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.198 I load: special tokens cache size = 25
0.00.040.442 I load: token to piece cache size = 0.2984 MB
0.00.040.460 I print_info: arch             = gptneox
0.00.040.461 I print_info: vocab_only       = 0
0.00.040.462 I print_info: n_ctx_train      = 2048
0.00.040.462 I print_info: n_embd           = 2048
0.00.040.462 I print_info: n_layer          = 24
0.00.040.466 I print_info: n_head           = 16
0.00.040.467 I print_info: n_head_kv        = 16
0.00.040.467 I print_info: n_rot            = 32
0.00.040.467 I print_info: n_swa            = 0
0.00.040.467 I print_info: n_embd_head_k    = 128
0.00.040.467 I print_info: n_embd_head_v    = 128
0.00.040.468 I print_info: n_gqa            = 1
0.00.040.468 I print_info: n_embd_k_gqa     = 2048
0.00.040.469 I print_info: n_embd_v_gqa     = 2048
0.00.040.470 I print_info: f_norm_eps       = 1.0e-05
0.00.040.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.471 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.472 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.472 I print_info: f_logit_scale    = 0.0e+00
0.00.040.473 I print_info: n_ff             = 8192
0.00.040.473 I print_info: n_expert         = 0
0.00.040.473 I print_info: n_expert_used    = 0
0.00.040.473 I print_info: causal attn      = 1
0.00.040.473 I print_info: pooling type     = 0
0.00.040.473 I print_info: rope type        = 2
0.00.040.474 I print_info: rope scaling     = linear
0.00.040.474 I print_info: freq_base_train  = 10000.0
0.00.040.474 I print_info: freq_scale_train = 1
0.00.040.476 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.476 I print_info: rope_finetuned   = unknown
0.00.040.476 I print_info: ssm_d_conv       = 0
0.00.040.476 I print_info: ssm_d_inner      = 0
0.00.040.476 I print_info: ssm_d_state      = 0
0.00.040.476 I print_info: ssm_dt_rank      = 0
0.00.040.476 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.477 I print_info: model type       = 1.4B
0.00.040.477 I print_info: model params     = 1.41 B
0.00.040.496 I print_info: general.name     = 1.4B
0.00.040.498 I print_info: vocab type       = BPE
0.00.040.498 I print_info: n_vocab          = 50304
0.00.040.498 I print_info: n_merges         = 50009
0.00.040.498 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.499 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.499 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.499 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.499 I print_info: LF token         = 187 ''
0.00.040.499 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.500 I print_info: max token length = 1024
0.00.040.500 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.130 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.136 I load_tensors: offloading output layer to GPU
0.00.634.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.172 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.634.175 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.635.514 I llama_init_from_model: n_seq_max     = 1
0.00.635.517 I llama_init_from_model: n_ctx         = 128
0.00.635.517 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.518 I llama_init_from_model: n_batch       = 128
0.00.635.518 I llama_init_from_model: n_ubatch      = 128
0.00.635.518 I llama_init_from_model: flash_attn    = 0
0.00.635.519 I llama_init_from_model: freq_base     = 10000.0
0.00.635.520 I llama_init_from_model: freq_scale    = 1
0.00.635.521 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.522 I ggml_metal_init: allocating
0.00.635.623 I ggml_metal_init: found device: Apple M4
0.00.635.640 I ggml_metal_init: picking default device: Apple M4
0.00.637.039 I ggml_metal_init: using embedded metal library
0.00.643.003 I ggml_metal_init: GPU name:   Apple M4
0.00.643.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.007 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.008 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.009 I ggml_metal_init: simdgroup reduction   = true
0.00.643.009 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.009 I ggml_metal_init: has residency sets    = true
0.00.643.010 I ggml_metal_init: has bfloat            = true
0.00.643.010 I ggml_metal_init: use bfloat            = true
0.00.643.011 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.603 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.006 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.663.010 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.663.038 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.371 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.373 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.373 I llama_init_from_model: graph nodes  = 967
0.00.666.374 I llama_init_from_model: graph splits = 2
0.00.666.377 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.899 I 
0.00.704.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.000 I perplexity: tokenizing the input ..
0.00.711.635 I perplexity: tokenization took 6.633 ms
0.00.711.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.842.332 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.843.663 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.843.692 I llama_perf_context_print:        load time =     695.87 ms
0.00.843.693 I llama_perf_context_print: prompt eval time =     130.46 ms /   128 tokens (    1.02 ms per token,   981.11 tokens per second)
0.00.843.693 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.843.694 I llama_perf_context_print:       total time =     138.80 ms /   129 tokens
0.00.844.084 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.077s
sys	0m0.153s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4863 (4e39a3c3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e505850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e505ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e506330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e508fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e509420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e509890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e509e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e50a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e50a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e50aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e50b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e50b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e50c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e50cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e50d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e50daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e50e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e50e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e50f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e50f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e50fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e510610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e510d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e5115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e511cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e512190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e512630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e512cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e513170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e513610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e5138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e513fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e514280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e514720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e514bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e515060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e515500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e5159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e515e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e5162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e516780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e516c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e5170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e517560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e517820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e517d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e518240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e518c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e5190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e519580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e519a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e519ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e51a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e51a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e51aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e51b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e51b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e51ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e51bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e51c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e51c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e51cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e51d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e51d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e51d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e51de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e51e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e51e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e51ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e51f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e51f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e51fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e51feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e520400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e520950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e520ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e5213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e521940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e521e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e5223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e522930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e522e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e5233d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e523920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e523e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e5243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e524910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e524e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e5253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e525900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e525e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e5263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e5268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e526e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e527390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e5278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e527e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e518750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e5282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e528a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e528fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e5294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e529a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e529f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e52a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e52aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e52af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e52b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e52ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e52bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e52c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e52ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e52cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e52d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e52d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e52dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e52e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e52e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e52eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e52efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e52f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e52f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e52fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e5304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e530770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e530c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e531170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e531670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e531b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e532070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e532570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e532a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e532f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e533470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e533970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e533e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e534370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e534870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e534d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e535270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e535770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e535c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e536170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e536670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e536b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e537070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e537570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e537a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e537f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e538470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e538970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e538e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e539370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e539870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e539d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e53a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e53a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e53ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e53b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e53b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e53bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e53c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e53c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e53ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e53cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e53d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e53d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e53de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e53e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e53e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e53ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e53f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e53f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e53fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e540170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e540670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e540b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e541070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e541570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e541a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e541f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e542470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e542970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e542e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e543370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e543870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e543d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e544270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e544770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e544c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e545170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e545670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e545b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e546070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e546620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e546bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e547180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e547730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e547c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e548130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e548630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e548d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e5491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e549470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e549a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e549f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e54a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e54aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e54af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e54b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e54bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e54bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e54c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e54ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e54d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e54d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e54db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e54e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e54e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e54ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e54f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e54f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e54fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e550330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e5508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e550e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e551440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e5519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e551fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e552550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e552b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e5530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e553660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e553c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e5541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e554770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e554d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e5552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e555880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e555e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e5563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e556990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e556f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e5574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e557aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e558050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e558600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e558bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e559160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e559710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e559cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e55a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e55a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e55add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e55b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e55b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e55bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e55c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e55ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e55cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e55d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e55db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e55e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e55e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e55ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e55f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e55f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e55fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e560270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e560770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e560c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e561170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e561670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e561b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e562070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e562570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e562a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e562f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e563470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e563970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e563e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e564370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11e564870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11e564d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11e565270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11e565770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11e565c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11e566170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11e566670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11e566b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11e567070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11e567570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e567a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e568480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e568ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e5692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e5699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e569ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e56a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e56a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e56ad70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.718.231 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e554a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e551700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e54ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e55e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e55bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e5599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e5577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e54fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e54d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e552260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e553370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e546e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e5588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e555590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e55d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e550040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e551150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e558310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e55a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e552dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e553ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e559420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e5560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e5566a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e550ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e551cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e55e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e5468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e55c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e54de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e557200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e54c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e54cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e546330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e54e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e55ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e554480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e567d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e55c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e552810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e554fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e558e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e5505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e55aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e54f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e55d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e55b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e556c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e55fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e54e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e55f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e547440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e54d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e55de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e557d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e559f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e55cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e55b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e553920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e504e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e528560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e569f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e513b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e54a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e5488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e56b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e56b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e56b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e56b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e56bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e56bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e56c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e56c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x105f04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105f044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105f04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x105f04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105f05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105f056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105f05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105f05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105f06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105f06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105f06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105f07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105f075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105f07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105f07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105f08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105f08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105f08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105f09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105f094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105f09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105f09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105f0a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105f0a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105f0ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105f0af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105f0b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105f0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105f0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105f0c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105f0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105f0ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105f0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105f0d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105f0d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105f0dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105f0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105f0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105f0e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105f0ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105f0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105f0f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105f0fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105f0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105f103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105f10830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105f10ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105f11110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105f11580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105f119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105f11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105f122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105f12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105f12bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105f13020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105f13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105f13900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105f13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105f141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105f14650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105f14ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105f14f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105f153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105f15810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105f15c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105f160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105f16560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105f169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105f16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105f172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105f17720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105f17b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105f18000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105f18470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105f188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105f18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105f191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105f19630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105f19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105f19f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105f1a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105f1a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105f1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105f1b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105f1b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105f1b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105f1be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105f1c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105f1c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105f1cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105f1cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105f1d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105f1d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105f1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105f1e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105f1ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105f1ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105f1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105f1f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105f1fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105f200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105f20530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105f209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105f20e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105f21280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105f216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105f21b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105f21fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105f22440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105f228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105f22d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105f23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105f23600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105f23a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105f23ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105f24350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105f247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105f24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105f250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105f25510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105f25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105f25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105f26260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105f266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105f26b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105f27070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105f274e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105f27950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105f27dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105f285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105f28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105f28e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105f29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105f299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105f29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105f2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105f2a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105f2b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105f2b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105f2b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105f2be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105f2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105f2c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105f2cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105f2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105f2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105f2e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105f2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105f2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105f2f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105f2f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105f2fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105f30280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105f30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105f30de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105f31390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105f31940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105f31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105f324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105f32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105f33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105f335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105f33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105f34110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105f346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105f34c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105f35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105f357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105f35d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105f36330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105f368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105f37440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105f379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105f37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105f38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105f38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105f390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105f39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105f39c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105f3a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105f3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105f3ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105f3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105f3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105f3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105f3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105f3c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105f3cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105f3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105f3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105f3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105f3e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105f3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105f3f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105f3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105f3fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105f40060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105f40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105f40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105f40f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105f41460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105f41960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105f41e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105f42360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105f42860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105f42d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105f43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105f43760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x105f43c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x105f44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x105f44660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x105f44b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x105f45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x105f45560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x105f45a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x105f45f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x105f46460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x105f46960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105f46e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105f47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105f47f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105f486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105f48dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105f49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105f49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105f49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105f4a160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e408780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e406590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e408da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e409210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e409680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e409c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e40a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e40a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e40ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e40b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e40b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e40bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e40c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e40cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e40d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e40de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e40e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e40ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e40f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e40fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e410290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e4109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e4110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e4117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e411f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e4123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e412850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e412cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e413190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e413630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e413ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e413f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e414230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e4146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e414b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e415010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e4154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e415950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e415df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e416290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e416730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e416bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e417070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e417510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e4179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e417e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e4182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e418790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e418c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e4190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e419570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e419a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e419eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e41a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e41a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e41ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e41b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e41b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e41b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e41bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e41c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e41c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e41cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e41d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e41d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e41dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e41e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e41e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e41eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e41f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e41f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e41faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e41ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e4204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e420a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e421000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e4215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e421b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e422110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e4226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e422c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e423220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e4237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e423d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e424330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e4248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e424e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e425440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e4259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e425fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e426550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e426b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e4270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e427660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e427c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e4281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e428770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e428d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e4292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e429880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e429e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e42a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e42a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e42af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e42b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e42baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e42c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e42c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e42cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e42d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e42d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e42dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e42e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e42e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e42ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e42f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e42f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e42fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e430120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e430620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e430b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e431020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e431520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e431a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e431f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e432420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e432920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e432e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e433320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e433820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e433d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e434220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e434720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e434c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e435120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e435620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e435b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e436020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e436520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e436a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e436f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e437420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e437920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e437e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e438320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e438820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e438d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e439220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e439720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e439c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e43a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e43a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e43ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e43b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e43b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e43ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e43bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e43c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e43c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e43ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e43d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e43d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e43dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e43e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e43e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e43ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e43f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e43f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e43fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e440020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e440520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e440a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e440f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e441420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e441920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e441e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e442320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e442820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e442d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e443220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e443720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e443c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e444120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e444620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e444b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e445020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e445520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e445a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e445f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e446420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e446920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e446e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e447320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e447820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e447dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e448380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e448930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e448ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e4493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e4498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e449de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e44a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e44a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e44ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e44b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e44b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e44bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e44c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e44c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e44cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e44d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e44d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e44dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e44e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e44e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e44ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e44f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e44f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e44fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e450420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e4509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e450f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e451530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e451ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e452090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e452640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e452bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e4531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e453750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e453d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e4542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e454860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e454e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e4553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e455970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e455f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e4564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e456a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e457030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e4575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e457b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e458140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e4586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e458ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e459250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e459800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e459db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e45a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e45a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e45aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e45b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e45ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e45bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e45c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e45cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e45d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e45d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e45dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e45e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e45e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e45ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e45f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e45f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e45fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e460410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e4609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e460f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e461520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e461a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e461f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e462420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e462920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e462e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e463320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e463820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e463d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e464220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e464720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e464c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e465120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e465620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e465b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11e466020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11e466520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11e466a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11e466f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11e467420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11e467920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11e467e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11e468320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11e468820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11e468d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e469220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e469c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e46a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e46aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e46b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e46b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e46bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e46c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e46c520 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.771s
user	0m0.277s
sys	0m0.328s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4863 (4e39a3c3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12760b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12760bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12760c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12760c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12760ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12760d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12760d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12760df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12760e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12760e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12760eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12760f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12760fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1276106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127610eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1276115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127611cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127612410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127612b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127613300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127613a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127614140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127614860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127615100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127615820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127615cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127616160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127616800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127616ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127617140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1276175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x127617a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127617d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1276181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127618680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127618b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127619460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127619900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127619da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12761a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12761a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12761ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12761b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12761b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12761b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12761be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12761c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12761ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12761cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12761d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12761d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12761dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12761e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12761e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12761eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12761ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12761f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12761f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12761fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1276200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1276209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1276217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1276225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127623d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1276242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127624810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127624d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1276252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127625800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x127625d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1276262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1276267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127626d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x127627290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1276277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127628280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1276287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127628d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127629270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1276297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12762a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12762a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12762ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12762b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12762b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12761c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12762bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12762c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12762c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12762ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12762d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12762d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12762de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12762e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12762e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12762ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12762f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12762f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12762fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127630380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1276308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127630d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1276316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127631ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127632dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1276344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127634e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1276352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1276360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1276369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x127637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1276377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127637c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1276385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127638a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127638ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x127639830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x127639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12763a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12763a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12763aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12763af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12763b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12763b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12763bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12763c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12763c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12763cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12763cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12763d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12763d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12763dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12763e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12763e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12763eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12763f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12763f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12763f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12763fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127640730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127640bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1276419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1276422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127642c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1276430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127644350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1276447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1276455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127645a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1276463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127647190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127647630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127648020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x127648570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x127648ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x127649010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1276494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x127649950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127649df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12764a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12764a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12764abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12764b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12764b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12764ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12764bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12764c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12764c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12764cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12764d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12764da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12764dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12764e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12764e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12764ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12764f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12764f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12764ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1276504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127650a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127651040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1276515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127651ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127652150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127652700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127652cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127653260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127653810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127654920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127655480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127655a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127655fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127656590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127656b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1276570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1276576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127657c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127658200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1276587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127658d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127659310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1276598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127659e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12765a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12765a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12765af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12765b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12765bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12765c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12765c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12765cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12765d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12765d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12765dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12765e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12765e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12765ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12765f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12765f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12765ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1276604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127660a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127661030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1276615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127661ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127661fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1276624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1276629e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127662ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1276633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1276638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127663de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1276642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1276647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127664ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1276651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1276656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127665be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1276660e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1276665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x127666ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x127666fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1276674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1276679e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x127667ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1276683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1276688e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x127668de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1276692e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127669cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12766a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12766ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12766b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12766b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12766bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12766c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12766c5e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117704ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117704f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1177053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117705830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117705ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117706110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117706580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1177069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117706e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1177073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117707850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117707ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1177089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1177091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1177099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11770a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11770a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11770af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11770b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11770be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11770c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11770cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11770d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11770da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11770e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11770e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11770e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11770eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11770f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11770f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11770fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11770ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117710620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117710ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117710f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117711400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1177118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117711d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1177121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117712680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117712b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117712fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117713460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117713900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117713da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117714240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1177146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117714b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117715020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1177154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117715960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117715e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1177162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117716be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117717080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117717520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1177177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117717aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117717f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117718380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1177187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117718c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1177190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117719540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1177199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117719e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11771a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11771a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11771ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11771afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11771b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11771b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11771bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11771c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11771c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11771ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11771cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11771d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11771d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11771dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11771e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11771e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11771e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11771ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11771f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11771f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11771fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11771ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117720430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1177208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117720d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117721180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1177215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117721a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117721ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117722340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1177227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117722c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117723090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117723500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117723970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117723de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117724250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1177246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117724b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117724fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117725880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117725cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117726160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1177265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117726a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117726eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117727320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117727790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117727c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117728070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1177284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117728dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117729230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1177296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117729b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117729f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11772a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11772a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11772acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11772b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11772b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11772ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11772be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11772c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11772c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11772cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11772d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11772d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11772d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11772dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11772e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11772e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11772eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11772ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11772f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11772f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11772fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117730120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117730590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117730a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117730e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1177312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117731750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117731bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117732030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1177324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117732910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117732d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1177331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117733660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117733ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117733f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1177343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117734820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117734c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117735100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117735570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117735cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117735fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117736420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117736890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117737170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1177375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117737a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117737ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117738330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1177387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117738c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117739080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1177394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117739dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11773a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11773a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11773ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11773af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11773b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11773b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11773bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11773c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11773c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11773ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11773cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11773d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11773d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11773dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11773e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11773e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11773e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11773edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11773f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11773f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11773fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117740120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1177406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117740bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1177412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117741750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117741bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117742090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1177428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117742ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117743150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117743700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117743cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117744260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117744810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117744dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117745370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117745920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117745ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117746480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117746a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117746fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117747590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117747b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1177480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1177486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117748c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117749200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1177497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117749d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11774a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11774a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11774ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11774b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11774b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11774bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11774c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11774cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11774d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11774d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11774dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11774e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11774e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11774ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11774f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11774f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11774fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1177503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117750970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117750f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1177514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117751a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117752030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1177525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117752b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117753140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1177536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117753ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117754250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117754800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117754db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117755360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117755910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117755ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117756470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117756a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117756f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117757420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117757920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117757e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117758320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117758820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117758d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117759220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117759720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117759c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11775a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11775a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11775ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11775b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11775b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11775ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11775bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11775c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11775c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11775ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11775d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11775d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11775dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11775e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11775e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11775f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11775f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11775ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117760690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117760950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1177610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117761580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117761a20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1288044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1288056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1288063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1288092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x128809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12880a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12880a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12880af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12880b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12880be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12880c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12880cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12880d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12880dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12880dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12880e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12880e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12880e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12880edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12880f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12880f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12880ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128810400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1288108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128810d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1288111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128811680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128811b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x128811fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128812460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128812900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128812da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128813240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1288136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128813b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128814020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1288144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128814960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128814e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1288152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128815be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128816080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128816520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1288169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128816e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128817120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1288173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128817850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128817cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128818130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1288185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128818a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128818e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1288192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128819760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x128819bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12881a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12881a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12881a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12881ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12881b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12881b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12881bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12881bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12881c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12881c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12881cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12881d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12881d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12881d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12881de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12881e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12881e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12881ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12881f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12881f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12881f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12881fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1288201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128820650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128820ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x128820f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1288213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128821810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128821c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1288220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128822810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128822cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1288232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128823e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1288243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128824960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128824f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1288254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128825a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128826020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1288265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128826b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128827130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1288276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128827c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128828190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128828690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128828b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128829090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x128829590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128829a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128829f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12882a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12882a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12882ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12882b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12882b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12882bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12882c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12882c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12882cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12882d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12882d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12882db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12882e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12882e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12882ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12882ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12882f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12882f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12882fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128830390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128830890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128830d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128831790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128831c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128832190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128832690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128832b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128833090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128833590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128833a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128833f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128834490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128834990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128834e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128835390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x128835890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128835d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128836290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128836790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128836c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128837190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128837690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128837b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128838090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128838590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128838a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128838f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128839490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128839990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128839e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12883a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12883a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12883ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12883b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12883b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12883bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12883c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12883c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12883cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12883d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12883d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12883da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12883df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12883e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12883e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12883ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12883f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12883f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12883fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128840290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x128840790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128840c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128841240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1288417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128841da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x128842350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x128842850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128842d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12770a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12770aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12770af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12770b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12770b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12770bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12770c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12770c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12770cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12770d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12770dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12770e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12770e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12770ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12770f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12770f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12770fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1277102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127710890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127710e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127711410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1277119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127711f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127712550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127712b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1277130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127713690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127713c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127714210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1277147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127714d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127715350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127715910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127715ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127716490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127716a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127717010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1277175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127717b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127718710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127718cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127719290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127719850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127719e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12771a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12771a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12771af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12771b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12771bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12771c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12771c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12771cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12771d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12771d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12771dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12771e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12771e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12771ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12771f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12771fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12771ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127720590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127720b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127721110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1277216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127721c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127722250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127722c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127723150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127723650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127723b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127724050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127724550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127724a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127724f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127725950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127725e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127726350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127726850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x127726d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x127727250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x127727750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x127727c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x127728150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x127728650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x127728b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x127729050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x127729550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x127729a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127729f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12772a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12772b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12772b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12772bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12772c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12772c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12772cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12772d0e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.231s
sys	0m0.177s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.46 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.72 sec*proc (2 tests)

Total Test time (real) =   1.73 sec
        1.75 real         0.52 user         0.22 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
