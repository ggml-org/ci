Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.535s
user	0m0.862s
sys	0m1.211s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target sha1
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-0
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-autorelease
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-barrier
[ 65%] Built target test-quantize-fns
[ 66%] Linking CXX executable ../../bin/llama-batched
[ 66%] Built target test-quantize-perf
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-eval-callback
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-embedding
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Built target llama-gritlm
[ 76%] Built target llama-gguf-split
[ 76%] Built target llama-imatrix
[ 76%] Built target llama-gbnf-validator
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Built target llama-infill
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Built target llama-bench
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Built target llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-create
[ 81%] Generating loading.html.hpp
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Built target llama-cli
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Built target llama-passkey
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Built target llama-lookup-stats
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-parallel
[ 86%] Built target llama-quantize
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Built target llama-save-load-state
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-speculative
[ 94%] Built target llama-tokenize
[ 94%] Built target llama-speculative-simple
[ 94%] Built target llama-gen-docs
[ 94%] Built target llama-tts
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.025s
user	0m5.960s
sys	0m9.400s

main: quantize time =  3159.67 ms
main:    total time =  3159.67 ms

main: quantize time =  1323.59 ms
main:    total time =  1323.59 ms

main: quantize time =  1326.99 ms
main:    total time =  1326.99 ms

main: quantize time =  2322.13 ms
main:    total time =  2322.13 ms

main: quantize time =  1574.89 ms
main:    total time =  1574.89 ms

main: quantize time =  4960.06 ms
main:    total time =  4960.06 ms

main: quantize time =  5657.34 ms
main:    total time =  5657.34 ms

main: quantize time =  6750.50 ms
main:    total time =  6750.50 ms

main: quantize time =  6244.34 ms
main:    total time =  6244.34 ms

main: quantize time =  4380.77 ms
main:    total time =  4380.77 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.118 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.223 I main: llama backend init
0.00.000.228 I main: load the model and apply lora adapter, if any
0.00.051.179 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.063.600 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.629 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.629 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.630 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.630 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.637 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.640 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.646 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.647 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.647 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.070.819 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.073.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.077.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.077.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.077.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.077.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.077.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.077.754 I llama_model_loader: - type  f32:  194 tensors
0.00.077.755 I llama_model_loader: - type  f16:   98 tensors
0.00.077.755 I print_info: file format = GGUF V3 (latest)
0.00.077.756 I print_info: file type   = all F32 (guessed)
0.00.077.758 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.097.939 I load: special tokens cache size = 25
0.00.103.980 I load: token to piece cache size = 0.2984 MB
0.00.103.985 I print_info: arch             = gptneox
0.00.103.985 I print_info: vocab_only       = 0
0.00.103.985 I print_info: n_ctx_train      = 2048
0.00.103.985 I print_info: n_embd           = 2048
0.00.103.985 I print_info: n_layer          = 24
0.00.103.990 I print_info: n_head           = 16
0.00.103.991 I print_info: n_head_kv        = 16
0.00.103.991 I print_info: n_rot            = 32
0.00.103.991 I print_info: n_swa            = 0
0.00.103.991 I print_info: n_embd_head_k    = 128
0.00.103.991 I print_info: n_embd_head_v    = 128
0.00.103.992 I print_info: n_gqa            = 1
0.00.103.993 I print_info: n_embd_k_gqa     = 2048
0.00.103.993 I print_info: n_embd_v_gqa     = 2048
0.00.103.994 I print_info: f_norm_eps       = 1.0e-05
0.00.103.994 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.994 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.994 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.994 I print_info: f_logit_scale    = 0.0e+00
0.00.103.995 I print_info: n_ff             = 8192
0.00.103.995 I print_info: n_expert         = 0
0.00.103.995 I print_info: n_expert_used    = 0
0.00.103.995 I print_info: causal attn      = 1
0.00.103.995 I print_info: pooling type     = 0
0.00.103.996 I print_info: rope type        = 2
0.00.103.996 I print_info: rope scaling     = linear
0.00.103.996 I print_info: freq_base_train  = 10000.0
0.00.103.996 I print_info: freq_scale_train = 1
0.00.103.997 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.997 I print_info: rope_finetuned   = unknown
0.00.103.997 I print_info: ssm_d_conv       = 0
0.00.103.997 I print_info: ssm_d_inner      = 0
0.00.103.997 I print_info: ssm_d_state      = 0
0.00.103.997 I print_info: ssm_dt_rank      = 0
0.00.103.997 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.998 I print_info: model type       = 1.4B
0.00.103.998 I print_info: model params     = 1.41 B
0.00.103.998 I print_info: general.name     = 1.4B
0.00.103.998 I print_info: vocab type       = BPE
0.00.103.999 I print_info: n_vocab          = 50304
0.00.103.999 I print_info: n_merges         = 50009
0.00.103.999 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.999 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.999 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.999 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.000 I print_info: LF token         = 128 'Ä'
0.00.104.000 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.000 I print_info: max token length = 1024
0.00.106.822 I load_tensors: offloading 24 repeating layers to GPU
0.00.106.824 I load_tensors: offloading output layer to GPU
0.00.106.825 I load_tensors: offloaded 25/25 layers to GPU
0.00.106.846 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.106.847 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.107.147 I llama_init_from_model: n_seq_max     = 1
0.00.107.149 I llama_init_from_model: n_ctx         = 2048
0.00.107.149 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.107.149 I llama_init_from_model: n_batch       = 2048
0.00.107.149 I llama_init_from_model: n_ubatch      = 512
0.00.107.149 I llama_init_from_model: flash_attn    = 0
0.00.107.150 I llama_init_from_model: freq_base     = 10000.0
0.00.107.150 I llama_init_from_model: freq_scale    = 1
0.00.107.150 I ggml_metal_init: allocating
0.00.107.153 I ggml_metal_init: found device: Apple M4
0.00.107.155 I ggml_metal_init: picking default device: Apple M4
0.00.107.832 I ggml_metal_init: using embedded metal library
0.00.152.200 I ggml_metal_init: GPU name:   Apple M4
0.00.152.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.152.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.152.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.152.206 I ggml_metal_init: simdgroup reduction   = true
0.00.152.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.152.207 I ggml_metal_init: has bfloat            = true
0.00.152.207 I ggml_metal_init: use bfloat            = true
0.00.152.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.152.209 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.226.791 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.249.269 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.249.299 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.249.319 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.250.263 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.250.264 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.250.264 I llama_init_from_model: graph nodes  = 967
0.00.250.265 I llama_init_from_model: graph splits = 2
0.00.250.268 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.250.384 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.250.385 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.332.756 I main: llama threadpool init, n_threads = 4
0.00.332.796 I 
0.00.332.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.332.835 I 
0.00.332.897 I sampler seed: 1234
0.00.332.903 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.332.928 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.332.929 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.332.929 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.183.137 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.02.183.141 I llama_perf_context_print:        load time =     281.57 ms
0.02.183.141 I llama_perf_context_print: prompt eval time =      54.13 ms /     7 tokens (    7.73 ms per token,   129.33 tokens per second)
0.02.183.142 I llama_perf_context_print:        eval time =    1793.26 ms /    63 runs   (   28.46 ms per token,    35.13 tokens per second)
0.02.183.143 I llama_perf_context_print:       total time =    1850.39 ms /    70 tokens
0.02.183.354 I ggml_metal_free: deallocating

real	0m2.506s
user	0m0.132s
sys	0m0.099s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.784 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.000 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.009 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.009 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.012 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.012 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.013 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.014 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.014 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.015 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.015 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.015 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.016 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.016 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.018 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.018 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.042 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.281 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.286 I llama_model_loader: - type  f32:  194 tensors
0.00.039.286 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.287 I print_info: file format = GGUF V3 (latest)
0.00.039.287 I print_info: file type   = Q8_0
0.00.039.289 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.063.068 I load: special tokens cache size = 25
0.00.070.033 I load: token to piece cache size = 0.2984 MB
0.00.070.037 I print_info: arch             = gptneox
0.00.070.038 I print_info: vocab_only       = 0
0.00.070.038 I print_info: n_ctx_train      = 2048
0.00.070.038 I print_info: n_embd           = 2048
0.00.070.038 I print_info: n_layer          = 24
0.00.070.044 I print_info: n_head           = 16
0.00.070.045 I print_info: n_head_kv        = 16
0.00.070.045 I print_info: n_rot            = 32
0.00.070.045 I print_info: n_swa            = 0
0.00.070.045 I print_info: n_embd_head_k    = 128
0.00.070.045 I print_info: n_embd_head_v    = 128
0.00.070.046 I print_info: n_gqa            = 1
0.00.070.047 I print_info: n_embd_k_gqa     = 2048
0.00.070.047 I print_info: n_embd_v_gqa     = 2048
0.00.070.048 I print_info: f_norm_eps       = 1.0e-05
0.00.070.049 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.049 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.049 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.052 I print_info: f_logit_scale    = 0.0e+00
0.00.070.053 I print_info: n_ff             = 8192
0.00.070.053 I print_info: n_expert         = 0
0.00.070.053 I print_info: n_expert_used    = 0
0.00.070.053 I print_info: causal attn      = 1
0.00.070.054 I print_info: pooling type     = 0
0.00.070.054 I print_info: rope type        = 2
0.00.070.054 I print_info: rope scaling     = linear
0.00.070.054 I print_info: freq_base_train  = 10000.0
0.00.070.055 I print_info: freq_scale_train = 1
0.00.070.055 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.055 I print_info: rope_finetuned   = unknown
0.00.070.055 I print_info: ssm_d_conv       = 0
0.00.070.055 I print_info: ssm_d_inner      = 0
0.00.070.055 I print_info: ssm_d_state      = 0
0.00.070.056 I print_info: ssm_dt_rank      = 0
0.00.070.056 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.056 I print_info: model type       = 1.4B
0.00.070.056 I print_info: model params     = 1.41 B
0.00.070.056 I print_info: general.name     = 1.4B
0.00.070.057 I print_info: vocab type       = BPE
0.00.070.057 I print_info: n_vocab          = 50304
0.00.070.058 I print_info: n_merges         = 50009
0.00.070.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.058 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.058 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.058 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.058 I print_info: LF token         = 128 'Ä'
0.00.070.059 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.059 I print_info: max token length = 1024
0.00.072.580 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.581 I load_tensors: offloading output layer to GPU
0.00.072.581 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.592 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.593 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.072.937 I llama_init_from_model: n_seq_max     = 1
0.00.072.938 I llama_init_from_model: n_ctx         = 2048
0.00.072.938 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.938 I llama_init_from_model: n_batch       = 2048
0.00.072.938 I llama_init_from_model: n_ubatch      = 512
0.00.072.938 I llama_init_from_model: flash_attn    = 0
0.00.072.939 I llama_init_from_model: freq_base     = 10000.0
0.00.072.939 I llama_init_from_model: freq_scale    = 1
0.00.072.940 I ggml_metal_init: allocating
0.00.072.942 I ggml_metal_init: found device: Apple M4
0.00.072.944 I ggml_metal_init: picking default device: Apple M4
0.00.073.753 I ggml_metal_init: using embedded metal library
0.00.076.732 I ggml_metal_init: GPU name:   Apple M4
0.00.076.734 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.735 I ggml_metal_init: simdgroup reduction   = true
0.00.076.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.735 I ggml_metal_init: has bfloat            = true
0.00.076.735 I ggml_metal_init: use bfloat            = true
0.00.076.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.737 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.659 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.113.280 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.303 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.328 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.114.695 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.114.698 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.114.698 I llama_init_from_model: graph nodes  = 967
0.00.114.698 I llama_init_from_model: graph splits = 2
0.00.114.703 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.416.889 I main: llama threadpool init, n_threads = 4
0.01.416.928 I 
0.01.416.957 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.416.957 I 
0.01.417.207 I sampler seed: 1234
0.01.417.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.417.223 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.417.225 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.417.225 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.497.149 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.02.497.149 I llama_perf_context_print:        load time =    1407.10 ms
0.02.497.150 I llama_perf_context_print: prompt eval time =      41.99 ms /     7 tokens (    6.00 ms per token,   166.69 tokens per second)
0.02.497.151 I llama_perf_context_print:        eval time =    1034.86 ms /    63 runs   (   16.43 ms per token,    60.88 tokens per second)
0.02.497.151 I llama_perf_context_print:       total time =    1080.26 ms /    70 tokens
0.02.497.376 I ggml_metal_free: deallocating

real	0m2.514s
user	0m0.118s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.017.072 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.937 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.938 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.940 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.941 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.941 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.941 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.943 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.946 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.323 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.590 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.591 I llama_model_loader: - type  f32:  194 tensors
0.00.045.591 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.591 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.592 I print_info: file format = GGUF V3 (latest)
0.00.045.593 I print_info: file type   = Q4_0
0.00.045.597 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.069.535 I load: special tokens cache size = 25
0.00.076.971 I load: token to piece cache size = 0.2984 MB
0.00.076.975 I print_info: arch             = gptneox
0.00.076.975 I print_info: vocab_only       = 0
0.00.076.975 I print_info: n_ctx_train      = 2048
0.00.076.976 I print_info: n_embd           = 2048
0.00.076.976 I print_info: n_layer          = 24
0.00.076.980 I print_info: n_head           = 16
0.00.076.981 I print_info: n_head_kv        = 16
0.00.076.981 I print_info: n_rot            = 32
0.00.076.981 I print_info: n_swa            = 0
0.00.076.981 I print_info: n_embd_head_k    = 128
0.00.076.981 I print_info: n_embd_head_v    = 128
0.00.076.982 I print_info: n_gqa            = 1
0.00.076.983 I print_info: n_embd_k_gqa     = 2048
0.00.076.983 I print_info: n_embd_v_gqa     = 2048
0.00.076.984 I print_info: f_norm_eps       = 1.0e-05
0.00.076.984 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.984 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.984 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.985 I print_info: f_logit_scale    = 0.0e+00
0.00.076.986 I print_info: n_ff             = 8192
0.00.076.986 I print_info: n_expert         = 0
0.00.076.986 I print_info: n_expert_used    = 0
0.00.076.986 I print_info: causal attn      = 1
0.00.076.986 I print_info: pooling type     = 0
0.00.076.986 I print_info: rope type        = 2
0.00.076.987 I print_info: rope scaling     = linear
0.00.076.987 I print_info: freq_base_train  = 10000.0
0.00.076.987 I print_info: freq_scale_train = 1
0.00.076.987 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.994 I print_info: rope_finetuned   = unknown
0.00.076.995 I print_info: ssm_d_conv       = 0
0.00.076.995 I print_info: ssm_d_inner      = 0
0.00.076.996 I print_info: ssm_d_state      = 0
0.00.076.996 I print_info: ssm_dt_rank      = 0
0.00.076.996 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.996 I print_info: model type       = 1.4B
0.00.076.997 I print_info: model params     = 1.41 B
0.00.076.997 I print_info: general.name     = 1.4B
0.00.076.998 I print_info: vocab type       = BPE
0.00.076.998 I print_info: n_vocab          = 50304
0.00.076.999 I print_info: n_merges         = 50009
0.00.076.999 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.999 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.999 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.001 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.001 I print_info: LF token         = 128 'Ä'
0.00.077.001 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.002 I print_info: max token length = 1024
0.00.079.239 I load_tensors: offloading 24 repeating layers to GPU
0.00.079.239 I load_tensors: offloading output layer to GPU
0.00.079.239 I load_tensors: offloaded 25/25 layers to GPU
0.00.079.246 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.079.247 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.079.614 I llama_init_from_model: n_seq_max     = 1
0.00.079.616 I llama_init_from_model: n_ctx         = 2048
0.00.079.616 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.079.616 I llama_init_from_model: n_batch       = 2048
0.00.079.616 I llama_init_from_model: n_ubatch      = 512
0.00.079.616 I llama_init_from_model: flash_attn    = 0
0.00.079.617 I llama_init_from_model: freq_base     = 10000.0
0.00.079.617 I llama_init_from_model: freq_scale    = 1
0.00.079.618 I ggml_metal_init: allocating
0.00.079.621 I ggml_metal_init: found device: Apple M4
0.00.079.623 I ggml_metal_init: picking default device: Apple M4
0.00.080.439 I ggml_metal_init: using embedded metal library
0.00.083.439 I ggml_metal_init: GPU name:   Apple M4
0.00.083.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.442 I ggml_metal_init: simdgroup reduction   = true
0.00.083.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.442 I ggml_metal_init: has bfloat            = true
0.00.083.443 I ggml_metal_init: use bfloat            = true
0.00.083.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.981 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.122.724 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.754 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.785 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.123.942 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.123.944 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.123.944 I llama_init_from_model: graph nodes  = 967
0.00.123.944 I llama_init_from_model: graph splits = 2
0.00.123.951 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.124.081 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.124.081 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.225 I main: llama threadpool init, n_threads = 4
0.00.715.268 I 
0.00.715.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.301 I 
0.00.715.526 I sampler seed: 1234
0.00.715.532 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.715.543 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.715.543 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.715.544 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.397.705 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57770.55 tokens per second)
0.01.397.705 I llama_perf_context_print:        load time =     698.15 ms
0.01.397.706 I llama_perf_context_print: prompt eval time =      46.60 ms /     7 tokens (    6.66 ms per token,   150.20 tokens per second)
0.01.397.707 I llama_perf_context_print:        eval time =     632.54 ms /    63 runs   (   10.04 ms per token,    99.60 tokens per second)
0.01.397.707 I llama_perf_context_print:       total time =     682.48 ms /    70 tokens
0.01.397.967 I ggml_metal_free: deallocating

real	0m1.415s
user	0m0.123s
sys	0m0.152s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.685 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.603 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.609 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.609 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.610 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.610 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.613 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.614 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.614 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.615 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.616 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.616 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.619 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.359 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.170 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.172 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.172 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.172 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.173 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.173 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.173 I llama_model_loader: - type  f32:  194 tensors
0.00.031.174 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.174 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.175 I print_info: file format = GGUF V3 (latest)
0.00.031.175 I print_info: file type   = Q4_1
0.00.031.176 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.050.492 I load: special tokens cache size = 25
0.00.056.440 I load: token to piece cache size = 0.2984 MB
0.00.056.443 I print_info: arch             = gptneox
0.00.056.443 I print_info: vocab_only       = 0
0.00.056.444 I print_info: n_ctx_train      = 2048
0.00.056.444 I print_info: n_embd           = 2048
0.00.056.444 I print_info: n_layer          = 24
0.00.056.447 I print_info: n_head           = 16
0.00.056.447 I print_info: n_head_kv        = 16
0.00.056.449 I print_info: n_rot            = 32
0.00.056.449 I print_info: n_swa            = 0
0.00.056.449 I print_info: n_embd_head_k    = 128
0.00.056.450 I print_info: n_embd_head_v    = 128
0.00.056.450 I print_info: n_gqa            = 1
0.00.056.451 I print_info: n_embd_k_gqa     = 2048
0.00.056.452 I print_info: n_embd_v_gqa     = 2048
0.00.056.452 I print_info: f_norm_eps       = 1.0e-05
0.00.056.453 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.453 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.453 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.453 I print_info: f_logit_scale    = 0.0e+00
0.00.056.454 I print_info: n_ff             = 8192
0.00.056.454 I print_info: n_expert         = 0
0.00.056.454 I print_info: n_expert_used    = 0
0.00.056.454 I print_info: causal attn      = 1
0.00.056.455 I print_info: pooling type     = 0
0.00.056.456 I print_info: rope type        = 2
0.00.056.458 I print_info: rope scaling     = linear
0.00.056.458 I print_info: freq_base_train  = 10000.0
0.00.056.458 I print_info: freq_scale_train = 1
0.00.056.459 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.459 I print_info: rope_finetuned   = unknown
0.00.056.463 I print_info: ssm_d_conv       = 0
0.00.056.463 I print_info: ssm_d_inner      = 0
0.00.056.463 I print_info: ssm_d_state      = 0
0.00.056.464 I print_info: ssm_dt_rank      = 0
0.00.056.464 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.464 I print_info: model type       = 1.4B
0.00.056.464 I print_info: model params     = 1.41 B
0.00.056.465 I print_info: general.name     = 1.4B
0.00.056.465 I print_info: vocab type       = BPE
0.00.056.466 I print_info: n_vocab          = 50304
0.00.056.466 I print_info: n_merges         = 50009
0.00.056.467 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.467 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.467 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.467 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.467 I print_info: LF token         = 128 'Ä'
0.00.056.468 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.468 I print_info: max token length = 1024
0.00.058.391 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.391 I load_tensors: offloading output layer to GPU
0.00.058.392 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.402 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.058.403 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.058.686 I llama_init_from_model: n_seq_max     = 1
0.00.058.687 I llama_init_from_model: n_ctx         = 2048
0.00.058.687 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.687 I llama_init_from_model: n_batch       = 2048
0.00.058.687 I llama_init_from_model: n_ubatch      = 512
0.00.058.687 I llama_init_from_model: flash_attn    = 0
0.00.058.688 I llama_init_from_model: freq_base     = 10000.0
0.00.058.688 I llama_init_from_model: freq_scale    = 1
0.00.058.689 I ggml_metal_init: allocating
0.00.058.692 I ggml_metal_init: found device: Apple M4
0.00.058.694 I ggml_metal_init: picking default device: Apple M4
0.00.059.292 I ggml_metal_init: using embedded metal library
0.00.061.613 I ggml_metal_init: GPU name:   Apple M4
0.00.061.614 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.615 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.615 I ggml_metal_init: simdgroup reduction   = true
0.00.061.616 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.616 I ggml_metal_init: has bfloat            = true
0.00.061.616 I ggml_metal_init: use bfloat            = true
0.00.061.616 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.617 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.308 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.706 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.722 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.743 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.827 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.829 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.829 I llama_init_from_model: graph nodes  = 967
0.00.091.830 I llama_init_from_model: graph splits = 2
0.00.091.832 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.986 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.986 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.824.023 I main: llama threadpool init, n_threads = 4
0.00.824.062 I 
0.00.824.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.824.087 I 
0.00.824.318 I sampler seed: 1234
0.00.824.322 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.824.373 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.824.378 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.824.378 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.549.779 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62280.70 tokens per second)
0.01.549.780 I llama_perf_context_print:        load time =     815.34 ms
0.01.549.780 I llama_perf_context_print: prompt eval time =      43.98 ms /     7 tokens (    6.28 ms per token,   159.16 tokens per second)
0.01.549.781 I llama_perf_context_print:        eval time =     678.51 ms /    63 runs   (   10.77 ms per token,    92.85 tokens per second)
0.01.549.781 I llama_perf_context_print:       total time =     725.76 ms /    70 tokens
0.01.550.042 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.109s
sys	0m0.145s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.020.291 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.400 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.040.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.406 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.406 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.414 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.415 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.418 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.419 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.736 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.318 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.320 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.050.321 I llama_model_loader: - type  f32:  194 tensors
0.00.050.321 I llama_model_loader: - type q5_0:   97 tensors
0.00.050.321 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.322 I print_info: file format = GGUF V3 (latest)
0.00.050.322 I print_info: file type   = Q5_0
0.00.050.323 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.075.912 I load: special tokens cache size = 25
0.00.085.772 I load: token to piece cache size = 0.2984 MB
0.00.085.777 I print_info: arch             = gptneox
0.00.085.777 I print_info: vocab_only       = 0
0.00.085.778 I print_info: n_ctx_train      = 2048
0.00.085.778 I print_info: n_embd           = 2048
0.00.085.778 I print_info: n_layer          = 24
0.00.085.782 I print_info: n_head           = 16
0.00.085.783 I print_info: n_head_kv        = 16
0.00.085.784 I print_info: n_rot            = 32
0.00.085.784 I print_info: n_swa            = 0
0.00.085.784 I print_info: n_embd_head_k    = 128
0.00.085.785 I print_info: n_embd_head_v    = 128
0.00.085.786 I print_info: n_gqa            = 1
0.00.085.787 I print_info: n_embd_k_gqa     = 2048
0.00.085.788 I print_info: n_embd_v_gqa     = 2048
0.00.085.789 I print_info: f_norm_eps       = 1.0e-05
0.00.085.789 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.790 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.790 I print_info: f_logit_scale    = 0.0e+00
0.00.085.791 I print_info: n_ff             = 8192
0.00.085.791 I print_info: n_expert         = 0
0.00.085.792 I print_info: n_expert_used    = 0
0.00.085.792 I print_info: causal attn      = 1
0.00.085.792 I print_info: pooling type     = 0
0.00.085.794 I print_info: rope type        = 2
0.00.085.795 I print_info: rope scaling     = linear
0.00.085.795 I print_info: freq_base_train  = 10000.0
0.00.085.796 I print_info: freq_scale_train = 1
0.00.085.796 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.796 I print_info: rope_finetuned   = unknown
0.00.085.797 I print_info: ssm_d_conv       = 0
0.00.085.797 I print_info: ssm_d_inner      = 0
0.00.085.797 I print_info: ssm_d_state      = 0
0.00.085.797 I print_info: ssm_dt_rank      = 0
0.00.085.798 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.798 I print_info: model type       = 1.4B
0.00.085.798 I print_info: model params     = 1.41 B
0.00.085.802 I print_info: general.name     = 1.4B
0.00.085.803 I print_info: vocab type       = BPE
0.00.085.803 I print_info: n_vocab          = 50304
0.00.085.803 I print_info: n_merges         = 50009
0.00.085.803 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.804 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.804 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.805 I print_info: LF token         = 128 'Ä'
0.00.085.805 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.805 I print_info: max token length = 1024
0.00.088.846 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.846 I load_tensors: offloading output layer to GPU
0.00.088.847 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.858 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.088.860 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.089.336 I llama_init_from_model: n_seq_max     = 1
0.00.089.338 I llama_init_from_model: n_ctx         = 2048
0.00.089.338 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.089.338 I llama_init_from_model: n_batch       = 2048
0.00.089.338 I llama_init_from_model: n_ubatch      = 512
0.00.089.339 I llama_init_from_model: flash_attn    = 0
0.00.089.339 I llama_init_from_model: freq_base     = 10000.0
0.00.089.340 I llama_init_from_model: freq_scale    = 1
0.00.089.340 I ggml_metal_init: allocating
0.00.089.345 I ggml_metal_init: found device: Apple M4
0.00.089.348 I ggml_metal_init: picking default device: Apple M4
0.00.090.239 I ggml_metal_init: using embedded metal library
0.00.094.383 I ggml_metal_init: GPU name:   Apple M4
0.00.094.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.386 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.387 I ggml_metal_init: simdgroup reduction   = true
0.00.094.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.387 I ggml_metal_init: has bfloat            = true
0.00.094.387 I ggml_metal_init: use bfloat            = true
0.00.094.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.631 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.128.869 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.128.897 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.128.920 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.129.999 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.130.000 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.130.001 I llama_init_from_model: graph nodes  = 967
0.00.130.001 I llama_init_from_model: graph splits = 2
0.00.130.004 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.130.138 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.130.139 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.291 I main: llama threadpool init, n_threads = 4
0.00.843.379 I 
0.00.843.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.843.460 I 
0.00.843.948 I sampler seed: 1234
0.00.843.955 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.843.985 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.843.987 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.843.987 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.645.429 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.01.645.430 I llama_perf_context_print:        load time =     823.00 ms
0.01.645.431 I llama_perf_context_print: prompt eval time =      50.01 ms /     7 tokens (    7.14 ms per token,   139.97 tokens per second)
0.01.645.431 I llama_perf_context_print:        eval time =     748.44 ms /    63 runs   (   11.88 ms per token,    84.18 tokens per second)
0.01.645.432 I llama_perf_context_print:       total time =     802.14 ms /    70 tokens
0.01.645.733 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.132s
sys	0m0.174s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.791 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.382 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.393 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.396 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.396 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.396 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.401 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.402 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.153 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.154 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.154 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.155 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.155 I llama_model_loader: - type  f32:  194 tensors
0.00.033.159 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.160 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.160 I print_info: file format = GGUF V3 (latest)
0.00.033.161 I print_info: file type   = Q5_1
0.00.033.162 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.052.481 I load: special tokens cache size = 25
0.00.058.587 I load: token to piece cache size = 0.2984 MB
0.00.058.590 I print_info: arch             = gptneox
0.00.058.590 I print_info: vocab_only       = 0
0.00.058.591 I print_info: n_ctx_train      = 2048
0.00.058.591 I print_info: n_embd           = 2048
0.00.058.591 I print_info: n_layer          = 24
0.00.058.595 I print_info: n_head           = 16
0.00.058.596 I print_info: n_head_kv        = 16
0.00.058.597 I print_info: n_rot            = 32
0.00.058.597 I print_info: n_swa            = 0
0.00.058.597 I print_info: n_embd_head_k    = 128
0.00.058.597 I print_info: n_embd_head_v    = 128
0.00.058.598 I print_info: n_gqa            = 1
0.00.058.599 I print_info: n_embd_k_gqa     = 2048
0.00.058.599 I print_info: n_embd_v_gqa     = 2048
0.00.058.600 I print_info: f_norm_eps       = 1.0e-05
0.00.058.600 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.601 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.601 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.601 I print_info: f_logit_scale    = 0.0e+00
0.00.058.602 I print_info: n_ff             = 8192
0.00.058.602 I print_info: n_expert         = 0
0.00.058.602 I print_info: n_expert_used    = 0
0.00.058.602 I print_info: causal attn      = 1
0.00.058.602 I print_info: pooling type     = 0
0.00.058.602 I print_info: rope type        = 2
0.00.058.603 I print_info: rope scaling     = linear
0.00.058.603 I print_info: freq_base_train  = 10000.0
0.00.058.603 I print_info: freq_scale_train = 1
0.00.058.603 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.603 I print_info: rope_finetuned   = unknown
0.00.058.604 I print_info: ssm_d_conv       = 0
0.00.058.604 I print_info: ssm_d_inner      = 0
0.00.058.604 I print_info: ssm_d_state      = 0
0.00.058.604 I print_info: ssm_dt_rank      = 0
0.00.058.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.604 I print_info: model type       = 1.4B
0.00.058.605 I print_info: model params     = 1.41 B
0.00.058.605 I print_info: general.name     = 1.4B
0.00.058.605 I print_info: vocab type       = BPE
0.00.058.605 I print_info: n_vocab          = 50304
0.00.058.607 I print_info: n_merges         = 50009
0.00.058.607 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.608 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.608 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.608 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.608 I print_info: LF token         = 128 'Ä'
0.00.058.608 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.608 I print_info: max token length = 1024
0.00.060.357 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.357 I load_tensors: offloading output layer to GPU
0.00.060.357 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.363 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.060.363 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.060.653 I llama_init_from_model: n_seq_max     = 1
0.00.060.655 I llama_init_from_model: n_ctx         = 2048
0.00.060.655 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.656 I llama_init_from_model: n_batch       = 2048
0.00.060.656 I llama_init_from_model: n_ubatch      = 512
0.00.060.656 I llama_init_from_model: flash_attn    = 0
0.00.060.656 I llama_init_from_model: freq_base     = 10000.0
0.00.060.656 I llama_init_from_model: freq_scale    = 1
0.00.060.657 I ggml_metal_init: allocating
0.00.060.660 I ggml_metal_init: found device: Apple M4
0.00.060.662 I ggml_metal_init: picking default device: Apple M4
0.00.061.353 I ggml_metal_init: using embedded metal library
0.00.063.758 I ggml_metal_init: GPU name:   Apple M4
0.00.063.760 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.760 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.761 I ggml_metal_init: simdgroup reduction   = true
0.00.063.761 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.761 I ggml_metal_init: has bfloat            = true
0.00.063.761 I ggml_metal_init: use bfloat            = true
0.00.063.762 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.763 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.947 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.690 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.712 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.743 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.093.713 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.093.714 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.093.714 I llama_init_from_model: graph nodes  = 967
0.00.093.715 I llama_init_from_model: graph splits = 2
0.00.093.719 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.850 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.206.062 I main: llama threadpool init, n_threads = 4
0.01.206.117 I 
0.01.206.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.206.146 I 
0.01.206.383 I sampler seed: 1234
0.01.206.387 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.206.424 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.206.427 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.206.427 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.036.232 I llama_perf_sampler_print:    sampling time =       1.52 ms /    71 runs   (    0.02 ms per token, 46772.07 tokens per second)
0.02.036.233 I llama_perf_context_print:        load time =    1197.27 ms
0.02.036.234 I llama_perf_context_print: prompt eval time =      42.36 ms /     7 tokens (    6.05 ms per token,   165.23 tokens per second)
0.02.036.234 I llama_perf_context_print:        eval time =     784.78 ms /    63 runs   (   12.46 ms per token,    80.28 tokens per second)
0.02.036.235 I llama_perf_context_print:       total time =     830.17 ms /    70 tokens
0.02.036.489 I ggml_metal_free: deallocating

real	0m2.060s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.413 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.028 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.033 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.035 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.041 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.045 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.047 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.904 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.713 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.714 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.715 I llama_model_loader: - type  f32:  194 tensors
0.00.025.716 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.716 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.716 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.716 I print_info: file format = GGUF V3 (latest)
0.00.025.717 I print_info: file type   = Q2_K - Medium
0.00.025.718 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.022 I load: special tokens cache size = 25
0.00.050.962 I load: token to piece cache size = 0.2984 MB
0.00.050.965 I print_info: arch             = gptneox
0.00.050.965 I print_info: vocab_only       = 0
0.00.050.965 I print_info: n_ctx_train      = 2048
0.00.050.965 I print_info: n_embd           = 2048
0.00.050.966 I print_info: n_layer          = 24
0.00.050.969 I print_info: n_head           = 16
0.00.050.969 I print_info: n_head_kv        = 16
0.00.050.970 I print_info: n_rot            = 32
0.00.050.970 I print_info: n_swa            = 0
0.00.050.970 I print_info: n_embd_head_k    = 128
0.00.050.970 I print_info: n_embd_head_v    = 128
0.00.050.971 I print_info: n_gqa            = 1
0.00.050.972 I print_info: n_embd_k_gqa     = 2048
0.00.050.972 I print_info: n_embd_v_gqa     = 2048
0.00.050.973 I print_info: f_norm_eps       = 1.0e-05
0.00.050.973 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.973 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.974 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.974 I print_info: f_logit_scale    = 0.0e+00
0.00.050.975 I print_info: n_ff             = 8192
0.00.050.975 I print_info: n_expert         = 0
0.00.050.975 I print_info: n_expert_used    = 0
0.00.050.975 I print_info: causal attn      = 1
0.00.050.975 I print_info: pooling type     = 0
0.00.050.975 I print_info: rope type        = 2
0.00.050.976 I print_info: rope scaling     = linear
0.00.050.979 I print_info: freq_base_train  = 10000.0
0.00.050.979 I print_info: freq_scale_train = 1
0.00.050.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.980 I print_info: rope_finetuned   = unknown
0.00.050.980 I print_info: ssm_d_conv       = 0
0.00.050.980 I print_info: ssm_d_inner      = 0
0.00.050.980 I print_info: ssm_d_state      = 0
0.00.050.980 I print_info: ssm_dt_rank      = 0
0.00.050.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.980 I print_info: model type       = 1.4B
0.00.050.981 I print_info: model params     = 1.41 B
0.00.050.981 I print_info: general.name     = 1.4B
0.00.050.982 I print_info: vocab type       = BPE
0.00.050.982 I print_info: n_vocab          = 50304
0.00.050.982 I print_info: n_merges         = 50009
0.00.050.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.983 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.983 I print_info: LF token         = 128 'Ä'
0.00.050.983 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.983 I print_info: max token length = 1024
0.00.052.808 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.809 I load_tensors: offloading output layer to GPU
0.00.052.809 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.819 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.821 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.101 I llama_init_from_model: n_seq_max     = 1
0.00.053.101 I llama_init_from_model: n_ctx         = 2048
0.00.053.102 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.102 I llama_init_from_model: n_batch       = 2048
0.00.053.102 I llama_init_from_model: n_ubatch      = 512
0.00.053.102 I llama_init_from_model: flash_attn    = 0
0.00.053.102 I llama_init_from_model: freq_base     = 10000.0
0.00.053.103 I llama_init_from_model: freq_scale    = 1
0.00.053.103 I ggml_metal_init: allocating
0.00.053.106 I ggml_metal_init: found device: Apple M4
0.00.053.108 I ggml_metal_init: picking default device: Apple M4
0.00.053.695 I ggml_metal_init: using embedded metal library
0.00.055.994 I ggml_metal_init: GPU name:   Apple M4
0.00.055.996 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.996 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.997 I ggml_metal_init: simdgroup reduction   = true
0.00.055.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.997 I ggml_metal_init: has bfloat            = true
0.00.055.997 I ggml_metal_init: use bfloat            = true
0.00.055.998 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.434 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.276 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.292 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.340 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.341 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.342 I llama_init_from_model: graph nodes  = 967
0.00.086.342 I llama_init_from_model: graph splits = 2
0.00.086.345 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.470 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.482.128 I main: llama threadpool init, n_threads = 4
0.00.482.174 I 
0.00.482.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.482.209 I 
0.00.482.445 I sampler seed: 1234
0.00.482.450 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.482.489 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.482.490 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.482.490 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.158.081 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.158.081 I llama_perf_context_print:        load time =     471.71 ms
0.01.158.082 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.68 tokens per second)
0.01.158.083 I llama_perf_context_print:        eval time =     636.83 ms /    63 runs   (   10.11 ms per token,    98.93 tokens per second)
0.01.158.084 I llama_perf_context_print:       total time =     675.96 ms /    70 tokens
0.01.158.311 I ggml_metal_free: deallocating

real	0m1.177s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.653 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.222 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.235 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.235 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.238 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.076 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.928 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.929 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.930 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.930 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.930 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.931 I llama_model_loader: - type  f32:  194 tensors
0.00.025.931 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.931 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.932 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.932 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.933 I print_info: file format = GGUF V3 (latest)
0.00.025.934 I print_info: file type   = Q3_K - Medium
0.00.025.935 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.243 I load: special tokens cache size = 25
0.00.051.249 I load: token to piece cache size = 0.2984 MB
0.00.051.252 I print_info: arch             = gptneox
0.00.051.253 I print_info: vocab_only       = 0
0.00.051.253 I print_info: n_ctx_train      = 2048
0.00.051.253 I print_info: n_embd           = 2048
0.00.051.253 I print_info: n_layer          = 24
0.00.051.257 I print_info: n_head           = 16
0.00.051.258 I print_info: n_head_kv        = 16
0.00.051.258 I print_info: n_rot            = 32
0.00.051.258 I print_info: n_swa            = 0
0.00.051.258 I print_info: n_embd_head_k    = 128
0.00.051.258 I print_info: n_embd_head_v    = 128
0.00.051.259 I print_info: n_gqa            = 1
0.00.051.260 I print_info: n_embd_k_gqa     = 2048
0.00.051.261 I print_info: n_embd_v_gqa     = 2048
0.00.051.261 I print_info: f_norm_eps       = 1.0e-05
0.00.051.261 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.262 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.262 I print_info: f_logit_scale    = 0.0e+00
0.00.051.263 I print_info: n_ff             = 8192
0.00.051.263 I print_info: n_expert         = 0
0.00.051.263 I print_info: n_expert_used    = 0
0.00.051.263 I print_info: causal attn      = 1
0.00.051.264 I print_info: pooling type     = 0
0.00.051.264 I print_info: rope type        = 2
0.00.051.264 I print_info: rope scaling     = linear
0.00.051.266 I print_info: freq_base_train  = 10000.0
0.00.051.266 I print_info: freq_scale_train = 1
0.00.051.267 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.267 I print_info: rope_finetuned   = unknown
0.00.051.267 I print_info: ssm_d_conv       = 0
0.00.051.267 I print_info: ssm_d_inner      = 0
0.00.051.267 I print_info: ssm_d_state      = 0
0.00.051.267 I print_info: ssm_dt_rank      = 0
0.00.051.267 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.268 I print_info: model type       = 1.4B
0.00.051.268 I print_info: model params     = 1.41 B
0.00.051.268 I print_info: general.name     = 1.4B
0.00.051.269 I print_info: vocab type       = BPE
0.00.051.269 I print_info: n_vocab          = 50304
0.00.051.269 I print_info: n_merges         = 50009
0.00.051.270 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.270 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.270 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.271 I print_info: LF token         = 128 'Ä'
0.00.051.272 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.272 I print_info: max token length = 1024
0.00.053.210 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.210 I load_tensors: offloading output layer to GPU
0.00.053.211 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.221 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.222 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.504 I llama_init_from_model: n_seq_max     = 1
0.00.053.505 I llama_init_from_model: n_ctx         = 2048
0.00.053.505 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.505 I llama_init_from_model: n_batch       = 2048
0.00.053.505 I llama_init_from_model: n_ubatch      = 512
0.00.053.506 I llama_init_from_model: flash_attn    = 0
0.00.053.506 I llama_init_from_model: freq_base     = 10000.0
0.00.053.506 I llama_init_from_model: freq_scale    = 1
0.00.053.507 I ggml_metal_init: allocating
0.00.053.510 I ggml_metal_init: found device: Apple M4
0.00.053.512 I ggml_metal_init: picking default device: Apple M4
0.00.054.104 I ggml_metal_init: using embedded metal library
0.00.056.456 I ggml_metal_init: GPU name:   Apple M4
0.00.056.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.458 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.458 I ggml_metal_init: simdgroup reduction   = true
0.00.056.458 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.458 I ggml_metal_init: has bfloat            = true
0.00.056.458 I ggml_metal_init: use bfloat            = true
0.00.056.459 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.459 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.301 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.904 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.921 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.945 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.967 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.969 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.969 I llama_init_from_model: graph nodes  = 967
0.00.086.969 I llama_init_from_model: graph splits = 2
0.00.086.972 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.088 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.747 I main: llama threadpool init, n_threads = 4
0.00.554.789 I 
0.00.554.818 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.819 I 
0.00.555.042 I sampler seed: 1234
0.00.555.047 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.555.059 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.555.059 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.555.059 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.293.461 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.293.462 I llama_perf_context_print:        load time =     545.09 ms
0.01.293.463 I llama_perf_context_print: prompt eval time =      40.51 ms /     7 tokens (    5.79 ms per token,   172.79 tokens per second)
0.01.293.464 I llama_perf_context_print:        eval time =     694.75 ms /    63 runs   (   11.03 ms per token,    90.68 tokens per second)
0.01.293.464 I llama_perf_context_print:       total time =     738.72 ms /    70 tokens
0.01.293.693 I ggml_metal_free: deallocating

real	0m1.311s
user	0m0.109s
sys	0m0.128s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.542 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.543 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.544 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.544 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.545 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.546 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.546 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.547 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.549 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.283 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.093 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.094 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.094 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.095 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.096 I llama_model_loader: - type  f32:  194 tensors
0.00.024.096 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.096 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.096 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.097 I print_info: file format = GGUF V3 (latest)
0.00.024.098 I print_info: file type   = Q4_K - Medium
0.00.024.098 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.374 I load: special tokens cache size = 25
0.00.049.521 I load: token to piece cache size = 0.2984 MB
0.00.049.524 I print_info: arch             = gptneox
0.00.049.525 I print_info: vocab_only       = 0
0.00.049.525 I print_info: n_ctx_train      = 2048
0.00.049.525 I print_info: n_embd           = 2048
0.00.049.525 I print_info: n_layer          = 24
0.00.049.528 I print_info: n_head           = 16
0.00.049.528 I print_info: n_head_kv        = 16
0.00.049.529 I print_info: n_rot            = 32
0.00.049.529 I print_info: n_swa            = 0
0.00.049.529 I print_info: n_embd_head_k    = 128
0.00.049.529 I print_info: n_embd_head_v    = 128
0.00.049.530 I print_info: n_gqa            = 1
0.00.049.531 I print_info: n_embd_k_gqa     = 2048
0.00.049.532 I print_info: n_embd_v_gqa     = 2048
0.00.049.532 I print_info: f_norm_eps       = 1.0e-05
0.00.049.533 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.533 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.533 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.533 I print_info: f_logit_scale    = 0.0e+00
0.00.049.535 I print_info: n_ff             = 8192
0.00.049.535 I print_info: n_expert         = 0
0.00.049.535 I print_info: n_expert_used    = 0
0.00.049.536 I print_info: causal attn      = 1
0.00.049.536 I print_info: pooling type     = 0
0.00.049.536 I print_info: rope type        = 2
0.00.049.537 I print_info: rope scaling     = linear
0.00.049.539 I print_info: freq_base_train  = 10000.0
0.00.049.539 I print_info: freq_scale_train = 1
0.00.049.540 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.540 I print_info: rope_finetuned   = unknown
0.00.049.540 I print_info: ssm_d_conv       = 0
0.00.049.540 I print_info: ssm_d_inner      = 0
0.00.049.540 I print_info: ssm_d_state      = 0
0.00.049.541 I print_info: ssm_dt_rank      = 0
0.00.049.541 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.541 I print_info: model type       = 1.4B
0.00.049.541 I print_info: model params     = 1.41 B
0.00.049.541 I print_info: general.name     = 1.4B
0.00.049.543 I print_info: vocab type       = BPE
0.00.049.543 I print_info: n_vocab          = 50304
0.00.049.544 I print_info: n_merges         = 50009
0.00.049.544 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.545 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.545 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.546 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.546 I print_info: LF token         = 128 'Ä'
0.00.049.546 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.546 I print_info: max token length = 1024
0.00.051.494 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.494 I load_tensors: offloading output layer to GPU
0.00.051.495 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.505 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.506 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.777 I llama_init_from_model: n_seq_max     = 1
0.00.051.777 I llama_init_from_model: n_ctx         = 2048
0.00.051.777 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.778 I llama_init_from_model: n_batch       = 2048
0.00.051.778 I llama_init_from_model: n_ubatch      = 512
0.00.051.778 I llama_init_from_model: flash_attn    = 0
0.00.051.778 I llama_init_from_model: freq_base     = 10000.0
0.00.051.779 I llama_init_from_model: freq_scale    = 1
0.00.051.779 I ggml_metal_init: allocating
0.00.051.782 I ggml_metal_init: found device: Apple M4
0.00.051.784 I ggml_metal_init: picking default device: Apple M4
0.00.052.353 I ggml_metal_init: using embedded metal library
0.00.054.682 I ggml_metal_init: GPU name:   Apple M4
0.00.054.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.684 I ggml_metal_init: simdgroup reduction   = true
0.00.054.684 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.684 I ggml_metal_init: has bfloat            = true
0.00.054.684 I ggml_metal_init: use bfloat            = true
0.00.054.685 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.963 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.148 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.166 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.189 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.123 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.124 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.125 I llama_init_from_model: graph nodes  = 967
0.00.084.125 I llama_init_from_model: graph splits = 2
0.00.084.128 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.264 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.265 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.372 I main: llama threadpool init, n_threads = 4
0.00.622.422 I 
0.00.622.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.479 I 
0.00.622.717 I sampler seed: 1234
0.00.622.723 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.757 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.758 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.758 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.378.019 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.378.020 I llama_perf_context_print:        load time =     613.48 ms
0.01.378.021 I llama_perf_context_print: prompt eval time =      51.04 ms /     7 tokens (    7.29 ms per token,   137.16 tokens per second)
0.01.378.021 I llama_perf_context_print:        eval time =     701.24 ms /    63 runs   (   11.13 ms per token,    89.84 tokens per second)
0.01.378.022 I llama_perf_context_print:       total time =     755.65 ms /    70 tokens
0.01.378.286 I ggml_metal_free: deallocating

real	0m1.396s
user	0m0.109s
sys	0m0.133s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.829 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.381 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.388 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.388 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.388 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.389 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.389 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.390 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.390 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.396 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.397 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.141 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.142 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.143 I llama_model_loader: - type  f32:  194 tensors
0.00.026.143 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.143 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.144 I print_info: file format = GGUF V3 (latest)
0.00.026.144 I print_info: file type   = Q5_K - Medium
0.00.026.145 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.482 I load: special tokens cache size = 25
0.00.051.568 I load: token to piece cache size = 0.2984 MB
0.00.051.571 I print_info: arch             = gptneox
0.00.051.571 I print_info: vocab_only       = 0
0.00.051.571 I print_info: n_ctx_train      = 2048
0.00.051.572 I print_info: n_embd           = 2048
0.00.051.572 I print_info: n_layer          = 24
0.00.051.575 I print_info: n_head           = 16
0.00.051.576 I print_info: n_head_kv        = 16
0.00.051.576 I print_info: n_rot            = 32
0.00.051.576 I print_info: n_swa            = 0
0.00.051.576 I print_info: n_embd_head_k    = 128
0.00.051.576 I print_info: n_embd_head_v    = 128
0.00.051.577 I print_info: n_gqa            = 1
0.00.051.578 I print_info: n_embd_k_gqa     = 2048
0.00.051.579 I print_info: n_embd_v_gqa     = 2048
0.00.051.579 I print_info: f_norm_eps       = 1.0e-05
0.00.051.579 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.580 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.580 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.580 I print_info: f_logit_scale    = 0.0e+00
0.00.051.581 I print_info: n_ff             = 8192
0.00.051.581 I print_info: n_expert         = 0
0.00.051.581 I print_info: n_expert_used    = 0
0.00.051.581 I print_info: causal attn      = 1
0.00.051.582 I print_info: pooling type     = 0
0.00.051.582 I print_info: rope type        = 2
0.00.051.582 I print_info: rope scaling     = linear
0.00.051.582 I print_info: freq_base_train  = 10000.0
0.00.051.583 I print_info: freq_scale_train = 1
0.00.051.583 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.583 I print_info: rope_finetuned   = unknown
0.00.051.583 I print_info: ssm_d_conv       = 0
0.00.051.583 I print_info: ssm_d_inner      = 0
0.00.051.584 I print_info: ssm_d_state      = 0
0.00.051.584 I print_info: ssm_dt_rank      = 0
0.00.051.584 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.584 I print_info: model type       = 1.4B
0.00.051.585 I print_info: model params     = 1.41 B
0.00.051.585 I print_info: general.name     = 1.4B
0.00.051.585 I print_info: vocab type       = BPE
0.00.051.586 I print_info: n_vocab          = 50304
0.00.051.586 I print_info: n_merges         = 50009
0.00.051.586 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.587 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.587 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.587 I print_info: LF token         = 128 'Ä'
0.00.051.587 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.588 I print_info: max token length = 1024
0.00.053.628 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.628 I load_tensors: offloading output layer to GPU
0.00.053.628 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.639 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.640 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.929 I llama_init_from_model: n_seq_max     = 1
0.00.053.930 I llama_init_from_model: n_ctx         = 2048
0.00.053.930 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.930 I llama_init_from_model: n_batch       = 2048
0.00.053.930 I llama_init_from_model: n_ubatch      = 512
0.00.053.931 I llama_init_from_model: flash_attn    = 0
0.00.053.931 I llama_init_from_model: freq_base     = 10000.0
0.00.053.931 I llama_init_from_model: freq_scale    = 1
0.00.053.932 I ggml_metal_init: allocating
0.00.053.935 I ggml_metal_init: found device: Apple M4
0.00.053.937 I ggml_metal_init: picking default device: Apple M4
0.00.054.542 I ggml_metal_init: using embedded metal library
0.00.057.293 I ggml_metal_init: GPU name:   Apple M4
0.00.057.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.295 I ggml_metal_init: simdgroup reduction   = true
0.00.057.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.296 I ggml_metal_init: has bfloat            = true
0.00.057.296 I ggml_metal_init: use bfloat            = true
0.00.057.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.191 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.621 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.642 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.660 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.811 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.813 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.813 I llama_init_from_model: graph nodes  = 967
0.00.087.813 I llama_init_from_model: graph splits = 2
0.00.087.816 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.953 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.979 I main: llama threadpool init, n_threads = 4
0.00.728.033 I 
0.00.728.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.069 I 
0.00.728.303 I sampler seed: 1234
0.00.728.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.728.326 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.728.327 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.728.327 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.582.086 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.582.086 I llama_perf_context_print:        load time =     717.14 ms
0.01.582.087 I llama_perf_context_print: prompt eval time =      55.35 ms /     7 tokens (    7.91 ms per token,   126.47 tokens per second)
0.01.582.088 I llama_perf_context_print:        eval time =     795.52 ms /    63 runs   (   12.63 ms per token,    79.19 tokens per second)
0.01.582.088 I llama_perf_context_print:       total time =     854.11 ms /    70 tokens
0.01.582.323 I ggml_metal_free: deallocating

real	0m1.600s
user	0m0.110s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.279 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.946 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.953 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.953 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.954 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.954 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.955 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.958 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.958 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.959 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.959 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.961 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.961 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.729 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.730 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.731 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.731 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.731 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.732 I llama_model_loader: - type  f32:  194 tensors
0.00.024.733 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.733 I print_info: file format = GGUF V3 (latest)
0.00.024.734 I print_info: file type   = Q6_K
0.00.024.735 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.193 I load: special tokens cache size = 25
0.00.051.146 I load: token to piece cache size = 0.2984 MB
0.00.051.151 I print_info: arch             = gptneox
0.00.051.151 I print_info: vocab_only       = 0
0.00.051.152 I print_info: n_ctx_train      = 2048
0.00.051.152 I print_info: n_embd           = 2048
0.00.051.152 I print_info: n_layer          = 24
0.00.051.157 I print_info: n_head           = 16
0.00.051.157 I print_info: n_head_kv        = 16
0.00.051.158 I print_info: n_rot            = 32
0.00.051.158 I print_info: n_swa            = 0
0.00.051.158 I print_info: n_embd_head_k    = 128
0.00.051.158 I print_info: n_embd_head_v    = 128
0.00.051.159 I print_info: n_gqa            = 1
0.00.051.160 I print_info: n_embd_k_gqa     = 2048
0.00.051.161 I print_info: n_embd_v_gqa     = 2048
0.00.051.162 I print_info: f_norm_eps       = 1.0e-05
0.00.051.163 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.163 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.164 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.164 I print_info: f_logit_scale    = 0.0e+00
0.00.051.164 I print_info: n_ff             = 8192
0.00.051.164 I print_info: n_expert         = 0
0.00.051.165 I print_info: n_expert_used    = 0
0.00.051.165 I print_info: causal attn      = 1
0.00.051.165 I print_info: pooling type     = 0
0.00.051.165 I print_info: rope type        = 2
0.00.051.165 I print_info: rope scaling     = linear
0.00.051.167 I print_info: freq_base_train  = 10000.0
0.00.051.168 I print_info: freq_scale_train = 1
0.00.051.168 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.168 I print_info: rope_finetuned   = unknown
0.00.051.168 I print_info: ssm_d_conv       = 0
0.00.051.168 I print_info: ssm_d_inner      = 0
0.00.051.169 I print_info: ssm_d_state      = 0
0.00.051.169 I print_info: ssm_dt_rank      = 0
0.00.051.169 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.169 I print_info: model type       = 1.4B
0.00.051.169 I print_info: model params     = 1.41 B
0.00.051.169 I print_info: general.name     = 1.4B
0.00.051.171 I print_info: vocab type       = BPE
0.00.051.171 I print_info: n_vocab          = 50304
0.00.051.171 I print_info: n_merges         = 50009
0.00.051.172 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.172 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.172 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.172 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.172 I print_info: LF token         = 128 'Ä'
0.00.051.172 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.173 I print_info: max token length = 1024
0.00.052.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.951 I load_tensors: offloading output layer to GPU
0.00.052.951 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.957 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.958 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.353 I llama_init_from_model: n_seq_max     = 1
0.00.053.354 I llama_init_from_model: n_ctx         = 2048
0.00.053.354 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.354 I llama_init_from_model: n_batch       = 2048
0.00.053.354 I llama_init_from_model: n_ubatch      = 512
0.00.053.355 I llama_init_from_model: flash_attn    = 0
0.00.053.355 I llama_init_from_model: freq_base     = 10000.0
0.00.053.355 I llama_init_from_model: freq_scale    = 1
0.00.053.356 I ggml_metal_init: allocating
0.00.053.360 I ggml_metal_init: found device: Apple M4
0.00.053.362 I ggml_metal_init: picking default device: Apple M4
0.00.054.010 I ggml_metal_init: using embedded metal library
0.00.056.472 I ggml_metal_init: GPU name:   Apple M4
0.00.056.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.474 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.474 I ggml_metal_init: simdgroup reduction   = true
0.00.056.475 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.475 I ggml_metal_init: has bfloat            = true
0.00.056.475 I ggml_metal_init: use bfloat            = true
0.00.056.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.476 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.688 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.916 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.942 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.962 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.930 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.933 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.933 I llama_init_from_model: graph nodes  = 967
0.00.086.933 I llama_init_from_model: graph splits = 2
0.00.086.937 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.064 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.788 I main: llama threadpool init, n_threads = 4
0.00.736.827 I 
0.00.736.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.888 I 
0.00.737.117 I sampler seed: 1234
0.00.737.121 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.737.142 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.737.142 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.737.142 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.617.370 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.617.371 I llama_perf_context_print:        load time =     727.50 ms
0.01.617.372 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.75 tokens per second)
0.01.617.372 I llama_perf_context_print:        eval time =     822.77 ms /    63 runs   (   13.06 ms per token,    76.57 tokens per second)
0.01.617.373 I llama_perf_context_print:       total time =     880.59 ms /    70 tokens
0.01.617.597 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.695 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.562 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.281 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.291 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.291 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.292 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.293 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.299 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.143 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.010 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.010 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.011 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.012 I llama_model_loader: - type  f32:  194 tensors
0.00.056.013 I llama_model_loader: - type  f16:   98 tensors
0.00.056.014 I print_info: file format = GGUF V3 (latest)
0.00.056.015 I print_info: file type   = all F32 (guessed)
0.00.056.018 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.993 I load: special tokens cache size = 25
0.00.091.948 I load: token to piece cache size = 0.2984 MB
0.00.091.951 I print_info: arch             = gptneox
0.00.091.951 I print_info: vocab_only       = 0
0.00.091.952 I print_info: n_ctx_train      = 2048
0.00.091.952 I print_info: n_embd           = 2048
0.00.091.952 I print_info: n_layer          = 24
0.00.091.955 I print_info: n_head           = 16
0.00.091.956 I print_info: n_head_kv        = 16
0.00.091.956 I print_info: n_rot            = 32
0.00.091.956 I print_info: n_swa            = 0
0.00.091.957 I print_info: n_embd_head_k    = 128
0.00.091.957 I print_info: n_embd_head_v    = 128
0.00.091.957 I print_info: n_gqa            = 1
0.00.091.958 I print_info: n_embd_k_gqa     = 2048
0.00.091.959 I print_info: n_embd_v_gqa     = 2048
0.00.091.959 I print_info: f_norm_eps       = 1.0e-05
0.00.091.959 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.960 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.960 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.960 I print_info: f_logit_scale    = 0.0e+00
0.00.091.960 I print_info: n_ff             = 8192
0.00.091.961 I print_info: n_expert         = 0
0.00.091.961 I print_info: n_expert_used    = 0
0.00.091.961 I print_info: causal attn      = 1
0.00.091.961 I print_info: pooling type     = 0
0.00.091.961 I print_info: rope type        = 2
0.00.091.961 I print_info: rope scaling     = linear
0.00.091.962 I print_info: freq_base_train  = 10000.0
0.00.091.962 I print_info: freq_scale_train = 1
0.00.091.962 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.962 I print_info: rope_finetuned   = unknown
0.00.091.963 I print_info: ssm_d_conv       = 0
0.00.091.963 I print_info: ssm_d_inner      = 0
0.00.091.965 I print_info: ssm_d_state      = 0
0.00.091.965 I print_info: ssm_dt_rank      = 0
0.00.091.965 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.965 I print_info: model type       = 1.4B
0.00.091.966 I print_info: model params     = 1.41 B
0.00.091.966 I print_info: general.name     = 1.4B
0.00.091.966 I print_info: vocab type       = BPE
0.00.091.966 I print_info: n_vocab          = 50304
0.00.091.967 I print_info: n_merges         = 50009
0.00.091.967 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.967 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.967 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.971 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.971 I print_info: LF token         = 128 'Ä'
0.00.091.972 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.973 I print_info: max token length = 1024
0.00.094.473 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.474 I load_tensors: offloading output layer to GPU
0.00.094.474 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.485 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.486 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.757 I llama_init_from_model: n_seq_max     = 1
0.00.094.758 I llama_init_from_model: n_ctx         = 128
0.00.094.758 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.758 I llama_init_from_model: n_batch       = 128
0.00.094.758 I llama_init_from_model: n_ubatch      = 128
0.00.094.758 I llama_init_from_model: flash_attn    = 0
0.00.094.759 I llama_init_from_model: freq_base     = 10000.0
0.00.094.759 I llama_init_from_model: freq_scale    = 1
0.00.094.759 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.760 I ggml_metal_init: allocating
0.00.094.763 I ggml_metal_init: found device: Apple M4
0.00.094.765 I ggml_metal_init: picking default device: Apple M4
0.00.095.382 I ggml_metal_init: using embedded metal library
0.00.098.018 I ggml_metal_init: GPU name:   Apple M4
0.00.098.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.021 I ggml_metal_init: simdgroup reduction   = true
0.00.098.021 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.021 I ggml_metal_init: has bfloat            = true
0.00.098.022 I ggml_metal_init: use bfloat            = true
0.00.098.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.349 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.593 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.606 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.622 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.109.534 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.109.535 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.109.536 I llama_init_from_model: graph nodes  = 967
0.00.109.536 I llama_init_from_model: graph splits = 2
0.00.109.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.537 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.116.269 I 
0.01.116.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.116.341 I perplexity: tokenizing the input ..
0.01.129.896 I perplexity: tokenization took 13.55 ms
0.01.129.901 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.250.726 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.252.378 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.252.410 I llama_perf_context_print:        load time =    1091.69 ms
0.01.252.410 I llama_perf_context_print: prompt eval time =     119.84 ms /   128 tokens (    0.94 ms per token,  1068.12 tokens per second)
0.01.252.411 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.252.411 I llama_perf_context_print:       total time =     136.15 ms /   129 tokens
0.01.252.828 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.118s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.423 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.436 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.440 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.440 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.441 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.444 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.305 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.215 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.216 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.217 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.217 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.218 I llama_model_loader: - type  f32:  194 tensors
0.00.025.218 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.219 I print_info: file format = GGUF V3 (latest)
0.00.025.219 I print_info: file type   = Q8_0
0.00.025.220 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.983 I load: special tokens cache size = 25
0.00.049.913 I load: token to piece cache size = 0.2984 MB
0.00.049.918 I print_info: arch             = gptneox
0.00.049.918 I print_info: vocab_only       = 0
0.00.049.918 I print_info: n_ctx_train      = 2048
0.00.049.919 I print_info: n_embd           = 2048
0.00.049.919 I print_info: n_layer          = 24
0.00.049.923 I print_info: n_head           = 16
0.00.049.924 I print_info: n_head_kv        = 16
0.00.049.924 I print_info: n_rot            = 32
0.00.049.924 I print_info: n_swa            = 0
0.00.049.924 I print_info: n_embd_head_k    = 128
0.00.049.924 I print_info: n_embd_head_v    = 128
0.00.049.925 I print_info: n_gqa            = 1
0.00.049.926 I print_info: n_embd_k_gqa     = 2048
0.00.049.927 I print_info: n_embd_v_gqa     = 2048
0.00.049.927 I print_info: f_norm_eps       = 1.0e-05
0.00.049.928 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.928 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.928 I print_info: f_logit_scale    = 0.0e+00
0.00.049.929 I print_info: n_ff             = 8192
0.00.049.929 I print_info: n_expert         = 0
0.00.049.929 I print_info: n_expert_used    = 0
0.00.049.929 I print_info: causal attn      = 1
0.00.049.929 I print_info: pooling type     = 0
0.00.049.929 I print_info: rope type        = 2
0.00.049.929 I print_info: rope scaling     = linear
0.00.049.930 I print_info: freq_base_train  = 10000.0
0.00.049.930 I print_info: freq_scale_train = 1
0.00.049.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.930 I print_info: rope_finetuned   = unknown
0.00.049.931 I print_info: ssm_d_conv       = 0
0.00.049.931 I print_info: ssm_d_inner      = 0
0.00.049.931 I print_info: ssm_d_state      = 0
0.00.049.931 I print_info: ssm_dt_rank      = 0
0.00.049.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.931 I print_info: model type       = 1.4B
0.00.049.932 I print_info: model params     = 1.41 B
0.00.049.934 I print_info: general.name     = 1.4B
0.00.049.935 I print_info: vocab type       = BPE
0.00.049.935 I print_info: n_vocab          = 50304
0.00.049.935 I print_info: n_merges         = 50009
0.00.049.935 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.935 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.936 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.936 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.936 I print_info: LF token         = 128 'Ä'
0.00.049.937 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.937 I print_info: max token length = 1024
0.00.051.711 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.711 I load_tensors: offloading output layer to GPU
0.00.051.711 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.717 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.051.717 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.052.035 I llama_init_from_model: n_seq_max     = 1
0.00.052.036 I llama_init_from_model: n_ctx         = 128
0.00.052.036 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.036 I llama_init_from_model: n_batch       = 128
0.00.052.037 I llama_init_from_model: n_ubatch      = 128
0.00.052.037 I llama_init_from_model: flash_attn    = 0
0.00.052.037 I llama_init_from_model: freq_base     = 10000.0
0.00.052.037 I llama_init_from_model: freq_scale    = 1
0.00.052.038 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.038 I ggml_metal_init: allocating
0.00.052.041 I ggml_metal_init: found device: Apple M4
0.00.052.043 I ggml_metal_init: picking default device: Apple M4
0.00.053.383 I ggml_metal_init: using embedded metal library
0.00.058.056 I ggml_metal_init: GPU name:   Apple M4
0.00.058.058 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.059 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.059 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.059 I ggml_metal_init: simdgroup reduction   = true
0.00.058.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.060 I ggml_metal_init: has bfloat            = true
0.00.058.060 I ggml_metal_init: use bfloat            = true
0.00.058.060 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.061 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.147 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.469 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.485 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.502 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.383 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.384 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.384 I llama_init_from_model: graph nodes  = 967
0.00.069.384 I llama_init_from_model: graph splits = 2
0.00.069.386 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.908.200 I 
0.00.908.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.908.233 I perplexity: tokenizing the input ..
0.00.916.201 I perplexity: tokenization took 7.967 ms
0.00.916.205 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.040.565 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.041.727 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.041.757 I llama_perf_context_print:        load time =     898.77 ms
0.01.041.758 I llama_perf_context_print: prompt eval time =     124.13 ms /   128 tokens (    0.97 ms per token,  1031.14 tokens per second)
0.01.041.759 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.041.760 I llama_perf_context_print:       total time =     133.56 ms /   129 tokens
0.01.042.284 I ggml_metal_free: deallocating

real	0m1.058s
user	0m0.078s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.245 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.339 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.347 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.348 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.349 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.349 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.350 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.231 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.255 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.134 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.136 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.137 I llama_model_loader: - type  f32:  194 tensors
0.00.025.137 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.137 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.138 I print_info: file format = GGUF V3 (latest)
0.00.025.138 I print_info: file type   = Q4_0
0.00.025.139 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.220 I load: special tokens cache size = 25
0.00.050.445 I load: token to piece cache size = 0.2984 MB
0.00.050.448 I print_info: arch             = gptneox
0.00.050.448 I print_info: vocab_only       = 0
0.00.050.448 I print_info: n_ctx_train      = 2048
0.00.050.448 I print_info: n_embd           = 2048
0.00.050.449 I print_info: n_layer          = 24
0.00.050.451 I print_info: n_head           = 16
0.00.050.452 I print_info: n_head_kv        = 16
0.00.050.452 I print_info: n_rot            = 32
0.00.050.452 I print_info: n_swa            = 0
0.00.050.452 I print_info: n_embd_head_k    = 128
0.00.050.452 I print_info: n_embd_head_v    = 128
0.00.050.455 I print_info: n_gqa            = 1
0.00.050.455 I print_info: n_embd_k_gqa     = 2048
0.00.050.456 I print_info: n_embd_v_gqa     = 2048
0.00.050.457 I print_info: f_norm_eps       = 1.0e-05
0.00.050.457 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.462 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.462 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.462 I print_info: f_logit_scale    = 0.0e+00
0.00.050.463 I print_info: n_ff             = 8192
0.00.050.463 I print_info: n_expert         = 0
0.00.050.463 I print_info: n_expert_used    = 0
0.00.050.463 I print_info: causal attn      = 1
0.00.050.463 I print_info: pooling type     = 0
0.00.050.469 I print_info: rope type        = 2
0.00.050.471 I print_info: rope scaling     = linear
0.00.050.472 I print_info: freq_base_train  = 10000.0
0.00.050.472 I print_info: freq_scale_train = 1
0.00.050.473 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.473 I print_info: rope_finetuned   = unknown
0.00.050.473 I print_info: ssm_d_conv       = 0
0.00.050.473 I print_info: ssm_d_inner      = 0
0.00.050.473 I print_info: ssm_d_state      = 0
0.00.050.474 I print_info: ssm_dt_rank      = 0
0.00.050.474 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.474 I print_info: model type       = 1.4B
0.00.050.475 I print_info: model params     = 1.41 B
0.00.050.475 I print_info: general.name     = 1.4B
0.00.050.475 I print_info: vocab type       = BPE
0.00.050.475 I print_info: n_vocab          = 50304
0.00.050.476 I print_info: n_merges         = 50009
0.00.050.476 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.476 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.476 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.476 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.477 I print_info: LF token         = 128 'Ä'
0.00.050.478 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.478 I print_info: max token length = 1024
0.00.052.384 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.384 I load_tensors: offloading output layer to GPU
0.00.052.385 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.395 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.396 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.732 I llama_init_from_model: n_seq_max     = 1
0.00.052.733 I llama_init_from_model: n_ctx         = 128
0.00.052.733 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.733 I llama_init_from_model: n_batch       = 128
0.00.052.733 I llama_init_from_model: n_ubatch      = 128
0.00.052.734 I llama_init_from_model: flash_attn    = 0
0.00.052.734 I llama_init_from_model: freq_base     = 10000.0
0.00.052.734 I llama_init_from_model: freq_scale    = 1
0.00.052.735 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.735 I ggml_metal_init: allocating
0.00.052.738 I ggml_metal_init: found device: Apple M4
0.00.052.740 I ggml_metal_init: picking default device: Apple M4
0.00.053.295 I ggml_metal_init: using embedded metal library
0.00.055.643 I ggml_metal_init: GPU name:   Apple M4
0.00.055.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.645 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.646 I ggml_metal_init: simdgroup reduction   = true
0.00.055.646 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.646 I ggml_metal_init: has bfloat            = true
0.00.055.646 I ggml_metal_init: use bfloat            = true
0.00.055.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.647 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.247 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.494 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.508 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.523 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.428 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.429 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.429 I llama_init_from_model: graph nodes  = 967
0.00.067.430 I llama_init_from_model: graph splits = 2
0.00.067.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.867 I 
0.00.626.920 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.926 I perplexity: tokenizing the input ..
0.00.634.694 I perplexity: tokenization took 7.767 ms
0.00.634.698 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.467 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.758.623 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.758.651 I llama_perf_context_print:        load time =     617.62 ms
0.00.758.652 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.53 tokens per second)
0.00.758.653 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.654 I llama_perf_context_print:       total time =     131.79 ms /   129 tokens
0.00.759.076 I ggml_metal_free: deallocating

real	0m0.774s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.797 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.799 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.870 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.920 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.921 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.922 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.923 I llama_model_loader: - type  f32:  194 tensors
0.00.024.923 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.924 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.924 I print_info: file format = GGUF V3 (latest)
0.00.024.925 I print_info: file type   = Q4_1
0.00.024.925 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.200 I load: special tokens cache size = 25
0.00.050.156 I load: token to piece cache size = 0.2984 MB
0.00.050.160 I print_info: arch             = gptneox
0.00.050.160 I print_info: vocab_only       = 0
0.00.050.160 I print_info: n_ctx_train      = 2048
0.00.050.160 I print_info: n_embd           = 2048
0.00.050.160 I print_info: n_layer          = 24
0.00.050.163 I print_info: n_head           = 16
0.00.050.164 I print_info: n_head_kv        = 16
0.00.050.164 I print_info: n_rot            = 32
0.00.050.164 I print_info: n_swa            = 0
0.00.050.165 I print_info: n_embd_head_k    = 128
0.00.050.166 I print_info: n_embd_head_v    = 128
0.00.050.167 I print_info: n_gqa            = 1
0.00.050.167 I print_info: n_embd_k_gqa     = 2048
0.00.050.168 I print_info: n_embd_v_gqa     = 2048
0.00.050.169 I print_info: f_norm_eps       = 1.0e-05
0.00.050.170 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.170 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.170 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.170 I print_info: f_logit_scale    = 0.0e+00
0.00.050.171 I print_info: n_ff             = 8192
0.00.050.171 I print_info: n_expert         = 0
0.00.050.171 I print_info: n_expert_used    = 0
0.00.050.171 I print_info: causal attn      = 1
0.00.050.171 I print_info: pooling type     = 0
0.00.050.171 I print_info: rope type        = 2
0.00.050.173 I print_info: rope scaling     = linear
0.00.050.174 I print_info: freq_base_train  = 10000.0
0.00.050.174 I print_info: freq_scale_train = 1
0.00.050.174 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.174 I print_info: rope_finetuned   = unknown
0.00.050.174 I print_info: ssm_d_conv       = 0
0.00.050.175 I print_info: ssm_d_inner      = 0
0.00.050.175 I print_info: ssm_d_state      = 0
0.00.050.175 I print_info: ssm_dt_rank      = 0
0.00.050.175 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.175 I print_info: model type       = 1.4B
0.00.050.176 I print_info: model params     = 1.41 B
0.00.050.176 I print_info: general.name     = 1.4B
0.00.050.176 I print_info: vocab type       = BPE
0.00.050.176 I print_info: n_vocab          = 50304
0.00.050.177 I print_info: n_merges         = 50009
0.00.050.177 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.177 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.177 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.177 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.182 I print_info: LF token         = 128 'Ä'
0.00.050.182 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.182 I print_info: max token length = 1024
0.00.052.140 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.140 I load_tensors: offloading output layer to GPU
0.00.052.140 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.150 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.152 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.435 I llama_init_from_model: n_seq_max     = 1
0.00.052.436 I llama_init_from_model: n_ctx         = 128
0.00.052.436 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.436 I llama_init_from_model: n_batch       = 128
0.00.052.436 I llama_init_from_model: n_ubatch      = 128
0.00.052.436 I llama_init_from_model: flash_attn    = 0
0.00.052.437 I llama_init_from_model: freq_base     = 10000.0
0.00.052.437 I llama_init_from_model: freq_scale    = 1
0.00.052.437 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.438 I ggml_metal_init: allocating
0.00.052.441 I ggml_metal_init: found device: Apple M4
0.00.052.442 I ggml_metal_init: picking default device: Apple M4
0.00.053.028 I ggml_metal_init: using embedded metal library
0.00.055.350 I ggml_metal_init: GPU name:   Apple M4
0.00.055.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.352 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.352 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.352 I ggml_metal_init: simdgroup reduction   = true
0.00.055.352 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.352 I ggml_metal_init: has bfloat            = true
0.00.055.353 I ggml_metal_init: use bfloat            = true
0.00.055.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.354 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.098 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.360 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.374 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.390 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.331 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.332 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.333 I llama_init_from_model: graph nodes  = 967
0.00.067.333 I llama_init_from_model: graph splits = 2
0.00.067.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.722.104 I 
0.00.722.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.722.142 I perplexity: tokenizing the input ..
0.00.730.223 I perplexity: tokenization took 8.079 ms
0.00.730.227 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.244 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.854.510 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.854.544 I llama_perf_context_print:        load time =     713.19 ms
0.00.854.545 I llama_perf_context_print: prompt eval time =     122.78 ms /   128 tokens (    0.96 ms per token,  1042.53 tokens per second)
0.00.854.546 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.546 I llama_perf_context_print:       total time =     132.44 ms /   129 tokens
0.00.855.099 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.078s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.118 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.899 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.903 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.905 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.906 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.906 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.906 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.908 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.908 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.908 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.910 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.911 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.912 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.913 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.913 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.647 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.411 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.411 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.412 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.413 I llama_model_loader: - type  f32:  194 tensors
0.00.026.413 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.413 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.414 I print_info: file format = GGUF V3 (latest)
0.00.026.414 I print_info: file type   = Q5_0
0.00.026.415 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.656 I load: special tokens cache size = 25
0.00.050.633 I load: token to piece cache size = 0.2984 MB
0.00.050.636 I print_info: arch             = gptneox
0.00.050.636 I print_info: vocab_only       = 0
0.00.050.636 I print_info: n_ctx_train      = 2048
0.00.050.636 I print_info: n_embd           = 2048
0.00.050.637 I print_info: n_layer          = 24
0.00.050.639 I print_info: n_head           = 16
0.00.050.640 I print_info: n_head_kv        = 16
0.00.050.640 I print_info: n_rot            = 32
0.00.050.641 I print_info: n_swa            = 0
0.00.050.642 I print_info: n_embd_head_k    = 128
0.00.050.642 I print_info: n_embd_head_v    = 128
0.00.050.643 I print_info: n_gqa            = 1
0.00.050.644 I print_info: n_embd_k_gqa     = 2048
0.00.050.644 I print_info: n_embd_v_gqa     = 2048
0.00.050.645 I print_info: f_norm_eps       = 1.0e-05
0.00.050.645 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.646 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.647 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.647 I print_info: f_logit_scale    = 0.0e+00
0.00.050.647 I print_info: n_ff             = 8192
0.00.050.648 I print_info: n_expert         = 0
0.00.050.648 I print_info: n_expert_used    = 0
0.00.050.648 I print_info: causal attn      = 1
0.00.050.648 I print_info: pooling type     = 0
0.00.050.648 I print_info: rope type        = 2
0.00.050.648 I print_info: rope scaling     = linear
0.00.050.649 I print_info: freq_base_train  = 10000.0
0.00.050.649 I print_info: freq_scale_train = 1
0.00.050.649 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.650 I print_info: rope_finetuned   = unknown
0.00.050.650 I print_info: ssm_d_conv       = 0
0.00.050.650 I print_info: ssm_d_inner      = 0
0.00.050.650 I print_info: ssm_d_state      = 0
0.00.050.650 I print_info: ssm_dt_rank      = 0
0.00.050.650 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.652 I print_info: model type       = 1.4B
0.00.050.652 I print_info: model params     = 1.41 B
0.00.050.653 I print_info: general.name     = 1.4B
0.00.050.653 I print_info: vocab type       = BPE
0.00.050.653 I print_info: n_vocab          = 50304
0.00.050.653 I print_info: n_merges         = 50009
0.00.050.654 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.654 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.654 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.654 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.654 I print_info: LF token         = 128 'Ä'
0.00.050.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.655 I print_info: max token length = 1024
0.00.052.455 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.456 I load_tensors: offloading output layer to GPU
0.00.052.456 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.461 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.462 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.922 I llama_init_from_model: n_seq_max     = 1
0.00.052.923 I llama_init_from_model: n_ctx         = 128
0.00.052.923 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.923 I llama_init_from_model: n_batch       = 128
0.00.052.923 I llama_init_from_model: n_ubatch      = 128
0.00.052.923 I llama_init_from_model: flash_attn    = 0
0.00.052.924 I llama_init_from_model: freq_base     = 10000.0
0.00.052.924 I llama_init_from_model: freq_scale    = 1
0.00.052.924 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.925 I ggml_metal_init: allocating
0.00.052.927 I ggml_metal_init: found device: Apple M4
0.00.052.929 I ggml_metal_init: picking default device: Apple M4
0.00.053.482 I ggml_metal_init: using embedded metal library
0.00.056.084 I ggml_metal_init: GPU name:   Apple M4
0.00.056.086 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.086 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.086 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.087 I ggml_metal_init: simdgroup reduction   = true
0.00.056.087 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.087 I ggml_metal_init: has bfloat            = true
0.00.056.087 I ggml_metal_init: use bfloat            = true
0.00.056.087 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.088 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.637 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.876 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.889 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.905 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.842 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.843 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.843 I llama_init_from_model: graph nodes  = 967
0.00.066.844 I llama_init_from_model: graph splits = 2
0.00.066.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.636 I 
0.00.792.693 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.721 I perplexity: tokenizing the input ..
0.00.800.444 I perplexity: tokenization took 7.721 ms
0.00.800.447 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.935.408 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.936.561 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.936.587 I llama_perf_context_print:        load time =     781.51 ms
0.00.936.590 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   950.01 tokens per second)
0.00.936.590 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.936.591 I llama_perf_context_print:       total time =     143.95 ms /   129 tokens
0.00.937.104 I ggml_metal_free: deallocating

real	0m0.952s
user	0m0.075s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.803 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.807 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.811 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.812 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.814 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.815 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.817 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.820 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.821 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.821 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.598 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.349 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.350 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.351 I llama_model_loader: - type  f32:  194 tensors
0.00.024.351 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.352 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.352 I print_info: file format = GGUF V3 (latest)
0.00.024.353 I print_info: file type   = Q5_1
0.00.024.353 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.313 I load: special tokens cache size = 25
0.00.049.260 I load: token to piece cache size = 0.2984 MB
0.00.049.263 I print_info: arch             = gptneox
0.00.049.263 I print_info: vocab_only       = 0
0.00.049.263 I print_info: n_ctx_train      = 2048
0.00.049.263 I print_info: n_embd           = 2048
0.00.049.264 I print_info: n_layer          = 24
0.00.049.266 I print_info: n_head           = 16
0.00.049.267 I print_info: n_head_kv        = 16
0.00.049.267 I print_info: n_rot            = 32
0.00.049.268 I print_info: n_swa            = 0
0.00.049.268 I print_info: n_embd_head_k    = 128
0.00.049.268 I print_info: n_embd_head_v    = 128
0.00.049.269 I print_info: n_gqa            = 1
0.00.049.270 I print_info: n_embd_k_gqa     = 2048
0.00.049.270 I print_info: n_embd_v_gqa     = 2048
0.00.049.271 I print_info: f_norm_eps       = 1.0e-05
0.00.049.271 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.271 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.271 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.272 I print_info: f_logit_scale    = 0.0e+00
0.00.049.272 I print_info: n_ff             = 8192
0.00.049.272 I print_info: n_expert         = 0
0.00.049.273 I print_info: n_expert_used    = 0
0.00.049.273 I print_info: causal attn      = 1
0.00.049.275 I print_info: pooling type     = 0
0.00.049.275 I print_info: rope type        = 2
0.00.049.275 I print_info: rope scaling     = linear
0.00.049.277 I print_info: freq_base_train  = 10000.0
0.00.049.279 I print_info: freq_scale_train = 1
0.00.049.279 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.279 I print_info: rope_finetuned   = unknown
0.00.049.279 I print_info: ssm_d_conv       = 0
0.00.049.279 I print_info: ssm_d_inner      = 0
0.00.049.279 I print_info: ssm_d_state      = 0
0.00.049.280 I print_info: ssm_dt_rank      = 0
0.00.049.280 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.280 I print_info: model type       = 1.4B
0.00.049.281 I print_info: model params     = 1.41 B
0.00.049.282 I print_info: general.name     = 1.4B
0.00.049.283 I print_info: vocab type       = BPE
0.00.049.283 I print_info: n_vocab          = 50304
0.00.049.283 I print_info: n_merges         = 50009
0.00.049.283 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.287 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.289 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.289 I print_info: LF token         = 128 'Ä'
0.00.049.290 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.290 I print_info: max token length = 1024
0.00.051.224 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.224 I load_tensors: offloading output layer to GPU
0.00.051.224 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.235 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.236 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.509 I llama_init_from_model: n_seq_max     = 1
0.00.051.509 I llama_init_from_model: n_ctx         = 128
0.00.051.510 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.510 I llama_init_from_model: n_batch       = 128
0.00.051.510 I llama_init_from_model: n_ubatch      = 128
0.00.051.510 I llama_init_from_model: flash_attn    = 0
0.00.051.510 I llama_init_from_model: freq_base     = 10000.0
0.00.051.511 I llama_init_from_model: freq_scale    = 1
0.00.051.511 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.511 I ggml_metal_init: allocating
0.00.051.514 I ggml_metal_init: found device: Apple M4
0.00.051.516 I ggml_metal_init: picking default device: Apple M4
0.00.052.117 I ggml_metal_init: using embedded metal library
0.00.054.454 I ggml_metal_init: GPU name:   Apple M4
0.00.054.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.456 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.457 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.457 I ggml_metal_init: simdgroup reduction   = true
0.00.054.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.457 I ggml_metal_init: has bfloat            = true
0.00.054.457 I ggml_metal_init: use bfloat            = true
0.00.054.458 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.458 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.996 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.280 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.294 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.309 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.228 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.229 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.230 I llama_init_from_model: graph nodes  = 967
0.00.066.230 I llama_init_from_model: graph splits = 2
0.00.066.231 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.231 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.792 I 
0.00.655.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.830 I perplexity: tokenizing the input ..
0.00.663.944 I perplexity: tokenization took 8.112 ms
0.00.663.948 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.156 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.800.425 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.800.446 I llama_perf_context_print:        load time =     646.95 ms
0.00.800.447 I llama_perf_context_print: prompt eval time =     134.98 ms /   128 tokens (    1.05 ms per token,   948.32 tokens per second)
0.00.800.448 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.448 I llama_perf_context_print:       total time =     144.65 ms /   129 tokens
0.00.800.783 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.076s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.482 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.493 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.497 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.497 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.498 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.498 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.499 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.500 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.286 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.005 I llama_model_loader: - type  f32:  194 tensors
0.00.026.005 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.005 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.006 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.006 I print_info: file format = GGUF V3 (latest)
0.00.026.006 I print_info: file type   = Q2_K - Medium
0.00.026.007 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.451 I load: special tokens cache size = 25
0.00.050.381 I load: token to piece cache size = 0.2984 MB
0.00.050.384 I print_info: arch             = gptneox
0.00.050.384 I print_info: vocab_only       = 0
0.00.050.384 I print_info: n_ctx_train      = 2048
0.00.050.384 I print_info: n_embd           = 2048
0.00.050.385 I print_info: n_layer          = 24
0.00.050.388 I print_info: n_head           = 16
0.00.050.388 I print_info: n_head_kv        = 16
0.00.050.389 I print_info: n_rot            = 32
0.00.050.389 I print_info: n_swa            = 0
0.00.050.389 I print_info: n_embd_head_k    = 128
0.00.050.389 I print_info: n_embd_head_v    = 128
0.00.050.390 I print_info: n_gqa            = 1
0.00.050.391 I print_info: n_embd_k_gqa     = 2048
0.00.050.391 I print_info: n_embd_v_gqa     = 2048
0.00.050.392 I print_info: f_norm_eps       = 1.0e-05
0.00.050.392 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.392 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.392 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.393 I print_info: f_logit_scale    = 0.0e+00
0.00.050.395 I print_info: n_ff             = 8192
0.00.050.396 I print_info: n_expert         = 0
0.00.050.396 I print_info: n_expert_used    = 0
0.00.050.396 I print_info: causal attn      = 1
0.00.050.396 I print_info: pooling type     = 0
0.00.050.396 I print_info: rope type        = 2
0.00.050.396 I print_info: rope scaling     = linear
0.00.050.397 I print_info: freq_base_train  = 10000.0
0.00.050.397 I print_info: freq_scale_train = 1
0.00.050.397 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.398 I print_info: rope_finetuned   = unknown
0.00.050.399 I print_info: ssm_d_conv       = 0
0.00.050.399 I print_info: ssm_d_inner      = 0
0.00.050.399 I print_info: ssm_d_state      = 0
0.00.050.400 I print_info: ssm_dt_rank      = 0
0.00.050.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.400 I print_info: model type       = 1.4B
0.00.050.400 I print_info: model params     = 1.41 B
0.00.050.400 I print_info: general.name     = 1.4B
0.00.050.401 I print_info: vocab type       = BPE
0.00.050.401 I print_info: n_vocab          = 50304
0.00.050.401 I print_info: n_merges         = 50009
0.00.050.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.402 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.403 I print_info: LF token         = 128 'Ä'
0.00.050.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.405 I print_info: max token length = 1024
0.00.052.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.265 I load_tensors: offloading output layer to GPU
0.00.052.265 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.276 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.277 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.626 I llama_init_from_model: n_seq_max     = 1
0.00.052.627 I llama_init_from_model: n_ctx         = 128
0.00.052.627 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.627 I llama_init_from_model: n_batch       = 128
0.00.052.627 I llama_init_from_model: n_ubatch      = 128
0.00.052.627 I llama_init_from_model: flash_attn    = 0
0.00.052.628 I llama_init_from_model: freq_base     = 10000.0
0.00.052.628 I llama_init_from_model: freq_scale    = 1
0.00.052.628 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.629 I ggml_metal_init: allocating
0.00.052.631 I ggml_metal_init: found device: Apple M4
0.00.052.633 I ggml_metal_init: picking default device: Apple M4
0.00.053.194 I ggml_metal_init: using embedded metal library
0.00.055.549 I ggml_metal_init: GPU name:   Apple M4
0.00.055.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.551 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.552 I ggml_metal_init: simdgroup reduction   = true
0.00.055.552 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.552 I ggml_metal_init: has bfloat            = true
0.00.055.552 I ggml_metal_init: use bfloat            = true
0.00.055.553 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.553 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.808 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.088 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.103 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.120 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.043 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.045 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.045 I llama_init_from_model: graph nodes  = 967
0.00.066.045 I llama_init_from_model: graph splits = 2
0.00.066.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.830 I 
0.00.416.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.869 I perplexity: tokenizing the input ..
0.00.424.908 I perplexity: tokenization took 8.037 ms
0.00.424.911 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.557.564 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.558.724 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.558.748 I llama_perf_context_print:        load time =     405.95 ms
0.00.558.749 I llama_perf_context_print: prompt eval time =     132.43 ms /   128 tokens (    1.03 ms per token,   966.56 tokens per second)
0.00.558.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.558.751 I llama_perf_context_print:       total time =     141.92 ms /   129 tokens
0.00.559.290 I ggml_metal_free: deallocating

real	0m0.575s
user	0m0.075s
sys	0m0.074s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.741 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.677 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.679 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.682 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.685 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.429 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.438 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.169 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.169 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.169 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.170 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.170 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.170 I llama_model_loader: - type  f32:  194 tensors
0.00.024.171 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.171 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.171 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.171 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.172 I print_info: file format = GGUF V3 (latest)
0.00.024.173 I print_info: file type   = Q3_K - Medium
0.00.024.173 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.483 I load: special tokens cache size = 25
0.00.048.416 I load: token to piece cache size = 0.2984 MB
0.00.048.419 I print_info: arch             = gptneox
0.00.048.419 I print_info: vocab_only       = 0
0.00.048.419 I print_info: n_ctx_train      = 2048
0.00.048.419 I print_info: n_embd           = 2048
0.00.048.420 I print_info: n_layer          = 24
0.00.048.422 I print_info: n_head           = 16
0.00.048.423 I print_info: n_head_kv        = 16
0.00.048.424 I print_info: n_rot            = 32
0.00.048.424 I print_info: n_swa            = 0
0.00.048.424 I print_info: n_embd_head_k    = 128
0.00.048.424 I print_info: n_embd_head_v    = 128
0.00.048.425 I print_info: n_gqa            = 1
0.00.048.426 I print_info: n_embd_k_gqa     = 2048
0.00.048.426 I print_info: n_embd_v_gqa     = 2048
0.00.048.427 I print_info: f_norm_eps       = 1.0e-05
0.00.048.427 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.428 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.429 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.429 I print_info: f_logit_scale    = 0.0e+00
0.00.048.430 I print_info: n_ff             = 8192
0.00.048.430 I print_info: n_expert         = 0
0.00.048.430 I print_info: n_expert_used    = 0
0.00.048.431 I print_info: causal attn      = 1
0.00.048.431 I print_info: pooling type     = 0
0.00.048.431 I print_info: rope type        = 2
0.00.048.431 I print_info: rope scaling     = linear
0.00.048.432 I print_info: freq_base_train  = 10000.0
0.00.048.432 I print_info: freq_scale_train = 1
0.00.048.432 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.432 I print_info: rope_finetuned   = unknown
0.00.048.432 I print_info: ssm_d_conv       = 0
0.00.048.433 I print_info: ssm_d_inner      = 0
0.00.048.433 I print_info: ssm_d_state      = 0
0.00.048.435 I print_info: ssm_dt_rank      = 0
0.00.048.435 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.435 I print_info: model type       = 1.4B
0.00.048.436 I print_info: model params     = 1.41 B
0.00.048.436 I print_info: general.name     = 1.4B
0.00.048.436 I print_info: vocab type       = BPE
0.00.048.436 I print_info: n_vocab          = 50304
0.00.048.437 I print_info: n_merges         = 50009
0.00.048.437 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.437 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.437 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.437 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.441 I print_info: LF token         = 128 'Ä'
0.00.048.442 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.442 I print_info: max token length = 1024
0.00.050.347 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.347 I load_tensors: offloading output layer to GPU
0.00.050.348 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.358 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.360 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.624 I llama_init_from_model: n_seq_max     = 1
0.00.050.625 I llama_init_from_model: n_ctx         = 128
0.00.050.625 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.625 I llama_init_from_model: n_batch       = 128
0.00.050.625 I llama_init_from_model: n_ubatch      = 128
0.00.050.626 I llama_init_from_model: flash_attn    = 0
0.00.050.626 I llama_init_from_model: freq_base     = 10000.0
0.00.050.626 I llama_init_from_model: freq_scale    = 1
0.00.050.626 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.627 I ggml_metal_init: allocating
0.00.050.629 I ggml_metal_init: found device: Apple M4
0.00.050.631 I ggml_metal_init: picking default device: Apple M4
0.00.051.203 I ggml_metal_init: using embedded metal library
0.00.053.533 I ggml_metal_init: GPU name:   Apple M4
0.00.053.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.535 I ggml_metal_init: simdgroup reduction   = true
0.00.053.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.536 I ggml_metal_init: has bfloat            = true
0.00.053.536 I ggml_metal_init: use bfloat            = true
0.00.053.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.867 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.137 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.150 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.177 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.121 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.122 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.122 I llama_init_from_model: graph nodes  = 967
0.00.065.123 I llama_init_from_model: graph splits = 2
0.00.065.124 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.124 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.463.596 I 
0.00.463.638 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.463.649 I perplexity: tokenizing the input ..
0.00.471.858 I perplexity: tokenization took 8.206 ms
0.00.471.862 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.604.163 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.605.393 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.605.426 I llama_perf_context_print:        load time =     454.85 ms
0.00.605.427 I llama_perf_context_print: prompt eval time =     132.07 ms /   128 tokens (    1.03 ms per token,   969.16 tokens per second)
0.00.605.427 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.605.428 I llama_perf_context_print:       total time =     141.83 ms /   129 tokens
0.00.605.996 I ggml_metal_free: deallocating

real	0m0.619s
user	0m0.076s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.262 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.006 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.011 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.013 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.014 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.016 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.017 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.017 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.017 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.894 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.764 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.764 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.764 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.764 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.765 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.765 I llama_model_loader: - type  f32:  194 tensors
0.00.024.765 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.765 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.766 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.766 I print_info: file format = GGUF V3 (latest)
0.00.024.767 I print_info: file type   = Q4_K - Medium
0.00.024.771 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.310 I load: special tokens cache size = 25
0.00.048.957 I load: token to piece cache size = 0.2984 MB
0.00.048.960 I print_info: arch             = gptneox
0.00.048.960 I print_info: vocab_only       = 0
0.00.048.961 I print_info: n_ctx_train      = 2048
0.00.048.961 I print_info: n_embd           = 2048
0.00.048.961 I print_info: n_layer          = 24
0.00.048.964 I print_info: n_head           = 16
0.00.048.965 I print_info: n_head_kv        = 16
0.00.048.966 I print_info: n_rot            = 32
0.00.048.966 I print_info: n_swa            = 0
0.00.048.966 I print_info: n_embd_head_k    = 128
0.00.048.966 I print_info: n_embd_head_v    = 128
0.00.048.967 I print_info: n_gqa            = 1
0.00.048.968 I print_info: n_embd_k_gqa     = 2048
0.00.048.969 I print_info: n_embd_v_gqa     = 2048
0.00.048.969 I print_info: f_norm_eps       = 1.0e-05
0.00.048.970 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.970 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.970 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.972 I print_info: f_logit_scale    = 0.0e+00
0.00.048.972 I print_info: n_ff             = 8192
0.00.048.973 I print_info: n_expert         = 0
0.00.048.973 I print_info: n_expert_used    = 0
0.00.048.973 I print_info: causal attn      = 1
0.00.048.974 I print_info: pooling type     = 0
0.00.048.975 I print_info: rope type        = 2
0.00.048.975 I print_info: rope scaling     = linear
0.00.048.975 I print_info: freq_base_train  = 10000.0
0.00.048.975 I print_info: freq_scale_train = 1
0.00.048.976 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.976 I print_info: rope_finetuned   = unknown
0.00.048.976 I print_info: ssm_d_conv       = 0
0.00.048.976 I print_info: ssm_d_inner      = 0
0.00.048.976 I print_info: ssm_d_state      = 0
0.00.048.976 I print_info: ssm_dt_rank      = 0
0.00.048.976 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.977 I print_info: model type       = 1.4B
0.00.048.977 I print_info: model params     = 1.41 B
0.00.048.977 I print_info: general.name     = 1.4B
0.00.048.981 I print_info: vocab type       = BPE
0.00.048.981 I print_info: n_vocab          = 50304
0.00.048.982 I print_info: n_merges         = 50009
0.00.048.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.982 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.982 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.982 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.983 I print_info: LF token         = 128 'Ä'
0.00.048.983 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.983 I print_info: max token length = 1024
0.00.050.919 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.919 I load_tensors: offloading output layer to GPU
0.00.050.920 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.930 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.931 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.216 I llama_init_from_model: n_seq_max     = 1
0.00.051.217 I llama_init_from_model: n_ctx         = 128
0.00.051.217 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.217 I llama_init_from_model: n_batch       = 128
0.00.051.217 I llama_init_from_model: n_ubatch      = 128
0.00.051.217 I llama_init_from_model: flash_attn    = 0
0.00.051.218 I llama_init_from_model: freq_base     = 10000.0
0.00.051.218 I llama_init_from_model: freq_scale    = 1
0.00.051.218 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.219 I ggml_metal_init: allocating
0.00.051.221 I ggml_metal_init: found device: Apple M4
0.00.051.223 I ggml_metal_init: picking default device: Apple M4
0.00.051.777 I ggml_metal_init: using embedded metal library
0.00.054.123 I ggml_metal_init: GPU name:   Apple M4
0.00.054.125 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.126 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.126 I ggml_metal_init: simdgroup reduction   = true
0.00.054.126 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.126 I ggml_metal_init: has bfloat            = true
0.00.054.126 I ggml_metal_init: use bfloat            = true
0.00.054.127 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.797 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.010 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.025 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.040 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.003 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.004 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.004 I llama_init_from_model: graph nodes  = 967
0.00.065.004 I llama_init_from_model: graph splits = 2
0.00.065.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.074 I 
0.00.641.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.150 I perplexity: tokenizing the input ..
0.00.648.932 I perplexity: tokenization took 7.779 ms
0.00.648.935 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.884 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.784.032 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.784.058 I llama_perf_context_print:        load time =     631.80 ms
0.00.784.059 I llama_perf_context_print: prompt eval time =     133.72 ms /   128 tokens (    1.04 ms per token,   957.20 tokens per second)
0.00.784.060 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.060 I llama_perf_context_print:       total time =     142.99 ms /   129 tokens
0.00.784.399 I ggml_metal_free: deallocating

real	0m0.798s
user	0m0.075s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.170 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.143 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.151 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.153 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.154 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.154 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.154 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.155 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.158 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.158 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.622 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.623 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.624 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.625 I llama_model_loader: - type  f32:  194 tensors
0.00.025.625 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.625 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.626 I print_info: file format = GGUF V3 (latest)
0.00.025.626 I print_info: file type   = Q5_K - Medium
0.00.025.626 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.059 I load: special tokens cache size = 25
0.00.050.009 I load: token to piece cache size = 0.2984 MB
0.00.050.012 I print_info: arch             = gptneox
0.00.050.012 I print_info: vocab_only       = 0
0.00.050.013 I print_info: n_ctx_train      = 2048
0.00.050.013 I print_info: n_embd           = 2048
0.00.050.013 I print_info: n_layer          = 24
0.00.050.015 I print_info: n_head           = 16
0.00.050.018 I print_info: n_head_kv        = 16
0.00.050.019 I print_info: n_rot            = 32
0.00.050.019 I print_info: n_swa            = 0
0.00.050.019 I print_info: n_embd_head_k    = 128
0.00.050.019 I print_info: n_embd_head_v    = 128
0.00.050.020 I print_info: n_gqa            = 1
0.00.050.020 I print_info: n_embd_k_gqa     = 2048
0.00.050.021 I print_info: n_embd_v_gqa     = 2048
0.00.050.022 I print_info: f_norm_eps       = 1.0e-05
0.00.050.022 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.022 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.022 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.022 I print_info: f_logit_scale    = 0.0e+00
0.00.050.023 I print_info: n_ff             = 8192
0.00.050.023 I print_info: n_expert         = 0
0.00.050.023 I print_info: n_expert_used    = 0
0.00.050.024 I print_info: causal attn      = 1
0.00.050.026 I print_info: pooling type     = 0
0.00.050.026 I print_info: rope type        = 2
0.00.050.026 I print_info: rope scaling     = linear
0.00.050.027 I print_info: freq_base_train  = 10000.0
0.00.050.027 I print_info: freq_scale_train = 1
0.00.050.027 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.027 I print_info: rope_finetuned   = unknown
0.00.050.027 I print_info: ssm_d_conv       = 0
0.00.050.028 I print_info: ssm_d_inner      = 0
0.00.050.028 I print_info: ssm_d_state      = 0
0.00.050.028 I print_info: ssm_dt_rank      = 0
0.00.050.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.028 I print_info: model type       = 1.4B
0.00.050.033 I print_info: model params     = 1.41 B
0.00.050.033 I print_info: general.name     = 1.4B
0.00.050.034 I print_info: vocab type       = BPE
0.00.050.034 I print_info: n_vocab          = 50304
0.00.050.034 I print_info: n_merges         = 50009
0.00.050.034 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.035 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.035 I print_info: LF token         = 128 'Ä'
0.00.050.035 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.036 I print_info: max token length = 1024
0.00.051.977 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.977 I load_tensors: offloading output layer to GPU
0.00.051.977 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.988 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.989 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.270 I llama_init_from_model: n_seq_max     = 1
0.00.052.271 I llama_init_from_model: n_ctx         = 128
0.00.052.271 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.271 I llama_init_from_model: n_batch       = 128
0.00.052.271 I llama_init_from_model: n_ubatch      = 128
0.00.052.272 I llama_init_from_model: flash_attn    = 0
0.00.052.272 I llama_init_from_model: freq_base     = 10000.0
0.00.052.272 I llama_init_from_model: freq_scale    = 1
0.00.052.273 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.273 I ggml_metal_init: allocating
0.00.052.275 I ggml_metal_init: found device: Apple M4
0.00.052.277 I ggml_metal_init: picking default device: Apple M4
0.00.052.848 I ggml_metal_init: using embedded metal library
0.00.055.177 I ggml_metal_init: GPU name:   Apple M4
0.00.055.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.179 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.179 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.180 I ggml_metal_init: simdgroup reduction   = true
0.00.055.180 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.180 I ggml_metal_init: has bfloat            = true
0.00.055.180 I ggml_metal_init: use bfloat            = true
0.00.055.180 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.181 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.705 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.976 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.989 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.006 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.973 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.974 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.975 I llama_init_from_model: graph nodes  = 967
0.00.065.975 I llama_init_from_model: graph splits = 2
0.00.065.976 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.252 I 
0.00.707.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.313 I perplexity: tokenizing the input ..
0.00.715.373 I perplexity: tokenization took 8.059 ms
0.00.715.376 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.856.482 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.857.809 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.857.835 I llama_perf_context_print:        load time =     697.08 ms
0.00.857.836 I llama_perf_context_print: prompt eval time =     140.87 ms /   128 tokens (    1.10 ms per token,   908.67 tokens per second)
0.00.857.837 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.857.837 I llama_perf_context_print:       total time =     150.59 ms /   129 tokens
0.00.858.393 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.075s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.242 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.830 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.836 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.836 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.837 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.837 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.840 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.840 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.843 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.844 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.845 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.631 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.425 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.427 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.427 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.428 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.428 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.428 I llama_model_loader: - type  f32:  194 tensors
0.00.024.429 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.429 I print_info: file format = GGUF V3 (latest)
0.00.024.430 I print_info: file type   = Q6_K
0.00.024.430 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.560 I load: special tokens cache size = 25
0.00.049.674 I load: token to piece cache size = 0.2984 MB
0.00.049.677 I print_info: arch             = gptneox
0.00.049.678 I print_info: vocab_only       = 0
0.00.049.678 I print_info: n_ctx_train      = 2048
0.00.049.678 I print_info: n_embd           = 2048
0.00.049.678 I print_info: n_layer          = 24
0.00.049.681 I print_info: n_head           = 16
0.00.049.682 I print_info: n_head_kv        = 16
0.00.049.682 I print_info: n_rot            = 32
0.00.049.683 I print_info: n_swa            = 0
0.00.049.683 I print_info: n_embd_head_k    = 128
0.00.049.683 I print_info: n_embd_head_v    = 128
0.00.049.684 I print_info: n_gqa            = 1
0.00.049.684 I print_info: n_embd_k_gqa     = 2048
0.00.049.687 I print_info: n_embd_v_gqa     = 2048
0.00.049.687 I print_info: f_norm_eps       = 1.0e-05
0.00.049.687 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.688 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.688 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.688 I print_info: f_logit_scale    = 0.0e+00
0.00.049.689 I print_info: n_ff             = 8192
0.00.049.689 I print_info: n_expert         = 0
0.00.049.689 I print_info: n_expert_used    = 0
0.00.049.689 I print_info: causal attn      = 1
0.00.049.689 I print_info: pooling type     = 0
0.00.049.689 I print_info: rope type        = 2
0.00.049.689 I print_info: rope scaling     = linear
0.00.049.690 I print_info: freq_base_train  = 10000.0
0.00.049.691 I print_info: freq_scale_train = 1
0.00.049.693 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.693 I print_info: rope_finetuned   = unknown
0.00.049.693 I print_info: ssm_d_conv       = 0
0.00.049.693 I print_info: ssm_d_inner      = 0
0.00.049.693 I print_info: ssm_d_state      = 0
0.00.049.693 I print_info: ssm_dt_rank      = 0
0.00.049.693 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.694 I print_info: model type       = 1.4B
0.00.049.694 I print_info: model params     = 1.41 B
0.00.049.694 I print_info: general.name     = 1.4B
0.00.049.697 I print_info: vocab type       = BPE
0.00.049.698 I print_info: n_vocab          = 50304
0.00.049.698 I print_info: n_merges         = 50009
0.00.049.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.700 I print_info: LF token         = 128 'Ä'
0.00.049.700 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.700 I print_info: max token length = 1024
0.00.051.461 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.461 I load_tensors: offloading output layer to GPU
0.00.051.461 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.467 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.469 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.727 I llama_init_from_model: n_seq_max     = 1
0.00.051.728 I llama_init_from_model: n_ctx         = 128
0.00.051.728 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.728 I llama_init_from_model: n_batch       = 128
0.00.051.728 I llama_init_from_model: n_ubatch      = 128
0.00.051.729 I llama_init_from_model: flash_attn    = 0
0.00.051.729 I llama_init_from_model: freq_base     = 10000.0
0.00.051.729 I llama_init_from_model: freq_scale    = 1
0.00.051.730 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.730 I ggml_metal_init: allocating
0.00.051.733 I ggml_metal_init: found device: Apple M4
0.00.051.735 I ggml_metal_init: picking default device: Apple M4
0.00.052.326 I ggml_metal_init: using embedded metal library
0.00.054.691 I ggml_metal_init: GPU name:   Apple M4
0.00.054.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.694 I ggml_metal_init: simdgroup reduction   = true
0.00.054.694 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.694 I ggml_metal_init: has bfloat            = true
0.00.054.694 I ggml_metal_init: use bfloat            = true
0.00.054.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.569 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.825 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.838 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.854 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.726 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.727 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.728 I llama_init_from_model: graph nodes  = 967
0.00.065.728 I llama_init_from_model: graph splits = 2
0.00.065.729 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.439 I 
0.00.373.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.485 I perplexity: tokenizing the input ..
0.00.380.815 I perplexity: tokenization took 7.328 ms
0.00.380.818 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.520.916 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.522.079 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.522.105 I llama_perf_context_print:        load time =     364.19 ms
0.00.522.105 I llama_perf_context_print: prompt eval time =     139.87 ms /   128 tokens (    1.09 ms per token,   915.12 tokens per second)
0.00.522.106 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.522.107 I llama_perf_context_print:       total time =     148.67 ms /   129 tokens
0.00.522.543 I ggml_metal_free: deallocating

real	0m0.536s
user	0m0.077s
sys	0m0.074s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.247 I build: 4509 (99487b57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.726 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.244 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.256 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.258 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.259 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.259 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.261 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.262 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.263 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.264 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.269 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.270 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.271 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.693 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.101 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.102 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.102 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.103 I llama_model_loader: - type  f32:  194 tensors
0.00.054.104 I llama_model_loader: - type  f16:   98 tensors
0.00.054.104 I print_info: file format = GGUF V3 (latest)
0.00.054.105 I print_info: file type   = all F32 (guessed)
0.00.054.113 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.684 I load: special tokens cache size = 25
0.00.088.383 I load: token to piece cache size = 0.2984 MB
0.00.088.386 I print_info: arch             = gptneox
0.00.088.386 I print_info: vocab_only       = 0
0.00.088.386 I print_info: n_ctx_train      = 2048
0.00.088.387 I print_info: n_embd           = 2048
0.00.088.387 I print_info: n_layer          = 24
0.00.088.390 I print_info: n_head           = 16
0.00.088.391 I print_info: n_head_kv        = 16
0.00.088.391 I print_info: n_rot            = 32
0.00.088.391 I print_info: n_swa            = 0
0.00.088.391 I print_info: n_embd_head_k    = 128
0.00.088.394 I print_info: n_embd_head_v    = 128
0.00.088.394 I print_info: n_gqa            = 1
0.00.088.395 I print_info: n_embd_k_gqa     = 2048
0.00.088.396 I print_info: n_embd_v_gqa     = 2048
0.00.088.396 I print_info: f_norm_eps       = 1.0e-05
0.00.088.397 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.397 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.397 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.397 I print_info: f_logit_scale    = 0.0e+00
0.00.088.398 I print_info: n_ff             = 8192
0.00.088.398 I print_info: n_expert         = 0
0.00.088.398 I print_info: n_expert_used    = 0
0.00.088.398 I print_info: causal attn      = 1
0.00.088.398 I print_info: pooling type     = 0
0.00.088.398 I print_info: rope type        = 2
0.00.088.399 I print_info: rope scaling     = linear
0.00.088.399 I print_info: freq_base_train  = 10000.0
0.00.088.399 I print_info: freq_scale_train = 1
0.00.088.400 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.400 I print_info: rope_finetuned   = unknown
0.00.088.400 I print_info: ssm_d_conv       = 0
0.00.088.400 I print_info: ssm_d_inner      = 0
0.00.088.400 I print_info: ssm_d_state      = 0
0.00.088.400 I print_info: ssm_dt_rank      = 0
0.00.088.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.401 I print_info: model type       = 1.4B
0.00.088.401 I print_info: model params     = 1.41 B
0.00.088.401 I print_info: general.name     = 1.4B
0.00.088.402 I print_info: vocab type       = BPE
0.00.088.402 I print_info: n_vocab          = 50304
0.00.088.402 I print_info: n_merges         = 50009
0.00.088.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.403 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.403 I print_info: LF token         = 128 'Ä'
0.00.088.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.403 I print_info: max token length = 1024
0.00.091.722 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.722 I load_tensors: offloading output layer to GPU
0.00.091.722 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.733 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.734 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.009 I llama_init_from_model: n_seq_max     = 1
0.00.092.010 I llama_init_from_model: n_ctx         = 128
0.00.092.011 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.011 I llama_init_from_model: n_batch       = 128
0.00.092.011 I llama_init_from_model: n_ubatch      = 128
0.00.092.011 I llama_init_from_model: flash_attn    = 0
0.00.092.012 I llama_init_from_model: freq_base     = 10000.0
0.00.092.012 I llama_init_from_model: freq_scale    = 1
0.00.092.012 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.013 I ggml_metal_init: allocating
0.00.092.016 I ggml_metal_init: found device: Apple M4
0.00.092.018 I ggml_metal_init: picking default device: Apple M4
0.00.092.654 I ggml_metal_init: using embedded metal library
0.00.095.441 I ggml_metal_init: GPU name:   Apple M4
0.00.095.443 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.444 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.444 I ggml_metal_init: simdgroup reduction   = true
0.00.095.444 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.444 I ggml_metal_init: has bfloat            = true
0.00.095.444 I ggml_metal_init: use bfloat            = true
0.00.095.445 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.507 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.850 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.868 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.884 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.106.780 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.106.781 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.106.782 I llama_init_from_model: graph nodes  = 967
0.00.106.782 I llama_init_from_model: graph splits = 2
0.00.106.783 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.783 I 
0.00.106.819 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.820 I compute_imatrix: tokenizing the input ..
0.00.113.703 I compute_imatrix: tokenization took 6.882 ms
0.00.113.705 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.590.300 I compute_imatrix: 1.48 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.594.189 I llama_perf_context_print:        load time =    1568.57 ms
0.01.594.190 I llama_perf_context_print: prompt eval time =    1475.95 ms /   128 tokens (   11.53 ms per token,    86.72 tokens per second)
0.01.594.191 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.594.191 I llama_perf_context_print:       total time =    1572.45 ms /   129 tokens
0.01.594.777 I ggml_metal_free: deallocating

real	0m1.787s
user	0m0.175s
sys	0m0.214s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4509 (99487b57)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e30a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e30aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e30aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e30b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e30bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e30c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e30c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e30cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e30d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e30d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e30dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e30e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e30ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e30f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e30fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e310310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e310a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e311150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e311870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e312040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e312760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e312e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e3135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e313e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e314560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e314820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e314e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e315aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e315fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e3162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e316740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e316a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e317290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e3177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e317a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e317f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e3183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e318870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e318d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e3191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e319650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e319af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e319f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e31a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e31a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e31ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e31b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e31bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e31c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e31c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e31ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e31d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e31da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e31e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e31e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e31ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e31f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e31f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e31fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e320280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e320540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e3209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e320e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e321320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e3217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e321c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e322100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e3225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e322a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e322ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e323380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e323820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e323cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e324210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e324760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e324cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e325200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e325750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e325ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e3261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e326740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e326c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e3271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e327730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e327c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e3281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e328720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e328c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e3291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e329710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e329c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e32a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e32a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e32ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e32b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e32b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e32bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e31b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e32c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e32c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e32cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e32d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e32d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e32dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e32e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e32e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e32ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e32f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e32f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e32fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e3302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e330820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e330d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e331210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e3316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e331b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e331ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e332490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e332930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e332dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e333270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e333710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e333bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e334050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e3344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e334990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e334e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e3352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e335770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e335c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e3360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e336550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e3369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e336e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e337330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e3377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e337c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e338110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e3385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e338a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e338ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e339390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e339830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e339cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e33a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e33a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e33aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e33af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e33b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e33b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e33bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e33c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e33c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e33cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e33cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e33d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e33d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e33dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e33e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e33e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e33eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e33f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e33f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e33f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e33fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e340290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e340730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e340bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e341070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e341510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e3419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e341e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e3422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e342790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e342c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e3430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e343570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e343a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e343eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e344350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e3447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e344c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e345130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e3455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e345a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e345f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e3463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e346850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e346cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e347190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e347630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e347ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e347f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e3484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e348a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e348f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e3494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e349770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e349d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e34a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e34a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e34b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e34b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e34b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e34bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e34c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e34cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e34d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e34d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e34dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e34e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e34e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e34ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e34f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e34f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e34fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e350270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e3507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e350d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e351260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e3517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e351d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e352250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e3527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e352cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e353240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e353790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e353ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e354230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e354780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e354cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e355220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e355770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e355cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e356210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e356760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e356cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e357200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e357750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e357ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e3581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e358740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e358c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e3591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e359730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e359c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e35a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e35a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e35ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e35b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e35b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e35bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e35c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e35c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e35cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e35d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e35d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e35dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e35e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e35e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e35ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e35f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e35f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e35fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e360170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e3606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e360c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e3610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e361550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e3619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e361e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e362330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e3627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e362c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e363110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e3635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e363a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e363ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e364390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e364830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e364cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e365170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e3656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e365de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e366500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e366c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e367340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e367600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e367df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e3680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e3686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.115.636 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.115.639 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15ff04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15ff04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15ff05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15ff05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15ff05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15ff06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15ff065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15ff06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15ff06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15ff07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15ff07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15ff07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15ff08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15ff09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15ff09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15ff0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15ff0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15ff0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15ff0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15ff0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15ff0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15ff0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15ff0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15ff0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15ff0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15ff0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15ff0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15ff0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15ff0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15ff0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15ff0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15ff0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15ff10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15ff10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15ff108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15ff10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15ff11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15ff11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15ff11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15ff11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15ff12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15ff127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15ff12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15ff130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15ff13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15ff13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15ff13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15ff14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15ff146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15ff14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15ff14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15ff15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15ff15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15ff15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15ff16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15ff165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15ff16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15ff17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15ff174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15ff17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15ff17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15ff18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15ff18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15ff18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15ff18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15ff193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15ff19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15ff19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15ff1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15ff1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15ff1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15ff1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15ff1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15ff1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15ff1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15ff1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15ff1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15ff1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15ff1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15ff1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15ff1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15ff1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15ff1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15ff1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15ff1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15ff1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15ff1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15ff1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15ff1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15ff1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15ff202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15ff20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15ff20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15ff21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15ff21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15ff218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15ff21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15ff221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15ff22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15ff22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15ff22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15ff23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15ff23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15ff23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15ff240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15ff24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15ff249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15ff24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15ff252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15ff25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15ff25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15ff25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15ff26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15ff268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15ff26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15ff271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15ff27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15ff27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15ff27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15ff28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15ff287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15ff28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15ff290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15ff29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15ff299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15ff29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15ff2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15ff2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15ff2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15ff2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15ff2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15ff2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15ff2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15ff2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15ff2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15ff2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15ff2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15ff2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15ff2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15ff2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15ff2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15ff2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15ff2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15ff2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15ff2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15ff2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15ff2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15ff2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15ff30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15ff30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15ff30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15ff31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15ff315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15ff31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15ff31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15ff32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15ff327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15ff32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15ff33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15ff334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15ff33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15ff33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15ff34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15ff346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15ff34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15ff34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15ff35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15ff35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15ff36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15ff365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15ff36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15ff36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15ff37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15ff37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15ff37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15ff38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15ff384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15ff38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15ff38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15ff39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15ff39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15ff39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15ff39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15ff3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15ff3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15ff3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15ff3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15ff3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15ff3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15ff3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15ff3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15ff3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15ff3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15ff3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15ff3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15ff3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15ff3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15ff3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15ff3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15ff3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15ff3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15ff3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15ff3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15ff3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15ff40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15ff40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15ff40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15ff40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15ff41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15ff41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15ff42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15ff42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15ff42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15ff433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15ff43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15ff43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15ff44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15ff44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15ff45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15ff45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15ff45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15ff461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15ff46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15ff46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15ff47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15ff478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15ff47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15ff48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15ff48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15ff48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15ff49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15ff49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15ff4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15ff4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15ff4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15ff4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15ff4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15ff4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15ff4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15ff4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15ff4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15ff4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15ff4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15ff4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15ff4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15ff4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15ff4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15ff4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15ff4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15ff502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15ff50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15ff50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15ff51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15ff519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15ff51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15ff52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15ff52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15ff530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15ff53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15ff53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15ff54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15ff547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15ff54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15ff55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15ff55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15ff55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15ff56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15ff56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15ff56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15ff57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15ff57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15ff57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15ff58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15ff58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15ff58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15ff59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15ff59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15ff59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15ff5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15ff5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15ff5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15ff5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15ff5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15ff5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15ff5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15ff5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15ff5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15ff5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15ff5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15ff5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15ff5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c7046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c704b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c704fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c705430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c7058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c705d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c706180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c7065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c706a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c706ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c707340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c707a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c708580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c708d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c709540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c709c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c70a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c70aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c70b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c70b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c70c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c70c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c70ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c70d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c70dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c70df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c70e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c70e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c70eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c70ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c70f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c70f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c70fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c710030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c7104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c710d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c7111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c711660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c711ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c711f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c7123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c712820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c712c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c713100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c713570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c7139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c713e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c7142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c714ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c715010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c7158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c715d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c7161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c716c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c7170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c717520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c717990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c717e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c718270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c7186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c718b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c718fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c719430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c7198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c719d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c71a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c71a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c71aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c71aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c71b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c71b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c71bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c71c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c71c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c71c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c71cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c71d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c71d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c71db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c71dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c71e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c71e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c71ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c71f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c71f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c71fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c71feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c720790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c721070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c7214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c721950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c721dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c722230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c7226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c722b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c722f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c7233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c723c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c723f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c7243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c724820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c724c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c725100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c725570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c7259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c725e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c7262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c726730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c726ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c727010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c727480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c7278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c727d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c7281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c728640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c728ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c728f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c729390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c729800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c729c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c72a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c72a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c72a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c72ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c72b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c72b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c72bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c72bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c72c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c72c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c72cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c72d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c72d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c72da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c72df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c72e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c72e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c72ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c72f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c72f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c72f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c72fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c730280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c7306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c730b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c730fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c731440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c7318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c731d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c732190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c732600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c732a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c732ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c733350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c7337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c733c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c7340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c734510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c734980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c734df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c735260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c7356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c735b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c735fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c736420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c736890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c737170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c7375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c737a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c737ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c738330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c7387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c738c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c739080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c7394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c739dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c73a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c73a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c73ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c73af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c73b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c73b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c73bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c73c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c73c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c73ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c73cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c73d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c73d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c73dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c73e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c73e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c73e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c73edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c73f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c73f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c73fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c73ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c7403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c740850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c740cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c741130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c741cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c741f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c742230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c7426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c742b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c742f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c7433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c743860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c743cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c744140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c7445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c744a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c744e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c745300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c745770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c745be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c746050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c7464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c746930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c746da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c747210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c747680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c747af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c747f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c7483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c748840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c748cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c749120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c749590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c749a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c749e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c74a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c74a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c74abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c74b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c74b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c74b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c74bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c74c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c74c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c74cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c74cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c74d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c74d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c74dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c74e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c74e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c74e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c74ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c74f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c74f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c74fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c750010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c750480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c7508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c750d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c7511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c751640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c751ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c751f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c752390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c752800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c752c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c7530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c753550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c7539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c753e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c7542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c754710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c754b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c754ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c755460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c7558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c756340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c756a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c757180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c7578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c757b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c757fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c7585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c758be0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.785s
user	0m0.274s
sys	0m0.308s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4509 (99487b57)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12970d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12970dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12970e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12970e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12970ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12970f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12970f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12970fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129710410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129710e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129711310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129711e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1297125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129712df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129714350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129714a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129715240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129715960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129716080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1297167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129717040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129717760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129717a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129718030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129718ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1297191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1297194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129719c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12971a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12971a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12971ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12971b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12971b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12971ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12971bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12971c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12971c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12971ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12971d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12971d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12971d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12971df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12971e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12971ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12971f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12971fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129720060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129720c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129721290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129721a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129721f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1297223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129722680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129722c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129723480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129723be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129724080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129724520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1297249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129724e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1297257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129725c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1297260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129726580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129726a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129726ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129727960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129728ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1297293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129729940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12972a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12972a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12972ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12972b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12972b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12972be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12972c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12972c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12972ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12972d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12972d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12972de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12972e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12972e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12972ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12971eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12972f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12972fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12972ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129730500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129730a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129730fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1297314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129731a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1297324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129732a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129732f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1297334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129733a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129733f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129734410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1297348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129734d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1297351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129735690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129735b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129736470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129736910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129736db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1297376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129737b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129738030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1297384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129738970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129738e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1297392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129739750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12973a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12973a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12973a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12973ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12973b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12973b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12973bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12973c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12973c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12973ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12973ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12973d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12973d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12973dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12973e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12973e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12973ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12973ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12973f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12973f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12973fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1297401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129740650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129740af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129740f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129741430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1297418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129741d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129742210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1297426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129742b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129742ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129743490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129743930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129743dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129744270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129744710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129744bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129745050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1297454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129745990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129745e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1297462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129746770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129746c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1297470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129747550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1297479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129747e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129748330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1297487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129748c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129749110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1297495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129749a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129749ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12974a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12974a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12974acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12974b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12974b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12974bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12974c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12974c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12974c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12974cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12974d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12974dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12974e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12974e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12974eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12974f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12974f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12974ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1297503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129750840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129750ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1297519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129751f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129752480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1297529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129752f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129753470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1297539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129753f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129754460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1297549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129754f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1297559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129756440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129756990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129756ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129757430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129757980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129757ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129758420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129758970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129758ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129759410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129759960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129759eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12975a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12975a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12975aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12975b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12975b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12975be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12975c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12975c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12975ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12975d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12975d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12975de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12975e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12975e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12975ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12975f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12975f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12975fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1297603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1297608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129760e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1297618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129761e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129762380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1297628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129762e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129763370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1297638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129763e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1297642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129764750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129764bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129765090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129765530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1297659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129766310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1297667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129766c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1297670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129767590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129767a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129767ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129768370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1297688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129768fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129769700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129769e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12976a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12976a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12976aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12976b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12976b8c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.104.508 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a8090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a80ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a80b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a80bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a80c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a80cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a80d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a80d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a80e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a80e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a80eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a80ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a80f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a80f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a80fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a8101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a8111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a8123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a8130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a8139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a8142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a8158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a8161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a8170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a8186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a81a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a81aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a81aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a81b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a81b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a81bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a81c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a81c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a81c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a81cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a81d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a81db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a81df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a81e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a81e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a81ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a81f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a81f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a81fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a81fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a8214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a8233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a8252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a8264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a8283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a8299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a82a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a82a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a82abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a82b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a82b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a82b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a82bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a82c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a82c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a82cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a82cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a82d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a82d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a82dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a82e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a82e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a82e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a82ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a82f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a82f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a82fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a8308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a8311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a8327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a8330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a8339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a8377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a8380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a83a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a83a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a83ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a83b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a83b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a83ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a83bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a83c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a83c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a83cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a83d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a83d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a83d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a83ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a83e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a83e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a83eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a83ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a83f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a83f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a83fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a8402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a8425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a8439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a8450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a8467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a8495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a84a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a84a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a84acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a84b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a84b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a84be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a84c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a84c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a84cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a84d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a84dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a84e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a84e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a84ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a84f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a84f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a84fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a8508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a8536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a8564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a85a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a85a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a85ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a85b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a85b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a85bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a85c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a85cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a85d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a85d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a85dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a85e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a85e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12976b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12974edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12974cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12974d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129720930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129722940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12974f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12971e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12971f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12971f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12971dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12971fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129716ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129722f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12972f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12976aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129719ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12971a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12974f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12974de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1297182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1297185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129718870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12976bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12976bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12976c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12976c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12976c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12976cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12976cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12976d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12976d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12976d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12976d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12976db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12976de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12976e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12976e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12976e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12976e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12976ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12976eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12976f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12976f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12976f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12976f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12976fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12976ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1297701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1297704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129770760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129770a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129770ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129770fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129771260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129771520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1297717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129771aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129771d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129772020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1297722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1297725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129772860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129772b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129772de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1297730a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129773360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129773620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1297738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129773ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129773e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129774120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1297743e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1297746a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129774960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129774c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129774ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1297751a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129775460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129775720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1297759e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129775ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129775f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129776220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1297764e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1297767a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129776a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129776d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129776fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1297772a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129777560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129777820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129777ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129777da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129778060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129778320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1297785e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1297788a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129778b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129778e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1297790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1297793a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129779660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129779920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129779be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129779ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12977a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12977a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12977a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12977a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12977ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12977af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12977b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12977b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12977b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12977ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12977bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12977bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12977c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12977c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12977c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12977caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12977cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12977d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12977d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12977d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12977d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12977db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12977dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12977e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12977e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12977e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12977e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12977eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12977ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12977f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12977f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12977f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12977f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12977fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12977fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1297801a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129780460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129780720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1297809e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129780ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129780f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129781220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1297814e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1297817a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129781a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129781d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129781fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1297822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129782560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129782820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129782ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129782da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129783060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129783320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1297835e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1297838a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129783b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129783e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1297840e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1297843a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129784660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129784920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129784be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129784ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129785160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129785420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1297856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1297859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129785c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129785f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1297861e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1297864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129786760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129786a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129786ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129786fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129787260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129787520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1297877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129787aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129787d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129788020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1297882e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1297885a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129788860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129788b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129788de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1297890a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129789360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129789620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1297898e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129789ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129789e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12978a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12978a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12978a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12978a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12978ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12978aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12978b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12978b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12978b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12978bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12978bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12978c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12978c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12978cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12978d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12978d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12978dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12978e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12978e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12978ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12978f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12978f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12978fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129790230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129790780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129790cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129791220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129791770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129791cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129792210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129792760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129792cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129793200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129793750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129793ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1297941f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129794740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129794c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1297951e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129795730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129795c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1297961d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129796720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129796c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1297971c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129797710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129797c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1297981b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129798700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129798c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1297991a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1297996f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129799c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12979a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12979a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12979ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12979b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12979b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12979bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12979c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12979c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12979cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12979d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12979d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12979dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12979e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12979e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12979e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12979e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12979ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12979f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12979f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12979fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12979ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1297a0430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1297a08a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1297a0d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1297a1180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1297a15f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1297a1a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1297a1ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1297a2340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1297a27b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1297a34a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1297a3bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1297a42e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1297a45a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1297a4a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1297a5010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1297a5620 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.933s
user	0m0.245s
sys	0m0.134s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
