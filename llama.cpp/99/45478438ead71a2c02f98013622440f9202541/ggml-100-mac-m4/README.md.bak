### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.63 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.62 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.27 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.30 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.30 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    1.00 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  177.13 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.67 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 219.57 sec*proc (28 tests)

Total Test time (real) = 219.58 sec

real	3m39.702s
user	7m32.559s
sys	0m6.509s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.10 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.35 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.43 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.30 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.39 sec*proc (28 tests)

Total Test time (real) =  51.41 sec

real	0m51.417s
user	1m11.676s
sys	0m5.684s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.083 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.417 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.323 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.330 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.332 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.333 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.334 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.335 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.336 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.337 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.337 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.338 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.338 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.341 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.342 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.343 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.343 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.344 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.344 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.345 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.809 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.811 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.812 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.812 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.812 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.813 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.025.813 I llama_model_loader: - type  f32:  124 tensors
0.00.025.814 I llama_model_loader: - type  f16:   73 tensors
0.00.025.814 I print_info: file format = GGUF V3 (latest)
0.00.025.815 I print_info: file type   = F16
0.00.025.822 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.813 I load: special tokens cache size = 5
0.00.031.785 I load: token to piece cache size = 0.2032 MB
0.00.031.789 I print_info: arch             = bert
0.00.031.789 I print_info: vocab_only       = 0
0.00.031.789 I print_info: n_ctx_train      = 512
0.00.031.789 I print_info: n_embd           = 384
0.00.031.790 I print_info: n_layer          = 12
0.00.031.793 I print_info: n_head           = 12
0.00.031.794 I print_info: n_head_kv        = 12
0.00.031.794 I print_info: n_rot            = 32
0.00.031.797 I print_info: n_swa            = 0
0.00.031.797 I print_info: n_embd_head_k    = 32
0.00.031.797 I print_info: n_embd_head_v    = 32
0.00.031.798 I print_info: n_gqa            = 1
0.00.031.799 I print_info: n_embd_k_gqa     = 384
0.00.031.799 I print_info: n_embd_v_gqa     = 384
0.00.031.800 I print_info: f_norm_eps       = 1.0e-12
0.00.031.802 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.802 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.802 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.803 I print_info: f_logit_scale    = 0.0e+00
0.00.031.803 I print_info: n_ff             = 1536
0.00.031.804 I print_info: n_expert         = 0
0.00.031.804 I print_info: n_expert_used    = 0
0.00.031.804 I print_info: causal attn      = 0
0.00.031.805 I print_info: pooling type     = 2
0.00.031.810 I print_info: rope type        = 2
0.00.031.810 I print_info: rope scaling     = linear
0.00.031.811 I print_info: freq_base_train  = 10000.0
0.00.031.811 I print_info: freq_scale_train = 1
0.00.031.811 I print_info: n_ctx_orig_yarn  = 512
0.00.031.812 I print_info: rope_finetuned   = unknown
0.00.031.813 I print_info: ssm_d_conv       = 0
0.00.031.814 I print_info: ssm_d_inner      = 0
0.00.031.814 I print_info: ssm_d_state      = 0
0.00.031.815 I print_info: ssm_dt_rank      = 0
0.00.031.815 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.815 I print_info: model type       = 33M
0.00.031.816 I print_info: model params     = 33.21 M
0.00.031.816 I print_info: general.name     = Bge Small
0.00.031.816 I print_info: vocab type       = WPM
0.00.031.818 I print_info: n_vocab          = 30522
0.00.031.819 I print_info: n_merges         = 0
0.00.031.819 I print_info: BOS token        = 101 '[CLS]'
0.00.031.819 I print_info: UNK token        = 100 '[UNK]'
0.00.031.819 I print_info: SEP token        = 102 '[SEP]'
0.00.031.820 I print_info: PAD token        = 0 '[PAD]'
0.00.031.820 I print_info: MASK token       = 103 '[MASK]'
0.00.031.820 I print_info: LF token         = 0 '[PAD]'
0.00.031.820 I print_info: max token length = 21
0.00.033.663 I load_tensors: offloading 12 repeating layers to GPU
0.00.033.664 I load_tensors: offloading output layer to GPU
0.00.033.664 I load_tensors: offloaded 13/13 layers to GPU
0.00.033.688 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.690 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.033.907 I llama_init_from_model: n_seq_max     = 1
0.00.033.909 I llama_init_from_model: n_ctx         = 512
0.00.033.909 I llama_init_from_model: n_ctx_per_seq = 512
0.00.033.909 I llama_init_from_model: n_batch       = 2048
0.00.033.909 I llama_init_from_model: n_ubatch      = 2048
0.00.033.910 I llama_init_from_model: flash_attn    = 0
0.00.033.910 I llama_init_from_model: freq_base     = 10000.0
0.00.033.910 I llama_init_from_model: freq_scale    = 1
0.00.033.911 I ggml_metal_init: allocating
0.00.033.915 I ggml_metal_init: found device: Apple M4
0.00.033.918 I ggml_metal_init: picking default device: Apple M4
0.00.034.717 I ggml_metal_init: using embedded metal library
0.00.038.779 I ggml_metal_init: GPU name:   Apple M4
0.00.038.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.038.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.038.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.038.783 I ggml_metal_init: simdgroup reduction   = true
0.00.038.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.038.784 I ggml_metal_init: has bfloat            = true
0.00.038.784 I ggml_metal_init: use bfloat            = true
0.00.038.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.038.785 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.050.587 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.187 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.209 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.211 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.051.933 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.051.934 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.051.934 I llama_init_from_model: graph nodes  = 429
0.00.051.935 I llama_init_from_model: graph splits = 2
0.00.051.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.051.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.059 I 
0.00.058.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.718 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.063.495 I llama_perf_context_print:        load time =      42.64 ms
0.00.063.496 I llama_perf_context_print: prompt eval time =       4.65 ms /     9 tokens (    0.52 ms per token,  1937.15 tokens per second)
0.00.063.497 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.063.497 I llama_perf_context_print:       total time =       5.44 ms /    10 tokens
0.00.063.650 I ggml_metal_free: deallocating

real	0m0.241s
user	0m0.047s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.178 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.879 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.883 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.884 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.884 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.887 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.887 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.887 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.888 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.889 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.889 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.889 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.890 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.892 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.892 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.894 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.894 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.894 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.895 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.313 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.944 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.946 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.946 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.947 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.947 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.947 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.947 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.948 I llama_model_loader: - type  f32:  124 tensors
0.00.014.948 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.949 I print_info: file format = GGUF V3 (latest)
0.00.014.949 I print_info: file type   = Q8_0
0.00.014.950 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.315 I load: special tokens cache size = 5
0.00.018.632 I load: token to piece cache size = 0.2032 MB
0.00.018.635 I print_info: arch             = bert
0.00.018.635 I print_info: vocab_only       = 0
0.00.018.636 I print_info: n_ctx_train      = 512
0.00.018.636 I print_info: n_embd           = 384
0.00.018.636 I print_info: n_layer          = 12
0.00.018.639 I print_info: n_head           = 12
0.00.018.640 I print_info: n_head_kv        = 12
0.00.018.641 I print_info: n_rot            = 32
0.00.018.641 I print_info: n_swa            = 0
0.00.018.641 I print_info: n_embd_head_k    = 32
0.00.018.641 I print_info: n_embd_head_v    = 32
0.00.018.642 I print_info: n_gqa            = 1
0.00.018.643 I print_info: n_embd_k_gqa     = 384
0.00.018.643 I print_info: n_embd_v_gqa     = 384
0.00.018.644 I print_info: f_norm_eps       = 1.0e-12
0.00.018.644 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.645 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.645 I print_info: f_logit_scale    = 0.0e+00
0.00.018.646 I print_info: n_ff             = 1536
0.00.018.646 I print_info: n_expert         = 0
0.00.018.646 I print_info: n_expert_used    = 0
0.00.018.646 I print_info: causal attn      = 0
0.00.018.646 I print_info: pooling type     = 2
0.00.018.646 I print_info: rope type        = 2
0.00.018.647 I print_info: rope scaling     = linear
0.00.018.647 I print_info: freq_base_train  = 10000.0
0.00.018.647 I print_info: freq_scale_train = 1
0.00.018.647 I print_info: n_ctx_orig_yarn  = 512
0.00.018.648 I print_info: rope_finetuned   = unknown
0.00.018.648 I print_info: ssm_d_conv       = 0
0.00.018.648 I print_info: ssm_d_inner      = 0
0.00.018.648 I print_info: ssm_d_state      = 0
0.00.018.648 I print_info: ssm_dt_rank      = 0
0.00.018.648 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.648 I print_info: model type       = 33M
0.00.018.649 I print_info: model params     = 33.21 M
0.00.018.649 I print_info: general.name     = Bge Small
0.00.018.649 I print_info: vocab type       = WPM
0.00.018.650 I print_info: n_vocab          = 30522
0.00.018.650 I print_info: n_merges         = 0
0.00.018.650 I print_info: BOS token        = 101 '[CLS]'
0.00.018.650 I print_info: UNK token        = 100 '[UNK]'
0.00.018.650 I print_info: SEP token        = 102 '[SEP]'
0.00.018.650 I print_info: PAD token        = 0 '[PAD]'
0.00.018.651 I print_info: MASK token       = 103 '[MASK]'
0.00.018.651 I print_info: LF token         = 0 '[PAD]'
0.00.018.651 I print_info: max token length = 21
0.00.019.895 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.895 I load_tensors: offloading output layer to GPU
0.00.019.897 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.904 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.905 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.046 I llama_init_from_model: n_seq_max     = 1
0.00.020.047 I llama_init_from_model: n_ctx         = 512
0.00.020.047 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.047 I llama_init_from_model: n_batch       = 2048
0.00.020.047 I llama_init_from_model: n_ubatch      = 2048
0.00.020.048 I llama_init_from_model: flash_attn    = 0
0.00.020.048 I llama_init_from_model: freq_base     = 10000.0
0.00.020.048 I llama_init_from_model: freq_scale    = 1
0.00.020.049 I ggml_metal_init: allocating
0.00.020.052 I ggml_metal_init: found device: Apple M4
0.00.020.056 I ggml_metal_init: picking default device: Apple M4
0.00.020.735 I ggml_metal_init: using embedded metal library
0.00.023.297 I ggml_metal_init: GPU name:   Apple M4
0.00.023.299 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.299 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.300 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.300 I ggml_metal_init: simdgroup reduction   = true
0.00.023.300 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.300 I ggml_metal_init: has bfloat            = true
0.00.023.301 I ggml_metal_init: use bfloat            = true
0.00.023.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.590 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.090 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.103 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.106 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.688 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.689 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.689 I llama_init_from_model: graph nodes  = 429
0.00.034.689 I llama_init_from_model: graph splits = 2
0.00.034.691 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.691 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.859 I 
0.00.039.891 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.413 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.880 I llama_perf_context_print:        load time =      30.68 ms
0.00.044.881 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2074.69 tokens per second)
0.00.044.882 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.882 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.045.100 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.108 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.151 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.821 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.828 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.026.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.829 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.026.830 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.026.833 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.026.834 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.026.834 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.026.835 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.026.835 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.026.836 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.026.838 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.838 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.839 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.026.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.030.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.031.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.732 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.034.733 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.733 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.034.734 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.034.734 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.034.734 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.034.735 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.034.735 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.034.735 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.034.736 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.034.736 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.034.736 I llama_model_loader: - type  f32:   40 tensors
0.00.034.737 I llama_model_loader: - type  f16:   30 tensors
0.00.034.737 I print_info: file format = GGUF V3 (latest)
0.00.034.738 I print_info: file type   = F16
0.00.034.739 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.046.780 W load: empty token at index 5
0.00.050.474 W load: model vocab missing newline token, using special_pad_id instead
0.00.051.661 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.051.688 I load: special tokens cache size = 5
0.00.305.997 I load: token to piece cache size = 1.5060 MB
0.00.306.007 I print_info: arch             = jina-bert-v2
0.00.306.008 I print_info: vocab_only       = 0
0.00.306.008 I print_info: n_ctx_train      = 8192
0.00.306.011 I print_info: n_embd           = 384
0.00.306.011 I print_info: n_layer          = 4
0.00.306.017 I print_info: n_head           = 12
0.00.306.018 I print_info: n_head_kv        = 12
0.00.306.018 I print_info: n_rot            = 32
0.00.306.018 I print_info: n_swa            = 0
0.00.306.018 I print_info: n_embd_head_k    = 32
0.00.306.019 I print_info: n_embd_head_v    = 32
0.00.306.024 I print_info: n_gqa            = 1
0.00.306.024 I print_info: n_embd_k_gqa     = 384
0.00.306.025 I print_info: n_embd_v_gqa     = 384
0.00.306.026 I print_info: f_norm_eps       = 1.0e-12
0.00.306.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.306.027 I print_info: f_clamp_kqv      = 0.0e+00
0.00.306.027 I print_info: f_max_alibi_bias = 8.0e+00
0.00.306.027 I print_info: f_logit_scale    = 0.0e+00
0.00.306.028 I print_info: n_ff             = 1536
0.00.306.031 I print_info: n_expert         = 0
0.00.306.031 I print_info: n_expert_used    = 0
0.00.306.031 I print_info: causal attn      = 0
0.00.306.031 I print_info: pooling type     = -1
0.00.306.031 I print_info: rope type        = -1
0.00.306.034 I print_info: rope scaling     = linear
0.00.306.034 I print_info: freq_base_train  = 10000.0
0.00.306.034 I print_info: freq_scale_train = 1
0.00.306.034 I print_info: n_ctx_orig_yarn  = 8192
0.00.306.035 I print_info: rope_finetuned   = unknown
0.00.306.035 I print_info: ssm_d_conv       = 0
0.00.306.035 I print_info: ssm_d_inner      = 0
0.00.306.036 I print_info: ssm_d_state      = 0
0.00.306.036 I print_info: ssm_dt_rank      = 0
0.00.306.036 I print_info: ssm_dt_b_c_rms   = 0
0.00.306.036 I print_info: model type       = 33M
0.00.306.036 I print_info: model params     = 32.90 M
0.00.306.037 I print_info: general.name     = Jina Bert Implementation
0.00.306.037 I print_info: vocab type       = BPE
0.00.306.038 I print_info: n_vocab          = 61056
0.00.306.038 I print_info: n_merges         = 39382
0.00.306.038 I print_info: BOS token        = 0 '<s>'
0.00.306.039 I print_info: EOS token        = 2 '</s>'
0.00.306.039 I print_info: UNK token        = 3 '<unk>'
0.00.306.040 I print_info: SEP token        = 2 '</s>'
0.00.306.040 I print_info: PAD token        = 1 '<pad>'
0.00.306.041 I print_info: MASK token       = 4 '<mask>'
0.00.306.041 I print_info: EOG token        = 2 '</s>'
0.00.306.041 I print_info: max token length = 45
0.00.307.216 I load_tensors: offloading 4 repeating layers to GPU
0.00.307.216 I load_tensors: offloading output layer to GPU
0.00.307.216 I load_tensors: offloaded 5/5 layers to GPU
0.00.307.238 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.307.239 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.307.414 I llama_init_from_model: n_seq_max     = 1
0.00.307.415 I llama_init_from_model: n_ctx         = 8192
0.00.307.415 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.307.415 I llama_init_from_model: n_batch       = 2048
0.00.307.415 I llama_init_from_model: n_ubatch      = 2048
0.00.307.415 I llama_init_from_model: flash_attn    = 0
0.00.307.416 I llama_init_from_model: freq_base     = 10000.0
0.00.307.416 I llama_init_from_model: freq_scale    = 1
0.00.307.417 I ggml_metal_init: allocating
0.00.307.420 I ggml_metal_init: found device: Apple M4
0.00.307.422 I ggml_metal_init: picking default device: Apple M4
0.00.308.265 I ggml_metal_init: using embedded metal library
0.00.311.152 I ggml_metal_init: GPU name:   Apple M4
0.00.311.153 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.311.154 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.311.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.311.154 I ggml_metal_init: simdgroup reduction   = true
0.00.311.155 I ggml_metal_init: simdgroup matrix mul. = true
0.00.311.155 I ggml_metal_init: has bfloat            = true
0.00.311.155 I ggml_metal_init: use bfloat            = true
0.00.311.155 I ggml_metal_init: hasUnifiedMemory      = true
0.00.311.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.320.715 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.323.113 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.323.134 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.323.138 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.323.751 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.323.752 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.323.753 I llama_init_from_model: graph nodes  = 154
0.00.323.753 I llama_init_from_model: graph splits = 2
0.00.323.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.323.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.336.139 I 
0.00.336.175 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.336.332 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.336.333 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.336.345 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.336.345 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.336.348 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.336.348 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.336.841 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.340.511 I llama_perf_context_print:        load time =     318.98 ms
0.00.340.513 I llama_perf_context_print: prompt eval time =       3.66 ms /    62 tokens (    0.06 ms per token, 16930.64 tokens per second)
0.00.340.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.340.515 I llama_perf_context_print:       total time =       4.37 ms /    63 tokens
0.00.340.736 I ggml_metal_free: deallocating

real	0m1.057s
user	0m0.315s
sys	0m0.037s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.181 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.295 I main: llama backend init
0.00.000.302 I main: load the model and apply lora adapter, if any
0.00.029.038 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.447 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.462 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.467 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.468 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.469 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.470 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.475 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.476 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.481 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.483 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.157 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.854 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.854 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.855 I llama_model_loader: - type  f32:  194 tensors
0.00.060.856 I llama_model_loader: - type  f16:   98 tensors
0.00.060.857 I print_info: file format = GGUF V3 (latest)
0.00.060.858 I print_info: file type   = all F32 (guessed)
0.00.060.859 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.088.501 I load: special tokens cache size = 25
0.00.095.247 I load: token to piece cache size = 0.2984 MB
0.00.095.251 I print_info: arch             = gptneox
0.00.095.251 I print_info: vocab_only       = 0
0.00.095.251 I print_info: n_ctx_train      = 2048
0.00.095.251 I print_info: n_embd           = 2048
0.00.095.251 I print_info: n_layer          = 24
0.00.095.255 I print_info: n_head           = 16
0.00.095.256 I print_info: n_head_kv        = 16
0.00.095.256 I print_info: n_rot            = 32
0.00.095.257 I print_info: n_swa            = 0
0.00.095.257 I print_info: n_embd_head_k    = 128
0.00.095.257 I print_info: n_embd_head_v    = 128
0.00.095.258 I print_info: n_gqa            = 1
0.00.095.258 I print_info: n_embd_k_gqa     = 2048
0.00.095.259 I print_info: n_embd_v_gqa     = 2048
0.00.095.259 I print_info: f_norm_eps       = 1.0e-05
0.00.095.260 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.095.260 I print_info: f_clamp_kqv      = 0.0e+00
0.00.095.260 I print_info: f_max_alibi_bias = 0.0e+00
0.00.095.260 I print_info: f_logit_scale    = 0.0e+00
0.00.095.261 I print_info: n_ff             = 8192
0.00.095.261 I print_info: n_expert         = 0
0.00.095.261 I print_info: n_expert_used    = 0
0.00.095.262 I print_info: causal attn      = 1
0.00.095.262 I print_info: pooling type     = 0
0.00.095.262 I print_info: rope type        = 2
0.00.095.262 I print_info: rope scaling     = linear
0.00.095.263 I print_info: freq_base_train  = 10000.0
0.00.095.263 I print_info: freq_scale_train = 1
0.00.095.263 I print_info: n_ctx_orig_yarn  = 2048
0.00.095.263 I print_info: rope_finetuned   = unknown
0.00.095.265 I print_info: ssm_d_conv       = 0
0.00.095.265 I print_info: ssm_d_inner      = 0
0.00.095.265 I print_info: ssm_d_state      = 0
0.00.095.265 I print_info: ssm_dt_rank      = 0
0.00.095.265 I print_info: ssm_dt_b_c_rms   = 0
0.00.095.265 I print_info: model type       = 1.4B
0.00.095.266 I print_info: model params     = 1.41 B
0.00.095.266 I print_info: general.name     = 1.4B
0.00.095.266 I print_info: vocab type       = BPE
0.00.095.267 I print_info: n_vocab          = 50304
0.00.095.267 I print_info: n_merges         = 50009
0.00.095.267 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.095.267 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.095.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.095.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.095.271 I print_info: LF token         = 128 'Ä'
0.00.095.272 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.095.272 I print_info: max token length = 1024
0.00.097.851 I load_tensors: offloading 24 repeating layers to GPU
0.00.097.852 I load_tensors: offloading output layer to GPU
0.00.097.852 I load_tensors: offloaded 25/25 layers to GPU
0.00.097.870 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.871 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.098.148 I llama_init_from_model: n_seq_max     = 1
0.00.098.149 I llama_init_from_model: n_ctx         = 2048
0.00.098.149 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.098.149 I llama_init_from_model: n_batch       = 2048
0.00.098.149 I llama_init_from_model: n_ubatch      = 512
0.00.098.150 I llama_init_from_model: flash_attn    = 0
0.00.098.150 I llama_init_from_model: freq_base     = 10000.0
0.00.098.150 I llama_init_from_model: freq_scale    = 1
0.00.098.151 I ggml_metal_init: allocating
0.00.098.153 I ggml_metal_init: found device: Apple M4
0.00.098.155 I ggml_metal_init: picking default device: Apple M4
0.00.098.803 I ggml_metal_init: using embedded metal library
0.00.117.644 I ggml_metal_init: GPU name:   Apple M4
0.00.117.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.117.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.117.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.117.647 I ggml_metal_init: simdgroup reduction   = true
0.00.117.647 I ggml_metal_init: simdgroup matrix mul. = true
0.00.117.647 I ggml_metal_init: has bfloat            = true
0.00.117.647 I ggml_metal_init: use bfloat            = true
0.00.117.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.117.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.158.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.180.534 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.180.563 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.180.582 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.181.532 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.181.534 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.181.534 I llama_init_from_model: graph nodes  = 967
0.00.181.534 I llama_init_from_model: graph splits = 2
0.00.181.537 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.181.657 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.181.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.261.982 I main: llama threadpool init, n_threads = 4
0.00.262.027 I 
0.00.262.069 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.262.070 I 
0.00.262.132 I sampler seed: 1234
0.00.262.137 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.262.162 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.262.163 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.262.164 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.091.515 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.02.091.516 I llama_perf_context_print:        load time =     232.93 ms
0.02.091.517 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.61 tokens per second)
0.02.091.517 I llama_perf_context_print:        eval time =    1782.98 ms /    63 runs   (   28.30 ms per token,    35.33 tokens per second)
0.02.091.519 I llama_perf_context_print:       total time =    1829.53 ms /    70 tokens
0.02.091.773 I ggml_metal_free: deallocating

real	0m2.397s
user	0m0.143s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.483 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.074 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.071 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.088 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.100 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.100 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.101 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.102 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.104 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.105 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.119 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.119 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.789 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.089 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.308 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.312 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.312 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.313 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.313 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.314 I llama_model_loader: - type  f32:  194 tensors
0.00.056.315 I llama_model_loader: - type  f16:   98 tensors
0.00.056.316 I print_info: file format = GGUF V3 (latest)
0.00.056.324 I print_info: file type   = all F32 (guessed)
0.00.056.326 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.661 I load: special tokens cache size = 25
0.00.092.700 I load: token to piece cache size = 0.2984 MB
0.00.092.704 I print_info: arch             = gptneox
0.00.092.704 I print_info: vocab_only       = 0
0.00.092.704 I print_info: n_ctx_train      = 2048
0.00.092.704 I print_info: n_embd           = 2048
0.00.092.704 I print_info: n_layer          = 24
0.00.092.707 I print_info: n_head           = 16
0.00.092.708 I print_info: n_head_kv        = 16
0.00.092.708 I print_info: n_rot            = 32
0.00.092.708 I print_info: n_swa            = 0
0.00.092.709 I print_info: n_embd_head_k    = 128
0.00.092.709 I print_info: n_embd_head_v    = 128
0.00.092.709 I print_info: n_gqa            = 1
0.00.092.710 I print_info: n_embd_k_gqa     = 2048
0.00.092.711 I print_info: n_embd_v_gqa     = 2048
0.00.092.711 I print_info: f_norm_eps       = 1.0e-05
0.00.092.712 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.712 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.712 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.712 I print_info: f_logit_scale    = 0.0e+00
0.00.092.713 I print_info: n_ff             = 8192
0.00.092.713 I print_info: n_expert         = 0
0.00.092.713 I print_info: n_expert_used    = 0
0.00.092.713 I print_info: causal attn      = 1
0.00.092.713 I print_info: pooling type     = 0
0.00.092.713 I print_info: rope type        = 2
0.00.092.714 I print_info: rope scaling     = linear
0.00.092.714 I print_info: freq_base_train  = 10000.0
0.00.092.714 I print_info: freq_scale_train = 1
0.00.092.714 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.715 I print_info: rope_finetuned   = unknown
0.00.092.716 I print_info: ssm_d_conv       = 0
0.00.092.716 I print_info: ssm_d_inner      = 0
0.00.092.716 I print_info: ssm_d_state      = 0
0.00.092.716 I print_info: ssm_dt_rank      = 0
0.00.092.718 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.718 I print_info: model type       = 1.4B
0.00.092.718 I print_info: model params     = 1.41 B
0.00.092.719 I print_info: general.name     = 1.4B
0.00.092.719 I print_info: vocab type       = BPE
0.00.092.719 I print_info: n_vocab          = 50304
0.00.092.719 I print_info: n_merges         = 50009
0.00.092.720 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.720 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.720 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.720 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.720 I print_info: LF token         = 128 'Ä'
0.00.092.724 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.724 I print_info: max token length = 1024
0.00.095.288 I load_tensors: offloading 24 repeating layers to GPU
0.00.095.288 I load_tensors: offloading output layer to GPU
0.00.095.288 I load_tensors: offloaded 25/25 layers to GPU
0.00.095.299 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.300 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.095.591 I llama_init_from_model: n_seq_max     = 1
0.00.095.592 I llama_init_from_model: n_ctx         = 128
0.00.095.592 I llama_init_from_model: n_ctx_per_seq = 128
0.00.095.592 I llama_init_from_model: n_batch       = 128
0.00.095.592 I llama_init_from_model: n_ubatch      = 128
0.00.095.593 I llama_init_from_model: flash_attn    = 0
0.00.095.593 I llama_init_from_model: freq_base     = 10000.0
0.00.095.593 I llama_init_from_model: freq_scale    = 1
0.00.095.594 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.594 I ggml_metal_init: allocating
0.00.095.597 I ggml_metal_init: found device: Apple M4
0.00.095.599 I ggml_metal_init: picking default device: Apple M4
0.00.096.267 I ggml_metal_init: using embedded metal library
0.00.098.997 I ggml_metal_init: GPU name:   Apple M4
0.00.098.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.999 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.000 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.000 I ggml_metal_init: simdgroup reduction   = true
0.00.099.000 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.000 I ggml_metal_init: has bfloat            = true
0.00.099.000 I ggml_metal_init: use bfloat            = true
0.00.099.001 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.200 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.564 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.580 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.599 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.110.510 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.110.511 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.110.511 I llama_init_from_model: graph nodes  = 967
0.00.110.511 I llama_init_from_model: graph splits = 2
0.00.110.513 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.254.970 I 
0.01.255.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.255.022 I perplexity: tokenizing the input ..
0.01.267.198 I perplexity: tokenization took 12.173 ms
0.01.267.205 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.390.327 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.392.308 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.392.377 I llama_perf_context_print:        load time =    1230.88 ms
0.01.392.379 I llama_perf_context_print: prompt eval time =     122.20 ms /   128 tokens (    0.95 ms per token,  1047.45 tokens per second)
0.01.392.380 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.392.381 I llama_perf_context_print:       total time =     137.41 ms /   129 tokens
0.01.393.267 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.129s
sys	0m0.229s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.600 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.955 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.965 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.969 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.969 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.970 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.970 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.970 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.971 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.972 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.972 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.972 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.973 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.976 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.976 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.183 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.311 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.318 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.319 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.319 I llama_model_loader: - type  f32:  194 tensors
0.00.041.320 I llama_model_loader: - type q8_0:   98 tensors
0.00.041.321 I print_info: file format = GGUF V3 (latest)
0.00.041.321 I print_info: file type   = Q8_0
0.00.041.323 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.066.524 I load: special tokens cache size = 25
0.00.072.496 I load: token to piece cache size = 0.2984 MB
0.00.072.521 I print_info: arch             = gptneox
0.00.072.521 I print_info: vocab_only       = 0
0.00.072.522 I print_info: n_ctx_train      = 2048
0.00.072.522 I print_info: n_embd           = 2048
0.00.072.522 I print_info: n_layer          = 24
0.00.072.527 I print_info: n_head           = 16
0.00.072.528 I print_info: n_head_kv        = 16
0.00.072.529 I print_info: n_rot            = 32
0.00.072.529 I print_info: n_swa            = 0
0.00.072.529 I print_info: n_embd_head_k    = 128
0.00.072.529 I print_info: n_embd_head_v    = 128
0.00.072.530 I print_info: n_gqa            = 1
0.00.072.531 I print_info: n_embd_k_gqa     = 2048
0.00.072.531 I print_info: n_embd_v_gqa     = 2048
0.00.072.532 I print_info: f_norm_eps       = 1.0e-05
0.00.072.533 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.533 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.533 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.533 I print_info: f_logit_scale    = 0.0e+00
0.00.072.534 I print_info: n_ff             = 8192
0.00.072.535 I print_info: n_expert         = 0
0.00.072.535 I print_info: n_expert_used    = 0
0.00.072.535 I print_info: causal attn      = 1
0.00.072.535 I print_info: pooling type     = 0
0.00.072.535 I print_info: rope type        = 2
0.00.072.538 I print_info: rope scaling     = linear
0.00.072.538 I print_info: freq_base_train  = 10000.0
0.00.072.538 I print_info: freq_scale_train = 1
0.00.072.538 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.539 I print_info: rope_finetuned   = unknown
0.00.072.539 I print_info: ssm_d_conv       = 0
0.00.072.539 I print_info: ssm_d_inner      = 0
0.00.072.539 I print_info: ssm_d_state      = 0
0.00.072.539 I print_info: ssm_dt_rank      = 0
0.00.072.541 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.541 I print_info: model type       = 1.4B
0.00.072.541 I print_info: model params     = 1.41 B
0.00.072.541 I print_info: general.name     = 1.4B
0.00.072.542 I print_info: vocab type       = BPE
0.00.072.542 I print_info: n_vocab          = 50304
0.00.072.542 I print_info: n_merges         = 50009
0.00.072.543 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.543 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.543 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.543 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.544 I print_info: LF token         = 128 'Ä'
0.00.072.544 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.544 I print_info: max token length = 1024
0.00.074.968 I load_tensors: offloading 24 repeating layers to GPU
0.00.074.968 I load_tensors: offloading output layer to GPU
0.00.074.968 I load_tensors: offloaded 25/25 layers to GPU
0.00.074.980 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.074.981 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.075.382 I llama_init_from_model: n_seq_max     = 1
0.00.075.383 I llama_init_from_model: n_ctx         = 2048
0.00.075.383 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.075.384 I llama_init_from_model: n_batch       = 2048
0.00.075.384 I llama_init_from_model: n_ubatch      = 512
0.00.075.384 I llama_init_from_model: flash_attn    = 0
0.00.075.385 I llama_init_from_model: freq_base     = 10000.0
0.00.075.385 I llama_init_from_model: freq_scale    = 1
0.00.075.386 I ggml_metal_init: allocating
0.00.075.390 I ggml_metal_init: found device: Apple M4
0.00.075.393 I ggml_metal_init: picking default device: Apple M4
0.00.076.236 I ggml_metal_init: using embedded metal library
0.00.079.009 I ggml_metal_init: GPU name:   Apple M4
0.00.079.014 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.015 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.015 I ggml_metal_init: simdgroup reduction   = true
0.00.079.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.015 I ggml_metal_init: has bfloat            = true
0.00.079.015 I ggml_metal_init: use bfloat            = true
0.00.079.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.017 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.768 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.115.198 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.115.220 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.115.243 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.116.347 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.116.348 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.116.348 I llama_init_from_model: graph nodes  = 967
0.00.116.349 I llama_init_from_model: graph splits = 2
0.00.116.352 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.116.468 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.133.474 I main: llama threadpool init, n_threads = 4
0.01.133.515 I 
0.01.133.545 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.133.546 I 
0.01.133.770 I sampler seed: 1234
0.01.133.774 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.133.817 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.133.818 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.133.819 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.236.147 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49964.81 tokens per second)
0.02.236.149 I llama_perf_context_print:        load time =    1123.87 ms
0.02.236.149 I llama_perf_context_print: prompt eval time =      39.82 ms /     7 tokens (    5.69 ms per token,   175.78 tokens per second)
0.02.236.150 I llama_perf_context_print:        eval time =    1059.90 ms /    63 runs   (   16.82 ms per token,    59.44 tokens per second)
0.02.236.152 I llama_perf_context_print:       total time =    1102.68 ms /    70 tokens
0.02.236.426 I ggml_metal_free: deallocating

real	0m2.257s
user	0m0.119s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.141 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.046 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.053 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.054 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.059 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.059 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.063 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.400 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.790 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.676 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.677 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.678 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.678 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.678 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.679 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.679 I llama_model_loader: - type  f32:  194 tensors
0.00.033.680 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.680 I print_info: file format = GGUF V3 (latest)
0.00.033.681 I print_info: file type   = Q8_0
0.00.033.682 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.420 I load: special tokens cache size = 25
0.00.062.451 I load: token to piece cache size = 0.2984 MB
0.00.062.454 I print_info: arch             = gptneox
0.00.062.454 I print_info: vocab_only       = 0
0.00.062.455 I print_info: n_ctx_train      = 2048
0.00.062.455 I print_info: n_embd           = 2048
0.00.062.455 I print_info: n_layer          = 24
0.00.062.458 I print_info: n_head           = 16
0.00.062.459 I print_info: n_head_kv        = 16
0.00.062.459 I print_info: n_rot            = 32
0.00.062.459 I print_info: n_swa            = 0
0.00.062.459 I print_info: n_embd_head_k    = 128
0.00.062.460 I print_info: n_embd_head_v    = 128
0.00.062.460 I print_info: n_gqa            = 1
0.00.062.461 I print_info: n_embd_k_gqa     = 2048
0.00.062.462 I print_info: n_embd_v_gqa     = 2048
0.00.062.462 I print_info: f_norm_eps       = 1.0e-05
0.00.062.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.463 I print_info: f_logit_scale    = 0.0e+00
0.00.062.466 I print_info: n_ff             = 8192
0.00.062.466 I print_info: n_expert         = 0
0.00.062.466 I print_info: n_expert_used    = 0
0.00.062.467 I print_info: causal attn      = 1
0.00.062.467 I print_info: pooling type     = 0
0.00.062.467 I print_info: rope type        = 2
0.00.062.467 I print_info: rope scaling     = linear
0.00.062.468 I print_info: freq_base_train  = 10000.0
0.00.062.468 I print_info: freq_scale_train = 1
0.00.062.468 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.469 I print_info: rope_finetuned   = unknown
0.00.062.469 I print_info: ssm_d_conv       = 0
0.00.062.469 I print_info: ssm_d_inner      = 0
0.00.062.469 I print_info: ssm_d_state      = 0
0.00.062.469 I print_info: ssm_dt_rank      = 0
0.00.062.469 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.469 I print_info: model type       = 1.4B
0.00.062.470 I print_info: model params     = 1.41 B
0.00.062.470 I print_info: general.name     = 1.4B
0.00.062.470 I print_info: vocab type       = BPE
0.00.062.471 I print_info: n_vocab          = 50304
0.00.062.471 I print_info: n_merges         = 50009
0.00.062.471 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.471 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.471 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.472 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.472 I print_info: LF token         = 128 'Ä'
0.00.062.472 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.472 I print_info: max token length = 1024
0.00.064.640 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.641 I load_tensors: offloading output layer to GPU
0.00.064.641 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.651 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.653 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.994 I llama_init_from_model: n_seq_max     = 1
0.00.064.995 I llama_init_from_model: n_ctx         = 128
0.00.064.995 I llama_init_from_model: n_ctx_per_seq = 128
0.00.064.995 I llama_init_from_model: n_batch       = 128
0.00.064.996 I llama_init_from_model: n_ubatch      = 128
0.00.064.996 I llama_init_from_model: flash_attn    = 0
0.00.064.996 I llama_init_from_model: freq_base     = 10000.0
0.00.064.996 I llama_init_from_model: freq_scale    = 1
0.00.064.997 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.997 I ggml_metal_init: allocating
0.00.065.000 I ggml_metal_init: found device: Apple M4
0.00.065.002 I ggml_metal_init: picking default device: Apple M4
0.00.065.598 I ggml_metal_init: using embedded metal library
0.00.067.974 I ggml_metal_init: GPU name:   Apple M4
0.00.067.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.976 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.976 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.976 I ggml_metal_init: simdgroup reduction   = true
0.00.067.977 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.977 I ggml_metal_init: has bfloat            = true
0.00.067.977 I ggml_metal_init: use bfloat            = true
0.00.067.977 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.978 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.137 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.078.457 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.470 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.486 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.079.419 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.079.420 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.079.420 I llama_init_from_model: graph nodes  = 967
0.00.079.421 I llama_init_from_model: graph splits = 2
0.00.079.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.079.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.791.995 I 
0.00.792.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.030 I perplexity: tokenizing the input ..
0.00.799.923 I perplexity: tokenization took 7.891 ms
0.00.799.929 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.924.577 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.925.733 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.925.761 I llama_perf_context_print:        load time =     779.95 ms
0.00.925.762 I llama_perf_context_print: prompt eval time =     124.42 ms /   128 tokens (    0.97 ms per token,  1028.77 tokens per second)
0.00.925.762 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.925.763 I llama_perf_context_print:       total time =     133.77 ms /   129 tokens
0.00.926.111 I ggml_metal_free: deallocating

real	0m0.946s
user	0m0.090s
sys	0m0.134s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.064 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.119 I main: llama backend init
0.00.000.122 I main: load the model and apply lora adapter, if any
0.00.016.955 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.608 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.615 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.617 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.618 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.622 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.623 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.623 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.624 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.497 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.500 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.501 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.501 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.502 I llama_model_loader: - type  f32:  194 tensors
0.00.042.503 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.504 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.505 I print_info: file format = GGUF V3 (latest)
0.00.042.505 I print_info: file type   = Q4_0
0.00.042.507 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.107 I load: special tokens cache size = 25
0.00.068.221 I load: token to piece cache size = 0.2984 MB
0.00.068.226 I print_info: arch             = gptneox
0.00.068.226 I print_info: vocab_only       = 0
0.00.068.226 I print_info: n_ctx_train      = 2048
0.00.068.226 I print_info: n_embd           = 2048
0.00.068.227 I print_info: n_layer          = 24
0.00.068.231 I print_info: n_head           = 16
0.00.068.232 I print_info: n_head_kv        = 16
0.00.068.232 I print_info: n_rot            = 32
0.00.068.232 I print_info: n_swa            = 0
0.00.068.232 I print_info: n_embd_head_k    = 128
0.00.068.232 I print_info: n_embd_head_v    = 128
0.00.068.235 I print_info: n_gqa            = 1
0.00.068.235 I print_info: n_embd_k_gqa     = 2048
0.00.068.236 I print_info: n_embd_v_gqa     = 2048
0.00.068.236 I print_info: f_norm_eps       = 1.0e-05
0.00.068.237 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.237 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.238 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.240 I print_info: f_logit_scale    = 0.0e+00
0.00.068.241 I print_info: n_ff             = 8192
0.00.068.241 I print_info: n_expert         = 0
0.00.068.241 I print_info: n_expert_used    = 0
0.00.068.241 I print_info: causal attn      = 1
0.00.068.241 I print_info: pooling type     = 0
0.00.068.241 I print_info: rope type        = 2
0.00.068.241 I print_info: rope scaling     = linear
0.00.068.242 I print_info: freq_base_train  = 10000.0
0.00.068.242 I print_info: freq_scale_train = 1
0.00.068.242 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.242 I print_info: rope_finetuned   = unknown
0.00.068.243 I print_info: ssm_d_conv       = 0
0.00.068.243 I print_info: ssm_d_inner      = 0
0.00.068.243 I print_info: ssm_d_state      = 0
0.00.068.243 I print_info: ssm_dt_rank      = 0
0.00.068.243 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.243 I print_info: model type       = 1.4B
0.00.068.244 I print_info: model params     = 1.41 B
0.00.068.244 I print_info: general.name     = 1.4B
0.00.068.245 I print_info: vocab type       = BPE
0.00.068.245 I print_info: n_vocab          = 50304
0.00.068.245 I print_info: n_merges         = 50009
0.00.068.245 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.245 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.245 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.246 I print_info: LF token         = 128 'Ä'
0.00.068.246 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.246 I print_info: max token length = 1024
0.00.070.369 I load_tensors: offloading 24 repeating layers to GPU
0.00.070.370 I load_tensors: offloading output layer to GPU
0.00.070.370 I load_tensors: offloaded 25/25 layers to GPU
0.00.070.381 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.070.381 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.070.669 I llama_init_from_model: n_seq_max     = 1
0.00.070.670 I llama_init_from_model: n_ctx         = 2048
0.00.070.670 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.070.671 I llama_init_from_model: n_batch       = 2048
0.00.070.671 I llama_init_from_model: n_ubatch      = 512
0.00.070.671 I llama_init_from_model: flash_attn    = 0
0.00.070.671 I llama_init_from_model: freq_base     = 10000.0
0.00.070.672 I llama_init_from_model: freq_scale    = 1
0.00.070.673 I ggml_metal_init: allocating
0.00.070.676 I ggml_metal_init: found device: Apple M4
0.00.070.678 I ggml_metal_init: picking default device: Apple M4
0.00.071.403 I ggml_metal_init: using embedded metal library
0.00.073.912 I ggml_metal_init: GPU name:   Apple M4
0.00.073.914 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.914 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.915 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.915 I ggml_metal_init: simdgroup reduction   = true
0.00.073.915 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.915 I ggml_metal_init: has bfloat            = true
0.00.073.915 I ggml_metal_init: use bfloat            = true
0.00.073.916 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.419 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.390 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.107.412 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.433 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.340 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.108.341 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.108.342 I llama_init_from_model: graph nodes  = 967
0.00.108.342 I llama_init_from_model: graph splits = 2
0.00.108.345 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.108.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.984.414 I main: llama threadpool init, n_threads = 4
0.00.984.504 I 
0.00.984.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.984.575 I 
0.00.985.084 I sampler seed: 1234
0.00.985.096 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.985.169 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.985.173 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.985.174 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.672.991 I llama_perf_sampler_print:    sampling time =       1.55 ms /    71 runs   (    0.02 ms per token, 45776.92 tokens per second)
0.01.672.992 I llama_perf_context_print:        load time =     967.45 ms
0.01.672.993 I llama_perf_context_print: prompt eval time =      50.97 ms /     7 tokens (    7.28 ms per token,   137.35 tokens per second)
0.01.672.994 I llama_perf_context_print:        eval time =     633.50 ms /    63 runs   (   10.06 ms per token,    99.45 tokens per second)
0.01.672.994 I llama_perf_context_print:       total time =     688.59 ms /    70 tokens
0.01.673.183 I ggml_metal_free: deallocating

real	0m1.706s
user	0m0.125s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.511 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.635 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.640 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.641 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.641 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.642 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.642 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.642 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.643 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.644 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.644 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.644 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.646 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.646 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.647 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.650 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.651 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.494 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.457 I llama_model_loader: - type  f32:  194 tensors
0.00.026.457 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.458 I print_info: file format = GGUF V3 (latest)
0.00.026.458 I print_info: file type   = Q4_0
0.00.026.459 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.471 I load: special tokens cache size = 25
0.00.051.229 I load: token to piece cache size = 0.2984 MB
0.00.051.232 I print_info: arch             = gptneox
0.00.051.232 I print_info: vocab_only       = 0
0.00.051.232 I print_info: n_ctx_train      = 2048
0.00.051.233 I print_info: n_embd           = 2048
0.00.051.233 I print_info: n_layer          = 24
0.00.051.236 I print_info: n_head           = 16
0.00.051.237 I print_info: n_head_kv        = 16
0.00.051.237 I print_info: n_rot            = 32
0.00.051.237 I print_info: n_swa            = 0
0.00.051.237 I print_info: n_embd_head_k    = 128
0.00.051.237 I print_info: n_embd_head_v    = 128
0.00.051.238 I print_info: n_gqa            = 1
0.00.051.239 I print_info: n_embd_k_gqa     = 2048
0.00.051.240 I print_info: n_embd_v_gqa     = 2048
0.00.051.240 I print_info: f_norm_eps       = 1.0e-05
0.00.051.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.243 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.243 I print_info: f_logit_scale    = 0.0e+00
0.00.051.244 I print_info: n_ff             = 8192
0.00.051.244 I print_info: n_expert         = 0
0.00.051.244 I print_info: n_expert_used    = 0
0.00.051.244 I print_info: causal attn      = 1
0.00.051.244 I print_info: pooling type     = 0
0.00.051.244 I print_info: rope type        = 2
0.00.051.245 I print_info: rope scaling     = linear
0.00.051.245 I print_info: freq_base_train  = 10000.0
0.00.051.245 I print_info: freq_scale_train = 1
0.00.051.246 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.246 I print_info: rope_finetuned   = unknown
0.00.051.246 I print_info: ssm_d_conv       = 0
0.00.051.246 I print_info: ssm_d_inner      = 0
0.00.051.246 I print_info: ssm_d_state      = 0
0.00.051.248 I print_info: ssm_dt_rank      = 0
0.00.051.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.249 I print_info: model type       = 1.4B
0.00.051.249 I print_info: model params     = 1.41 B
0.00.051.249 I print_info: general.name     = 1.4B
0.00.051.250 I print_info: vocab type       = BPE
0.00.051.250 I print_info: n_vocab          = 50304
0.00.051.250 I print_info: n_merges         = 50009
0.00.051.251 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.251 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.255 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.255 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.255 I print_info: LF token         = 128 'Ä'
0.00.051.255 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.255 I print_info: max token length = 1024
0.00.053.178 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.179 I load_tensors: offloading output layer to GPU
0.00.053.179 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.190 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.191 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.475 I llama_init_from_model: n_seq_max     = 1
0.00.053.476 I llama_init_from_model: n_ctx         = 128
0.00.053.477 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.477 I llama_init_from_model: n_batch       = 128
0.00.053.477 I llama_init_from_model: n_ubatch      = 128
0.00.053.477 I llama_init_from_model: flash_attn    = 0
0.00.053.478 I llama_init_from_model: freq_base     = 10000.0
0.00.053.478 I llama_init_from_model: freq_scale    = 1
0.00.053.478 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.479 I ggml_metal_init: allocating
0.00.053.482 I ggml_metal_init: found device: Apple M4
0.00.053.484 I ggml_metal_init: picking default device: Apple M4
0.00.054.037 I ggml_metal_init: using embedded metal library
0.00.056.389 I ggml_metal_init: GPU name:   Apple M4
0.00.056.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.391 I ggml_metal_init: simdgroup reduction   = true
0.00.056.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.392 I ggml_metal_init: has bfloat            = true
0.00.056.392 I ggml_metal_init: use bfloat            = true
0.00.056.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.385 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.646 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.660 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.678 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.549 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.550 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.550 I llama_init_from_model: graph nodes  = 967
0.00.068.550 I llama_init_from_model: graph splits = 2
0.00.068.551 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.552 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.472 I 
0.00.589.511 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.516 I perplexity: tokenizing the input ..
0.00.597.565 I perplexity: tokenization took 8.047 ms
0.00.597.569 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.030 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.721.425 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.721.464 I llama_perf_context_print:        load time =     578.96 ms
0.00.721.465 I llama_perf_context_print: prompt eval time =     122.21 ms /   128 tokens (    0.95 ms per token,  1047.34 tokens per second)
0.00.721.466 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.721.466 I llama_perf_context_print:       total time =     131.99 ms /   129 tokens
0.00.721.985 I ggml_metal_free: deallocating

real	0m0.738s
user	0m0.076s
sys	0m0.097s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.725 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.436 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.439 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.439 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.439 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.440 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.440 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.444 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.291 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.343 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.134 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.136 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.136 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.137 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.137 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.137 I llama_model_loader: - type  f32:  194 tensors
0.00.033.138 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.138 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.139 I print_info: file format = GGUF V3 (latest)
0.00.033.139 I print_info: file type   = Q4_1
0.00.033.140 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.054.185 I load: special tokens cache size = 25
0.00.060.283 I load: token to piece cache size = 0.2984 MB
0.00.060.286 I print_info: arch             = gptneox
0.00.060.286 I print_info: vocab_only       = 0
0.00.060.286 I print_info: n_ctx_train      = 2048
0.00.060.286 I print_info: n_embd           = 2048
0.00.060.287 I print_info: n_layer          = 24
0.00.060.290 I print_info: n_head           = 16
0.00.060.290 I print_info: n_head_kv        = 16
0.00.060.291 I print_info: n_rot            = 32
0.00.060.291 I print_info: n_swa            = 0
0.00.060.291 I print_info: n_embd_head_k    = 128
0.00.060.291 I print_info: n_embd_head_v    = 128
0.00.060.292 I print_info: n_gqa            = 1
0.00.060.293 I print_info: n_embd_k_gqa     = 2048
0.00.060.294 I print_info: n_embd_v_gqa     = 2048
0.00.060.294 I print_info: f_norm_eps       = 1.0e-05
0.00.060.294 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.295 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.295 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.295 I print_info: f_logit_scale    = 0.0e+00
0.00.060.296 I print_info: n_ff             = 8192
0.00.060.296 I print_info: n_expert         = 0
0.00.060.296 I print_info: n_expert_used    = 0
0.00.060.296 I print_info: causal attn      = 1
0.00.060.296 I print_info: pooling type     = 0
0.00.060.298 I print_info: rope type        = 2
0.00.060.300 I print_info: rope scaling     = linear
0.00.060.300 I print_info: freq_base_train  = 10000.0
0.00.060.300 I print_info: freq_scale_train = 1
0.00.060.301 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.301 I print_info: rope_finetuned   = unknown
0.00.060.301 I print_info: ssm_d_conv       = 0
0.00.060.301 I print_info: ssm_d_inner      = 0
0.00.060.302 I print_info: ssm_d_state      = 0
0.00.060.302 I print_info: ssm_dt_rank      = 0
0.00.060.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.302 I print_info: model type       = 1.4B
0.00.060.302 I print_info: model params     = 1.41 B
0.00.060.303 I print_info: general.name     = 1.4B
0.00.060.303 I print_info: vocab type       = BPE
0.00.060.303 I print_info: n_vocab          = 50304
0.00.060.303 I print_info: n_merges         = 50009
0.00.060.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.305 I print_info: LF token         = 128 'Ä'
0.00.060.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.306 I print_info: max token length = 1024
0.00.062.258 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.258 I load_tensors: offloading output layer to GPU
0.00.062.258 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.269 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.062.270 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.062.551 I llama_init_from_model: n_seq_max     = 1
0.00.062.552 I llama_init_from_model: n_ctx         = 2048
0.00.062.552 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.552 I llama_init_from_model: n_batch       = 2048
0.00.062.553 I llama_init_from_model: n_ubatch      = 512
0.00.062.553 I llama_init_from_model: flash_attn    = 0
0.00.062.553 I llama_init_from_model: freq_base     = 10000.0
0.00.062.553 I llama_init_from_model: freq_scale    = 1
0.00.062.554 I ggml_metal_init: allocating
0.00.062.557 I ggml_metal_init: found device: Apple M4
0.00.062.559 I ggml_metal_init: picking default device: Apple M4
0.00.063.160 I ggml_metal_init: using embedded metal library
0.00.065.510 I ggml_metal_init: GPU name:   Apple M4
0.00.065.512 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.513 I ggml_metal_init: simdgroup reduction   = true
0.00.065.513 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.513 I ggml_metal_init: has bfloat            = true
0.00.065.513 I ggml_metal_init: use bfloat            = true
0.00.065.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.236 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.983 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.001 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.020 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.096.134 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.096.136 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.096.136 I llama_init_from_model: graph nodes  = 967
0.00.096.136 I llama_init_from_model: graph splits = 2
0.00.096.139 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.628 I main: llama threadpool init, n_threads = 4
0.00.716.672 I 
0.00.716.710 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.711 I 
0.00.716.943 I sampler seed: 1234
0.00.716.948 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.716.982 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.716.993 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.716.993 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.437.457 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63620.07 tokens per second)
0.01.437.459 I llama_perf_context_print:        load time =     707.89 ms
0.01.437.459 I llama_perf_context_print: prompt eval time =      41.44 ms /     7 tokens (    5.92 ms per token,   168.92 tokens per second)
0.01.437.460 I llama_perf_context_print:        eval time =     676.13 ms /    63 runs   (   10.73 ms per token,    93.18 tokens per second)
0.01.437.460 I llama_perf_context_print:       total time =     720.84 ms /    70 tokens
0.01.437.654 I ggml_metal_free: deallocating

real	0m1.456s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.133 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.276 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.276 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.281 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.282 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.283 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.286 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.286 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.287 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.287 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.010 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.725 I llama_model_loader: - type  f32:  194 tensors
0.00.024.726 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.726 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.726 I print_info: file format = GGUF V3 (latest)
0.00.024.727 I print_info: file type   = Q4_1
0.00.024.728 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.122 I load: special tokens cache size = 25
0.00.049.115 I load: token to piece cache size = 0.2984 MB
0.00.049.118 I print_info: arch             = gptneox
0.00.049.119 I print_info: vocab_only       = 0
0.00.049.119 I print_info: n_ctx_train      = 2048
0.00.049.119 I print_info: n_embd           = 2048
0.00.049.119 I print_info: n_layer          = 24
0.00.049.122 I print_info: n_head           = 16
0.00.049.123 I print_info: n_head_kv        = 16
0.00.049.123 I print_info: n_rot            = 32
0.00.049.123 I print_info: n_swa            = 0
0.00.049.125 I print_info: n_embd_head_k    = 128
0.00.049.125 I print_info: n_embd_head_v    = 128
0.00.049.126 I print_info: n_gqa            = 1
0.00.049.126 I print_info: n_embd_k_gqa     = 2048
0.00.049.127 I print_info: n_embd_v_gqa     = 2048
0.00.049.128 I print_info: f_norm_eps       = 1.0e-05
0.00.049.128 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.128 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.128 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.128 I print_info: f_logit_scale    = 0.0e+00
0.00.049.129 I print_info: n_ff             = 8192
0.00.049.129 I print_info: n_expert         = 0
0.00.049.130 I print_info: n_expert_used    = 0
0.00.049.130 I print_info: causal attn      = 1
0.00.049.130 I print_info: pooling type     = 0
0.00.049.130 I print_info: rope type        = 2
0.00.049.130 I print_info: rope scaling     = linear
0.00.049.132 I print_info: freq_base_train  = 10000.0
0.00.049.133 I print_info: freq_scale_train = 1
0.00.049.133 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.134 I print_info: rope_finetuned   = unknown
0.00.049.134 I print_info: ssm_d_conv       = 0
0.00.049.134 I print_info: ssm_d_inner      = 0
0.00.049.134 I print_info: ssm_d_state      = 0
0.00.049.134 I print_info: ssm_dt_rank      = 0
0.00.049.134 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.135 I print_info: model type       = 1.4B
0.00.049.135 I print_info: model params     = 1.41 B
0.00.049.135 I print_info: general.name     = 1.4B
0.00.049.136 I print_info: vocab type       = BPE
0.00.049.136 I print_info: n_vocab          = 50304
0.00.049.136 I print_info: n_merges         = 50009
0.00.049.136 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.140 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.140 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.141 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.142 I print_info: LF token         = 128 'Ä'
0.00.049.142 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.142 I print_info: max token length = 1024
0.00.050.868 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.868 I load_tensors: offloading output layer to GPU
0.00.050.868 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.874 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.874 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.301 I llama_init_from_model: n_seq_max     = 1
0.00.051.302 I llama_init_from_model: n_ctx         = 128
0.00.051.302 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.302 I llama_init_from_model: n_batch       = 128
0.00.051.302 I llama_init_from_model: n_ubatch      = 128
0.00.051.302 I llama_init_from_model: flash_attn    = 0
0.00.051.303 I llama_init_from_model: freq_base     = 10000.0
0.00.051.303 I llama_init_from_model: freq_scale    = 1
0.00.051.304 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.304 I ggml_metal_init: allocating
0.00.051.306 I ggml_metal_init: found device: Apple M4
0.00.051.308 I ggml_metal_init: picking default device: Apple M4
0.00.051.880 I ggml_metal_init: using embedded metal library
0.00.054.240 I ggml_metal_init: GPU name:   Apple M4
0.00.054.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.242 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.242 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.242 I ggml_metal_init: simdgroup reduction   = true
0.00.054.243 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.243 I ggml_metal_init: has bfloat            = true
0.00.054.243 I ggml_metal_init: use bfloat            = true
0.00.054.243 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.709 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.967 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.981 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.999 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.858 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.859 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.859 I llama_init_from_model: graph nodes  = 967
0.00.066.859 I llama_init_from_model: graph splits = 2
0.00.066.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.860 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.585 I 
0.00.657.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.646 I perplexity: tokenizing the input ..
0.00.665.205 I perplexity: tokenization took 7.557 ms
0.00.665.209 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.003 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.788.402 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.788.425 I llama_perf_context_print:        load time =     648.45 ms
0.00.788.426 I llama_perf_context_print: prompt eval time =     121.56 ms /   128 tokens (    0.95 ms per token,  1052.95 tokens per second)
0.00.788.427 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.427 I llama_perf_context_print:       total time =     130.84 ms /   129 tokens
0.00.788.766 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.076s
sys	0m0.078s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.017.126 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.472 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.024.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.478 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.478 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.479 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.479 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.479 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.481 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.481 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.035 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.036 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.037 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.037 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.033.038 I llama_model_loader: - type  f32:  194 tensors
0.00.033.038 I llama_model_loader: - type q5_0:   97 tensors
0.00.033.038 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.039 I print_info: file format = GGUF V3 (latest)
0.00.033.039 I print_info: file type   = Q5_0
0.00.033.041 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.054.624 I load: special tokens cache size = 25
0.00.060.545 I load: token to piece cache size = 0.2984 MB
0.00.060.548 I print_info: arch             = gptneox
0.00.060.548 I print_info: vocab_only       = 0
0.00.060.549 I print_info: n_ctx_train      = 2048
0.00.060.549 I print_info: n_embd           = 2048
0.00.060.549 I print_info: n_layer          = 24
0.00.060.552 I print_info: n_head           = 16
0.00.060.553 I print_info: n_head_kv        = 16
0.00.060.553 I print_info: n_rot            = 32
0.00.060.553 I print_info: n_swa            = 0
0.00.060.554 I print_info: n_embd_head_k    = 128
0.00.060.554 I print_info: n_embd_head_v    = 128
0.00.060.555 I print_info: n_gqa            = 1
0.00.060.555 I print_info: n_embd_k_gqa     = 2048
0.00.060.556 I print_info: n_embd_v_gqa     = 2048
0.00.060.557 I print_info: f_norm_eps       = 1.0e-05
0.00.060.557 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.557 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.558 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.558 I print_info: f_logit_scale    = 0.0e+00
0.00.060.559 I print_info: n_ff             = 8192
0.00.060.560 I print_info: n_expert         = 0
0.00.060.560 I print_info: n_expert_used    = 0
0.00.060.560 I print_info: causal attn      = 1
0.00.060.560 I print_info: pooling type     = 0
0.00.060.560 I print_info: rope type        = 2
0.00.060.563 I print_info: rope scaling     = linear
0.00.060.563 I print_info: freq_base_train  = 10000.0
0.00.060.563 I print_info: freq_scale_train = 1
0.00.060.563 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.564 I print_info: rope_finetuned   = unknown
0.00.060.564 I print_info: ssm_d_conv       = 0
0.00.060.564 I print_info: ssm_d_inner      = 0
0.00.060.564 I print_info: ssm_d_state      = 0
0.00.060.564 I print_info: ssm_dt_rank      = 0
0.00.060.564 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.565 I print_info: model type       = 1.4B
0.00.060.565 I print_info: model params     = 1.41 B
0.00.060.565 I print_info: general.name     = 1.4B
0.00.060.566 I print_info: vocab type       = BPE
0.00.060.566 I print_info: n_vocab          = 50304
0.00.060.566 I print_info: n_merges         = 50009
0.00.060.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.571 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.571 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.571 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.571 I print_info: LF token         = 128 'Ä'
0.00.060.571 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.571 I print_info: max token length = 1024
0.00.062.541 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.541 I load_tensors: offloading output layer to GPU
0.00.062.541 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.552 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.062.553 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.062.824 I llama_init_from_model: n_seq_max     = 1
0.00.062.825 I llama_init_from_model: n_ctx         = 2048
0.00.062.825 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.825 I llama_init_from_model: n_batch       = 2048
0.00.062.825 I llama_init_from_model: n_ubatch      = 512
0.00.062.825 I llama_init_from_model: flash_attn    = 0
0.00.062.826 I llama_init_from_model: freq_base     = 10000.0
0.00.062.826 I llama_init_from_model: freq_scale    = 1
0.00.062.827 I ggml_metal_init: allocating
0.00.062.830 I ggml_metal_init: found device: Apple M4
0.00.062.832 I ggml_metal_init: picking default device: Apple M4
0.00.063.429 I ggml_metal_init: using embedded metal library
0.00.065.922 I ggml_metal_init: GPU name:   Apple M4
0.00.065.923 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.924 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.924 I ggml_metal_init: simdgroup reduction   = true
0.00.065.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.924 I ggml_metal_init: has bfloat            = true
0.00.065.924 I ggml_metal_init: use bfloat            = true
0.00.065.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.263 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.433 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.453 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.472 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.097.600 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.097.602 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.097.602 I llama_init_from_model: graph nodes  = 967
0.00.097.603 I llama_init_from_model: graph splits = 2
0.00.097.605 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.751 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.427 I main: llama threadpool init, n_threads = 4
0.00.800.474 I 
0.00.800.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.506 I 
0.00.800.739 I sampler seed: 1234
0.00.800.745 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.780 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.792 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.792 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.586.638 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.586.639 I llama_perf_context_print:        load time =     783.29 ms
0.01.586.639 I llama_perf_context_print: prompt eval time =      46.86 ms /     7 tokens (    6.69 ms per token,   149.37 tokens per second)
0.01.586.640 I llama_perf_context_print:        eval time =     736.11 ms /    63 runs   (   11.68 ms per token,    85.58 tokens per second)
0.01.586.640 I llama_perf_context_print:       total time =     786.22 ms /    70 tokens
0.01.586.825 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.113s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.372 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.142 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.155 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.156 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.156 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.156 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.157 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.158 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.158 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.159 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.909 I llama_model_loader: - type  f32:  194 tensors
0.00.025.910 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.910 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.911 I print_info: file format = GGUF V3 (latest)
0.00.025.911 I print_info: file type   = Q5_0
0.00.025.912 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.830 I load: special tokens cache size = 25
0.00.051.876 I load: token to piece cache size = 0.2984 MB
0.00.051.881 I print_info: arch             = gptneox
0.00.051.882 I print_info: vocab_only       = 0
0.00.051.882 I print_info: n_ctx_train      = 2048
0.00.051.882 I print_info: n_embd           = 2048
0.00.051.882 I print_info: n_layer          = 24
0.00.051.887 I print_info: n_head           = 16
0.00.051.888 I print_info: n_head_kv        = 16
0.00.051.888 I print_info: n_rot            = 32
0.00.051.890 I print_info: n_swa            = 0
0.00.051.891 I print_info: n_embd_head_k    = 128
0.00.051.891 I print_info: n_embd_head_v    = 128
0.00.051.891 I print_info: n_gqa            = 1
0.00.051.892 I print_info: n_embd_k_gqa     = 2048
0.00.051.893 I print_info: n_embd_v_gqa     = 2048
0.00.051.894 I print_info: f_norm_eps       = 1.0e-05
0.00.051.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.895 I print_info: f_logit_scale    = 0.0e+00
0.00.051.895 I print_info: n_ff             = 8192
0.00.051.895 I print_info: n_expert         = 0
0.00.051.895 I print_info: n_expert_used    = 0
0.00.051.896 I print_info: causal attn      = 1
0.00.051.896 I print_info: pooling type     = 0
0.00.051.896 I print_info: rope type        = 2
0.00.051.896 I print_info: rope scaling     = linear
0.00.051.897 I print_info: freq_base_train  = 10000.0
0.00.051.897 I print_info: freq_scale_train = 1
0.00.051.897 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.898 I print_info: rope_finetuned   = unknown
0.00.051.898 I print_info: ssm_d_conv       = 0
0.00.051.898 I print_info: ssm_d_inner      = 0
0.00.051.899 I print_info: ssm_d_state      = 0
0.00.051.899 I print_info: ssm_dt_rank      = 0
0.00.051.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.899 I print_info: model type       = 1.4B
0.00.051.900 I print_info: model params     = 1.41 B
0.00.051.900 I print_info: general.name     = 1.4B
0.00.051.900 I print_info: vocab type       = BPE
0.00.051.901 I print_info: n_vocab          = 50304
0.00.051.901 I print_info: n_merges         = 50009
0.00.051.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.902 I print_info: LF token         = 128 'Ä'
0.00.051.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.902 I print_info: max token length = 1024
0.00.053.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.602 I load_tensors: offloading output layer to GPU
0.00.053.602 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.612 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.613 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.886 I llama_init_from_model: n_seq_max     = 1
0.00.053.887 I llama_init_from_model: n_ctx         = 128
0.00.053.887 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.888 I llama_init_from_model: n_batch       = 128
0.00.053.888 I llama_init_from_model: n_ubatch      = 128
0.00.053.888 I llama_init_from_model: flash_attn    = 0
0.00.053.888 I llama_init_from_model: freq_base     = 10000.0
0.00.053.889 I llama_init_from_model: freq_scale    = 1
0.00.053.889 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.890 I ggml_metal_init: allocating
0.00.053.893 I ggml_metal_init: found device: Apple M4
0.00.053.895 I ggml_metal_init: picking default device: Apple M4
0.00.054.532 I ggml_metal_init: using embedded metal library
0.00.056.898 I ggml_metal_init: GPU name:   Apple M4
0.00.056.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.901 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.901 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.901 I ggml_metal_init: simdgroup reduction   = true
0.00.056.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.902 I ggml_metal_init: has bfloat            = true
0.00.056.902 I ggml_metal_init: use bfloat            = true
0.00.056.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.903 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.259 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.541 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.557 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.575 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.424 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.425 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.425 I llama_init_from_model: graph nodes  = 967
0.00.069.426 I llama_init_from_model: graph splits = 2
0.00.069.427 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.364 I 
0.00.730.440 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.452 I perplexity: tokenizing the input ..
0.00.738.541 I perplexity: tokenization took 8.087 ms
0.00.738.545 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.872.573 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.873.989 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.874.013 I llama_perf_context_print:        load time =     720.98 ms
0.00.874.014 I llama_perf_context_print: prompt eval time =     133.79 ms /   128 tokens (    1.05 ms per token,   956.71 tokens per second)
0.00.874.014 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.874.015 I llama_perf_context_print:       total time =     143.65 ms /   129 tokens
0.00.874.323 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.080s
sys	0m0.126s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.747 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.478 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.485 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.485 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.485 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.487 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.488 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.488 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.489 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.491 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.491 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.255 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.025 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.026 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.027 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.027 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.028 I llama_model_loader: - type  f32:  194 tensors
0.00.025.028 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.028 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.029 I print_info: file format = GGUF V3 (latest)
0.00.025.029 I print_info: file type   = Q5_1
0.00.025.030 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.261 I load: special tokens cache size = 25
0.00.050.218 I load: token to piece cache size = 0.2984 MB
0.00.050.221 I print_info: arch             = gptneox
0.00.050.221 I print_info: vocab_only       = 0
0.00.050.222 I print_info: n_ctx_train      = 2048
0.00.050.222 I print_info: n_embd           = 2048
0.00.050.222 I print_info: n_layer          = 24
0.00.050.225 I print_info: n_head           = 16
0.00.050.225 I print_info: n_head_kv        = 16
0.00.050.226 I print_info: n_rot            = 32
0.00.050.226 I print_info: n_swa            = 0
0.00.050.226 I print_info: n_embd_head_k    = 128
0.00.050.226 I print_info: n_embd_head_v    = 128
0.00.050.227 I print_info: n_gqa            = 1
0.00.050.228 I print_info: n_embd_k_gqa     = 2048
0.00.050.229 I print_info: n_embd_v_gqa     = 2048
0.00.050.229 I print_info: f_norm_eps       = 1.0e-05
0.00.050.229 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.230 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.230 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.230 I print_info: f_logit_scale    = 0.0e+00
0.00.050.231 I print_info: n_ff             = 8192
0.00.050.231 I print_info: n_expert         = 0
0.00.050.231 I print_info: n_expert_used    = 0
0.00.050.231 I print_info: causal attn      = 1
0.00.050.231 I print_info: pooling type     = 0
0.00.050.231 I print_info: rope type        = 2
0.00.050.233 I print_info: rope scaling     = linear
0.00.050.234 I print_info: freq_base_train  = 10000.0
0.00.050.234 I print_info: freq_scale_train = 1
0.00.050.234 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.235 I print_info: rope_finetuned   = unknown
0.00.050.235 I print_info: ssm_d_conv       = 0
0.00.050.235 I print_info: ssm_d_inner      = 0
0.00.050.235 I print_info: ssm_d_state      = 0
0.00.050.235 I print_info: ssm_dt_rank      = 0
0.00.050.235 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.236 I print_info: model type       = 1.4B
0.00.050.236 I print_info: model params     = 1.41 B
0.00.050.236 I print_info: general.name     = 1.4B
0.00.050.237 I print_info: vocab type       = BPE
0.00.050.237 I print_info: n_vocab          = 50304
0.00.050.237 I print_info: n_merges         = 50009
0.00.050.238 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.238 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.238 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.240 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.240 I print_info: LF token         = 128 'Ä'
0.00.050.240 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.240 I print_info: max token length = 1024
0.00.052.002 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.002 I load_tensors: offloading output layer to GPU
0.00.052.002 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.008 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.008 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.265 I llama_init_from_model: n_seq_max     = 1
0.00.052.266 I llama_init_from_model: n_ctx         = 2048
0.00.052.266 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.266 I llama_init_from_model: n_batch       = 2048
0.00.052.266 I llama_init_from_model: n_ubatch      = 512
0.00.052.266 I llama_init_from_model: flash_attn    = 0
0.00.052.267 I llama_init_from_model: freq_base     = 10000.0
0.00.052.267 I llama_init_from_model: freq_scale    = 1
0.00.052.267 I ggml_metal_init: allocating
0.00.052.271 I ggml_metal_init: found device: Apple M4
0.00.052.272 I ggml_metal_init: picking default device: Apple M4
0.00.052.852 I ggml_metal_init: using embedded metal library
0.00.055.198 I ggml_metal_init: GPU name:   Apple M4
0.00.055.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.201 I ggml_metal_init: simdgroup reduction   = true
0.00.055.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.201 I ggml_metal_init: has bfloat            = true
0.00.055.201 I ggml_metal_init: use bfloat            = true
0.00.055.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.794 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.697 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.713 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.736 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.849 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.850 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.851 I llama_init_from_model: graph nodes  = 967
0.00.086.851 I llama_init_from_model: graph splits = 2
0.00.086.854 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.994 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.995 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.584 I main: llama threadpool init, n_threads = 4
0.00.933.636 I 
0.00.933.672 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.673 I 
0.00.933.916 I sampler seed: 1234
0.00.933.922 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.933.961 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.933.966 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.933.966 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.772.657 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.772.657 I llama_perf_context_print:        load time =     924.83 ms
0.01.772.658 I llama_perf_context_print: prompt eval time =      42.24 ms /     7 tokens (    6.03 ms per token,   165.73 tokens per second)
0.01.772.659 I llama_perf_context_print:        eval time =     793.47 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.772.659 I llama_perf_context_print:       total time =     839.08 ms /    70 tokens
0.01.772.921 I ggml_metal_free: deallocating

real	0m1.793s
user	0m0.110s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.363 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.010 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.014 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.016 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.016 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.016 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.017 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.017 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.019 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.019 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.020 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.020 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.022 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.023 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.023 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.835 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.837 I llama_model_loader: - type  f32:  194 tensors
0.00.027.837 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.837 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.838 I print_info: file format = GGUF V3 (latest)
0.00.027.839 I print_info: file type   = Q5_1
0.00.027.840 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.048.020 I load: special tokens cache size = 25
0.00.054.201 I load: token to piece cache size = 0.2984 MB
0.00.054.206 I print_info: arch             = gptneox
0.00.054.207 I print_info: vocab_only       = 0
0.00.054.207 I print_info: n_ctx_train      = 2048
0.00.054.207 I print_info: n_embd           = 2048
0.00.054.207 I print_info: n_layer          = 24
0.00.054.211 I print_info: n_head           = 16
0.00.054.223 I print_info: n_head_kv        = 16
0.00.054.226 I print_info: n_rot            = 32
0.00.054.227 I print_info: n_swa            = 0
0.00.054.227 I print_info: n_embd_head_k    = 128
0.00.054.227 I print_info: n_embd_head_v    = 128
0.00.054.235 I print_info: n_gqa            = 1
0.00.054.235 I print_info: n_embd_k_gqa     = 2048
0.00.054.236 I print_info: n_embd_v_gqa     = 2048
0.00.054.236 I print_info: f_norm_eps       = 1.0e-05
0.00.054.237 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.237 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.237 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.237 I print_info: f_logit_scale    = 0.0e+00
0.00.054.238 I print_info: n_ff             = 8192
0.00.054.238 I print_info: n_expert         = 0
0.00.054.238 I print_info: n_expert_used    = 0
0.00.054.238 I print_info: causal attn      = 1
0.00.054.238 I print_info: pooling type     = 0
0.00.054.239 I print_info: rope type        = 2
0.00.054.239 I print_info: rope scaling     = linear
0.00.054.239 I print_info: freq_base_train  = 10000.0
0.00.054.239 I print_info: freq_scale_train = 1
0.00.054.240 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.240 I print_info: rope_finetuned   = unknown
0.00.054.240 I print_info: ssm_d_conv       = 0
0.00.054.240 I print_info: ssm_d_inner      = 0
0.00.054.240 I print_info: ssm_d_state      = 0
0.00.054.240 I print_info: ssm_dt_rank      = 0
0.00.054.240 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.241 I print_info: model type       = 1.4B
0.00.054.243 I print_info: model params     = 1.41 B
0.00.054.243 I print_info: general.name     = 1.4B
0.00.054.243 I print_info: vocab type       = BPE
0.00.054.244 I print_info: n_vocab          = 50304
0.00.054.244 I print_info: n_merges         = 50009
0.00.054.244 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.244 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.244 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.245 I print_info: LF token         = 128 'Ä'
0.00.054.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.245 I print_info: max token length = 1024
0.00.056.320 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.320 I load_tensors: offloading output layer to GPU
0.00.056.320 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.331 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.333 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.056.604 I llama_init_from_model: n_seq_max     = 1
0.00.056.606 I llama_init_from_model: n_ctx         = 128
0.00.056.606 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.606 I llama_init_from_model: n_batch       = 128
0.00.056.606 I llama_init_from_model: n_ubatch      = 128
0.00.056.606 I llama_init_from_model: flash_attn    = 0
0.00.056.606 I llama_init_from_model: freq_base     = 10000.0
0.00.056.607 I llama_init_from_model: freq_scale    = 1
0.00.056.607 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.607 I ggml_metal_init: allocating
0.00.056.611 I ggml_metal_init: found device: Apple M4
0.00.056.613 I ggml_metal_init: picking default device: Apple M4
0.00.057.243 I ggml_metal_init: using embedded metal library
0.00.061.419 I ggml_metal_init: GPU name:   Apple M4
0.00.061.421 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.421 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.422 I ggml_metal_init: simdgroup reduction   = true
0.00.061.422 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.423 I ggml_metal_init: has bfloat            = true
0.00.061.423 I ggml_metal_init: use bfloat            = true
0.00.061.424 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.776 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.075 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.089 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.107 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.073.011 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.073.012 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.073.012 I llama_init_from_model: graph nodes  = 967
0.00.073.013 I llama_init_from_model: graph splits = 2
0.00.073.014 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.320 I 
0.00.721.358 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.363 I perplexity: tokenizing the input ..
0.00.728.301 I perplexity: tokenization took 6.936 ms
0.00.728.305 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.586 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.864.812 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.864.834 I llama_perf_context_print:        load time =     709.95 ms
0.00.864.837 I llama_perf_context_print: prompt eval time =     135.04 ms /   128 tokens (    1.05 ms per token,   947.88 tokens per second)
0.00.864.838 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.838 I llama_perf_context_print:       total time =     143.52 ms /   129 tokens
0.00.865.184 I ggml_metal_free: deallocating

real	0m0.882s
user	0m0.078s
sys	0m0.106s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.020.664 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.027.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.834 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.835 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.835 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.836 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.836 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.837 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.837 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.838 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.838 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.838 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.839 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.839 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.841 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.841 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.841 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.782 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.976 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.221 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.222 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.223 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.223 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.223 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.037.224 I llama_model_loader: - type  f32:  194 tensors
0.00.037.224 I llama_model_loader: - type q2_K:   49 tensors
0.00.037.224 I llama_model_loader: - type q3_K:   48 tensors
0.00.037.225 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.225 I print_info: file format = GGUF V3 (latest)
0.00.037.226 I print_info: file type   = Q2_K - Medium
0.00.037.231 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.061.158 I load: special tokens cache size = 25
0.00.069.975 I load: token to piece cache size = 0.2984 MB
0.00.069.978 I print_info: arch             = gptneox
0.00.069.979 I print_info: vocab_only       = 0
0.00.069.979 I print_info: n_ctx_train      = 2048
0.00.069.979 I print_info: n_embd           = 2048
0.00.069.979 I print_info: n_layer          = 24
0.00.069.983 I print_info: n_head           = 16
0.00.069.984 I print_info: n_head_kv        = 16
0.00.069.984 I print_info: n_rot            = 32
0.00.069.984 I print_info: n_swa            = 0
0.00.069.984 I print_info: n_embd_head_k    = 128
0.00.069.985 I print_info: n_embd_head_v    = 128
0.00.069.985 I print_info: n_gqa            = 1
0.00.069.986 I print_info: n_embd_k_gqa     = 2048
0.00.069.987 I print_info: n_embd_v_gqa     = 2048
0.00.069.988 I print_info: f_norm_eps       = 1.0e-05
0.00.069.988 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.988 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.991 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.992 I print_info: f_logit_scale    = 0.0e+00
0.00.069.992 I print_info: n_ff             = 8192
0.00.069.993 I print_info: n_expert         = 0
0.00.069.993 I print_info: n_expert_used    = 0
0.00.069.993 I print_info: causal attn      = 1
0.00.069.994 I print_info: pooling type     = 0
0.00.069.994 I print_info: rope type        = 2
0.00.069.994 I print_info: rope scaling     = linear
0.00.069.995 I print_info: freq_base_train  = 10000.0
0.00.069.995 I print_info: freq_scale_train = 1
0.00.069.995 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.996 I print_info: rope_finetuned   = unknown
0.00.069.996 I print_info: ssm_d_conv       = 0
0.00.069.996 I print_info: ssm_d_inner      = 0
0.00.069.996 I print_info: ssm_d_state      = 0
0.00.069.996 I print_info: ssm_dt_rank      = 0
0.00.069.997 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.997 I print_info: model type       = 1.4B
0.00.069.997 I print_info: model params     = 1.41 B
0.00.069.998 I print_info: general.name     = 1.4B
0.00.069.998 I print_info: vocab type       = BPE
0.00.069.998 I print_info: n_vocab          = 50304
0.00.069.999 I print_info: n_merges         = 50009
0.00.069.999 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.999 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.999 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.000 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.002 I print_info: LF token         = 128 'Ä'
0.00.070.003 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.003 I print_info: max token length = 1024
0.00.072.475 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.476 I load_tensors: offloading output layer to GPU
0.00.072.476 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.487 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.072.488 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.072.937 I llama_init_from_model: n_seq_max     = 1
0.00.072.938 I llama_init_from_model: n_ctx         = 2048
0.00.072.938 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.938 I llama_init_from_model: n_batch       = 2048
0.00.072.938 I llama_init_from_model: n_ubatch      = 512
0.00.072.939 I llama_init_from_model: flash_attn    = 0
0.00.072.939 I llama_init_from_model: freq_base     = 10000.0
0.00.072.940 I llama_init_from_model: freq_scale    = 1
0.00.072.940 I ggml_metal_init: allocating
0.00.072.944 I ggml_metal_init: found device: Apple M4
0.00.072.947 I ggml_metal_init: picking default device: Apple M4
0.00.073.752 I ggml_metal_init: using embedded metal library
0.00.077.260 I ggml_metal_init: GPU name:   Apple M4
0.00.077.262 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.263 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.263 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.263 I ggml_metal_init: simdgroup reduction   = true
0.00.077.263 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.264 I ggml_metal_init: has bfloat            = true
0.00.077.264 I ggml_metal_init: use bfloat            = true
0.00.077.264 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.265 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.640 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.111.692 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.111.711 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.111.734 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.112.754 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.112.756 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.112.756 I llama_init_from_model: graph nodes  = 967
0.00.112.756 I llama_init_from_model: graph splits = 2
0.00.112.760 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.112.892 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.112.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.511.954 I main: llama threadpool init, n_threads = 4
0.00.512.001 I 
0.00.512.034 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.512.035 I 
0.00.512.254 I sampler seed: 1234
0.00.512.260 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.512.271 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.512.272 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.512.272 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.191.884 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60891.94 tokens per second)
0.01.191.884 I llama_perf_context_print:        load time =     491.28 ms
0.01.191.885 I llama_perf_context_print: prompt eval time =      35.79 ms /     7 tokens (    5.11 ms per token,   195.57 tokens per second)
0.01.191.886 I llama_perf_context_print:        eval time =     640.91 ms /    63 runs   (   10.17 ms per token,    98.30 tokens per second)
0.01.191.886 I llama_perf_context_print:       total time =     679.93 ms /    70 tokens
0.01.192.084 I ggml_metal_free: deallocating

real	0m1.210s
user	0m0.123s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.513 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.354 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.359 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.361 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.366 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.370 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.943 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.943 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.944 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.944 I llama_model_loader: - type  f32:  194 tensors
0.00.023.944 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.945 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.945 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.945 I print_info: file format = GGUF V3 (latest)
0.00.023.946 I print_info: file type   = Q2_K - Medium
0.00.023.951 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.042.985 I load: special tokens cache size = 25
0.00.048.918 I load: token to piece cache size = 0.2984 MB
0.00.048.921 I print_info: arch             = gptneox
0.00.048.921 I print_info: vocab_only       = 0
0.00.048.921 I print_info: n_ctx_train      = 2048
0.00.048.921 I print_info: n_embd           = 2048
0.00.048.922 I print_info: n_layer          = 24
0.00.048.925 I print_info: n_head           = 16
0.00.048.925 I print_info: n_head_kv        = 16
0.00.048.925 I print_info: n_rot            = 32
0.00.048.926 I print_info: n_swa            = 0
0.00.048.926 I print_info: n_embd_head_k    = 128
0.00.048.926 I print_info: n_embd_head_v    = 128
0.00.048.927 I print_info: n_gqa            = 1
0.00.048.928 I print_info: n_embd_k_gqa     = 2048
0.00.048.928 I print_info: n_embd_v_gqa     = 2048
0.00.048.929 I print_info: f_norm_eps       = 1.0e-05
0.00.048.929 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.930 I print_info: f_logit_scale    = 0.0e+00
0.00.048.931 I print_info: n_ff             = 8192
0.00.048.931 I print_info: n_expert         = 0
0.00.048.931 I print_info: n_expert_used    = 0
0.00.048.931 I print_info: causal attn      = 1
0.00.048.931 I print_info: pooling type     = 0
0.00.048.931 I print_info: rope type        = 2
0.00.048.934 I print_info: rope scaling     = linear
0.00.048.934 I print_info: freq_base_train  = 10000.0
0.00.048.934 I print_info: freq_scale_train = 1
0.00.048.935 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.935 I print_info: rope_finetuned   = unknown
0.00.048.935 I print_info: ssm_d_conv       = 0
0.00.048.935 I print_info: ssm_d_inner      = 0
0.00.048.935 I print_info: ssm_d_state      = 0
0.00.048.935 I print_info: ssm_dt_rank      = 0
0.00.048.935 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.936 I print_info: model type       = 1.4B
0.00.048.936 I print_info: model params     = 1.41 B
0.00.048.936 I print_info: general.name     = 1.4B
0.00.048.937 I print_info: vocab type       = BPE
0.00.048.937 I print_info: n_vocab          = 50304
0.00.048.937 I print_info: n_merges         = 50009
0.00.048.937 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.937 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.937 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.938 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.942 I print_info: LF token         = 128 'Ä'
0.00.048.942 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.943 I print_info: max token length = 1024
0.00.050.811 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.811 I load_tensors: offloading output layer to GPU
0.00.050.812 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.822 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.823 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.090 I llama_init_from_model: n_seq_max     = 1
0.00.051.091 I llama_init_from_model: n_ctx         = 128
0.00.051.091 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.091 I llama_init_from_model: n_batch       = 128
0.00.051.092 I llama_init_from_model: n_ubatch      = 128
0.00.051.092 I llama_init_from_model: flash_attn    = 0
0.00.051.092 I llama_init_from_model: freq_base     = 10000.0
0.00.051.092 I llama_init_from_model: freq_scale    = 1
0.00.051.093 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.093 I ggml_metal_init: allocating
0.00.051.096 I ggml_metal_init: found device: Apple M4
0.00.051.098 I ggml_metal_init: picking default device: Apple M4
0.00.051.674 I ggml_metal_init: using embedded metal library
0.00.054.009 I ggml_metal_init: GPU name:   Apple M4
0.00.054.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.010 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.011 I ggml_metal_init: simdgroup reduction   = true
0.00.054.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.011 I ggml_metal_init: has bfloat            = true
0.00.054.011 I ggml_metal_init: use bfloat            = true
0.00.054.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.012 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.580 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.854 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.867 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.883 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.838 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.839 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.839 I llama_init_from_model: graph nodes  = 967
0.00.065.839 I llama_init_from_model: graph splits = 2
0.00.065.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.840 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.420.193 I 
0.00.420.237 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.420.245 I perplexity: tokenizing the input ..
0.00.428.297 I perplexity: tokenization took 8.049 ms
0.00.428.300 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.679 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.903 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.941 I llama_perf_context_print:        load time =     411.67 ms
0.00.561.942 I llama_perf_context_print: prompt eval time =     132.16 ms /   128 tokens (    1.03 ms per token,   968.55 tokens per second)
0.00.561.943 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.943 I llama_perf_context_print:       total time =     141.75 ms /   129 tokens
0.00.562.526 I ggml_metal_free: deallocating

real	0m0.576s
user	0m0.078s
sys	0m0.065s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.946 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.111 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.116 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.119 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.119 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.120 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.126 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.482 I llama_model_loader: - type  f32:  194 tensors
0.00.026.483 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.483 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.483 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.483 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.484 I print_info: file format = GGUF V3 (latest)
0.00.026.484 I print_info: file type   = Q3_K - Medium
0.00.026.485 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.047 I load: special tokens cache size = 25
0.00.051.021 I load: token to piece cache size = 0.2984 MB
0.00.051.024 I print_info: arch             = gptneox
0.00.051.024 I print_info: vocab_only       = 0
0.00.051.024 I print_info: n_ctx_train      = 2048
0.00.051.025 I print_info: n_embd           = 2048
0.00.051.025 I print_info: n_layer          = 24
0.00.051.028 I print_info: n_head           = 16
0.00.051.028 I print_info: n_head_kv        = 16
0.00.051.029 I print_info: n_rot            = 32
0.00.051.029 I print_info: n_swa            = 0
0.00.051.029 I print_info: n_embd_head_k    = 128
0.00.051.029 I print_info: n_embd_head_v    = 128
0.00.051.030 I print_info: n_gqa            = 1
0.00.051.031 I print_info: n_embd_k_gqa     = 2048
0.00.051.033 I print_info: n_embd_v_gqa     = 2048
0.00.051.034 I print_info: f_norm_eps       = 1.0e-05
0.00.051.034 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.035 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.035 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.035 I print_info: f_logit_scale    = 0.0e+00
0.00.051.036 I print_info: n_ff             = 8192
0.00.051.037 I print_info: n_expert         = 0
0.00.051.037 I print_info: n_expert_used    = 0
0.00.051.037 I print_info: causal attn      = 1
0.00.051.037 I print_info: pooling type     = 0
0.00.051.037 I print_info: rope type        = 2
0.00.051.038 I print_info: rope scaling     = linear
0.00.051.039 I print_info: freq_base_train  = 10000.0
0.00.051.040 I print_info: freq_scale_train = 1
0.00.051.040 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.040 I print_info: rope_finetuned   = unknown
0.00.051.040 I print_info: ssm_d_conv       = 0
0.00.051.040 I print_info: ssm_d_inner      = 0
0.00.051.040 I print_info: ssm_d_state      = 0
0.00.051.041 I print_info: ssm_dt_rank      = 0
0.00.051.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.041 I print_info: model type       = 1.4B
0.00.051.041 I print_info: model params     = 1.41 B
0.00.051.043 I print_info: general.name     = 1.4B
0.00.051.043 I print_info: vocab type       = BPE
0.00.051.043 I print_info: n_vocab          = 50304
0.00.051.044 I print_info: n_merges         = 50009
0.00.051.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.045 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.046 I print_info: LF token         = 128 'Ä'
0.00.051.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.046 I print_info: max token length = 1024
0.00.052.786 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.786 I load_tensors: offloading output layer to GPU
0.00.052.787 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.792 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.793 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.082 I llama_init_from_model: n_seq_max     = 1
0.00.053.082 I llama_init_from_model: n_ctx         = 2048
0.00.053.083 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.083 I llama_init_from_model: n_batch       = 2048
0.00.053.083 I llama_init_from_model: n_ubatch      = 512
0.00.053.083 I llama_init_from_model: flash_attn    = 0
0.00.053.084 I llama_init_from_model: freq_base     = 10000.0
0.00.053.084 I llama_init_from_model: freq_scale    = 1
0.00.053.084 I ggml_metal_init: allocating
0.00.053.087 I ggml_metal_init: found device: Apple M4
0.00.053.089 I ggml_metal_init: picking default device: Apple M4
0.00.053.670 I ggml_metal_init: using embedded metal library
0.00.055.993 I ggml_metal_init: GPU name:   Apple M4
0.00.055.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.995 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.995 I ggml_metal_init: simdgroup reduction   = true
0.00.055.995 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.995 I ggml_metal_init: has bfloat            = true
0.00.055.996 I ggml_metal_init: use bfloat            = true
0.00.055.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.997 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.661 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.512 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.533 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.556 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.509 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.510 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.511 I llama_init_from_model: graph nodes  = 967
0.00.085.511 I llama_init_from_model: graph splits = 2
0.00.085.513 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.634 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.074 I main: llama threadpool init, n_threads = 4
0.00.629.127 I 
0.00.629.157 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.158 I 
0.00.629.395 I sampler seed: 1234
0.00.629.400 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.440 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.440 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.440 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.386.358 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.386.359 I llama_perf_context_print:        load time =     618.12 ms
0.01.386.360 I llama_perf_context_print: prompt eval time =      47.44 ms /     7 tokens (    6.78 ms per token,   147.56 tokens per second)
0.01.386.360 I llama_perf_context_print:        eval time =     706.49 ms /    63 runs   (   11.21 ms per token,    89.17 tokens per second)
0.01.386.361 I llama_perf_context_print:       total time =     757.29 ms /    70 tokens
0.01.386.567 I ggml_metal_free: deallocating

real	0m1.402s
user	0m0.108s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.658 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.351 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.353 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.355 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.356 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.356 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.356 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.357 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.357 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.358 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.077 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.034 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.667 I llama_model_loader: - type  f32:  194 tensors
0.00.023.667 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.668 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.668 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.668 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.669 I print_info: file format = GGUF V3 (latest)
0.00.023.669 I print_info: file type   = Q3_K - Medium
0.00.023.670 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.041.955 I load: special tokens cache size = 25
0.00.048.016 I load: token to piece cache size = 0.2984 MB
0.00.048.018 I print_info: arch             = gptneox
0.00.048.019 I print_info: vocab_only       = 0
0.00.048.019 I print_info: n_ctx_train      = 2048
0.00.048.019 I print_info: n_embd           = 2048
0.00.048.019 I print_info: n_layer          = 24
0.00.048.022 I print_info: n_head           = 16
0.00.048.023 I print_info: n_head_kv        = 16
0.00.048.023 I print_info: n_rot            = 32
0.00.048.023 I print_info: n_swa            = 0
0.00.048.023 I print_info: n_embd_head_k    = 128
0.00.048.023 I print_info: n_embd_head_v    = 128
0.00.048.024 I print_info: n_gqa            = 1
0.00.048.025 I print_info: n_embd_k_gqa     = 2048
0.00.048.025 I print_info: n_embd_v_gqa     = 2048
0.00.048.026 I print_info: f_norm_eps       = 1.0e-05
0.00.048.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.026 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.027 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.027 I print_info: f_logit_scale    = 0.0e+00
0.00.048.027 I print_info: n_ff             = 8192
0.00.048.028 I print_info: n_expert         = 0
0.00.048.028 I print_info: n_expert_used    = 0
0.00.048.028 I print_info: causal attn      = 1
0.00.048.028 I print_info: pooling type     = 0
0.00.048.028 I print_info: rope type        = 2
0.00.048.031 I print_info: rope scaling     = linear
0.00.048.031 I print_info: freq_base_train  = 10000.0
0.00.048.032 I print_info: freq_scale_train = 1
0.00.048.032 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.032 I print_info: rope_finetuned   = unknown
0.00.048.032 I print_info: ssm_d_conv       = 0
0.00.048.032 I print_info: ssm_d_inner      = 0
0.00.048.033 I print_info: ssm_d_state      = 0
0.00.048.033 I print_info: ssm_dt_rank      = 0
0.00.048.034 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.034 I print_info: model type       = 1.4B
0.00.048.034 I print_info: model params     = 1.41 B
0.00.048.035 I print_info: general.name     = 1.4B
0.00.048.035 I print_info: vocab type       = BPE
0.00.048.035 I print_info: n_vocab          = 50304
0.00.048.035 I print_info: n_merges         = 50009
0.00.048.036 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.036 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.036 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.036 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.036 I print_info: LF token         = 128 'Ä'
0.00.048.040 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.041 I print_info: max token length = 1024
0.00.049.880 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.880 I load_tensors: offloading output layer to GPU
0.00.049.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.891 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.049.892 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.158 I llama_init_from_model: n_seq_max     = 1
0.00.050.159 I llama_init_from_model: n_ctx         = 128
0.00.050.159 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.159 I llama_init_from_model: n_batch       = 128
0.00.050.159 I llama_init_from_model: n_ubatch      = 128
0.00.050.159 I llama_init_from_model: flash_attn    = 0
0.00.050.160 I llama_init_from_model: freq_base     = 10000.0
0.00.050.160 I llama_init_from_model: freq_scale    = 1
0.00.050.160 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.161 I ggml_metal_init: allocating
0.00.050.163 I ggml_metal_init: found device: Apple M4
0.00.050.165 I ggml_metal_init: picking default device: Apple M4
0.00.050.736 I ggml_metal_init: using embedded metal library
0.00.053.106 I ggml_metal_init: GPU name:   Apple M4
0.00.053.108 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.108 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.108 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.108 I ggml_metal_init: simdgroup reduction   = true
0.00.053.109 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.109 I ggml_metal_init: has bfloat            = true
0.00.053.109 I ggml_metal_init: use bfloat            = true
0.00.053.109 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.110 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.408 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.756 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.769 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.785 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.621 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.622 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.622 I llama_init_from_model: graph nodes  = 967
0.00.063.622 I llama_init_from_model: graph splits = 2
0.00.063.623 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.794 I 
0.00.479.845 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.479.852 I perplexity: tokenizing the input ..
0.00.487.646 I perplexity: tokenization took 7.793 ms
0.00.487.650 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.912 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.621.146 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.621.174 I llama_perf_context_print:        load time =     471.13 ms
0.00.621.176 I llama_perf_context_print: prompt eval time =     132.02 ms /   128 tokens (    1.03 ms per token,   969.59 tokens per second)
0.00.621.177 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.178 I llama_perf_context_print:       total time =     141.39 ms /   129 tokens
0.00.621.715 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.075s
sys	0m0.092s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.632 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.879 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.894 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.742 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.741 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.593 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.594 I llama_model_loader: - type  f32:  194 tensors
0.00.025.594 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.594 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.594 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.595 I print_info: file format = GGUF V3 (latest)
0.00.025.595 I print_info: file type   = Q4_K - Medium
0.00.025.596 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.874 I load: special tokens cache size = 25
0.00.051.895 I load: token to piece cache size = 0.2984 MB
0.00.051.900 I print_info: arch             = gptneox
0.00.051.900 I print_info: vocab_only       = 0
0.00.051.900 I print_info: n_ctx_train      = 2048
0.00.051.900 I print_info: n_embd           = 2048
0.00.051.900 I print_info: n_layer          = 24
0.00.051.905 I print_info: n_head           = 16
0.00.051.906 I print_info: n_head_kv        = 16
0.00.051.906 I print_info: n_rot            = 32
0.00.051.906 I print_info: n_swa            = 0
0.00.051.906 I print_info: n_embd_head_k    = 128
0.00.051.906 I print_info: n_embd_head_v    = 128
0.00.051.910 I print_info: n_gqa            = 1
0.00.051.910 I print_info: n_embd_k_gqa     = 2048
0.00.051.911 I print_info: n_embd_v_gqa     = 2048
0.00.051.912 I print_info: f_norm_eps       = 1.0e-05
0.00.051.912 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.912 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.912 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.912 I print_info: f_logit_scale    = 0.0e+00
0.00.051.913 I print_info: n_ff             = 8192
0.00.051.913 I print_info: n_expert         = 0
0.00.051.914 I print_info: n_expert_used    = 0
0.00.051.914 I print_info: causal attn      = 1
0.00.051.914 I print_info: pooling type     = 0
0.00.051.915 I print_info: rope type        = 2
0.00.051.916 I print_info: rope scaling     = linear
0.00.051.916 I print_info: freq_base_train  = 10000.0
0.00.051.916 I print_info: freq_scale_train = 1
0.00.051.917 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.917 I print_info: rope_finetuned   = unknown
0.00.051.917 I print_info: ssm_d_conv       = 0
0.00.051.918 I print_info: ssm_d_inner      = 0
0.00.051.918 I print_info: ssm_d_state      = 0
0.00.051.919 I print_info: ssm_dt_rank      = 0
0.00.051.919 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.919 I print_info: model type       = 1.4B
0.00.051.919 I print_info: model params     = 1.41 B
0.00.051.919 I print_info: general.name     = 1.4B
0.00.051.920 I print_info: vocab type       = BPE
0.00.051.920 I print_info: n_vocab          = 50304
0.00.051.920 I print_info: n_merges         = 50009
0.00.051.920 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.920 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.921 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.921 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.921 I print_info: LF token         = 128 'Ä'
0.00.051.921 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.921 I print_info: max token length = 1024
0.00.053.809 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.809 I load_tensors: offloading output layer to GPU
0.00.053.809 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.820 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.822 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.216 I llama_init_from_model: n_seq_max     = 1
0.00.054.216 I llama_init_from_model: n_ctx         = 2048
0.00.054.217 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.217 I llama_init_from_model: n_batch       = 2048
0.00.054.217 I llama_init_from_model: n_ubatch      = 512
0.00.054.217 I llama_init_from_model: flash_attn    = 0
0.00.054.217 I llama_init_from_model: freq_base     = 10000.0
0.00.054.218 I llama_init_from_model: freq_scale    = 1
0.00.054.218 I ggml_metal_init: allocating
0.00.054.222 I ggml_metal_init: found device: Apple M4
0.00.054.225 I ggml_metal_init: picking default device: Apple M4
0.00.054.869 I ggml_metal_init: using embedded metal library
0.00.057.259 I ggml_metal_init: GPU name:   Apple M4
0.00.057.260 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.261 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.262 I ggml_metal_init: simdgroup reduction   = true
0.00.057.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.262 I ggml_metal_init: has bfloat            = true
0.00.057.262 I ggml_metal_init: use bfloat            = true
0.00.057.262 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.526 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.034 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.057 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.077 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.115 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.117 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.117 I llama_init_from_model: graph nodes  = 967
0.00.088.118 I llama_init_from_model: graph splits = 2
0.00.088.120 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.249 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.112.951 I main: llama threadpool init, n_threads = 4
0.01.113.069 I 
0.01.113.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.113.153 I 
0.01.113.652 I sampler seed: 1234
0.01.113.662 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.113.744 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.113.746 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.113.746 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.883.416 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.01.883.417 I llama_perf_context_print:        load time =    1103.30 ms
0.01.883.417 I llama_perf_context_print: prompt eval time =      58.81 ms /     7 tokens (    8.40 ms per token,   119.02 tokens per second)
0.01.883.418 I llama_perf_context_print:        eval time =     707.67 ms /    63 runs   (   11.23 ms per token,    89.02 tokens per second)
0.01.883.418 I llama_perf_context_print:       total time =     770.48 ms /    70 tokens
0.01.883.703 I ggml_metal_free: deallocating

real	0m1.902s
user	0m0.123s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.093 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.548 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.550 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.552 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.554 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.555 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.556 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.557 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.557 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.383 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.208 I llama_model_loader: - type  f32:  194 tensors
0.00.025.208 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.209 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.209 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.209 I print_info: file format = GGUF V3 (latest)
0.00.025.210 I print_info: file type   = Q4_K - Medium
0.00.025.211 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.191 I load: special tokens cache size = 25
0.00.050.126 I load: token to piece cache size = 0.2984 MB
0.00.050.129 I print_info: arch             = gptneox
0.00.050.130 I print_info: vocab_only       = 0
0.00.050.130 I print_info: n_ctx_train      = 2048
0.00.050.130 I print_info: n_embd           = 2048
0.00.050.130 I print_info: n_layer          = 24
0.00.050.133 I print_info: n_head           = 16
0.00.050.134 I print_info: n_head_kv        = 16
0.00.050.134 I print_info: n_rot            = 32
0.00.050.134 I print_info: n_swa            = 0
0.00.050.134 I print_info: n_embd_head_k    = 128
0.00.050.134 I print_info: n_embd_head_v    = 128
0.00.050.135 I print_info: n_gqa            = 1
0.00.050.136 I print_info: n_embd_k_gqa     = 2048
0.00.050.137 I print_info: n_embd_v_gqa     = 2048
0.00.050.137 I print_info: f_norm_eps       = 1.0e-05
0.00.050.138 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.138 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.138 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.141 I print_info: f_logit_scale    = 0.0e+00
0.00.050.142 I print_info: n_ff             = 8192
0.00.050.142 I print_info: n_expert         = 0
0.00.050.142 I print_info: n_expert_used    = 0
0.00.050.142 I print_info: causal attn      = 1
0.00.050.143 I print_info: pooling type     = 0
0.00.050.143 I print_info: rope type        = 2
0.00.050.143 I print_info: rope scaling     = linear
0.00.050.143 I print_info: freq_base_train  = 10000.0
0.00.050.144 I print_info: freq_scale_train = 1
0.00.050.144 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.146 I print_info: rope_finetuned   = unknown
0.00.050.146 I print_info: ssm_d_conv       = 0
0.00.050.146 I print_info: ssm_d_inner      = 0
0.00.050.146 I print_info: ssm_d_state      = 0
0.00.050.146 I print_info: ssm_dt_rank      = 0
0.00.050.146 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.146 I print_info: model type       = 1.4B
0.00.050.147 I print_info: model params     = 1.41 B
0.00.050.147 I print_info: general.name     = 1.4B
0.00.050.147 I print_info: vocab type       = BPE
0.00.050.148 I print_info: n_vocab          = 50304
0.00.050.148 I print_info: n_merges         = 50009
0.00.050.148 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.152 I print_info: LF token         = 128 'Ä'
0.00.050.153 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.153 I print_info: max token length = 1024
0.00.052.030 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.030 I load_tensors: offloading output layer to GPU
0.00.052.031 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.041 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.042 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.309 I llama_init_from_model: n_seq_max     = 1
0.00.052.310 I llama_init_from_model: n_ctx         = 128
0.00.052.310 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.310 I llama_init_from_model: n_batch       = 128
0.00.052.310 I llama_init_from_model: n_ubatch      = 128
0.00.052.310 I llama_init_from_model: flash_attn    = 0
0.00.052.311 I llama_init_from_model: freq_base     = 10000.0
0.00.052.311 I llama_init_from_model: freq_scale    = 1
0.00.052.311 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.312 I ggml_metal_init: allocating
0.00.052.315 I ggml_metal_init: found device: Apple M4
0.00.052.316 I ggml_metal_init: picking default device: Apple M4
0.00.052.864 I ggml_metal_init: using embedded metal library
0.00.055.179 I ggml_metal_init: GPU name:   Apple M4
0.00.055.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.182 I ggml_metal_init: simdgroup reduction   = true
0.00.055.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.182 I ggml_metal_init: has bfloat            = true
0.00.055.182 I ggml_metal_init: use bfloat            = true
0.00.055.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.183 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.565 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.794 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.809 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.833 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.668 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.669 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.669 I llama_init_from_model: graph nodes  = 967
0.00.066.669 I llama_init_from_model: graph splits = 2
0.00.066.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.302 I 
0.00.541.341 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.346 I perplexity: tokenizing the input ..
0.00.549.204 I perplexity: tokenization took 7.855 ms
0.00.549.208 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.683.298 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.684.486 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.684.513 I llama_perf_context_print:        load time =     531.20 ms
0.00.684.515 I llama_perf_context_print: prompt eval time =     133.86 ms /   128 tokens (    1.05 ms per token,   956.19 tokens per second)
0.00.684.515 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.684.516 I llama_perf_context_print:       total time =     143.21 ms /   129 tokens
0.00.684.871 I ggml_metal_free: deallocating

real	0m0.700s
user	0m0.077s
sys	0m0.094s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.133 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.737 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.742 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.744 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.744 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.745 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.745 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.745 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.746 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.749 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.750 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.750 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.752 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.752 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.573 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.571 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.355 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.356 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.356 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.357 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.357 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.357 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.358 I llama_model_loader: - type  f32:  194 tensors
0.00.026.358 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.358 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.359 I print_info: file format = GGUF V3 (latest)
0.00.026.359 I print_info: file type   = Q5_K - Medium
0.00.026.364 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.590 I load: special tokens cache size = 25
0.00.051.507 I load: token to piece cache size = 0.2984 MB
0.00.051.510 I print_info: arch             = gptneox
0.00.051.510 I print_info: vocab_only       = 0
0.00.051.511 I print_info: n_ctx_train      = 2048
0.00.051.511 I print_info: n_embd           = 2048
0.00.051.511 I print_info: n_layer          = 24
0.00.051.514 I print_info: n_head           = 16
0.00.051.515 I print_info: n_head_kv        = 16
0.00.051.515 I print_info: n_rot            = 32
0.00.051.515 I print_info: n_swa            = 0
0.00.051.515 I print_info: n_embd_head_k    = 128
0.00.051.515 I print_info: n_embd_head_v    = 128
0.00.051.516 I print_info: n_gqa            = 1
0.00.051.517 I print_info: n_embd_k_gqa     = 2048
0.00.051.518 I print_info: n_embd_v_gqa     = 2048
0.00.051.519 I print_info: f_norm_eps       = 1.0e-05
0.00.051.519 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.519 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.519 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.520 I print_info: f_logit_scale    = 0.0e+00
0.00.051.520 I print_info: n_ff             = 8192
0.00.051.520 I print_info: n_expert         = 0
0.00.051.521 I print_info: n_expert_used    = 0
0.00.051.521 I print_info: causal attn      = 1
0.00.051.521 I print_info: pooling type     = 0
0.00.051.521 I print_info: rope type        = 2
0.00.051.521 I print_info: rope scaling     = linear
0.00.051.523 I print_info: freq_base_train  = 10000.0
0.00.051.524 I print_info: freq_scale_train = 1
0.00.051.524 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.525 I print_info: rope_finetuned   = unknown
0.00.051.525 I print_info: ssm_d_conv       = 0
0.00.051.525 I print_info: ssm_d_inner      = 0
0.00.051.525 I print_info: ssm_d_state      = 0
0.00.051.525 I print_info: ssm_dt_rank      = 0
0.00.051.525 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.527 I print_info: model type       = 1.4B
0.00.051.527 I print_info: model params     = 1.41 B
0.00.051.528 I print_info: general.name     = 1.4B
0.00.051.528 I print_info: vocab type       = BPE
0.00.051.528 I print_info: n_vocab          = 50304
0.00.051.529 I print_info: n_merges         = 50009
0.00.051.529 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.530 I print_info: LF token         = 128 'Ä'
0.00.051.530 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.530 I print_info: max token length = 1024
0.00.053.535 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.536 I load_tensors: offloading output layer to GPU
0.00.053.536 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.547 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.548 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.821 I llama_init_from_model: n_seq_max     = 1
0.00.053.821 I llama_init_from_model: n_ctx         = 2048
0.00.053.822 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.822 I llama_init_from_model: n_batch       = 2048
0.00.053.822 I llama_init_from_model: n_ubatch      = 512
0.00.053.822 I llama_init_from_model: flash_attn    = 0
0.00.053.822 I llama_init_from_model: freq_base     = 10000.0
0.00.053.823 I llama_init_from_model: freq_scale    = 1
0.00.053.823 I ggml_metal_init: allocating
0.00.053.826 I ggml_metal_init: found device: Apple M4
0.00.053.828 I ggml_metal_init: picking default device: Apple M4
0.00.054.416 I ggml_metal_init: using embedded metal library
0.00.056.811 I ggml_metal_init: GPU name:   Apple M4
0.00.056.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.813 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.813 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.814 I ggml_metal_init: simdgroup reduction   = true
0.00.056.814 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.814 I ggml_metal_init: has bfloat            = true
0.00.056.814 I ggml_metal_init: use bfloat            = true
0.00.056.814 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.815 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.461 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.045 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.063 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.081 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.294 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.295 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.295 I llama_init_from_model: graph nodes  = 967
0.00.087.296 I llama_init_from_model: graph splits = 2
0.00.087.298 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.783 I main: llama threadpool init, n_threads = 4
0.00.715.828 I 
0.00.715.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.860 I 
0.00.716.096 I sampler seed: 1234
0.00.716.101 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.716.159 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.716.163 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.716.163 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.556.597 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.556.598 I llama_perf_context_print:        load time =     704.64 ms
0.01.556.600 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.38 ms per token,   135.59 tokens per second)
0.01.556.601 I llama_perf_context_print:        eval time =     785.76 ms /    63 runs   (   12.47 ms per token,    80.18 tokens per second)
0.01.556.601 I llama_perf_context_print:       total time =     840.82 ms /    70 tokens
0.01.556.829 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.110s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.823 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.573 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.574 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.574 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.575 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.315 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.080 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.080 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.080 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.081 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.081 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.081 I llama_model_loader: - type  f32:  194 tensors
0.00.024.082 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.082 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.082 I print_info: file format = GGUF V3 (latest)
0.00.024.083 I print_info: file type   = Q5_K - Medium
0.00.024.084 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.268 I load: special tokens cache size = 25
0.00.049.288 I load: token to piece cache size = 0.2984 MB
0.00.049.291 I print_info: arch             = gptneox
0.00.049.292 I print_info: vocab_only       = 0
0.00.049.292 I print_info: n_ctx_train      = 2048
0.00.049.292 I print_info: n_embd           = 2048
0.00.049.292 I print_info: n_layer          = 24
0.00.049.295 I print_info: n_head           = 16
0.00.049.296 I print_info: n_head_kv        = 16
0.00.049.296 I print_info: n_rot            = 32
0.00.049.296 I print_info: n_swa            = 0
0.00.049.296 I print_info: n_embd_head_k    = 128
0.00.049.296 I print_info: n_embd_head_v    = 128
0.00.049.297 I print_info: n_gqa            = 1
0.00.049.298 I print_info: n_embd_k_gqa     = 2048
0.00.049.299 I print_info: n_embd_v_gqa     = 2048
0.00.049.299 I print_info: f_norm_eps       = 1.0e-05
0.00.049.300 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.300 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.300 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.300 I print_info: f_logit_scale    = 0.0e+00
0.00.049.301 I print_info: n_ff             = 8192
0.00.049.303 I print_info: n_expert         = 0
0.00.049.303 I print_info: n_expert_used    = 0
0.00.049.303 I print_info: causal attn      = 1
0.00.049.303 I print_info: pooling type     = 0
0.00.049.304 I print_info: rope type        = 2
0.00.049.304 I print_info: rope scaling     = linear
0.00.049.304 I print_info: freq_base_train  = 10000.0
0.00.049.306 I print_info: freq_scale_train = 1
0.00.049.306 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.306 I print_info: rope_finetuned   = unknown
0.00.049.306 I print_info: ssm_d_conv       = 0
0.00.049.306 I print_info: ssm_d_inner      = 0
0.00.049.306 I print_info: ssm_d_state      = 0
0.00.049.307 I print_info: ssm_dt_rank      = 0
0.00.049.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.307 I print_info: model type       = 1.4B
0.00.049.307 I print_info: model params     = 1.41 B
0.00.049.308 I print_info: general.name     = 1.4B
0.00.049.308 I print_info: vocab type       = BPE
0.00.049.308 I print_info: n_vocab          = 50304
0.00.049.308 I print_info: n_merges         = 50009
0.00.049.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.309 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.313 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.313 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.313 I print_info: LF token         = 128 'Ä'
0.00.049.313 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.313 I print_info: max token length = 1024
0.00.051.254 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.254 I load_tensors: offloading output layer to GPU
0.00.051.254 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.265 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.266 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.538 I llama_init_from_model: n_seq_max     = 1
0.00.051.539 I llama_init_from_model: n_ctx         = 128
0.00.051.539 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.539 I llama_init_from_model: n_batch       = 128
0.00.051.539 I llama_init_from_model: n_ubatch      = 128
0.00.051.539 I llama_init_from_model: flash_attn    = 0
0.00.051.540 I llama_init_from_model: freq_base     = 10000.0
0.00.051.540 I llama_init_from_model: freq_scale    = 1
0.00.051.540 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.541 I ggml_metal_init: allocating
0.00.051.544 I ggml_metal_init: found device: Apple M4
0.00.051.546 I ggml_metal_init: picking default device: Apple M4
0.00.052.097 I ggml_metal_init: using embedded metal library
0.00.054.440 I ggml_metal_init: GPU name:   Apple M4
0.00.054.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.442 I ggml_metal_init: simdgroup reduction   = true
0.00.054.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.442 I ggml_metal_init: has bfloat            = true
0.00.054.443 I ggml_metal_init: use bfloat            = true
0.00.054.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.745 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.947 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.960 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.986 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.869 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.870 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.870 I llama_init_from_model: graph nodes  = 967
0.00.065.870 I llama_init_from_model: graph splits = 2
0.00.065.871 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.651 I 
0.00.631.748 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.758 I perplexity: tokenizing the input ..
0.00.639.328 I perplexity: tokenization took 7.569 ms
0.00.639.331 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.298 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.781.446 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.781.471 I llama_perf_context_print:        load time =     622.82 ms
0.00.781.472 I llama_perf_context_print: prompt eval time =     140.73 ms /   128 tokens (    1.10 ms per token,   909.52 tokens per second)
0.00.781.472 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.473 I llama_perf_context_print:       total time =     149.82 ms /   129 tokens
0.00.782.041 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.077s
sys	0m0.118s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.898 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.555 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.563 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.564 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.565 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.567 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.570 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.570 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.404 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.446 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.292 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.293 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.294 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.295 I llama_model_loader: - type  f32:  194 tensors
0.00.024.295 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.296 I print_info: file format = GGUF V3 (latest)
0.00.024.297 I print_info: file type   = Q6_K
0.00.024.298 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.522 I load: special tokens cache size = 25
0.00.049.611 I load: token to piece cache size = 0.2984 MB
0.00.049.614 I print_info: arch             = gptneox
0.00.049.614 I print_info: vocab_only       = 0
0.00.049.614 I print_info: n_ctx_train      = 2048
0.00.049.615 I print_info: n_embd           = 2048
0.00.049.615 I print_info: n_layer          = 24
0.00.049.618 I print_info: n_head           = 16
0.00.049.619 I print_info: n_head_kv        = 16
0.00.049.619 I print_info: n_rot            = 32
0.00.049.619 I print_info: n_swa            = 0
0.00.049.619 I print_info: n_embd_head_k    = 128
0.00.049.619 I print_info: n_embd_head_v    = 128
0.00.049.620 I print_info: n_gqa            = 1
0.00.049.621 I print_info: n_embd_k_gqa     = 2048
0.00.049.622 I print_info: n_embd_v_gqa     = 2048
0.00.049.622 I print_info: f_norm_eps       = 1.0e-05
0.00.049.623 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.623 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.623 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.623 I print_info: f_logit_scale    = 0.0e+00
0.00.049.624 I print_info: n_ff             = 8192
0.00.049.624 I print_info: n_expert         = 0
0.00.049.624 I print_info: n_expert_used    = 0
0.00.049.625 I print_info: causal attn      = 1
0.00.049.625 I print_info: pooling type     = 0
0.00.049.625 I print_info: rope type        = 2
0.00.049.625 I print_info: rope scaling     = linear
0.00.049.627 I print_info: freq_base_train  = 10000.0
0.00.049.628 I print_info: freq_scale_train = 1
0.00.049.628 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.628 I print_info: rope_finetuned   = unknown
0.00.049.628 I print_info: ssm_d_conv       = 0
0.00.049.628 I print_info: ssm_d_inner      = 0
0.00.049.628 I print_info: ssm_d_state      = 0
0.00.049.629 I print_info: ssm_dt_rank      = 0
0.00.049.629 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.629 I print_info: model type       = 1.4B
0.00.049.629 I print_info: model params     = 1.41 B
0.00.049.629 I print_info: general.name     = 1.4B
0.00.049.630 I print_info: vocab type       = BPE
0.00.049.630 I print_info: n_vocab          = 50304
0.00.049.630 I print_info: n_merges         = 50009
0.00.049.631 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: LF token         = 128 'Ä'
0.00.049.632 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.632 I print_info: max token length = 1024
0.00.051.389 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.390 I load_tensors: offloading output layer to GPU
0.00.051.390 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.395 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.396 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.664 I llama_init_from_model: n_seq_max     = 1
0.00.051.665 I llama_init_from_model: n_ctx         = 2048
0.00.051.665 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.665 I llama_init_from_model: n_batch       = 2048
0.00.051.665 I llama_init_from_model: n_ubatch      = 512
0.00.051.666 I llama_init_from_model: flash_attn    = 0
0.00.051.666 I llama_init_from_model: freq_base     = 10000.0
0.00.051.666 I llama_init_from_model: freq_scale    = 1
0.00.051.667 I ggml_metal_init: allocating
0.00.051.670 I ggml_metal_init: found device: Apple M4
0.00.051.672 I ggml_metal_init: picking default device: Apple M4
0.00.052.242 I ggml_metal_init: using embedded metal library
0.00.054.548 I ggml_metal_init: GPU name:   Apple M4
0.00.054.550 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.550 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.551 I ggml_metal_init: simdgroup reduction   = true
0.00.054.551 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.551 I ggml_metal_init: has bfloat            = true
0.00.054.551 I ggml_metal_init: use bfloat            = true
0.00.054.552 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.125 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.875 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.895 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.916 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.037 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.039 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.039 I llama_init_from_model: graph nodes  = 967
0.00.085.039 I llama_init_from_model: graph splits = 2
0.00.085.046 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.164 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.270 I main: llama threadpool init, n_threads = 4
0.00.745.345 I 
0.00.745.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.375 I 
0.00.745.610 I sampler seed: 1234
0.00.745.615 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.655 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.656 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.656 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.624.946 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.624.947 I llama_perf_context_print:        load time =     736.37 ms
0.01.624.948 I llama_perf_context_print: prompt eval time =      54.45 ms /     7 tokens (    7.78 ms per token,   128.57 tokens per second)
0.01.624.950 I llama_perf_context_print:        eval time =     822.12 ms /    63 runs   (   13.05 ms per token,    76.63 tokens per second)
0.01.624.951 I llama_perf_context_print:       total time =     879.68 ms /    70 tokens
0.01.625.153 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.109s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4511 (99454784) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.100 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.480 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.486 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.491 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.492 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.493 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.493 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.232 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.032 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.033 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.034 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.034 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.035 I llama_model_loader: - type  f32:  194 tensors
0.00.024.035 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.036 I print_info: file format = GGUF V3 (latest)
0.00.024.036 I print_info: file type   = Q6_K
0.00.024.037 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.213 I load: special tokens cache size = 25
0.00.047.853 I load: token to piece cache size = 0.2984 MB
0.00.047.855 I print_info: arch             = gptneox
0.00.047.856 I print_info: vocab_only       = 0
0.00.047.856 I print_info: n_ctx_train      = 2048
0.00.047.856 I print_info: n_embd           = 2048
0.00.047.856 I print_info: n_layer          = 24
0.00.047.859 I print_info: n_head           = 16
0.00.047.860 I print_info: n_head_kv        = 16
0.00.047.860 I print_info: n_rot            = 32
0.00.047.861 I print_info: n_swa            = 0
0.00.047.861 I print_info: n_embd_head_k    = 128
0.00.047.861 I print_info: n_embd_head_v    = 128
0.00.047.862 I print_info: n_gqa            = 1
0.00.047.862 I print_info: n_embd_k_gqa     = 2048
0.00.047.863 I print_info: n_embd_v_gqa     = 2048
0.00.047.864 I print_info: f_norm_eps       = 1.0e-05
0.00.047.864 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.864 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.864 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.865 I print_info: f_logit_scale    = 0.0e+00
0.00.047.865 I print_info: n_ff             = 8192
0.00.047.866 I print_info: n_expert         = 0
0.00.047.866 I print_info: n_expert_used    = 0
0.00.047.866 I print_info: causal attn      = 1
0.00.047.866 I print_info: pooling type     = 0
0.00.047.866 I print_info: rope type        = 2
0.00.047.866 I print_info: rope scaling     = linear
0.00.047.867 I print_info: freq_base_train  = 10000.0
0.00.047.867 I print_info: freq_scale_train = 1
0.00.047.867 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.867 I print_info: rope_finetuned   = unknown
0.00.047.868 I print_info: ssm_d_conv       = 0
0.00.047.868 I print_info: ssm_d_inner      = 0
0.00.047.868 I print_info: ssm_d_state      = 0
0.00.047.868 I print_info: ssm_dt_rank      = 0
0.00.047.868 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.869 I print_info: model type       = 1.4B
0.00.047.870 I print_info: model params     = 1.41 B
0.00.047.870 I print_info: general.name     = 1.4B
0.00.047.870 I print_info: vocab type       = BPE
0.00.047.871 I print_info: n_vocab          = 50304
0.00.047.871 I print_info: n_merges         = 50009
0.00.047.871 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.871 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.873 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.873 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.873 I print_info: LF token         = 128 'Ä'
0.00.047.873 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.874 I print_info: max token length = 1024
0.00.049.843 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.843 I load_tensors: offloading output layer to GPU
0.00.049.843 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.854 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.049.855 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.120 I llama_init_from_model: n_seq_max     = 1
0.00.050.120 I llama_init_from_model: n_ctx         = 128
0.00.050.121 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.121 I llama_init_from_model: n_batch       = 128
0.00.050.121 I llama_init_from_model: n_ubatch      = 128
0.00.050.121 I llama_init_from_model: flash_attn    = 0
0.00.050.121 I llama_init_from_model: freq_base     = 10000.0
0.00.050.122 I llama_init_from_model: freq_scale    = 1
0.00.050.122 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.122 I ggml_metal_init: allocating
0.00.050.125 I ggml_metal_init: found device: Apple M4
0.00.050.126 I ggml_metal_init: picking default device: Apple M4
0.00.050.692 I ggml_metal_init: using embedded metal library
0.00.053.010 I ggml_metal_init: GPU name:   Apple M4
0.00.053.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.013 I ggml_metal_init: simdgroup reduction   = true
0.00.053.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.013 I ggml_metal_init: has bfloat            = true
0.00.053.013 I ggml_metal_init: use bfloat            = true
0.00.053.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.928 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.161 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.174 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.191 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.124 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.125 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.126 I llama_init_from_model: graph nodes  = 967
0.00.064.126 I llama_init_from_model: graph splits = 2
0.00.064.127 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.127 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.483.616 I 
0.00.483.655 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.483.658 I perplexity: tokenizing the input ..
0.00.491.750 I perplexity: tokenization took 8.09 ms
0.00.491.758 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.631.683 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.632.853 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.632.882 I llama_perf_context_print:        load time =     474.51 ms
0.00.632.883 I llama_perf_context_print: prompt eval time =     139.70 ms /   128 tokens (    1.09 ms per token,   916.26 tokens per second)
0.00.632.884 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.632.885 I llama_perf_context_print:       total time =     149.27 ms /   129 tokens
0.00.633.309 I ggml_metal_free: deallocating

real	0m0.648s
user	0m0.075s
sys	0m0.093s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4511 (99454784)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153c0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153c0aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153c0b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153c0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153c0bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153c0c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153c0c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153c0cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153c0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153c0d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153c0dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153c0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153c0ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153c0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153c0fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153c10350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153c10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153c11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153c118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153c12080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153c127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153c12ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153c135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153c13e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153c145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153c14860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153c14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153c15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153c16020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153c162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153c16780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153c16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153c172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153c17810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153c17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153c17f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153c18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153c188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153c18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153c191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153c19690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153c19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153c19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153c1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153c1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153c1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153c1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153c1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153c1c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153c1c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153c1cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153c1d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153c1dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153c1e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153c1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153c1ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153c1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153c1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153c1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153c202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153c20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153c20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153c20ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153c21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153c21800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153c21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153c22140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153c225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153c22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153c22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153c233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153c23860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153c23d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153c24250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153c247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153c24cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153c25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153c25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153c25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153c26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153c26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153c26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x153c27220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153c27770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153c27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153c28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153c28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153c28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153c29200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153c29750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153c29ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153c2a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153c2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153c2ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153c2b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153c2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153c2bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153c1b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153c2c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153c2c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153c2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153c2d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153c2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153c2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153c2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153c2e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153c2edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153c2f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153c2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153c2fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153c30310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153c30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x153c30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153c31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153c316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153c31b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153c32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153c324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153c32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153c32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153c332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153c33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153c33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153c34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153c34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153c349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153c34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153c35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153c357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153c35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153c360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153c36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153c36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153c36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153c37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153c37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153c37cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153c38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153c385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153c38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153c38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153c393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153c39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153c39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153c3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153c3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153c3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153c3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153c3b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153c3b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153c3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153c3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153c3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153c3cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153c3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153c3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153c3d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153c3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153c3e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153c3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153c3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153c3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153c3f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153c3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153c3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153c402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153c40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153c40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153c410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153c41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153c419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153c41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153c42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153c427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153c42c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153c43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153c435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153c43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153c43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153c44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153c44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153c44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153c45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153c45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153c45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153c45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153c463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153c46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153c46d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153c471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153c47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153c47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153c47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153c48500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153c48a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153c48fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153c494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153c497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153c49dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153c4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153c4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153c4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x153c4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153c4b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153c4bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153c4c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153c4cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153c4d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153c4d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153c4db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153c4e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153c4e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153c4ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153c4f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153c4f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153c4fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153c502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153c50800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153c50d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153c512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153c517f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153c51d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153c52290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153c527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153c52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153c53280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153c537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153c53d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153c54270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153c547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153c54d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153c55260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153c557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153c55d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153c56250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153c567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153c56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153c57240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153c57790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153c57ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153c58230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153c58780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153c58cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153c59220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153c59770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153c59cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153c5a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153c5a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153c5acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153c5b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153c5b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153c5bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153c5c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153c5c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153c5cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153c5d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153c5d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153c5dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153c5e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153c5e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153c5ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153c5f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153c5f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153c5fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153c601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153c60700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153c60c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153c610f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153c61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153c61a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153c61ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153c62370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153c62810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153c62cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153c63150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153c635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153c63a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153c63f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153c643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153c64870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153c64d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153c651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153c65700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153c65e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153c66540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153c66c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153c67380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153c67640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x153c67e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153c680f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153c68700 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.140.420 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.140.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153a04ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153a05150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153a055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153a05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153a05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153a06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153a06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153a06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153a07060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153a074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153a07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153a08050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153a08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153a09320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153a09b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153a0a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153a0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153a0b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153a0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153a0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153a0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153a0cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153a0d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153a0db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153a0e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153a0e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153a0e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153a0ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153a0f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153a0f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153a0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153a0fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153a10360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153a10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153a10a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153a10f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153a11370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153a117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153a11c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153a120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153a12530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153a129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153a12e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153a13280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153a136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153a13b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153a13fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153a14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153a148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153a14d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153a15190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153a15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153a15a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153a15ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153b04410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153b046d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153b04c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153b05160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153b055d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153b05a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153b05eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153b06320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153b06790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153b06c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153b07070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153b074e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153b07950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153b07dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153b08230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153b086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153b08b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153b08f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153b093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153b09860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153b09cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153b0a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153b0a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153b0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153b0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153b0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153b0b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153b0bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x153b0c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153b0c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153b0c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153b0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153b0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153b0d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153b0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153b0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153b0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153b0e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153b0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153b0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153b0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153b0fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153b0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153b102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153b10750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153b10bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153b11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153b114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153b11910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153b11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153b121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153b12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153b12ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153b12f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153b133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153b13820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153b13c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153b14100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x153b14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153b149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153b14e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153b152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153b15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153b15ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153b16010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153b16480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153b168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153b16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153b171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153b17640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153b17ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153b17f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153b18390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153b18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153b18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153b190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153b19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153b199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153b19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153b1a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153b1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153b1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153b1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153b1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153b1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153b1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153b1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153b1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153b1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153b1cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153b1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153b1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153b1dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153b1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153b1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153b1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153b1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153b1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153b1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153b1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153b1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153b20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153b208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153b20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153b21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153b21600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153b21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153b21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153b22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153b227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153b22c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153b230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153b23cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153b23f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153b24250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153b246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153b24b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153b24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153b25410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153b25880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153b25cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153b26160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153b265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153b26a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153b26eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153b27320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153b27790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153b27c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153b28070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153b284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153b28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153b28dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153b29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153b296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153b29b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153b29f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153b2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153b2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153b2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153b2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153b2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153b2ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153b2be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153b2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153b2c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153b2cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153b2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153b2d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x153b2da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153b2df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153b2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153b2e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153b2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153b2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153b2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153b2fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153b30690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153b30950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153b30f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153b314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153b31a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153b32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153b32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153b32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153b33190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153b33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153b33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153b342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153b34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153b34e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153b35410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153b359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153b35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153b36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153b36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153b370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153b37690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153b37c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153b38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153b387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153b38d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153b39350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153b39910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153b39ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153b3a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153b3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153b3b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153b3b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153b3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153b3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153b3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153b3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153b3d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153b3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153b3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153b3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153b3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153b3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153b3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153b3fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153b40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153b40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153b40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153b411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153b41790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153b41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153b42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153b428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153b42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153b43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153b43a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153b43fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153b44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153b44b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153b45050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153b45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153b45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153b45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153b46450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153b46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153b46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153b47350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153b47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153b47d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153b48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153b48750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153b48c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153b49150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153b49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153b4a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153b4a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153b4aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153b4b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153b4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x153b4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153b4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153b4c940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153b49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153b3a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153b39610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153b36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153b33a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153b43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153b40910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153b3e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153b3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153b34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153b31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153b36dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153b37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153b3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153b3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153b42010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153b35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153b3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153b38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153b3ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153b356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153b43710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153b328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153b311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153b33450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153b43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153b39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153b41490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153b37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153b39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153b3db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153b35110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153b3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153b3f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153b33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153b425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153b3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153b3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153b44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153b32e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153b44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153b32310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153b42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153b3c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153b3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153b41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153b40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153b384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153b4bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153b4d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153b4d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153b4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153b4d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153b4db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153b4dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153b4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153b4e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153b4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153b4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153b4eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153b4ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153b4f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153b4f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153b4f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153b4f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153b4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153b4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153b501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153b50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153b50720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153b509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153b50ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153b50f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153b51220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153b514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153b517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153b51a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153b51d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153b51fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153b522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153b52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153b52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x153b52ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153b52da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153b53060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153b53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153b535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153b538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153b53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153b53e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153b540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153b543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153b54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153b54920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153b54be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153b54ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153b55160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153b55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153b556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153b559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153b55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153b55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153b561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153b564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153b56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153b56a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153b56ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153b56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153b57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153b57520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153b577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153b57aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x153b57d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153b58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153b582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153b585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153b58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153b58b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153b58de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153b590a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153b59360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153b59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153b598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153b59ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153b59e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153b5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153b5a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153b5a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153b5a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153b5ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153b5aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153b5b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153b5b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153b5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153b5b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153b5bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153b5bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153b5c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153b5c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153b5c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153b5ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153b5cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153b5cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153b5d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153b5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153b5d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153b5dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153b5dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153b5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153b5e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153b5e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153b5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153b5eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153b5ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153b5f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153b5f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153b5f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153b5f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153b5fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153b5fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153b60160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153b60420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153b606e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153b609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153b60c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153b60f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153b611e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153b615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153b618a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153b61b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153b61fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153b62440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153b628b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153b62d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153b63190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153b63600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153b63a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153b63ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153b64350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153b647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153b64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153b650a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153b65510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153b65980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153b65df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153b66260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153b666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153b66b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153b66fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153b67420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153b67890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153b67d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153b68170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153b685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153b68a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153b68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153b69330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153b697a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153b69c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153b6a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153a08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153a04880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x153a161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153a0ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153a16460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153a16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153a16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153a170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153a17390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153a17650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153a17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153a18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153a18430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153a186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153a189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153a18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153a18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153a191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153a194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153a19770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153a19a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153a19cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153a19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153a1a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153a1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153a1a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153a1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153a1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153a1b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153a1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153a1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153a1b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153a1bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153a1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153a1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153a1c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153a1c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153a1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153a1cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153a1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153a1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153a1d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153a1d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153a1d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153a1dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153a1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153a1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153a1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153a1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153a1e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153a1ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153a1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153a1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153a1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153a1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153a204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153a20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153a20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153a214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153a219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153a21f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153a22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153a229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153a22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153a23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153a239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153a241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153a244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153a249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153a24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153a253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153a258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153a25da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153a262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153a267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153a26ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153a271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153a276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153a27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153a280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153a285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153a28aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153a294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153a29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153a2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153a2aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153a2acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x153a2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153a2b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153a2bd90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.854s
user	0m0.297s
sys	0m0.334s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4511 (99454784)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ee0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ee0c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ee0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ee0cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ee0d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ee0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ee0e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ee0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ee0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ee0f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ee0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ee0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ee105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ee10d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ee11570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ee11c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ee123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ee12ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ee131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ee139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ee140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ee14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ee14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ee157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ee15ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ee161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ee167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ee17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ee17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ee17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ee180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ee18380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ee18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ee19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ee19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ee198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ee19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ee1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ee1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ee1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ee1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ee1b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ee1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ee1bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ee1c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ee1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ee1cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ee1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ee1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ee1e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ee1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ee1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ee1f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ee1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ee20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ee206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ee20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ee20e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ee21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ee21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ee21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ee22360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ee22800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ee22ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ee23140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ee235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ee23a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ee23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ee243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ee24860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ee24d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ee251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ee25640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ee25b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ee260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ee26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ee26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ee270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ee27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ee27b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ee280c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ee28610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ee28b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ee290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ee29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ee29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ee2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ee2a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ee2ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ee2b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ee2b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ee2bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ee2c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ee2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ee2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ee2d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ee2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ee1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ee2da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ee2e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ee2e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ee2ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ee2f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ee2f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ee2fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ee301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ee30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ee30c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ee311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ee31700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ee31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ee321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ee326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ee32b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ee33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ee334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ee33970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ee33e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ee342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ee34750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ee34bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ee35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ee35530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ee359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ee35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ee36310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ee367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ee36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ee370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ee37590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ee37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ee37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ee38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ee38810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ee38cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ee39150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ee395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ee39a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ee39f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ee3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ee3a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ee3ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ee3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ee3b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ee3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ee3bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ee3c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ee3c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ee3cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ee3d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ee3d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ee3db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ee3dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ee3e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ee3e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ee3edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ee3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ee3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ee3fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ee40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ee404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ee40990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ee40e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ee412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ee41770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ee41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ee420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ee42550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ee429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ee42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ee43330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ee437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ee43c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ee44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ee445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ee44a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ee44ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ee45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ee45830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ee45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ee46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ee46610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ee46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ee46f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ee473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ee47890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ee47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ee481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ee48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ee48b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ee48fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ee49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ee498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ee49e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ee4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ee4a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ee4ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ee4b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ee4b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ee4bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ee4c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ee4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ee4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ee4d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ee4d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ee4de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ee4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ee4eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ee4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ee4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ee4fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ee50160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ee506b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ee50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ee51150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ee516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ee51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ee52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ee52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ee52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ee53130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ee53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ee53bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ee54120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ee54670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ee54bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ee55110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ee55660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ee55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ee56100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ee56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ee56ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ee570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ee57640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ee57b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ee580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ee58630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ee58b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ee590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ee59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ee59b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ee5a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ee5a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ee5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ee5b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ee5b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ee5bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ee5c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ee5c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ee5cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ee5d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ee5d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ee5db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ee5e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ee5e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ee5eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ee5f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ee5f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ee5fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ee60060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ee605b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ee60b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ee61050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ee615a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ee61af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ee62040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ee62590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ee62a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ee62ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ee63370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ee63810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ee63cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ee64150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ee645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ee64a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ee64f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ee653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ee65870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ee65d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ee661b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ee66650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ee66af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ee67040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ee67760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ee67e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ee685a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ee68cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ee68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ee69770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ee69a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ee6a040 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.016 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ee69cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ee4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ee4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ee4bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ee1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ee1eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ee210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ee4db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ee16460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ee1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ee1d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ee1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ee1c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ee1e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ee15460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ee216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ee2dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ee69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ee18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ee18900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ee4e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ee4c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ee16a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ee16d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ee16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ee6a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ee6a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ee6aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ee6ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ee6afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ee6b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ee6b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ee6b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ee6baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ee6bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ee6c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ee6c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ee6c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ee6c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ee6cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ee6cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ee6d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ee6d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ee6d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ee6d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ee6dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ee6de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ee6e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ee6e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ee6e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ee6e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ee6ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ee6eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ee6f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ee6f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ee6f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ee6f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ee6fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ee6ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ee70220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ee704e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ee707a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ee70a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ee70d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ee70fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ee712a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ee71560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ee71820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ee71ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ef04430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ef048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ef04d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ef05180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ef055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ef05a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ef05ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ef06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ef067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ef06c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ef07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ef07500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ef07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ef07de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ef08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ef086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ef08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ef08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ef09410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ef09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ef09cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ef0a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ef0a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ef0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ef0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ef0b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ef0b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ef0bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ef0c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ef0c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ef0c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ef0cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ef0d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ef0d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ef0db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ef0df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ef0e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ef0e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ef0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ef0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ef0f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ef0fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ef0fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ef10300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ef10770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ef10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ef11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ef114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ef11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ef11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ef12210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ef12680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ef12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ef12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ef133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ef13840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ef13cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ef14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ef14590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ef14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ef14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ef152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ef15750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ef15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ef16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ef164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ef16910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ef16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ef171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ef17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ef17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ef17f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ef183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ef18820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ef18c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ef19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ef19570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ef199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ef19e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ef1a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ef1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ef1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ef1b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ef1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ef1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ef1bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ef1c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ef1c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ef1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ef1cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ef1d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ef1d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ef1dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ef1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ef1e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ef1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ef1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ef1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ef1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ef1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ef20450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ef208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ef20d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ef211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ef21610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ef21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ef21ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ef22360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ef227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ef22c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ef230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ef23520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ef23990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ef23e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ef24270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ef246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ef24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ef24fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ef25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ef258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ef25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ef26180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ef265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ef26a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ef26ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150004080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1500044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150004960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150004dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150005240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1500056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150005b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150005f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x150006400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150006870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150006ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x150007150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1500075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150007b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150007fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150008420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150008f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150009230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1500094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150009960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150009dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15000a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15000a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15000ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15000af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15000b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15000b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15000bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15000c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15000c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15000ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15000cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15000d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15000d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15000dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15000e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15000e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15000e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15000edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15000f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15000f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15000fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15000ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1500103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150010850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150010cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150011130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1500115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150011a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150011e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1500122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150012760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150012bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150013040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1500134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150013920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150013d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150014200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150014670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150014ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150014f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1500153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150015830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150015ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x150016110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150016580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1500169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150016e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1500172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150017740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150017bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150018020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150018490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x150018900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150018d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1500191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150019650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150019ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150019f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15001a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15001a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15001ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15001b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15001b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15001b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15001be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15001c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15001c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15001cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15001d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15001dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15001e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15001eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15001ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15001f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15001f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15001fea0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15001f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15001fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15001ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1500086e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x150020160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150020420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1500206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1500209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150020d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x150021000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1500212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x150021580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x150021b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150022120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150022750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150022c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1500231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x150023710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x150023c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150024420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150024960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150024ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1500253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150025920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150025e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1500263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150026660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150026be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150026ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150027160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150027420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1500276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1500279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150027c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150027f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1500281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1500284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150028760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150028a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150028ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150028fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150029260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150029520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1500297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x150029aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x150029d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15002a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15002a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15002a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15002a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15002ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15002ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15002b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15002b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15002b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15002b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15002bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15002be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15002c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15002c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15002c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15002c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15002cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15002cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15002d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15002d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15002d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15002d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15002dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15002df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15002e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15002e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15002e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15002ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15002ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15002efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15002f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15002f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15002f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15002fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15002fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150030060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150030320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1500305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1500308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150030b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x150030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1500310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150031b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1500320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150032620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150032b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1500330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x150033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x150033b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1500340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x150034600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x150034b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1500350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1500355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150035b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1500361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150036490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x150036750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150036bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150037030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1500374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150037910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x150037d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1500381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x150038660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150038ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x150039040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1500394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150039920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x150039d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15003a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15003a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15003aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15003af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15003b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15003b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15003bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15003c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15003c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15003c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15003ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15003d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15003d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15003dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15003e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15003e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15003e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15003ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15003f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15003f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15003fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15003ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1500403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150040810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150040c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1500410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150041560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1500419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150041e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1500422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150042720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150042b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150043000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150043470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1500438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150043d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1500441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150044630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150044aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150044f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150045380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1500457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150045c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1500460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x150046540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1500469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150046e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x150047290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150047700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x150047b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x150047fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150048450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1500488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150048d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1500491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150049610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150049a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150049ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15004a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15004a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15004ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15004b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15004b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15004b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15004be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15004c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15004c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15004cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15004cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15004d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15004d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15004dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15004e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15004e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15004ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15004eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15004f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15004f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15004fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150050090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150050500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150050970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150050de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150051250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1500517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150051cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150052160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1500525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150052a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150052eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1500533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1500538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150054450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150054710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150054cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150055290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150055850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150055e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1500563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x150056990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150056f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150057510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x150057ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150058090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150058650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150058c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1500591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150059790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150059d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15005a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15005a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15005ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15005b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15005ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15005bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15005c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15005cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15005d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15005d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15005dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15005e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15005e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15005edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15005f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15005f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15005ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1500604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150060a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150061050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150061610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150061bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150062190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150062750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150062d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1500632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150063890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150063e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150064410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1500649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150064f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x150065550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150065b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1500660d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150066690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150066c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150067210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1500677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150067d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150068350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x150068910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150068e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150069310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150069810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150069d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15006a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15006a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15006ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15006b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15006b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15006bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15006c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15006c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15006ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15006cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15006d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15006de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15006e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15006ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15006f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15006f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15006fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1500700f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x150070700 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.917s
user	0m0.242s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.59 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.15 sec*proc (2 tests)

Total Test time (real) =   1.16 sec
        1.22 real         0.71 user         0.07 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
