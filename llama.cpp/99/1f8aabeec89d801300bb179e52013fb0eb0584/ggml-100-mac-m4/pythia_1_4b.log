Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:44 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.611s
user	0m0.710s
sys	0m0.959s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  7%] Built target xxhash
[  7%] Built target build_info
[  7%] Built target sha256
[  7%] Built target sha1
[  8%] Linking CXX shared library libggml-base.dylib
[  8%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/cpu-feats-x86.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 12%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Built target llama-gguf
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-run
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Built target llava
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target test-c
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-run
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target llama-simple-chat
[ 32%] Built target llama-simple
[ 33%] Linking CXX static library libllava_static.a
[ 33%] Built target common
[ 33%] Built target llava_static
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Linking CXX executable ../bin/test-arg-parser
[ 42%] Linking CXX executable ../bin/test-grammar-parser
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-chat-template
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Built target test-arg-parser
[ 47%] Built target test-log
[ 47%] Built target test-grammar-parser
[ 47%] Built target test-chat-template
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Built target test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Built target test-llama-grammar
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 56%] Linking CXX executable ../bin/test-quantize-fns
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Built target test-backend-ops
[ 61%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../../bin/llama-batched
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target test-quantize-perf
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-rope
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Built target llama-batched-bench
[ 65%] Built target test-json-schema-to-grammar
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Built target llama-batched
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-lookup
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-embedding
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-eval-callback
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-gbnf-validator
[ 74%] Built target llama-infill
[ 74%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Built target llama-lookahead
[ 74%] Built target llama-bench
[ 74%] Built target llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Generating loading.html.hpp
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Generating completion.js.hpp
[ 84%] Built target llama-lookup-create
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-lookup-merge
[ 84%] Built target llama-cli
[ 84%] Built target llama-lookup-stats
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Generating deps_daisyui.min.css.hpp
[ 85%] Built target llama-passkey
[ 85%] Built target llama-parallel
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Built target llama-perplexity
[ 88%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Built target llama-retrieval
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-speculative-simple
[ 95%] Built target llama-speculative
[ 95%] Built target llama-save-load-state
[ 95%] Built target llama-tokenize
[ 95%] Generating deps_markdown-it.js.hpp
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Generating deps_tailwindcss.js.hpp
[ 96%] Generating deps_vue.esm-browser.js.hpp
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Generating index.html.hpp
[ 98%] Built target llama-minicpmv-cli
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.629s
user	0m6.035s
sys	0m9.311s

main: quantize time =  5630.62 ms
main:    total time =  5630.62 ms

main: quantize time =  1843.43 ms
main:    total time =  1843.43 ms

main: quantize time =  2019.84 ms
main:    total time =  2019.84 ms

main: quantize time =  2643.20 ms
main:    total time =  2643.20 ms

main: quantize time =  2688.61 ms
main:    total time =  2688.61 ms

main: quantize time =  5181.70 ms
main:    total time =  5181.70 ms

main: quantize time =  5718.55 ms
main:    total time =  5718.55 ms

main: quantize time =  6870.09 ms
main:    total time =  6870.09 ms

main: quantize time =  6076.08 ms
main:    total time =  6076.08 ms

main: quantize time =  4607.73 ms
main:    total time =  4607.73 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.115 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.234 I main: llama backend init
0.00.000.242 I main: load the model and apply lora adapter, if any
0.00.031.982 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.096 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.131 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.136 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.141 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.146 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.425 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.970 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.973 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.974 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.975 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.976 I llama_model_loader: - type  f32:  194 tensors
0.00.062.976 I llama_model_loader: - type  f16:   98 tensors
0.00.096.002 I llm_load_vocab: special tokens cache size = 25
0.00.103.058 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.103.061 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.103.062 I llm_load_print_meta: arch             = gptneox
0.00.103.062 I llm_load_print_meta: vocab type       = BPE
0.00.103.062 I llm_load_print_meta: n_vocab          = 50304
0.00.103.062 I llm_load_print_meta: n_merges         = 50009
0.00.103.062 I llm_load_print_meta: vocab_only       = 0
0.00.103.063 I llm_load_print_meta: n_ctx_train      = 2048
0.00.103.063 I llm_load_print_meta: n_embd           = 2048
0.00.103.063 I llm_load_print_meta: n_layer          = 24
0.00.103.066 I llm_load_print_meta: n_head           = 16
0.00.103.067 I llm_load_print_meta: n_head_kv        = 16
0.00.103.067 I llm_load_print_meta: n_rot            = 32
0.00.103.067 I llm_load_print_meta: n_swa            = 0
0.00.103.068 I llm_load_print_meta: n_embd_head_k    = 128
0.00.103.068 I llm_load_print_meta: n_embd_head_v    = 128
0.00.103.068 I llm_load_print_meta: n_gqa            = 1
0.00.103.069 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.103.089 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.103.090 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.103.091 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.103.091 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.103.091 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.103.091 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.103.092 I llm_load_print_meta: n_ff             = 8192
0.00.103.092 I llm_load_print_meta: n_expert         = 0
0.00.103.092 I llm_load_print_meta: n_expert_used    = 0
0.00.103.092 I llm_load_print_meta: causal attn      = 1
0.00.103.093 I llm_load_print_meta: pooling type     = 0
0.00.103.093 I llm_load_print_meta: rope type        = 2
0.00.103.093 I llm_load_print_meta: rope scaling     = linear
0.00.103.093 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.103.093 I llm_load_print_meta: freq_scale_train = 1
0.00.103.094 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.103.094 I llm_load_print_meta: rope_finetuned   = unknown
0.00.103.094 I llm_load_print_meta: ssm_d_conv       = 0
0.00.103.094 I llm_load_print_meta: ssm_d_inner      = 0
0.00.103.094 I llm_load_print_meta: ssm_d_state      = 0
0.00.103.094 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.103.094 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.103.095 I llm_load_print_meta: model type       = 1.4B
0.00.103.095 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.103.096 I llm_load_print_meta: model params     = 1.41 B
0.00.103.096 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.103.096 I llm_load_print_meta: general.name     = 1.4B
0.00.103.097 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.103.097 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.103.099 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.103.099 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.103.100 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.103.100 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.103.100 I llm_load_print_meta: max token length = 1024
0.00.105.722 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.105.723 I llm_load_tensors: offloading output layer to GPU
0.00.105.723 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.105.740 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.105.741 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.106.672 I llama_new_context_with_model: n_seq_max     = 1
0.00.106.672 I llama_new_context_with_model: n_ctx         = 2048
0.00.106.673 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.106.673 I llama_new_context_with_model: n_batch       = 2048
0.00.106.673 I llama_new_context_with_model: n_ubatch      = 512
0.00.106.673 I llama_new_context_with_model: flash_attn    = 0
0.00.106.673 I llama_new_context_with_model: freq_base     = 10000.0
0.00.106.674 I llama_new_context_with_model: freq_scale    = 1
0.00.106.674 I ggml_metal_init: allocating
0.00.106.677 I ggml_metal_init: found device: Apple M4
0.00.106.679 I ggml_metal_init: picking default device: Apple M4
0.00.107.330 I ggml_metal_init: using embedded metal library
0.00.121.282 I ggml_metal_init: GPU name:   Apple M4
0.00.121.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.285 I ggml_metal_init: simdgroup reduction   = true
0.00.121.285 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.285 I ggml_metal_init: has bfloat            = true
0.00.121.285 I ggml_metal_init: use bfloat            = true
0.00.121.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.286 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.158.831 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.158.838 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.158.858 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.159.825 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.159.826 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.159.827 I llama_new_context_with_model: graph nodes  = 967
0.00.159.827 I llama_new_context_with_model: graph splits = 2
0.00.159.850 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.234.092 I main: llama threadpool init, n_threads = 4
0.00.234.123 I 
0.00.234.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.234.162 I 
0.00.234.232 I sampler seed: 1234
0.00.234.237 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.234.272 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.234.273 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.234.273 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.090.207 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.02.090.208 I llama_perf_context_print:        load time =     202.09 ms
0.02.090.209 I llama_perf_context_print: prompt eval time =      37.44 ms /     7 tokens (    5.35 ms per token,   186.97 tokens per second)
0.02.090.210 I llama_perf_context_print:        eval time =    1815.58 ms /    63 runs   (   28.82 ms per token,    34.70 tokens per second)
0.02.090.211 I llama_perf_context_print:       total time =    1856.12 ms /    70 tokens
0.02.090.388 I ggml_metal_free: deallocating

real	0m2.395s
user	0m0.146s
sys	0m0.094s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.756 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.060 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.060 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.061 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.064 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.875 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.785 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.787 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.787 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.788 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.789 I llama_model_loader: - type  f32:  194 tensors
0.00.034.789 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.873 I llm_load_vocab: special tokens cache size = 25
0.00.063.803 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.806 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.807 I llm_load_print_meta: arch             = gptneox
0.00.063.807 I llm_load_print_meta: vocab type       = BPE
0.00.063.807 I llm_load_print_meta: n_vocab          = 50304
0.00.063.808 I llm_load_print_meta: n_merges         = 50009
0.00.063.808 I llm_load_print_meta: vocab_only       = 0
0.00.063.808 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.808 I llm_load_print_meta: n_embd           = 2048
0.00.063.809 I llm_load_print_meta: n_layer          = 24
0.00.063.816 I llm_load_print_meta: n_head           = 16
0.00.063.817 I llm_load_print_meta: n_head_kv        = 16
0.00.063.817 I llm_load_print_meta: n_rot            = 32
0.00.063.817 I llm_load_print_meta: n_swa            = 0
0.00.063.817 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.817 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.818 I llm_load_print_meta: n_gqa            = 1
0.00.063.819 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.832 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.833 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.834 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.834 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.834 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.834 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.835 I llm_load_print_meta: n_ff             = 8192
0.00.063.835 I llm_load_print_meta: n_expert         = 0
0.00.063.835 I llm_load_print_meta: n_expert_used    = 0
0.00.063.835 I llm_load_print_meta: causal attn      = 1
0.00.063.835 I llm_load_print_meta: pooling type     = 0
0.00.063.836 I llm_load_print_meta: rope type        = 2
0.00.063.837 I llm_load_print_meta: rope scaling     = linear
0.00.063.837 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.839 I llm_load_print_meta: freq_scale_train = 1
0.00.063.840 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.840 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.840 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.840 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.840 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.840 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.840 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.840 I llm_load_print_meta: model type       = 1.4B
0.00.063.841 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.841 I llm_load_print_meta: model params     = 1.41 B
0.00.063.842 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.842 I llm_load_print_meta: general.name     = 1.4B
0.00.063.842 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.842 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.842 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.845 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.846 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.846 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.846 I llm_load_print_meta: max token length = 1024
0.00.066.317 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.318 I llm_load_tensors: offloading output layer to GPU
0.00.066.318 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.328 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.330 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.303 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.304 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.304 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.304 I llama_new_context_with_model: n_batch       = 2048
0.00.067.305 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.305 I llama_new_context_with_model: flash_attn    = 0
0.00.067.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.306 I llama_new_context_with_model: freq_scale    = 1
0.00.067.306 I ggml_metal_init: allocating
0.00.067.313 I ggml_metal_init: found device: Apple M4
0.00.067.316 I ggml_metal_init: picking default device: Apple M4
0.00.068.046 I ggml_metal_init: using embedded metal library
0.00.070.203 I ggml_metal_init: GPU name:   Apple M4
0.00.070.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.206 I ggml_metal_init: simdgroup reduction   = true
0.00.070.206 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.207 I ggml_metal_init: has bfloat            = true
0.00.070.207 I ggml_metal_init: use bfloat            = true
0.00.070.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.260 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.274 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.304 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.477 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.479 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.480 I llama_new_context_with_model: graph nodes  = 967
0.00.107.480 I llama_new_context_with_model: graph splits = 2
0.00.107.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.815.635 I main: llama threadpool init, n_threads = 4
0.01.815.696 I 
0.01.815.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.815.746 I 
0.01.816.288 I sampler seed: 1234
0.01.816.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.816.337 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.816.339 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.816.339 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.906.615 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55468.75 tokens per second)
0.02.906.616 I llama_perf_context_print:        load time =    1805.87 ms
0.02.906.616 I llama_perf_context_print: prompt eval time =      42.59 ms /     7 tokens (    6.08 ms per token,   164.37 tokens per second)
0.02.906.617 I llama_perf_context_print:        eval time =    1044.75 ms /    63 runs   (   16.58 ms per token,    60.30 tokens per second)
0.02.906.621 I llama_perf_context_print:       total time =    1090.98 ms /    70 tokens
0.02.906.816 I ggml_metal_free: deallocating

real	0m2.923s
user	0m0.119s
sys	0m0.266s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.011.146 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.765 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.771 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.773 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.774 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.777 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.777 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.778 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.778 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.783 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.783 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.677 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.595 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.597 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.598 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.599 I llama_model_loader: - type  f32:  194 tensors
0.00.026.599 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.599 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.046 I llm_load_vocab: special tokens cache size = 25
0.00.054.034 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.038 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.038 I llm_load_print_meta: arch             = gptneox
0.00.054.038 I llm_load_print_meta: vocab type       = BPE
0.00.054.039 I llm_load_print_meta: n_vocab          = 50304
0.00.054.039 I llm_load_print_meta: n_merges         = 50009
0.00.054.039 I llm_load_print_meta: vocab_only       = 0
0.00.054.039 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.039 I llm_load_print_meta: n_embd           = 2048
0.00.054.040 I llm_load_print_meta: n_layer          = 24
0.00.054.044 I llm_load_print_meta: n_head           = 16
0.00.054.045 I llm_load_print_meta: n_head_kv        = 16
0.00.054.045 I llm_load_print_meta: n_rot            = 32
0.00.054.045 I llm_load_print_meta: n_swa            = 0
0.00.054.046 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.046 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.046 I llm_load_print_meta: n_gqa            = 1
0.00.054.047 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.059 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.059 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.060 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.060 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.060 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.061 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.061 I llm_load_print_meta: n_ff             = 8192
0.00.054.061 I llm_load_print_meta: n_expert         = 0
0.00.054.061 I llm_load_print_meta: n_expert_used    = 0
0.00.054.062 I llm_load_print_meta: causal attn      = 1
0.00.054.063 I llm_load_print_meta: pooling type     = 0
0.00.054.063 I llm_load_print_meta: rope type        = 2
0.00.054.063 I llm_load_print_meta: rope scaling     = linear
0.00.054.063 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.064 I llm_load_print_meta: freq_scale_train = 1
0.00.054.064 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.064 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.064 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.064 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.064 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.065 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.065 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.065 I llm_load_print_meta: model type       = 1.4B
0.00.054.065 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.066 I llm_load_print_meta: model params     = 1.41 B
0.00.054.067 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.067 I llm_load_print_meta: general.name     = 1.4B
0.00.054.068 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.068 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.068 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.068 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.070 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.071 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.071 I llm_load_print_meta: max token length = 1024
0.00.055.923 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.923 I llm_load_tensors: offloading output layer to GPU
0.00.055.923 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.934 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.935 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.879 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.880 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.880 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.880 I llama_new_context_with_model: n_batch       = 2048
0.00.056.880 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.881 I llama_new_context_with_model: flash_attn    = 0
0.00.056.881 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.881 I llama_new_context_with_model: freq_scale    = 1
0.00.056.882 I ggml_metal_init: allocating
0.00.056.889 I ggml_metal_init: found device: Apple M4
0.00.056.893 I ggml_metal_init: picking default device: Apple M4
0.00.057.579 I ggml_metal_init: using embedded metal library
0.00.059.750 I ggml_metal_init: GPU name:   Apple M4
0.00.059.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.753 I ggml_metal_init: simdgroup reduction   = true
0.00.059.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.753 I ggml_metal_init: has bfloat            = true
0.00.059.753 I ggml_metal_init: use bfloat            = true
0.00.059.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.050 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.062 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.100 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.208 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.210 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.210 I llama_new_context_with_model: graph nodes  = 967
0.00.093.210 I llama_new_context_with_model: graph splits = 2
0.00.093.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.750 I main: llama threadpool init, n_threads = 4
0.00.671.786 I 
0.00.671.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.671.811 I 
0.00.672.029 I sampler seed: 1234
0.00.672.035 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.672.078 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.672.078 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.672.078 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.356.476 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.356.477 I llama_perf_context_print:        load time =     660.60 ms
0.01.356.478 I llama_perf_context_print: prompt eval time =      36.55 ms /     7 tokens (    5.22 ms per token,   191.51 tokens per second)
0.01.356.478 I llama_perf_context_print:        eval time =     644.92 ms /    63 runs   (   10.24 ms per token,    97.69 tokens per second)
0.01.356.478 I llama_perf_context_print:       total time =     684.73 ms /    70 tokens
0.01.356.676 I ggml_metal_free: deallocating

real	0m1.374s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.013.441 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.286 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.295 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.296 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.296 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.297 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.297 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.299 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.170 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.208 I llama_model_loader: - type  f32:  194 tensors
0.00.034.208 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.208 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.083 I llm_load_vocab: special tokens cache size = 25
0.00.064.251 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.254 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.254 I llm_load_print_meta: arch             = gptneox
0.00.064.254 I llm_load_print_meta: vocab type       = BPE
0.00.064.255 I llm_load_print_meta: n_vocab          = 50304
0.00.064.255 I llm_load_print_meta: n_merges         = 50009
0.00.064.255 I llm_load_print_meta: vocab_only       = 0
0.00.064.255 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.255 I llm_load_print_meta: n_embd           = 2048
0.00.064.255 I llm_load_print_meta: n_layer          = 24
0.00.064.258 I llm_load_print_meta: n_head           = 16
0.00.064.259 I llm_load_print_meta: n_head_kv        = 16
0.00.064.259 I llm_load_print_meta: n_rot            = 32
0.00.064.259 I llm_load_print_meta: n_swa            = 0
0.00.064.259 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.259 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.260 I llm_load_print_meta: n_gqa            = 1
0.00.064.261 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.273 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.273 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.275 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.277 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.277 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.277 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.278 I llm_load_print_meta: n_ff             = 8192
0.00.064.278 I llm_load_print_meta: n_expert         = 0
0.00.064.278 I llm_load_print_meta: n_expert_used    = 0
0.00.064.278 I llm_load_print_meta: causal attn      = 1
0.00.064.278 I llm_load_print_meta: pooling type     = 0
0.00.064.278 I llm_load_print_meta: rope type        = 2
0.00.064.278 I llm_load_print_meta: rope scaling     = linear
0.00.064.279 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.279 I llm_load_print_meta: freq_scale_train = 1
0.00.064.279 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.279 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.280 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.280 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.281 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.281 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.281 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.281 I llm_load_print_meta: model type       = 1.4B
0.00.064.282 I llm_load_print_meta: model ftype      = Q4_1
0.00.064.282 I llm_load_print_meta: model params     = 1.41 B
0.00.064.283 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.064.283 I llm_load_print_meta: general.name     = 1.4B
0.00.064.283 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.283 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.283 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.283 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.284 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.284 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.284 I llm_load_print_meta: max token length = 1024
0.00.066.307 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.307 I llm_load_tensors: offloading output layer to GPU
0.00.066.308 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.317 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.066.319 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.067.348 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.349 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.349 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.350 I llama_new_context_with_model: n_batch       = 2048
0.00.067.350 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.350 I llama_new_context_with_model: flash_attn    = 0
0.00.067.350 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.351 I llama_new_context_with_model: freq_scale    = 1
0.00.067.351 I ggml_metal_init: allocating
0.00.067.357 I ggml_metal_init: found device: Apple M4
0.00.067.359 I ggml_metal_init: picking default device: Apple M4
0.00.067.993 I ggml_metal_init: using embedded metal library
0.00.070.162 I ggml_metal_init: GPU name:   Apple M4
0.00.070.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.165 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.165 I ggml_metal_init: simdgroup reduction   = true
0.00.070.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.166 I ggml_metal_init: has bfloat            = true
0.00.070.166 I ggml_metal_init: use bfloat            = true
0.00.070.166 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.168 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.678 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.691 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.714 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.789 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.790 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.791 I llama_new_context_with_model: graph nodes  = 967
0.00.102.791 I llama_new_context_with_model: graph splits = 2
0.00.102.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.864.685 I main: llama threadpool init, n_threads = 4
0.00.864.727 I 
0.00.864.755 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.864.757 I 
0.00.864.982 I sampler seed: 1234
0.00.864.987 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.865.031 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.865.034 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.865.035 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.607.091 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.607.092 I llama_perf_context_print:        load time =     851.24 ms
0.01.607.093 I llama_perf_context_print: prompt eval time =      39.99 ms /     7 tokens (    5.71 ms per token,   175.05 tokens per second)
0.01.607.094 I llama_perf_context_print:        eval time =     699.22 ms /    63 runs   (   11.10 ms per token,    90.10 tokens per second)
0.01.607.094 I llama_perf_context_print:       total time =     742.41 ms /    70 tokens
0.01.607.275 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.114s
sys	0m0.173s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.420 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.792 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.617 I llama_model_loader: - type  f32:  194 tensors
0.00.025.617 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.617 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.330 I llm_load_vocab: special tokens cache size = 25
0.00.052.220 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.224 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.225 I llm_load_print_meta: arch             = gptneox
0.00.052.225 I llm_load_print_meta: vocab type       = BPE
0.00.052.225 I llm_load_print_meta: n_vocab          = 50304
0.00.052.225 I llm_load_print_meta: n_merges         = 50009
0.00.052.226 I llm_load_print_meta: vocab_only       = 0
0.00.052.226 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.226 I llm_load_print_meta: n_embd           = 2048
0.00.052.226 I llm_load_print_meta: n_layer          = 24
0.00.052.230 I llm_load_print_meta: n_head           = 16
0.00.052.231 I llm_load_print_meta: n_head_kv        = 16
0.00.052.231 I llm_load_print_meta: n_rot            = 32
0.00.052.232 I llm_load_print_meta: n_swa            = 0
0.00.052.232 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.232 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.233 I llm_load_print_meta: n_gqa            = 1
0.00.052.235 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.249 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.249 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.249 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.250 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.250 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.250 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.250 I llm_load_print_meta: n_ff             = 8192
0.00.052.250 I llm_load_print_meta: n_expert         = 0
0.00.052.250 I llm_load_print_meta: n_expert_used    = 0
0.00.052.252 I llm_load_print_meta: causal attn      = 1
0.00.052.252 I llm_load_print_meta: pooling type     = 0
0.00.052.253 I llm_load_print_meta: rope type        = 2
0.00.052.253 I llm_load_print_meta: rope scaling     = linear
0.00.052.253 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.253 I llm_load_print_meta: freq_scale_train = 1
0.00.052.254 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.254 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.254 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.254 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.254 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.254 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.254 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.254 I llm_load_print_meta: model type       = 1.4B
0.00.052.255 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.256 I llm_load_print_meta: model params     = 1.41 B
0.00.052.256 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.256 I llm_load_print_meta: general.name     = 1.4B
0.00.052.256 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.256 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.257 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.257 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.257 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.257 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.257 I llm_load_print_meta: max token length = 1024
0.00.054.317 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.317 I llm_load_tensors: offloading output layer to GPU
0.00.054.317 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.328 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.329 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.349 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.350 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.350 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.351 I llama_new_context_with_model: n_batch       = 2048
0.00.055.351 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.351 I llama_new_context_with_model: flash_attn    = 0
0.00.055.352 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.352 I llama_new_context_with_model: freq_scale    = 1
0.00.055.352 I ggml_metal_init: allocating
0.00.055.356 I ggml_metal_init: found device: Apple M4
0.00.055.358 I ggml_metal_init: picking default device: Apple M4
0.00.055.951 I ggml_metal_init: using embedded metal library
0.00.057.970 I ggml_metal_init: GPU name:   Apple M4
0.00.057.971 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.972 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.972 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.972 I ggml_metal_init: simdgroup reduction   = true
0.00.057.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.973 I ggml_metal_init: has bfloat            = true
0.00.057.973 I ggml_metal_init: use bfloat            = true
0.00.057.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.526 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.536 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.554 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.623 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.625 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.625 I llama_new_context_with_model: graph nodes  = 967
0.00.088.625 I llama_new_context_with_model: graph splits = 2
0.00.088.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.207 I main: llama threadpool init, n_threads = 4
0.00.712.247 I 
0.00.712.275 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.712.277 I 
0.00.712.495 I sampler seed: 1234
0.00.712.499 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.534 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.536 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.536 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.524.929 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.524.930 I llama_perf_context_print:        load time =     702.78 ms
0.01.524.930 I llama_perf_context_print: prompt eval time =      36.68 ms /     7 tokens (    5.24 ms per token,   190.84 tokens per second)
0.01.524.931 I llama_perf_context_print:        eval time =     772.79 ms /    63 runs   (   12.27 ms per token,    81.52 tokens per second)
0.01.524.931 I llama_perf_context_print:       total time =     812.72 ms /    70 tokens
0.01.525.114 I ggml_metal_free: deallocating

real	0m1.542s
user	0m0.108s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.208 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.697 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.698 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.699 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.701 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.701 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.704 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.498 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.345 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.346 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.346 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.347 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.347 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.347 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.348 I llama_model_loader: - type  f32:  194 tensors
0.00.026.348 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.349 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.624 I llm_load_vocab: special tokens cache size = 25
0.00.052.669 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.672 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.672 I llm_load_print_meta: arch             = gptneox
0.00.052.673 I llm_load_print_meta: vocab type       = BPE
0.00.052.673 I llm_load_print_meta: n_vocab          = 50304
0.00.052.673 I llm_load_print_meta: n_merges         = 50009
0.00.052.673 I llm_load_print_meta: vocab_only       = 0
0.00.052.673 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.674 I llm_load_print_meta: n_embd           = 2048
0.00.052.674 I llm_load_print_meta: n_layer          = 24
0.00.052.677 I llm_load_print_meta: n_head           = 16
0.00.052.679 I llm_load_print_meta: n_head_kv        = 16
0.00.052.680 I llm_load_print_meta: n_rot            = 32
0.00.052.680 I llm_load_print_meta: n_swa            = 0
0.00.052.680 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.680 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.681 I llm_load_print_meta: n_gqa            = 1
0.00.052.682 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.694 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.695 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.696 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.696 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.696 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.696 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.697 I llm_load_print_meta: n_ff             = 8192
0.00.052.697 I llm_load_print_meta: n_expert         = 0
0.00.052.697 I llm_load_print_meta: n_expert_used    = 0
0.00.052.699 I llm_load_print_meta: causal attn      = 1
0.00.052.700 I llm_load_print_meta: pooling type     = 0
0.00.052.700 I llm_load_print_meta: rope type        = 2
0.00.052.700 I llm_load_print_meta: rope scaling     = linear
0.00.052.700 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.701 I llm_load_print_meta: freq_scale_train = 1
0.00.052.701 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.701 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.701 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.701 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.702 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.702 I llm_load_print_meta: model type       = 1.4B
0.00.052.702 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.703 I llm_load_print_meta: model params     = 1.41 B
0.00.052.703 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.703 I llm_load_print_meta: general.name     = 1.4B
0.00.052.704 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.704 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.704 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.704 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.704 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.705 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.705 I llm_load_print_meta: max token length = 1024
0.00.054.728 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.729 I llm_load_tensors: offloading output layer to GPU
0.00.054.729 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.739 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.740 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.666 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.667 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.667 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.667 I llama_new_context_with_model: n_batch       = 2048
0.00.055.667 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.668 I llama_new_context_with_model: flash_attn    = 0
0.00.055.668 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.668 I llama_new_context_with_model: freq_scale    = 1
0.00.055.669 I ggml_metal_init: allocating
0.00.055.672 I ggml_metal_init: found device: Apple M4
0.00.055.674 I ggml_metal_init: picking default device: Apple M4
0.00.056.232 I ggml_metal_init: using embedded metal library
0.00.058.149 I ggml_metal_init: GPU name:   Apple M4
0.00.058.151 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.151 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.151 I ggml_metal_init: simdgroup reduction   = true
0.00.058.153 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.153 I ggml_metal_init: has bfloat            = true
0.00.058.154 I ggml_metal_init: use bfloat            = true
0.00.058.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.155 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.446 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.456 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.487 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.589 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.590 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.591 I llama_new_context_with_model: graph nodes  = 967
0.00.088.591 I llama_new_context_with_model: graph splits = 2
0.00.088.605 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.837.983 I main: llama threadpool init, n_threads = 4
0.00.838.018 I 
0.00.838.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.838.047 I 
0.00.838.258 I sampler seed: 1234
0.00.838.263 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.838.304 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.838.306 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.838.306 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.683.240 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.683.241 I llama_perf_context_print:        load time =     827.77 ms
0.01.683.242 I llama_perf_context_print: prompt eval time =      36.56 ms /     7 tokens (    5.22 ms per token,   191.44 tokens per second)
0.01.683.243 I llama_perf_context_print:        eval time =     805.36 ms /    63 runs   (   12.78 ms per token,    78.23 tokens per second)
0.01.683.243 I llama_perf_context_print:       total time =     845.26 ms /    70 tokens
0.01.683.420 I ggml_metal_free: deallocating

real	0m1.702s
user	0m0.108s
sys	0m0.188s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.248 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.738 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.745 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.747 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.748 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.748 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.748 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.749 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.750 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.751 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.751 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.583 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.294 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.295 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.296 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.296 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.297 I llama_model_loader: - type  f32:  194 tensors
0.00.023.297 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.297 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.297 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.690 I llm_load_vocab: special tokens cache size = 25
0.00.049.640 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.642 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.643 I llm_load_print_meta: arch             = gptneox
0.00.049.643 I llm_load_print_meta: vocab type       = BPE
0.00.049.643 I llm_load_print_meta: n_vocab          = 50304
0.00.049.643 I llm_load_print_meta: n_merges         = 50009
0.00.049.644 I llm_load_print_meta: vocab_only       = 0
0.00.049.644 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.644 I llm_load_print_meta: n_embd           = 2048
0.00.049.644 I llm_load_print_meta: n_layer          = 24
0.00.049.647 I llm_load_print_meta: n_head           = 16
0.00.049.648 I llm_load_print_meta: n_head_kv        = 16
0.00.049.648 I llm_load_print_meta: n_rot            = 32
0.00.049.648 I llm_load_print_meta: n_swa            = 0
0.00.049.648 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.648 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.649 I llm_load_print_meta: n_gqa            = 1
0.00.049.650 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.662 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.663 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.663 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.664 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.664 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.664 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.664 I llm_load_print_meta: n_ff             = 8192
0.00.049.665 I llm_load_print_meta: n_expert         = 0
0.00.049.665 I llm_load_print_meta: n_expert_used    = 0
0.00.049.665 I llm_load_print_meta: causal attn      = 1
0.00.049.665 I llm_load_print_meta: pooling type     = 0
0.00.049.665 I llm_load_print_meta: rope type        = 2
0.00.049.665 I llm_load_print_meta: rope scaling     = linear
0.00.049.666 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.666 I llm_load_print_meta: freq_scale_train = 1
0.00.049.666 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.666 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.667 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.667 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.667 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.667 I llm_load_print_meta: model type       = 1.4B
0.00.049.667 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.668 I llm_load_print_meta: model params     = 1.41 B
0.00.049.668 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.668 I llm_load_print_meta: general.name     = 1.4B
0.00.049.669 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.669 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.671 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.671 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.671 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.672 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.672 I llm_load_print_meta: max token length = 1024
0.00.051.556 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.556 I llm_load_tensors: offloading output layer to GPU
0.00.051.556 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.566 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.567 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.545 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.546 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.546 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.547 I llama_new_context_with_model: n_batch       = 2048
0.00.052.547 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.547 I llama_new_context_with_model: flash_attn    = 0
0.00.052.547 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.548 I llama_new_context_with_model: freq_scale    = 1
0.00.052.548 I ggml_metal_init: allocating
0.00.052.551 I ggml_metal_init: found device: Apple M4
0.00.052.553 I ggml_metal_init: picking default device: Apple M4
0.00.053.110 I ggml_metal_init: using embedded metal library
0.00.055.041 I ggml_metal_init: GPU name:   Apple M4
0.00.055.042 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.043 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.043 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.043 I ggml_metal_init: simdgroup reduction   = true
0.00.055.043 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.043 I ggml_metal_init: has bfloat            = true
0.00.055.044 I ggml_metal_init: use bfloat            = true
0.00.055.044 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.045 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.102 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.109 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.128 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.146 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.148 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.148 I llama_new_context_with_model: graph nodes  = 967
0.00.083.148 I llama_new_context_with_model: graph splits = 2
0.00.083.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.696 I main: llama threadpool init, n_threads = 4
0.00.472.735 I 
0.00.472.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.472.763 I 
0.00.472.924 I sampler seed: 1234
0.00.472.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.472.952 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.472.952 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.472.952 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.153.681 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55642.63 tokens per second)
0.01.153.682 I llama_perf_context_print:        load time =     463.44 ms
0.01.153.683 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.32 tokens per second)
0.01.153.683 I llama_perf_context_print:        eval time =     642.21 ms /    63 runs   (   10.19 ms per token,    98.10 tokens per second)
0.01.153.684 I llama_perf_context_print:       total time =     680.99 ms /    70 tokens
0.01.153.875 I ggml_metal_free: deallocating

real	0m1.173s
user	0m0.107s
sys	0m0.107s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.960 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.254 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.259 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.265 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.266 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.266 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.266 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.270 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.270 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.270 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.271 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.272 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.272 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.273 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.274 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.275 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.275 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.248 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.294 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.217 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.218 I llama_model_loader: - type  f32:  194 tensors
0.00.025.218 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.218 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.218 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.219 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.240 I llm_load_vocab: special tokens cache size = 25
0.00.052.272 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.274 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.275 I llm_load_print_meta: arch             = gptneox
0.00.052.275 I llm_load_print_meta: vocab type       = BPE
0.00.052.275 I llm_load_print_meta: n_vocab          = 50304
0.00.052.275 I llm_load_print_meta: n_merges         = 50009
0.00.052.276 I llm_load_print_meta: vocab_only       = 0
0.00.052.276 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.276 I llm_load_print_meta: n_embd           = 2048
0.00.052.276 I llm_load_print_meta: n_layer          = 24
0.00.052.279 I llm_load_print_meta: n_head           = 16
0.00.052.280 I llm_load_print_meta: n_head_kv        = 16
0.00.052.280 I llm_load_print_meta: n_rot            = 32
0.00.052.282 I llm_load_print_meta: n_swa            = 0
0.00.052.283 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.283 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.284 I llm_load_print_meta: n_gqa            = 1
0.00.052.284 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.297 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.297 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.298 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.298 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.299 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.299 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.300 I llm_load_print_meta: n_ff             = 8192
0.00.052.300 I llm_load_print_meta: n_expert         = 0
0.00.052.300 I llm_load_print_meta: n_expert_used    = 0
0.00.052.300 I llm_load_print_meta: causal attn      = 1
0.00.052.301 I llm_load_print_meta: pooling type     = 0
0.00.052.301 I llm_load_print_meta: rope type        = 2
0.00.052.301 I llm_load_print_meta: rope scaling     = linear
0.00.052.302 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.302 I llm_load_print_meta: freq_scale_train = 1
0.00.052.302 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.302 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.302 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.302 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.303 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.303 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.303 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.303 I llm_load_print_meta: model type       = 1.4B
0.00.052.313 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.313 I llm_load_print_meta: model params     = 1.41 B
0.00.052.313 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.314 I llm_load_print_meta: general.name     = 1.4B
0.00.052.314 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.314 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.314 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.314 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.315 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.315 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.315 I llm_load_print_meta: max token length = 1024
0.00.054.309 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.309 I llm_load_tensors: offloading output layer to GPU
0.00.054.310 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.319 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.321 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.668 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.668 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.668 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.669 I llama_new_context_with_model: n_batch       = 2048
0.00.055.669 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.669 I llama_new_context_with_model: flash_attn    = 0
0.00.055.669 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.670 I llama_new_context_with_model: freq_scale    = 1
0.00.055.670 I ggml_metal_init: allocating
0.00.055.676 I ggml_metal_init: found device: Apple M4
0.00.055.678 I ggml_metal_init: picking default device: Apple M4
0.00.056.210 I ggml_metal_init: using embedded metal library
0.00.058.124 I ggml_metal_init: GPU name:   Apple M4
0.00.058.125 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.126 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.126 I ggml_metal_init: simdgroup reduction   = true
0.00.058.126 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.126 I ggml_metal_init: has bfloat            = true
0.00.058.126 I ggml_metal_init: use bfloat            = true
0.00.058.127 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.606 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.615 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.635 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.617 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.618 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.618 I llama_new_context_with_model: graph nodes  = 967
0.00.086.618 I llama_new_context_with_model: graph splits = 2
0.00.086.632 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.552.715 I main: llama threadpool init, n_threads = 4
0.00.552.751 I 
0.00.552.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.552.781 I 
0.00.553.007 I sampler seed: 1234
0.00.553.011 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.553.053 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.553.072 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.553.072 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.303.191 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.303.192 I llama_perf_context_print:        load time =     542.75 ms
0.01.303.193 I llama_perf_context_print: prompt eval time =      39.62 ms /     7 tokens (    5.66 ms per token,   176.70 tokens per second)
0.01.303.193 I llama_perf_context_print:        eval time =     707.45 ms /    63 runs   (   11.23 ms per token,    89.05 tokens per second)
0.01.303.194 I llama_perf_context_print:       total time =     750.48 ms /    70 tokens
0.01.303.378 I ggml_metal_free: deallocating

real	0m1.321s
user	0m0.108s
sys	0m0.129s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.698 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.523 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.530 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.532 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.532 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.533 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.533 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.534 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.535 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.535 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.536 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.536 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.536 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.423 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.423 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.424 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.424 I llama_model_loader: - type  f32:  194 tensors
0.00.024.425 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.425 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.425 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.538 I llm_load_vocab: special tokens cache size = 25
0.00.051.572 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.575 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.575 I llm_load_print_meta: arch             = gptneox
0.00.051.575 I llm_load_print_meta: vocab type       = BPE
0.00.051.576 I llm_load_print_meta: n_vocab          = 50304
0.00.051.576 I llm_load_print_meta: n_merges         = 50009
0.00.051.576 I llm_load_print_meta: vocab_only       = 0
0.00.051.576 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.576 I llm_load_print_meta: n_embd           = 2048
0.00.051.577 I llm_load_print_meta: n_layer          = 24
0.00.051.580 I llm_load_print_meta: n_head           = 16
0.00.051.580 I llm_load_print_meta: n_head_kv        = 16
0.00.051.580 I llm_load_print_meta: n_rot            = 32
0.00.051.581 I llm_load_print_meta: n_swa            = 0
0.00.051.581 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.581 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.582 I llm_load_print_meta: n_gqa            = 1
0.00.051.583 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.595 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.596 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.598 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.598 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.598 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.598 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.599 I llm_load_print_meta: n_ff             = 8192
0.00.051.600 I llm_load_print_meta: n_expert         = 0
0.00.051.602 I llm_load_print_meta: n_expert_used    = 0
0.00.051.603 I llm_load_print_meta: causal attn      = 1
0.00.051.603 I llm_load_print_meta: pooling type     = 0
0.00.051.603 I llm_load_print_meta: rope type        = 2
0.00.051.604 I llm_load_print_meta: rope scaling     = linear
0.00.051.604 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.604 I llm_load_print_meta: freq_scale_train = 1
0.00.051.604 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.605 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.605 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.605 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.605 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.605 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.605 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.605 I llm_load_print_meta: model type       = 1.4B
0.00.051.615 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.615 I llm_load_print_meta: model params     = 1.41 B
0.00.051.616 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.616 I llm_load_print_meta: general.name     = 1.4B
0.00.051.616 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.616 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.617 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.617 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.617 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.617 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.618 I llm_load_print_meta: max token length = 1024
0.00.053.646 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.646 I llm_load_tensors: offloading output layer to GPU
0.00.053.646 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.656 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.657 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.623 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.624 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.624 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.624 I llama_new_context_with_model: n_batch       = 2048
0.00.054.625 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.625 I llama_new_context_with_model: flash_attn    = 0
0.00.054.625 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.626 I llama_new_context_with_model: freq_scale    = 1
0.00.054.626 I ggml_metal_init: allocating
0.00.054.631 I ggml_metal_init: found device: Apple M4
0.00.054.634 I ggml_metal_init: picking default device: Apple M4
0.00.055.165 I ggml_metal_init: using embedded metal library
0.00.057.089 I ggml_metal_init: GPU name:   Apple M4
0.00.057.091 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.091 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.093 I ggml_metal_init: simdgroup reduction   = true
0.00.057.093 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.093 I ggml_metal_init: has bfloat            = true
0.00.057.093 I ggml_metal_init: use bfloat            = true
0.00.057.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.243 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.254 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.272 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.224 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.225 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.225 I llama_new_context_with_model: graph nodes  = 967
0.00.085.226 I llama_new_context_with_model: graph splits = 2
0.00.085.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.826 I main: llama threadpool init, n_threads = 4
0.00.636.867 I 
0.00.636.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.636.895 I 
0.00.637.127 I sampler seed: 1234
0.00.637.132 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.637.167 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.637.168 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.637.168 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.388.819 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.388.819 I llama_perf_context_print:        load time =     628.12 ms
0.01.388.820 I llama_perf_context_print: prompt eval time =      36.51 ms /     7 tokens (    5.22 ms per token,   191.74 tokens per second)
0.01.388.821 I llama_perf_context_print:        eval time =     712.08 ms /    63 runs   (   11.30 ms per token,    88.47 tokens per second)
0.01.388.821 I llama_perf_context_print:       total time =     752.00 ms /    70 tokens
0.01.388.991 I ggml_metal_free: deallocating

real	0m1.406s
user	0m0.109s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.173 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.655 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.664 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.664 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.665 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.666 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.667 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.667 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.667 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.668 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.668 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.668 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.670 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.470 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.253 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.254 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.255 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.256 I llama_model_loader: - type  f32:  194 tensors
0.00.025.256 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.257 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.556 I llm_load_vocab: special tokens cache size = 25
0.00.051.332 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.334 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.334 I llm_load_print_meta: arch             = gptneox
0.00.051.335 I llm_load_print_meta: vocab type       = BPE
0.00.051.335 I llm_load_print_meta: n_vocab          = 50304
0.00.051.335 I llm_load_print_meta: n_merges         = 50009
0.00.051.335 I llm_load_print_meta: vocab_only       = 0
0.00.051.335 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.336 I llm_load_print_meta: n_embd           = 2048
0.00.051.336 I llm_load_print_meta: n_layer          = 24
0.00.051.338 I llm_load_print_meta: n_head           = 16
0.00.051.339 I llm_load_print_meta: n_head_kv        = 16
0.00.051.339 I llm_load_print_meta: n_rot            = 32
0.00.051.340 I llm_load_print_meta: n_swa            = 0
0.00.051.340 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.340 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.341 I llm_load_print_meta: n_gqa            = 1
0.00.051.341 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.348 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.349 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.349 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.349 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.351 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.352 I llm_load_print_meta: n_ff             = 8192
0.00.051.352 I llm_load_print_meta: n_expert         = 0
0.00.051.352 I llm_load_print_meta: n_expert_used    = 0
0.00.051.353 I llm_load_print_meta: causal attn      = 1
0.00.051.353 I llm_load_print_meta: pooling type     = 0
0.00.051.353 I llm_load_print_meta: rope type        = 2
0.00.051.353 I llm_load_print_meta: rope scaling     = linear
0.00.051.354 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.354 I llm_load_print_meta: freq_scale_train = 1
0.00.051.354 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.355 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.355 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.355 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.355 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.355 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.355 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.356 I llm_load_print_meta: model type       = 1.4B
0.00.051.360 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.361 I llm_load_print_meta: model params     = 1.41 B
0.00.051.361 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.361 I llm_load_print_meta: general.name     = 1.4B
0.00.051.362 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.362 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.362 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.362 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.363 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.364 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.364 I llm_load_print_meta: max token length = 1024
0.00.053.185 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.185 I llm_load_tensors: offloading output layer to GPU
0.00.053.186 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.191 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.191 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.455 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.456 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.456 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.456 I llama_new_context_with_model: n_batch       = 2048
0.00.054.456 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.457 I llama_new_context_with_model: flash_attn    = 0
0.00.054.457 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.457 I llama_new_context_with_model: freq_scale    = 1
0.00.054.458 I ggml_metal_init: allocating
0.00.054.463 I ggml_metal_init: found device: Apple M4
0.00.054.466 I ggml_metal_init: picking default device: Apple M4
0.00.055.025 I ggml_metal_init: using embedded metal library
0.00.056.972 I ggml_metal_init: GPU name:   Apple M4
0.00.056.973 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.974 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.974 I ggml_metal_init: simdgroup reduction   = true
0.00.056.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.975 I ggml_metal_init: has bfloat            = true
0.00.056.975 I ggml_metal_init: use bfloat            = true
0.00.056.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.976 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.316 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.324 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.342 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.381 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.382 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.383 I llama_new_context_with_model: graph nodes  = 967
0.00.086.383 I llama_new_context_with_model: graph splits = 2
0.00.086.397 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.608 I main: llama threadpool init, n_threads = 4
0.00.711.649 I 
0.00.711.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.711.699 I 
0.00.711.924 I sampler seed: 1234
0.00.711.928 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.944 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.945 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.945 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.555.151 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.555.152 I llama_perf_context_print:        load time =     701.43 ms
0.01.555.155 I llama_perf_context_print: prompt eval time =      38.72 ms /     7 tokens (    5.53 ms per token,   180.81 tokens per second)
0.01.555.156 I llama_perf_context_print:        eval time =     801.53 ms /    63 runs   (   12.72 ms per token,    78.60 tokens per second)
0.01.555.156 I llama_perf_context_print:       total time =     843.55 ms /    70 tokens
0.01.555.336 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.107s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.048 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.174 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.176 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.176 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.177 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.180 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.151 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.991 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.991 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.992 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.993 I llama_model_loader: - type  f32:  194 tensors
0.00.024.993 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.363 I llm_load_vocab: special tokens cache size = 25
0.00.051.374 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.379 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.379 I llm_load_print_meta: arch             = gptneox
0.00.051.379 I llm_load_print_meta: vocab type       = BPE
0.00.051.381 I llm_load_print_meta: n_vocab          = 50304
0.00.051.381 I llm_load_print_meta: n_merges         = 50009
0.00.051.382 I llm_load_print_meta: vocab_only       = 0
0.00.051.382 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.382 I llm_load_print_meta: n_embd           = 2048
0.00.051.382 I llm_load_print_meta: n_layer          = 24
0.00.051.385 I llm_load_print_meta: n_head           = 16
0.00.051.386 I llm_load_print_meta: n_head_kv        = 16
0.00.051.386 I llm_load_print_meta: n_rot            = 32
0.00.051.386 I llm_load_print_meta: n_swa            = 0
0.00.051.386 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.386 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.387 I llm_load_print_meta: n_gqa            = 1
0.00.051.388 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.395 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.396 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.396 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.396 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.397 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.397 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.398 I llm_load_print_meta: n_ff             = 8192
0.00.051.398 I llm_load_print_meta: n_expert         = 0
0.00.051.398 I llm_load_print_meta: n_expert_used    = 0
0.00.051.398 I llm_load_print_meta: causal attn      = 1
0.00.051.398 I llm_load_print_meta: pooling type     = 0
0.00.051.398 I llm_load_print_meta: rope type        = 2
0.00.051.399 I llm_load_print_meta: rope scaling     = linear
0.00.051.399 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.399 I llm_load_print_meta: freq_scale_train = 1
0.00.051.399 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.400 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.400 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.400 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.400 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.400 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.400 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.400 I llm_load_print_meta: model type       = 1.4B
0.00.051.405 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.405 I llm_load_print_meta: model params     = 1.41 B
0.00.051.407 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.407 I llm_load_print_meta: general.name     = 1.4B
0.00.051.408 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.408 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.409 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.409 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.409 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.410 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.410 I llm_load_print_meta: max token length = 1024
0.00.053.167 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.167 I llm_load_tensors: offloading output layer to GPU
0.00.053.167 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.172 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.173 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.071 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.072 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.072 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.072 I llama_new_context_with_model: n_batch       = 2048
0.00.054.072 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.073 I llama_new_context_with_model: flash_attn    = 0
0.00.054.073 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.073 I llama_new_context_with_model: freq_scale    = 1
0.00.054.074 I ggml_metal_init: allocating
0.00.054.079 I ggml_metal_init: found device: Apple M4
0.00.054.081 I ggml_metal_init: picking default device: Apple M4
0.00.054.634 I ggml_metal_init: using embedded metal library
0.00.056.563 I ggml_metal_init: GPU name:   Apple M4
0.00.056.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.565 I ggml_metal_init: simdgroup reduction   = true
0.00.056.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.565 I ggml_metal_init: has bfloat            = true
0.00.056.566 I ggml_metal_init: use bfloat            = true
0.00.056.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.382 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.387 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.406 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.438 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.440 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.440 I llama_new_context_with_model: graph nodes  = 967
0.00.084.440 I llama_new_context_with_model: graph splits = 2
0.00.084.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.929 I main: llama threadpool init, n_threads = 4
0.00.776.966 I 
0.00.776.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.776.995 I 
0.00.777.216 I sampler seed: 1234
0.00.777.221 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.286 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.291 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.291 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.647.999 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 62008.73 tokens per second)
0.01.648.000 I llama_perf_context_print:        load time =     767.88 ms
0.01.648.001 I llama_perf_context_print: prompt eval time =      38.43 ms /     7 tokens (    5.49 ms per token,   182.16 tokens per second)
0.01.648.002 I llama_perf_context_print:        eval time =     829.35 ms /    63 runs   (   13.16 ms per token,    75.96 tokens per second)
0.01.648.002 I llama_perf_context_print:       total time =     871.07 ms /    70 tokens
0.01.648.187 I ggml_metal_free: deallocating

real	0m1.664s
user	0m0.107s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.557 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.289 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.070 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.087 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.092 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.094 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.096 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.099 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.099 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.100 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.106 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.107 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.107 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.629 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.336 I llama_model_loader: - type  f32:  194 tensors
0.00.053.337 I llama_model_loader: - type  f16:   98 tensors
0.00.089.939 I llm_load_vocab: special tokens cache size = 25
0.00.097.485 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.097.488 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.097.488 I llm_load_print_meta: arch             = gptneox
0.00.097.489 I llm_load_print_meta: vocab type       = BPE
0.00.097.489 I llm_load_print_meta: n_vocab          = 50304
0.00.097.489 I llm_load_print_meta: n_merges         = 50009
0.00.097.489 I llm_load_print_meta: vocab_only       = 0
0.00.097.489 I llm_load_print_meta: n_ctx_train      = 2048
0.00.097.490 I llm_load_print_meta: n_embd           = 2048
0.00.097.490 I llm_load_print_meta: n_layer          = 24
0.00.097.494 I llm_load_print_meta: n_head           = 16
0.00.097.494 I llm_load_print_meta: n_head_kv        = 16
0.00.097.494 I llm_load_print_meta: n_rot            = 32
0.00.097.495 I llm_load_print_meta: n_swa            = 0
0.00.097.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.097.495 I llm_load_print_meta: n_embd_head_v    = 128
0.00.097.496 I llm_load_print_meta: n_gqa            = 1
0.00.097.497 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.097.509 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.097.509 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.097.512 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.097.513 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.097.513 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.097.513 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.097.514 I llm_load_print_meta: n_ff             = 8192
0.00.097.514 I llm_load_print_meta: n_expert         = 0
0.00.097.514 I llm_load_print_meta: n_expert_used    = 0
0.00.097.514 I llm_load_print_meta: causal attn      = 1
0.00.097.514 I llm_load_print_meta: pooling type     = 0
0.00.097.514 I llm_load_print_meta: rope type        = 2
0.00.097.515 I llm_load_print_meta: rope scaling     = linear
0.00.097.516 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.097.516 I llm_load_print_meta: freq_scale_train = 1
0.00.097.516 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.097.516 I llm_load_print_meta: rope_finetuned   = unknown
0.00.097.516 I llm_load_print_meta: ssm_d_conv       = 0
0.00.097.516 I llm_load_print_meta: ssm_d_inner      = 0
0.00.097.517 I llm_load_print_meta: ssm_d_state      = 0
0.00.097.517 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.097.517 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.517 I llm_load_print_meta: model type       = 1.4B
0.00.097.518 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.097.518 I llm_load_print_meta: model params     = 1.41 B
0.00.097.519 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.097.519 I llm_load_print_meta: general.name     = 1.4B
0.00.097.519 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.519 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.519 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.520 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.520 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.097.520 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.520 I llm_load_print_meta: max token length = 1024
0.00.100.290 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.290 I llm_load_tensors: offloading output layer to GPU
0.00.100.290 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.300 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.301 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.346 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.347 I llama_new_context_with_model: n_ctx         = 128
0.00.101.347 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.101.347 I llama_new_context_with_model: n_batch       = 128
0.00.101.347 I llama_new_context_with_model: n_ubatch      = 128
0.00.101.347 I llama_new_context_with_model: flash_attn    = 0
0.00.101.348 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.348 I llama_new_context_with_model: freq_scale    = 1
0.00.101.348 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.101.349 I ggml_metal_init: allocating
0.00.101.358 I ggml_metal_init: found device: Apple M4
0.00.101.360 I ggml_metal_init: picking default device: Apple M4
0.00.101.977 I ggml_metal_init: using embedded metal library
0.00.104.259 I ggml_metal_init: GPU name:   Apple M4
0.00.104.261 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.104.261 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.104.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.104.262 I ggml_metal_init: simdgroup reduction   = true
0.00.104.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.104.262 I ggml_metal_init: has bfloat            = true
0.00.104.262 I ggml_metal_init: use bfloat            = true
0.00.104.262 I ggml_metal_init: hasUnifiedMemory      = true
0.00.104.263 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.305 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.113.310 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.113.323 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.265 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.114.266 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.114.266 I llama_new_context_with_model: graph nodes  = 967
0.00.114.267 I llama_new_context_with_model: graph splits = 2
0.00.114.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.214.508 I 
0.01.214.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.214.549 I perplexity: tokenizing the input ..
0.01.222.339 I perplexity: tokenization took 7.787 ms
0.01.222.345 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.340.403 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.342.243 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.342.285 I llama_perf_context_print:        load time =    1191.21 ms
0.01.342.287 I llama_perf_context_print: prompt eval time =     117.82 ms /   128 tokens (    0.92 ms per token,  1086.39 tokens per second)
0.01.342.288 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.342.288 I llama_perf_context_print:       total time =     127.78 ms /   129 tokens
0.01.342.889 I ggml_metal_free: deallocating

real	0m1.542s
user	0m0.123s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.245 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.022 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.032 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.033 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.033 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.035 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.039 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.039 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.550 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.885 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.831 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.832 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.832 I llama_model_loader: - type  f32:  194 tensors
0.00.029.833 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.965 I llm_load_vocab: special tokens cache size = 25
0.00.060.841 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.845 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.846 I llm_load_print_meta: arch             = gptneox
0.00.060.846 I llm_load_print_meta: vocab type       = BPE
0.00.060.851 I llm_load_print_meta: n_vocab          = 50304
0.00.060.851 I llm_load_print_meta: n_merges         = 50009
0.00.060.852 I llm_load_print_meta: vocab_only       = 0
0.00.060.853 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.853 I llm_load_print_meta: n_embd           = 2048
0.00.060.854 I llm_load_print_meta: n_layer          = 24
0.00.060.856 I llm_load_print_meta: n_head           = 16
0.00.060.857 I llm_load_print_meta: n_head_kv        = 16
0.00.060.857 I llm_load_print_meta: n_rot            = 32
0.00.060.858 I llm_load_print_meta: n_swa            = 0
0.00.060.858 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.858 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.859 I llm_load_print_meta: n_gqa            = 1
0.00.060.859 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.866 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.867 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.867 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.868 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.868 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.868 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.868 I llm_load_print_meta: n_ff             = 8192
0.00.060.869 I llm_load_print_meta: n_expert         = 0
0.00.060.869 I llm_load_print_meta: n_expert_used    = 0
0.00.060.869 I llm_load_print_meta: causal attn      = 1
0.00.060.870 I llm_load_print_meta: pooling type     = 0
0.00.060.870 I llm_load_print_meta: rope type        = 2
0.00.060.870 I llm_load_print_meta: rope scaling     = linear
0.00.060.871 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.871 I llm_load_print_meta: freq_scale_train = 1
0.00.060.871 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.871 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.872 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.872 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.872 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.872 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.872 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.872 I llm_load_print_meta: model type       = 1.4B
0.00.060.873 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.873 I llm_load_print_meta: model params     = 1.41 B
0.00.060.874 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.874 I llm_load_print_meta: general.name     = 1.4B
0.00.060.874 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.875 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.875 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.875 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.875 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.876 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.876 I llm_load_print_meta: max token length = 1024
0.00.062.740 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.740 I llm_load_tensors: offloading output layer to GPU
0.00.062.741 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.746 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.747 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.861 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.862 I llama_new_context_with_model: n_ctx         = 128
0.00.063.862 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.862 I llama_new_context_with_model: n_batch       = 128
0.00.063.862 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.862 I llama_new_context_with_model: flash_attn    = 0
0.00.063.863 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.863 I llama_new_context_with_model: freq_scale    = 1
0.00.063.863 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.864 I ggml_metal_init: allocating
0.00.063.867 I ggml_metal_init: found device: Apple M4
0.00.063.870 I ggml_metal_init: picking default device: Apple M4
0.00.064.437 I ggml_metal_init: using embedded metal library
0.00.066.571 I ggml_metal_init: GPU name:   Apple M4
0.00.066.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.574 I ggml_metal_init: simdgroup reduction   = true
0.00.066.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.574 I ggml_metal_init: has bfloat            = true
0.00.066.574 I ggml_metal_init: use bfloat            = true
0.00.066.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.358 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.362 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.377 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.312 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.313 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.314 I llama_new_context_with_model: graph nodes  = 967
0.00.077.314 I llama_new_context_with_model: graph splits = 2
0.00.077.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.918.272 I 
0.00.918.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.918.369 I perplexity: tokenizing the input ..
0.00.926.366 I perplexity: tokenization took 7.995 ms
0.00.926.371 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.048.886 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.050.146 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.050.172 I llama_perf_context_print:        load time =     908.01 ms
0.01.050.173 I llama_perf_context_print: prompt eval time =     122.28 ms /   128 tokens (    0.96 ms per token,  1046.78 tokens per second)
0.01.050.173 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.050.174 I llama_perf_context_print:       total time =     131.91 ms /   129 tokens
0.01.050.546 I ggml_metal_free: deallocating

real	0m1.068s
user	0m0.089s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.127 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.973 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.979 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.980 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.980 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.981 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.981 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.982 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.983 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.983 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.985 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.985 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.807 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.698 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.699 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.699 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.699 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.699 I llama_model_loader: - type  f32:  194 tensors
0.00.025.700 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.700 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.912 I llm_load_vocab: special tokens cache size = 25
0.00.052.925 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.928 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.928 I llm_load_print_meta: arch             = gptneox
0.00.052.929 I llm_load_print_meta: vocab type       = BPE
0.00.052.929 I llm_load_print_meta: n_vocab          = 50304
0.00.052.929 I llm_load_print_meta: n_merges         = 50009
0.00.052.929 I llm_load_print_meta: vocab_only       = 0
0.00.052.930 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.930 I llm_load_print_meta: n_embd           = 2048
0.00.052.930 I llm_load_print_meta: n_layer          = 24
0.00.052.933 I llm_load_print_meta: n_head           = 16
0.00.052.933 I llm_load_print_meta: n_head_kv        = 16
0.00.052.934 I llm_load_print_meta: n_rot            = 32
0.00.052.934 I llm_load_print_meta: n_swa            = 0
0.00.052.934 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.934 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.937 I llm_load_print_meta: n_gqa            = 1
0.00.052.938 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.950 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.951 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.951 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.951 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.951 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.952 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.952 I llm_load_print_meta: n_ff             = 8192
0.00.052.952 I llm_load_print_meta: n_expert         = 0
0.00.052.952 I llm_load_print_meta: n_expert_used    = 0
0.00.052.953 I llm_load_print_meta: causal attn      = 1
0.00.052.953 I llm_load_print_meta: pooling type     = 0
0.00.052.953 I llm_load_print_meta: rope type        = 2
0.00.052.953 I llm_load_print_meta: rope scaling     = linear
0.00.052.954 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.954 I llm_load_print_meta: freq_scale_train = 1
0.00.052.954 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.955 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.955 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.955 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.955 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.955 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.956 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.956 I llm_load_print_meta: model type       = 1.4B
0.00.052.956 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.957 I llm_load_print_meta: model params     = 1.41 B
0.00.052.957 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.957 I llm_load_print_meta: general.name     = 1.4B
0.00.052.958 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.958 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.958 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.958 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.958 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.959 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.959 I llm_load_print_meta: max token length = 1024
0.00.054.966 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.966 I llm_load_tensors: offloading output layer to GPU
0.00.054.966 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.976 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.977 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.943 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.944 I llama_new_context_with_model: n_ctx         = 128
0.00.055.944 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.945 I llama_new_context_with_model: n_batch       = 128
0.00.055.945 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.945 I llama_new_context_with_model: flash_attn    = 0
0.00.055.945 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.945 I llama_new_context_with_model: freq_scale    = 1
0.00.055.946 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.946 I ggml_metal_init: allocating
0.00.055.950 I ggml_metal_init: found device: Apple M4
0.00.055.952 I ggml_metal_init: picking default device: Apple M4
0.00.056.493 I ggml_metal_init: using embedded metal library
0.00.058.444 I ggml_metal_init: GPU name:   Apple M4
0.00.058.446 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.446 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.447 I ggml_metal_init: simdgroup reduction   = true
0.00.058.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.447 I ggml_metal_init: has bfloat            = true
0.00.058.447 I ggml_metal_init: use bfloat            = true
0.00.058.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.448 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.271 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.273 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.288 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.249 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.250 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.250 I llama_new_context_with_model: graph nodes  = 967
0.00.069.250 I llama_new_context_with_model: graph splits = 2
0.00.069.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.158 I 
0.00.575.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.575.195 I perplexity: tokenizing the input ..
0.00.582.879 I perplexity: tokenization took 7.682 ms
0.00.582.883 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.706.095 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.707.337 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.707.367 I llama_perf_context_print:        load time =     564.03 ms
0.00.707.368 I llama_perf_context_print: prompt eval time =     122.96 ms /   128 tokens (    0.96 ms per token,  1040.96 tokens per second)
0.00.707.369 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.370 I llama_perf_context_print:       total time =     132.21 ms /   129 tokens
0.00.707.882 I ggml_metal_free: deallocating

real	0m0.723s
user	0m0.077s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.882 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.278 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.283 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.283 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.284 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.284 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.284 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.285 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.285 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.285 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.286 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.286 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.286 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.290 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.290 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.125 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.956 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.958 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.959 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.959 I llama_model_loader: - type  f32:  194 tensors
0.00.023.960 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.960 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.144 I llm_load_vocab: special tokens cache size = 25
0.00.050.019 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.022 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.022 I llm_load_print_meta: arch             = gptneox
0.00.050.023 I llm_load_print_meta: vocab type       = BPE
0.00.050.023 I llm_load_print_meta: n_vocab          = 50304
0.00.050.023 I llm_load_print_meta: n_merges         = 50009
0.00.050.023 I llm_load_print_meta: vocab_only       = 0
0.00.050.023 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.024 I llm_load_print_meta: n_embd           = 2048
0.00.050.024 I llm_load_print_meta: n_layer          = 24
0.00.050.027 I llm_load_print_meta: n_head           = 16
0.00.050.028 I llm_load_print_meta: n_head_kv        = 16
0.00.050.028 I llm_load_print_meta: n_rot            = 32
0.00.050.028 I llm_load_print_meta: n_swa            = 0
0.00.050.028 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.028 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.029 I llm_load_print_meta: n_gqa            = 1
0.00.050.030 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.043 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.044 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.044 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.044 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.045 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.045 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.045 I llm_load_print_meta: n_ff             = 8192
0.00.050.046 I llm_load_print_meta: n_expert         = 0
0.00.050.046 I llm_load_print_meta: n_expert_used    = 0
0.00.050.046 I llm_load_print_meta: causal attn      = 1
0.00.050.046 I llm_load_print_meta: pooling type     = 0
0.00.050.046 I llm_load_print_meta: rope type        = 2
0.00.050.046 I llm_load_print_meta: rope scaling     = linear
0.00.050.048 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.050 I llm_load_print_meta: freq_scale_train = 1
0.00.050.050 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.050 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.050 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.050 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.050 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.050 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.051 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.051 I llm_load_print_meta: model type       = 1.4B
0.00.050.051 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.051 I llm_load_print_meta: model params     = 1.41 B
0.00.050.052 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.052 I llm_load_print_meta: general.name     = 1.4B
0.00.050.053 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.053 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.053 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.054 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.054 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.054 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.054 I llm_load_print_meta: max token length = 1024
0.00.052.079 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.079 I llm_load_tensors: offloading output layer to GPU
0.00.052.079 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.089 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.090 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.059 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.060 I llama_new_context_with_model: n_ctx         = 128
0.00.053.060 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.060 I llama_new_context_with_model: n_batch       = 128
0.00.053.061 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.061 I llama_new_context_with_model: flash_attn    = 0
0.00.053.061 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.061 I llama_new_context_with_model: freq_scale    = 1
0.00.053.062 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.062 I ggml_metal_init: allocating
0.00.053.065 I ggml_metal_init: found device: Apple M4
0.00.053.067 I ggml_metal_init: picking default device: Apple M4
0.00.053.604 I ggml_metal_init: using embedded metal library
0.00.055.517 I ggml_metal_init: GPU name:   Apple M4
0.00.055.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.519 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.519 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.520 I ggml_metal_init: simdgroup reduction   = true
0.00.055.520 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.520 I ggml_metal_init: has bfloat            = true
0.00.055.520 I ggml_metal_init: use bfloat            = true
0.00.055.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.522 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.846 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.851 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.865 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.785 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.786 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.786 I llama_new_context_with_model: graph nodes  = 967
0.00.065.786 I llama_new_context_with_model: graph splits = 2
0.00.065.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.505 I 
0.00.652.538 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.652.555 I perplexity: tokenizing the input ..
0.00.660.287 I perplexity: tokenization took 7.73 ms
0.00.660.290 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.783.858 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.785.149 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.785.176 I llama_perf_context_print:        load time =     643.62 ms
0.00.785.177 I llama_perf_context_print: prompt eval time =     123.33 ms /   128 tokens (    0.96 ms per token,  1037.85 tokens per second)
0.00.785.178 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.178 I llama_perf_context_print:       total time =     132.67 ms /   129 tokens
0.00.785.605 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.075s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.956 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.820 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.831 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.831 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.832 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.832 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.835 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.835 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.731 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.748 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.673 I llama_model_loader: - type  f32:  194 tensors
0.00.024.673 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.673 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.681 I llm_load_vocab: special tokens cache size = 25
0.00.051.738 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.741 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.741 I llm_load_print_meta: arch             = gptneox
0.00.051.741 I llm_load_print_meta: vocab type       = BPE
0.00.051.742 I llm_load_print_meta: n_vocab          = 50304
0.00.051.742 I llm_load_print_meta: n_merges         = 50009
0.00.051.742 I llm_load_print_meta: vocab_only       = 0
0.00.051.742 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.742 I llm_load_print_meta: n_embd           = 2048
0.00.051.743 I llm_load_print_meta: n_layer          = 24
0.00.051.745 I llm_load_print_meta: n_head           = 16
0.00.051.746 I llm_load_print_meta: n_head_kv        = 16
0.00.051.746 I llm_load_print_meta: n_rot            = 32
0.00.051.746 I llm_load_print_meta: n_swa            = 0
0.00.051.747 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.747 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.747 I llm_load_print_meta: n_gqa            = 1
0.00.051.748 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.763 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.763 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.764 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.764 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.764 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.764 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.766 I llm_load_print_meta: n_ff             = 8192
0.00.051.766 I llm_load_print_meta: n_expert         = 0
0.00.051.766 I llm_load_print_meta: n_expert_used    = 0
0.00.051.766 I llm_load_print_meta: causal attn      = 1
0.00.051.767 I llm_load_print_meta: pooling type     = 0
0.00.051.767 I llm_load_print_meta: rope type        = 2
0.00.051.767 I llm_load_print_meta: rope scaling     = linear
0.00.051.767 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.768 I llm_load_print_meta: freq_scale_train = 1
0.00.051.768 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.768 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.768 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.768 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.768 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.769 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.769 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.769 I llm_load_print_meta: model type       = 1.4B
0.00.051.770 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.770 I llm_load_print_meta: model params     = 1.41 B
0.00.051.771 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.771 I llm_load_print_meta: general.name     = 1.4B
0.00.051.771 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.771 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.771 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.771 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.772 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.773 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.773 I llm_load_print_meta: max token length = 1024
0.00.053.750 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.750 I llm_load_tensors: offloading output layer to GPU
0.00.053.751 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.761 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.762 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.049 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.050 I llama_new_context_with_model: n_ctx         = 128
0.00.055.050 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.050 I llama_new_context_with_model: n_batch       = 128
0.00.055.050 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.050 I llama_new_context_with_model: flash_attn    = 0
0.00.055.051 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.051 I llama_new_context_with_model: freq_scale    = 1
0.00.055.051 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.052 I ggml_metal_init: allocating
0.00.055.058 I ggml_metal_init: found device: Apple M4
0.00.055.060 I ggml_metal_init: picking default device: Apple M4
0.00.055.628 I ggml_metal_init: using embedded metal library
0.00.057.579 I ggml_metal_init: GPU name:   Apple M4
0.00.057.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.581 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.582 I ggml_metal_init: simdgroup reduction   = true
0.00.057.582 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.582 I ggml_metal_init: has bfloat            = true
0.00.057.582 I ggml_metal_init: use bfloat            = true
0.00.057.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.583 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.858 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.861 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.876 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.803 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.805 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.805 I llama_new_context_with_model: graph nodes  = 967
0.00.067.805 I llama_new_context_with_model: graph splits = 2
0.00.067.818 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.191 I 
0.00.642.217 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.642.220 I perplexity: tokenizing the input ..
0.00.650.168 I perplexity: tokenization took 7.946 ms
0.00.650.172 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.599 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.786.912 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.786.935 I llama_perf_context_print:        load time =     632.23 ms
0.00.786.935 I llama_perf_context_print: prompt eval time =     135.17 ms /   128 tokens (    1.06 ms per token,   946.94 tokens per second)
0.00.786.936 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.937 I llama_perf_context_print:       total time =     144.75 ms /   129 tokens
0.00.787.398 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.076s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.239 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.134 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.140 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.141 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.141 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.144 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.145 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.145 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.147 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.147 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.150 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.150 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.926 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.797 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.798 I llama_model_loader: - type  f32:  194 tensors
0.00.023.798 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.799 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.990 I llm_load_vocab: special tokens cache size = 25
0.00.049.759 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.764 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.765 I llm_load_print_meta: arch             = gptneox
0.00.049.765 I llm_load_print_meta: vocab type       = BPE
0.00.049.765 I llm_load_print_meta: n_vocab          = 50304
0.00.049.765 I llm_load_print_meta: n_merges         = 50009
0.00.049.766 I llm_load_print_meta: vocab_only       = 0
0.00.049.766 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.766 I llm_load_print_meta: n_embd           = 2048
0.00.049.766 I llm_load_print_meta: n_layer          = 24
0.00.049.769 I llm_load_print_meta: n_head           = 16
0.00.049.772 I llm_load_print_meta: n_head_kv        = 16
0.00.049.773 I llm_load_print_meta: n_rot            = 32
0.00.049.773 I llm_load_print_meta: n_swa            = 0
0.00.049.773 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.773 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.774 I llm_load_print_meta: n_gqa            = 1
0.00.049.775 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.781 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.782 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.783 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.783 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.783 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.783 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.784 I llm_load_print_meta: n_ff             = 8192
0.00.049.784 I llm_load_print_meta: n_expert         = 0
0.00.049.784 I llm_load_print_meta: n_expert_used    = 0
0.00.049.784 I llm_load_print_meta: causal attn      = 1
0.00.049.784 I llm_load_print_meta: pooling type     = 0
0.00.049.784 I llm_load_print_meta: rope type        = 2
0.00.049.785 I llm_load_print_meta: rope scaling     = linear
0.00.049.785 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.786 I llm_load_print_meta: freq_scale_train = 1
0.00.049.786 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.787 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.787 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.787 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.787 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.787 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.788 I llm_load_print_meta: model type       = 1.4B
0.00.049.789 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.789 I llm_load_print_meta: model params     = 1.41 B
0.00.049.790 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.790 I llm_load_print_meta: general.name     = 1.4B
0.00.049.790 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.790 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.790 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.792 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: max token length = 1024
0.00.051.588 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.588 I llm_load_tensors: offloading output layer to GPU
0.00.051.589 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.593 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.594 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.574 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.575 I llama_new_context_with_model: n_ctx         = 128
0.00.052.575 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.575 I llama_new_context_with_model: n_batch       = 128
0.00.052.575 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.575 I llama_new_context_with_model: flash_attn    = 0
0.00.052.576 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.576 I llama_new_context_with_model: freq_scale    = 1
0.00.052.576 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.577 I ggml_metal_init: allocating
0.00.052.582 I ggml_metal_init: found device: Apple M4
0.00.052.584 I ggml_metal_init: picking default device: Apple M4
0.00.053.125 I ggml_metal_init: using embedded metal library
0.00.055.098 I ggml_metal_init: GPU name:   Apple M4
0.00.055.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.100 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.100 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.101 I ggml_metal_init: simdgroup reduction   = true
0.00.055.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.101 I ggml_metal_init: has bfloat            = true
0.00.055.101 I ggml_metal_init: use bfloat            = true
0.00.055.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.161 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.166 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.181 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.120 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.121 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.121 I llama_new_context_with_model: graph nodes  = 967
0.00.065.122 I llama_new_context_with_model: graph splits = 2
0.00.065.129 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.960 I 
0.00.768.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.768.051 I perplexity: tokenizing the input ..
0.00.776.271 I perplexity: tokenization took 8.218 ms
0.00.776.275 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.911.251 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.912.508 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.912.534 I llama_perf_context_print:        load time =     758.71 ms
0.00.912.535 I llama_perf_context_print: prompt eval time =     134.73 ms /   128 tokens (    1.05 ms per token,   950.07 tokens per second)
0.00.912.536 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.912.537 I llama_perf_context_print:       total time =     144.58 ms /   129 tokens
0.00.913.035 I ggml_metal_free: deallocating

real	0m0.926s
user	0m0.075s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.741 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.445 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.446 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.447 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.447 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.448 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.450 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.450 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.450 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.278 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.144 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.144 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.145 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.145 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.145 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.146 I llama_model_loader: - type  f32:  194 tensors
0.00.024.146 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.146 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.146 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.296 I llm_load_vocab: special tokens cache size = 25
0.00.050.208 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.211 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.211 I llm_load_print_meta: arch             = gptneox
0.00.050.211 I llm_load_print_meta: vocab type       = BPE
0.00.050.212 I llm_load_print_meta: n_vocab          = 50304
0.00.050.212 I llm_load_print_meta: n_merges         = 50009
0.00.050.212 I llm_load_print_meta: vocab_only       = 0
0.00.050.212 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.212 I llm_load_print_meta: n_embd           = 2048
0.00.050.212 I llm_load_print_meta: n_layer          = 24
0.00.050.215 I llm_load_print_meta: n_head           = 16
0.00.050.216 I llm_load_print_meta: n_head_kv        = 16
0.00.050.216 I llm_load_print_meta: n_rot            = 32
0.00.050.217 I llm_load_print_meta: n_swa            = 0
0.00.050.217 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.217 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.218 I llm_load_print_meta: n_gqa            = 1
0.00.050.219 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.231 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.233 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.233 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.233 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.234 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.234 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.234 I llm_load_print_meta: n_ff             = 8192
0.00.050.234 I llm_load_print_meta: n_expert         = 0
0.00.050.235 I llm_load_print_meta: n_expert_used    = 0
0.00.050.235 I llm_load_print_meta: causal attn      = 1
0.00.050.235 I llm_load_print_meta: pooling type     = 0
0.00.050.235 I llm_load_print_meta: rope type        = 2
0.00.050.235 I llm_load_print_meta: rope scaling     = linear
0.00.050.236 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.237 I llm_load_print_meta: freq_scale_train = 1
0.00.050.237 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.237 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.237 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.237 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.237 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.238 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.238 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.238 I llm_load_print_meta: model type       = 1.4B
0.00.050.239 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.240 I llm_load_print_meta: model params     = 1.41 B
0.00.050.240 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.241 I llm_load_print_meta: general.name     = 1.4B
0.00.050.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: max token length = 1024
0.00.052.145 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.145 I llm_load_tensors: offloading output layer to GPU
0.00.052.146 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.156 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.157 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.027 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.028 I llama_new_context_with_model: n_ctx         = 128
0.00.053.028 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.028 I llama_new_context_with_model: n_batch       = 128
0.00.053.029 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.029 I llama_new_context_with_model: flash_attn    = 0
0.00.053.029 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.030 I llama_new_context_with_model: freq_scale    = 1
0.00.053.030 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.030 I ggml_metal_init: allocating
0.00.053.035 I ggml_metal_init: found device: Apple M4
0.00.053.038 I ggml_metal_init: picking default device: Apple M4
0.00.053.579 I ggml_metal_init: using embedded metal library
0.00.055.529 I ggml_metal_init: GPU name:   Apple M4
0.00.055.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.531 I ggml_metal_init: simdgroup reduction   = true
0.00.055.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.531 I ggml_metal_init: has bfloat            = true
0.00.055.532 I ggml_metal_init: use bfloat            = true
0.00.055.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.652 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.656 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.670 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.549 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.550 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.551 I llama_new_context_with_model: graph nodes  = 967
0.00.065.551 I llama_new_context_with_model: graph splits = 2
0.00.065.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.439.166 I 
0.00.439.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.439.246 I perplexity: tokenizing the input ..
0.00.447.445 I perplexity: tokenization took 8.201 ms
0.00.447.450 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.579.895 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.581.154 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.581.183 I llama_perf_context_print:        load time =     429.42 ms
0.00.581.184 I llama_perf_context_print: prompt eval time =     132.20 ms /   128 tokens (    1.03 ms per token,   968.24 tokens per second)
0.00.581.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.581.186 I llama_perf_context_print:       total time =     142.02 ms /   129 tokens
0.00.581.659 I ggml_metal_free: deallocating

real	0m0.596s
user	0m0.075s
sys	0m0.081s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.731 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.206 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.245 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.991 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.994 I llama_model_loader: - type  f32:  194 tensors
0.00.022.995 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.995 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.995 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.995 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.143 I llm_load_vocab: special tokens cache size = 25
0.00.049.147 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.149 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.150 I llm_load_print_meta: arch             = gptneox
0.00.049.150 I llm_load_print_meta: vocab type       = BPE
0.00.049.150 I llm_load_print_meta: n_vocab          = 50304
0.00.049.150 I llm_load_print_meta: n_merges         = 50009
0.00.049.151 I llm_load_print_meta: vocab_only       = 0
0.00.049.151 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.151 I llm_load_print_meta: n_embd           = 2048
0.00.049.151 I llm_load_print_meta: n_layer          = 24
0.00.049.154 I llm_load_print_meta: n_head           = 16
0.00.049.155 I llm_load_print_meta: n_head_kv        = 16
0.00.049.155 I llm_load_print_meta: n_rot            = 32
0.00.049.156 I llm_load_print_meta: n_swa            = 0
0.00.049.156 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.156 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.157 I llm_load_print_meta: n_gqa            = 1
0.00.049.157 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.170 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.170 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.171 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.171 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.171 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.171 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.172 I llm_load_print_meta: n_ff             = 8192
0.00.049.172 I llm_load_print_meta: n_expert         = 0
0.00.049.172 I llm_load_print_meta: n_expert_used    = 0
0.00.049.172 I llm_load_print_meta: causal attn      = 1
0.00.049.172 I llm_load_print_meta: pooling type     = 0
0.00.049.173 I llm_load_print_meta: rope type        = 2
0.00.049.175 I llm_load_print_meta: rope scaling     = linear
0.00.049.175 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.176 I llm_load_print_meta: freq_scale_train = 1
0.00.049.176 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.176 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.176 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.176 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.176 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.176 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.177 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.177 I llm_load_print_meta: model type       = 1.4B
0.00.049.187 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.187 I llm_load_print_meta: model params     = 1.41 B
0.00.049.189 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.189 I llm_load_print_meta: general.name     = 1.4B
0.00.049.189 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.189 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.189 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.189 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.190 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.190 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.190 I llm_load_print_meta: max token length = 1024
0.00.051.153 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.153 I llm_load_tensors: offloading output layer to GPU
0.00.051.153 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.163 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.164 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.100 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.101 I llama_new_context_with_model: n_ctx         = 128
0.00.052.101 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.101 I llama_new_context_with_model: n_batch       = 128
0.00.052.101 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.101 I llama_new_context_with_model: flash_attn    = 0
0.00.052.102 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.102 I llama_new_context_with_model: freq_scale    = 1
0.00.052.102 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.103 I ggml_metal_init: allocating
0.00.052.105 I ggml_metal_init: found device: Apple M4
0.00.052.107 I ggml_metal_init: picking default device: Apple M4
0.00.052.648 I ggml_metal_init: using embedded metal library
0.00.054.528 I ggml_metal_init: GPU name:   Apple M4
0.00.054.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.531 I ggml_metal_init: simdgroup reduction   = true
0.00.054.532 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.532 I ggml_metal_init: has bfloat            = true
0.00.054.532 I ggml_metal_init: use bfloat            = true
0.00.054.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.646 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.651 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.672 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.585 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.585 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.586 I llama_new_context_with_model: graph nodes  = 967
0.00.064.586 I llama_new_context_with_model: graph splits = 2
0.00.064.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.287 I 
0.00.508.330 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.508.335 I perplexity: tokenizing the input ..
0.00.516.257 I perplexity: tokenization took 7.921 ms
0.00.516.261 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.648.680 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.649.932 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.649.960 I llama_perf_context_print:        load time =     499.55 ms
0.00.649.962 I llama_perf_context_print: prompt eval time =     132.18 ms /   128 tokens (    1.03 ms per token,   968.41 tokens per second)
0.00.649.962 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.963 I llama_perf_context_print:       total time =     141.67 ms /   129 tokens
0.00.650.303 I ggml_metal_free: deallocating

real	0m0.663s
user	0m0.075s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.488 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.347 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.352 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.355 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.357 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.357 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.357 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.358 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.358 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.358 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.360 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.360 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.361 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.137 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.215 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.111 I llama_model_loader: - type  f32:  194 tensors
0.00.024.111 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.112 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.112 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.130 I llm_load_vocab: special tokens cache size = 25
0.00.051.143 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.148 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.148 I llm_load_print_meta: arch             = gptneox
0.00.051.149 I llm_load_print_meta: vocab type       = BPE
0.00.051.149 I llm_load_print_meta: n_vocab          = 50304
0.00.051.152 I llm_load_print_meta: n_merges         = 50009
0.00.051.152 I llm_load_print_meta: vocab_only       = 0
0.00.051.152 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.152 I llm_load_print_meta: n_embd           = 2048
0.00.051.153 I llm_load_print_meta: n_layer          = 24
0.00.051.156 I llm_load_print_meta: n_head           = 16
0.00.051.157 I llm_load_print_meta: n_head_kv        = 16
0.00.051.158 I llm_load_print_meta: n_rot            = 32
0.00.051.158 I llm_load_print_meta: n_swa            = 0
0.00.051.158 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.158 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.159 I llm_load_print_meta: n_gqa            = 1
0.00.051.160 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.171 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.172 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.172 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.173 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.173 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.174 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.174 I llm_load_print_meta: n_ff             = 8192
0.00.051.175 I llm_load_print_meta: n_expert         = 0
0.00.051.176 I llm_load_print_meta: n_expert_used    = 0
0.00.051.176 I llm_load_print_meta: causal attn      = 1
0.00.051.176 I llm_load_print_meta: pooling type     = 0
0.00.051.176 I llm_load_print_meta: rope type        = 2
0.00.051.177 I llm_load_print_meta: rope scaling     = linear
0.00.051.179 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.179 I llm_load_print_meta: freq_scale_train = 1
0.00.051.179 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.179 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.179 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.179 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.180 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.180 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.181 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.181 I llm_load_print_meta: model type       = 1.4B
0.00.051.191 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.191 I llm_load_print_meta: model params     = 1.41 B
0.00.051.192 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.192 I llm_load_print_meta: general.name     = 1.4B
0.00.051.192 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.193 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.193 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.193 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.193 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.194 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.194 I llm_load_print_meta: max token length = 1024
0.00.053.242 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.242 I llm_load_tensors: offloading output layer to GPU
0.00.053.243 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.252 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.253 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.183 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.184 I llama_new_context_with_model: n_ctx         = 128
0.00.054.184 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.184 I llama_new_context_with_model: n_batch       = 128
0.00.054.184 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.185 I llama_new_context_with_model: flash_attn    = 0
0.00.054.185 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.185 I llama_new_context_with_model: freq_scale    = 1
0.00.054.186 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.186 I ggml_metal_init: allocating
0.00.054.189 I ggml_metal_init: found device: Apple M4
0.00.054.191 I ggml_metal_init: picking default device: Apple M4
0.00.054.736 I ggml_metal_init: using embedded metal library
0.00.056.661 I ggml_metal_init: GPU name:   Apple M4
0.00.056.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.664 I ggml_metal_init: simdgroup reduction   = true
0.00.056.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.664 I ggml_metal_init: has bfloat            = true
0.00.056.665 I ggml_metal_init: use bfloat            = true
0.00.056.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.009 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.015 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.030 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.961 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.963 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.963 I llama_new_context_with_model: graph nodes  = 967
0.00.066.963 I llama_new_context_with_model: graph splits = 2
0.00.066.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.582 I 
0.00.598.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.598.648 I perplexity: tokenizing the input ..
0.00.607.014 I perplexity: tokenization took 8.364 ms
0.00.607.020 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.741.721 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.742.975 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.743.004 I llama_perf_context_print:        load time =     589.09 ms
0.00.743.005 I llama_perf_context_print: prompt eval time =     134.46 ms /   128 tokens (    1.05 ms per token,   951.93 tokens per second)
0.00.743.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.006 I llama_perf_context_print:       total time =     144.42 ms /   129 tokens
0.00.743.444 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.077s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.298 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.298 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.299 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.299 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.299 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.300 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.300 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.301 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.301 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.302 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.305 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.068 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.069 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.069 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.069 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.070 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.070 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.071 I llama_model_loader: - type  f32:  194 tensors
0.00.023.071 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.071 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.142 I llm_load_vocab: special tokens cache size = 25
0.00.049.011 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.014 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.014 I llm_load_print_meta: arch             = gptneox
0.00.049.015 I llm_load_print_meta: vocab type       = BPE
0.00.049.015 I llm_load_print_meta: n_vocab          = 50304
0.00.049.015 I llm_load_print_meta: n_merges         = 50009
0.00.049.015 I llm_load_print_meta: vocab_only       = 0
0.00.049.015 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.016 I llm_load_print_meta: n_embd           = 2048
0.00.049.016 I llm_load_print_meta: n_layer          = 24
0.00.049.019 I llm_load_print_meta: n_head           = 16
0.00.049.019 I llm_load_print_meta: n_head_kv        = 16
0.00.049.020 I llm_load_print_meta: n_rot            = 32
0.00.049.020 I llm_load_print_meta: n_swa            = 0
0.00.049.020 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.020 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.021 I llm_load_print_meta: n_gqa            = 1
0.00.049.022 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.034 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.035 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.035 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.035 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.035 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.036 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.038 I llm_load_print_meta: n_ff             = 8192
0.00.049.038 I llm_load_print_meta: n_expert         = 0
0.00.049.038 I llm_load_print_meta: n_expert_used    = 0
0.00.049.038 I llm_load_print_meta: causal attn      = 1
0.00.049.038 I llm_load_print_meta: pooling type     = 0
0.00.049.038 I llm_load_print_meta: rope type        = 2
0.00.049.039 I llm_load_print_meta: rope scaling     = linear
0.00.049.042 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.043 I llm_load_print_meta: freq_scale_train = 1
0.00.049.043 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.043 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.043 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.043 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.043 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.044 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.044 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.047 I llm_load_print_meta: model type       = 1.4B
0.00.049.057 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.058 I llm_load_print_meta: model params     = 1.41 B
0.00.049.058 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.060 I llm_load_print_meta: general.name     = 1.4B
0.00.049.060 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.060 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.061 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.061 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.061 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.061 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.062 I llm_load_print_meta: max token length = 1024
0.00.051.071 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.072 I llm_load_tensors: offloading output layer to GPU
0.00.051.072 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.082 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.083 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.017 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.017 I llama_new_context_with_model: n_ctx         = 128
0.00.052.018 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.018 I llama_new_context_with_model: n_batch       = 128
0.00.052.018 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.018 I llama_new_context_with_model: flash_attn    = 0
0.00.052.018 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.019 I llama_new_context_with_model: freq_scale    = 1
0.00.052.019 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.019 I ggml_metal_init: allocating
0.00.052.022 I ggml_metal_init: found device: Apple M4
0.00.052.024 I ggml_metal_init: picking default device: Apple M4
0.00.052.563 I ggml_metal_init: using embedded metal library
0.00.054.474 I ggml_metal_init: GPU name:   Apple M4
0.00.054.475 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.476 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.476 I ggml_metal_init: simdgroup reduction   = true
0.00.054.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.478 I ggml_metal_init: has bfloat            = true
0.00.054.478 I ggml_metal_init: use bfloat            = true
0.00.054.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.549 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.552 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.567 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.460 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.462 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.462 I llama_new_context_with_model: graph nodes  = 967
0.00.064.462 I llama_new_context_with_model: graph splits = 2
0.00.064.475 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.721 I 
0.00.678.794 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.678.801 I perplexity: tokenizing the input ..
0.00.686.856 I perplexity: tokenization took 8.053 ms
0.00.686.859 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.761 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.829.025 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.829.053 I llama_perf_context_print:        load time =     670.05 ms
0.00.829.055 I llama_perf_context_print: prompt eval time =     140.68 ms /   128 tokens (    1.10 ms per token,   909.90 tokens per second)
0.00.829.056 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.056 I llama_perf_context_print:       total time =     150.33 ms /   129 tokens
0.00.829.461 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.075s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.079 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.845 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.850 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.852 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.852 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.852 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.853 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.853 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.854 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.854 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.854 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.855 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.855 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.855 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.856 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.858 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.859 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.676 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.654 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.655 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.656 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.656 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.657 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.657 I llama_model_loader: - type  f32:  194 tensors
0.00.024.658 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.541 I llm_load_vocab: special tokens cache size = 25
0.00.051.539 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.541 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.541 I llm_load_print_meta: arch             = gptneox
0.00.051.542 I llm_load_print_meta: vocab type       = BPE
0.00.051.542 I llm_load_print_meta: n_vocab          = 50304
0.00.051.542 I llm_load_print_meta: n_merges         = 50009
0.00.051.542 I llm_load_print_meta: vocab_only       = 0
0.00.051.542 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.543 I llm_load_print_meta: n_embd           = 2048
0.00.051.543 I llm_load_print_meta: n_layer          = 24
0.00.051.545 I llm_load_print_meta: n_head           = 16
0.00.051.546 I llm_load_print_meta: n_head_kv        = 16
0.00.051.549 I llm_load_print_meta: n_rot            = 32
0.00.051.549 I llm_load_print_meta: n_swa            = 0
0.00.051.549 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.549 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.550 I llm_load_print_meta: n_gqa            = 1
0.00.051.551 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.563 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.563 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.564 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.564 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.564 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.564 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.565 I llm_load_print_meta: n_ff             = 8192
0.00.051.565 I llm_load_print_meta: n_expert         = 0
0.00.051.565 I llm_load_print_meta: n_expert_used    = 0
0.00.051.566 I llm_load_print_meta: causal attn      = 1
0.00.051.566 I llm_load_print_meta: pooling type     = 0
0.00.051.566 I llm_load_print_meta: rope type        = 2
0.00.051.566 I llm_load_print_meta: rope scaling     = linear
0.00.051.566 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.568 I llm_load_print_meta: freq_scale_train = 1
0.00.051.569 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.569 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.569 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.569 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.569 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.569 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.570 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.570 I llm_load_print_meta: model type       = 1.4B
0.00.051.580 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.580 I llm_load_print_meta: model params     = 1.41 B
0.00.051.580 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.581 I llm_load_print_meta: general.name     = 1.4B
0.00.051.581 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.582 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.582 I llm_load_print_meta: max token length = 1024
0.00.053.641 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.641 I llm_load_tensors: offloading output layer to GPU
0.00.053.641 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.651 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.652 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.551 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.552 I llama_new_context_with_model: n_ctx         = 128
0.00.054.552 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.552 I llama_new_context_with_model: n_batch       = 128
0.00.054.552 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.553 I llama_new_context_with_model: flash_attn    = 0
0.00.054.553 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.553 I llama_new_context_with_model: freq_scale    = 1
0.00.054.554 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.554 I ggml_metal_init: allocating
0.00.054.557 I ggml_metal_init: found device: Apple M4
0.00.054.559 I ggml_metal_init: picking default device: Apple M4
0.00.055.108 I ggml_metal_init: using embedded metal library
0.00.057.048 I ggml_metal_init: GPU name:   Apple M4
0.00.057.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.050 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.050 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.051 I ggml_metal_init: simdgroup reduction   = true
0.00.057.051 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.051 I ggml_metal_init: has bfloat            = true
0.00.057.051 I ggml_metal_init: use bfloat            = true
0.00.057.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.377 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.382 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.397 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.295 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.295 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.296 I llama_new_context_with_model: graph nodes  = 967
0.00.067.296 I llama_new_context_with_model: graph splits = 2
0.00.067.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.450 I 
0.00.360.477 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.360.480 I perplexity: tokenizing the input ..
0.00.368.555 I perplexity: tokenization took 8.073 ms
0.00.368.558 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.509.353 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.510.509 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.510.547 I llama_perf_context_print:        load time =     350.37 ms
0.00.510.548 I llama_perf_context_print: prompt eval time =     140.45 ms /   128 tokens (    1.10 ms per token,   911.34 tokens per second)
0.00.510.549 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.510.549 I llama_perf_context_print:       total time =     150.10 ms /   129 tokens
0.00.511.022 I ggml_metal_free: deallocating

real	0m0.526s
user	0m0.077s
sys	0m0.087s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.268 I build: 4239 (991f8aab) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.423 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.320 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.325 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.331 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.331 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.332 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.334 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.334 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.334 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.335 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.336 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.338 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.780 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.036 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.038 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.038 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.039 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.039 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.040 I llama_model_loader: - type  f32:  194 tensors
0.00.050.041 I llama_model_loader: - type  f16:   98 tensors
0.00.079.641 I llm_load_vocab: special tokens cache size = 25
0.00.086.083 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.086 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.087 I llm_load_print_meta: arch             = gptneox
0.00.086.087 I llm_load_print_meta: vocab type       = BPE
0.00.086.087 I llm_load_print_meta: n_vocab          = 50304
0.00.086.087 I llm_load_print_meta: n_merges         = 50009
0.00.086.088 I llm_load_print_meta: vocab_only       = 0
0.00.086.088 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.090 I llm_load_print_meta: n_embd           = 2048
0.00.086.090 I llm_load_print_meta: n_layer          = 24
0.00.086.092 I llm_load_print_meta: n_head           = 16
0.00.086.093 I llm_load_print_meta: n_head_kv        = 16
0.00.086.093 I llm_load_print_meta: n_rot            = 32
0.00.086.095 I llm_load_print_meta: n_swa            = 0
0.00.086.095 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.095 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.096 I llm_load_print_meta: n_gqa            = 1
0.00.086.096 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.109 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.109 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.110 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.110 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.110 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.110 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.111 I llm_load_print_meta: n_ff             = 8192
0.00.086.111 I llm_load_print_meta: n_expert         = 0
0.00.086.111 I llm_load_print_meta: n_expert_used    = 0
0.00.086.111 I llm_load_print_meta: causal attn      = 1
0.00.086.111 I llm_load_print_meta: pooling type     = 0
0.00.086.111 I llm_load_print_meta: rope type        = 2
0.00.086.112 I llm_load_print_meta: rope scaling     = linear
0.00.086.112 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.112 I llm_load_print_meta: freq_scale_train = 1
0.00.086.112 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.112 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.113 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.113 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.113 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.113 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.113 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.113 I llm_load_print_meta: model type       = 1.4B
0.00.086.114 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.115 I llm_load_print_meta: model params     = 1.41 B
0.00.086.115 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.116 I llm_load_print_meta: general.name     = 1.4B
0.00.086.116 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.116 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.116 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.116 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.117 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.117 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.117 I llm_load_print_meta: max token length = 1024
0.00.088.706 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.706 I llm_load_tensors: offloading output layer to GPU
0.00.088.706 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.716 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.717 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.687 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.688 I llama_new_context_with_model: n_ctx         = 128
0.00.089.688 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.688 I llama_new_context_with_model: n_batch       = 128
0.00.089.688 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.688 I llama_new_context_with_model: flash_attn    = 0
0.00.089.689 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.689 I llama_new_context_with_model: freq_scale    = 1
0.00.089.690 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.690 I ggml_metal_init: allocating
0.00.089.694 I ggml_metal_init: found device: Apple M4
0.00.089.696 I ggml_metal_init: picking default device: Apple M4
0.00.090.270 I ggml_metal_init: using embedded metal library
0.00.092.293 I ggml_metal_init: GPU name:   Apple M4
0.00.092.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.296 I ggml_metal_init: simdgroup reduction   = true
0.00.092.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.296 I ggml_metal_init: has bfloat            = true
0.00.092.296 I ggml_metal_init: use bfloat            = true
0.00.092.297 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.855 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.858 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.880 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.686 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.687 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.687 I llama_new_context_with_model: graph nodes  = 967
0.00.101.688 I llama_new_context_with_model: graph splits = 2
0.00.101.700 I 
0.00.101.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.101.731 I compute_imatrix: tokenizing the input ..
0.00.108.818 I compute_imatrix: tokenization took 7.087 ms
0.00.108.820 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.716.389 I compute_imatrix: 1.61 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.719.248 I llama_perf_context_print:        load time =    1694.97 ms
0.01.719.249 I llama_perf_context_print: prompt eval time =    1606.63 ms /   128 tokens (   12.55 ms per token,    79.67 tokens per second)
0.01.719.252 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.719.252 I llama_perf_context_print:       total time =    1697.82 ms /   129 tokens
0.01.720.232 I ggml_metal_free: deallocating

real	0m1.904s
user	0m0.166s
sys	0m0.249s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4239 (991f8aab)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c60a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c60a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c60ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c60b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c60b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c60bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c60c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c60ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c60cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c60d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c60d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c60deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c60e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c60f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c60f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c6100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c6107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c610ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c611610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c611de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c612500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c612c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c613340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c613be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c614300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c6145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c614bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c615840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c615d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c616040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c6164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c6167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c617030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c617570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c617830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c617cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c618170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c618610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c618ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c618f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c6193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c619890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c619d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c61a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c61a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c61aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c61b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c61b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c61bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c61c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c61cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c61d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c61d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c61de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c61e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c61eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c61ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c61f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c61f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c620020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c6202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c620780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c620c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c6210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c621560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c621a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c621ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c622340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c6227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c623120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c6235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c623a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c623f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c6243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c624ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c625620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c625ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c625f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c626400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c6268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c626d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c6271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c627b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c628460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c628900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c628da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c6296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c629b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c62a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c62a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c62a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c61b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c62afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c62b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c62b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c62bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c62c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c62c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c62cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c62d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c62d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c62d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c62ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c62e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c62e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c62ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c62f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c62f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c62f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c62fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c6302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c630790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c630c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c6310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c631570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c631eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c632350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c6327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c632c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c633130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c6335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c633a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c633f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c6343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c634850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c634cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c635190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c635630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c635ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c635f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c636410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c6368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c636d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c6371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c637690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c637b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c637fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c638470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c638910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c638db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c639250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c6396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c639b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c63a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c63a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c63a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c63aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c63b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c63b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c63beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c63c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c63c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c63cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c63d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c63d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c63dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c63e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c63ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c63f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c63f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c63fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c6407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c640d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c641280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c6417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c641d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c642270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c6427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c642d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c643260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c6437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c643d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c6447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c644cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c645240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c645790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c645ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c646230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c646780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c646cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c647220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c647770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c647cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c648210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c648760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c648cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c649200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c649750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c649ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c64a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c64a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c64ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c64b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c64b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c64bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c64c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c64c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c64cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c64d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c64d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c64dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c64e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c64e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c64ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c64f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c64f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c64fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c650190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c6506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c650c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c651180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c6516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c651c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c652170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c6526c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c652b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c653000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c6534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c653940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c653de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c654280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c654720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c654bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c655060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c655500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c6559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c655e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c6562e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c656830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c656f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c657670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c657d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c6584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c658770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c658d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c659390 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.162.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f306100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f306570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f3069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f306e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f3072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f307730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f307ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f308010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f304080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f3044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f304960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f308640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f309160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f309910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f30a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f30a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f30af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f30b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f30bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f30c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f30cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f30d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f30dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f30e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f30e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f30ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f30ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f30f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f30f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f30fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f3100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f3105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f310a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f310d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f311190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f311600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f311b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f312060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f312560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f312a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f312f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f313460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f313960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f313e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f314360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f3147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f314c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f3150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f315520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f315990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f315e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f316270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f3166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f316b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f316fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f317790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f317c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f317ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f318500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f318cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f319190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f319630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f319ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f319f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f31a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f31a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f31ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f31b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f31b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f31bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f31bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f31c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f31c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f31cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f31d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f31d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f31db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f31e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f31e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f31e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f31ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f31f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f31f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f31fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f320090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f320530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f3209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f320e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f321310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f3217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f321c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f3220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f322590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f322a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f322ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f323370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f323810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f323cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f324150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f3245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f324a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f324f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f3253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f325870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f325d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f3261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f326650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f326af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f326f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f327430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f3278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f327d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f328210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f3286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f328b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f328ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f329490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f329930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f329dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f32a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f32a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f32abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f32b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f32b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f32b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f32be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f32c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f32c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f32cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f32d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f32d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f32d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f32de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f32e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f32e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f32ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f32f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f32f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f32fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f32fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f330390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f330830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f330cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f331170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f331610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f331ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f331f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f3323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f332890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f332d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f3331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f333670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f333b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f334060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f3345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f334b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f335050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f335310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f335920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f335f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f336540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f336b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f337160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f337950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f337df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f338290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f338730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f338ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f339430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f339980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f339ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f33a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f33a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f33aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f33b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f33b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f33beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f33c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f33c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f33cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f33d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f33d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f33de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f33e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f33e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f33ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f33f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f33f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f33fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f3403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f340910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f340e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f3413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f341900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f341e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f3423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f3428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f342e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f343390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f3438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f343e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f344380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f3448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f344e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f345370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f3458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f345e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f346360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f3468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f346e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f347350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f3478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f347df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f348340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f348890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f348de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f349330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f349880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f349dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f34a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f34a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f34adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f34b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f34b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f34bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f34c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f34c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f34cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f34cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f34d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f34d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f34dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f34e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f34e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f34eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f34efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f34f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f34f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f3500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f350810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f350f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f351650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f351910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f351f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f352530 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f306560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f3069d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f306e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f3072b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f307720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f307b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f308000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f308470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f3088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f308d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f3091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f3097a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f30a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f30a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f30aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f30b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f30bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f30c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f30cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f30d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f30dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f30e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f30ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f30f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f30f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f30fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f3100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f310530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f3109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f310e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f311280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f3116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f311b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f311e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f312290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f312700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f312b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f312fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f313450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f3138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f313d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f3141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f314610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f314a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f314ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f315360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f3157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f315c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f3160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f316520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f316990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f316e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f317270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f3176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f317b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f317fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f318430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f3188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f318d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f319180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f3195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f319a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f319ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f31a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f31a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f31ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f31b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f31b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f31b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f31bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f31c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f31c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f31cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f31cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f31d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f31d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f31dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f31e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f31e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f31ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f31eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f31f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f31f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f31fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f320070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f3204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f320950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f320dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f321230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f3216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f321b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f321f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f3223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f322860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f322cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f323140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f3235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f323a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f323e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f324300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f324770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f324be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f325050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f3254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f325930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f325da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f326210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f326680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f326af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f326f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f3273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f327840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f327cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f328120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f328590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f328a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f328e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f3292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f329750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f329bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f32a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f32a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f32a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f32ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f32b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f32b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f32bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f32bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f32c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f32c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f32cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f32d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f32d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f32d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f32de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f32e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f32e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f32eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f32f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f32f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f32f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f32fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f3301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f330640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f330ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f330f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f331390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f331800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f331c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f3320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f332550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f3329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f332e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f3332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f333710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f333b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f333ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f334460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f3348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f334d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f3351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f335620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f335a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f335f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f336370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f3367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f336c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f3373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f337840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f337cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f338120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f338590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f338a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f338e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f3392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f339750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f339bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f33a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f33a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f33a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f33ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f33b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f33b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f33bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f33bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f33c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f33c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f33cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f33d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f33d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f33d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f33de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f33e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f33e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f33eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f33f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f33f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f33f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f33fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f3401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f340640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f340ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f340f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f341390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f341800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f341c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f3420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f342550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f3429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f342e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f3432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f343710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f343b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f343ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f344460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f3448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f344d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f3451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f345620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f345a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f345f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f346370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f3467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f346c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f3470c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f347530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f3479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f347e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f348280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f3486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f348b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f348fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f349440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f3498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f349d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f34a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f34a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f34aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f34b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f34b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f34bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f34c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f34caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f34cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f34d380 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.808s
user	0m0.293s
sys	0m0.303s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4239 (991f8aab)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15460c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15460c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15460cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15460d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15460d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15460dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15460e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15460e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15460ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15460f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15460f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15460fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x154610720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x154610ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1546116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x154611e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x154612520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x154612c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x154613360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154613b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154614250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154614970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154615090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154615930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154616050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154616310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154616920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154617590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154617ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154617d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154618230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1546184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154618d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1546192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154619580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154619a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154619ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15461a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15461a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15461aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15461b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15461b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15461ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15461bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15461c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15461c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15461ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15461d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15461dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15461e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15461e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15461ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15461f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15461fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x154620370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x154620810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x154620cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x154620f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x154621580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x154621d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154622030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1546224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154622970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x154622e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1546232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x154623750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x154623bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154624090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x154624530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1546249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x154624e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154625310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1546257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154625c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1546260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154626590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154626a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154626ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154627370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154627810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x154627cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154628150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1546285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x154628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154628f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1546293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154629870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154629d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15462a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15462a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15462aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15462af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15462b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15462b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15462bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15462c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15462c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15461d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15462cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15462d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15462d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15462dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15462df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15462e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15462e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15462ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15462f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15462f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15462fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15462ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154630480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x154630920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x154630dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154631260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154631700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154631ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x154632040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1546324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x154632980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x154632e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1546332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x154633760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x154633c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1546340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154634540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1546349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x154634e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154635320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1546357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154635c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154636100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1546365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154636a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154636ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154637380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154637820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x154637cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154638160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154638600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x154638aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154638f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1546393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154639880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154639d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15463a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15463a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15463ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15463afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15463b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15463b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15463bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15463c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15463c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15463cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15463d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15463d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15463dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15463dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15463e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15463eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15463f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15463f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15463fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154640500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1546409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154640e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1546412e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154641a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x154641fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154642530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154642a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154643520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154643a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154643fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x154644510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154644a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154644fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154645500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154645a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x154645fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1546464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154646a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x154646f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1546474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154647a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x154647f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1546484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154648a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154648f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1546494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154649a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154649f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15464a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15464aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15464af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15464b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15464b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15464bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15464c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15464c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15464cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15464d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15464d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15464df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15464e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15464e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15464ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15464f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15464f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15464ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154650450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1546509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154650ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x154651440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154651990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154651ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154652430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154652980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154652ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154653420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x154653970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154653ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154654410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1546548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154654d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1546551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154655690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154655b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154655fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154656470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154656910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x154656db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154657250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1546576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154657b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154658030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154658580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154658ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1546593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154659ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15465a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15465a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15465aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15465b0e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.116.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15460c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15460cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15460e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15460ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15460da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15460b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15464b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15464b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15464b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15464be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15464c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15464c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15464ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15464d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15464ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15464e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15464ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15464f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15464f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154650330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154650a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154651110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x154651800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x154651ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1546525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x154652a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154652ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x154653330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1546537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x154653c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x154654080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1546544f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154654960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x154654c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154655090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154655500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154655de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154656250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1546566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154656b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154656fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154657410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154657880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154657cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154658160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1546585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154658a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154658eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154659320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x154659790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x154659c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15465a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15465a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15465a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15465adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15465b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15464a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15464a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15464a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x154619460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1546198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x154619d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15461a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15461a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15461aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15461af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15461b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15461b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15461bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15461c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15461c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15461c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15461ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15461d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15461d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15461db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15461dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15461e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15461e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15461ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15461f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15461f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15461fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15461fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154620350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1546207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154620c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1546210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154621510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154621980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x154621df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154622260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1546226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x154622b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154622fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154623420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154623890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154623d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154624170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1546245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154624a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x154624ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154625330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1546257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x154625c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x154626080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1546264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x154626960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154626dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x154627240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1546276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x154627b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x154627f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154628870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x154628ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x154629150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1546295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x154629a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x154629ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15462a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15462a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15462abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15462b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15462b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15462b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15462bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15462c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15462c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15462cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15462cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15462d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15462d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15462dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15462e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15462e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15462ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15462ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15462f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15462f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15462fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x154630040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1546304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154630920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154630d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154631200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x154631670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154631ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154631f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1546323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x154632ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154633110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154633580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1546339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154633e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1546342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154634740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x154634bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x154635020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x154635490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x154635900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x154635d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1546361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x154636650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x154636ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x154637240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1546376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x154637b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x154637f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x154638400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x154638870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x154638ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x154639150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1546395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154639a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x154639ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15463a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15463a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15463abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15463b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15463b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15463b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15463bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15463c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15463c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15463cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15463cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15463d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15463d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15463dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15463e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15463e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15463ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15463ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15463f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15463f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15463fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154640040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1546404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154640920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x154641200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154641670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154641ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x154641f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1546423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x154642830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x154642ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x154643110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x154643580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1546439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x154643e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1546442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x154644740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x154644bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x154645020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x154645490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x154645900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x154645d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1546461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x154646650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x154646ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x154646f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1546473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154647810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x154647c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1546480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x154648560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1546489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154648e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1546492b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154649720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15460f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15460f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15460faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15460ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x154610600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154610cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1546113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154611ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154611f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1546123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154612820 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x154709950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x154709dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15470a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15470a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15470ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15470af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15470b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15470b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15470bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15470c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15470c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15470ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15470d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15470df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15470e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15470eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15470f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15470fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x154710410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x154710b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x154711260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x154711980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1547120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1547127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x154712ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1547131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x154713460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1547138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154713d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1547141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1547146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x154714bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x154715030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1547152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x154715760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x154715bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x154716130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x154716630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154716b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x154717030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154717530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154717a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x154717f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x154718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x154718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x154718da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x154719210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x154719680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x154719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x154719f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15471a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15471a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15471acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15471b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15471b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15471bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15471c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15471c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15471cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15471d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15471d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15471dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15471e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15471e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15471e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15471ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15471f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15471f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15471fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x154720100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1547205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x154720a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x154720ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x154721380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x154721820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x154721cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x154722160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x154722600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x154722aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x154722f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1547233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x154723880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x154723d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1547241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x154724660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x154724b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x154724fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x154725440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1547258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x154725d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x154726220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1547266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x154726b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x154727000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1547274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x154727940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x154727de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x154728280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x154728720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x154728bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x154729060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x154729500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1547299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x154729e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15472a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15472a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15472ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15472b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15472b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15472ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15472bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15472c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15472c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15472cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15472d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15472d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15472da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15472df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15472e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15472e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15472ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15472f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15472f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15472fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15472ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x154730400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1547308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x154730d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1547311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x154731680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x154731b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x154731fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x154732460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x154732900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x154732da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x154733240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1547336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x154733b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x154734020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1547344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x154734960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x154734e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1547352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x154735740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x154735be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x154736080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x154736520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1547369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x154736e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x154737300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1547377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x154737c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1547380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x154738630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x154738b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1547390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x154739620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1547398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x154739ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15473a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15473ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15473b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15473b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15473bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15473c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15473c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15473cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15473d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15473da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15473df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15473e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15473e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15473ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15473f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15473f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15473ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x154740480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1547409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x154740f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x154741470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1547419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x154741f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x154742460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1547429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x154742f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x154743450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1547439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x154743ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x154744440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x154744990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x154744ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x154745430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x154745980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x154745ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x154746420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x154746970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x154746ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x154747410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x154747960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x154747eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x154748400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x154748950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x154748ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1547493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x154749940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x154749e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15474a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15474a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15474ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15474b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15474b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15474be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15474c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15474c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15474ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15474d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15474d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15474de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15474e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15474e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15474ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15474f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15474f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15474fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1547502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x154750770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x154750c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1547510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x154751550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1547519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x154751e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x154752330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1547527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x154752c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x154753110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1547535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x154753a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x154753fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1547546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x154754de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x154755500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x154755c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x154755ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1547564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x154756b00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.994s
user	0m0.256s
sys	0m0.142s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
