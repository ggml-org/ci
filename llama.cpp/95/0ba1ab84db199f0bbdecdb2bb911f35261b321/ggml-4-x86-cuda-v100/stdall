Requirement already satisfied: numpy~=1.24.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.7.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)
Requirement already satisfied: typing-extensions in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.2.0)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)
Requirement already satisfied: triton==2.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.7.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.7.0-py3-none-any.whl size=3229 sha256=e77d3489e0beea138863799a05abb421092af60b67ffc67b376508ee3f0720bf
  Stored in directory: /tmp/pip-ephem-wheel-cache-ggcxvkjl/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.7.0
    Uninstalling gguf-0.7.0:
      Successfully uninstalled gguf-0.7.0
Successfully installed gguf-0.7.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_debug.log
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.0s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m3.254s
user	0m2.423s
sys	0m0.835s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_debug-make.log
+ make -j
[  1%] Generating build details from Git
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target build_info
[  6%] Built target ggml
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CUDA static library libggml_static.a
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target gguf
[  9%] Built target llama
[ 10%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 10%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 11%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 11%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 13%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 18%] Linking CXX executable ../bin/test-c
[ 19%] Linking CXX executable ../../bin/quantize
[ 20%] Linking CXX executable ../../bin/quantize-stats
[ 20%] Built target llava
[ 21%] Linking CXX static library libllava_static.a
[ 21%] Linking CXX executable ../../bin/benchmark
[ 22%] Linking CXX static library libcommon.a
[ 22%] Built target test-c
[ 22%] Built target llava_static
[ 22%] Built target common
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 29%] Linking CXX executable ../bin/test-quantize-perf
[ 29%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 33%] Built target benchmark
[ 35%] Linking CXX executable ../bin/test-chat-template
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-quantize-fns
[ 37%] Linking CXX executable ../bin/test-sampling
[ 38%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 43%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-grad0
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-rope
[ 55%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 58%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 61%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 62%] Linking CXX executable ../../bin/baby-llama
[ 62%] Linking CXX executable ../bin/test-llama-grammar
[ 62%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 64%] Linking CXX executable ../../bin/batched
[ 64%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../../bin/batched-bench
[ 66%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 67%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 68%] Linking CXX executable ../../bin/embedding
[ 69%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 70%] Linking CXX executable ../../bin/finetune
[ 71%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 72%] Linking CXX executable ../../bin/beam-search
[ 73%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 74%] Linking CXX executable ../../bin/infill
[ 74%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 75%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 76%] Linking CXX executable ../../bin/main
[ 77%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-bench
[ 79%] Linking CXX executable ../../bin/llava-cli
[ 79%] Linking CXX executable ../../bin/tokenize
[ 80%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 81%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 82%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 83%] Linking CXX executable ../../bin/parallel
[ 84%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 85%] Linking CXX executable ../../bin/save-load-state
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 87%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 87%] Linking CXX executable ../../bin/perplexity
[ 88%] Linking CXX executable ../../bin/simple
[ 89%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 89%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 90%] Linking CXX executable ../../bin/passkey
[ 90%] Linking CXX executable ../../bin/speculative
[ 91%] Linking CXX executable ../../bin/lookahead
[ 91%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 92%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 93%] Linking CXX executable ../../bin/lookup
[ 94%] Linking CXX executable ../../bin/train-text-from-scratch
[ 95%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 95%] Built target test-quantize-perf
[ 96%] Linking CXX executable ../../bin/imatrix
[ 97%] Linking CXX executable ../../bin/vdot
[ 98%] Linking CXX executable ../../bin/q8dot
[ 99%] Linking CXX executable ../../bin/export-lora
[ 99%] Built target test-grammar-parser
[ 99%] Built target test-grad0
[ 99%] Built target test-quantize-fns
[ 99%] Built target quantize
[ 99%] Built target test-backend-ops
[ 99%] Built target test-rope
[ 99%] Built target quantize-stats
[ 99%] Built target convert-llama2c-to-ggml
[ 99%] Built target export-lora
[ 99%] Built target q8dot
[ 99%] Built target vdot
[ 99%] Built target test-chat-template
[ 99%] Built target test-sampling
[ 99%] Built target test-autorelease
[ 99%] Built target test-model-load-cancel
[ 99%] Built target test-llama-grammar
[ 99%] Built target baby-llama
[ 99%] Built target test-tokenizer-0-llama
[ 99%] Built target infill
[ 99%] Built target lookup
[ 99%] Built target embedding
[ 99%] Built target finetune
[ 99%] Built target test-tokenizer-1-llama
[ 99%] Built target test-tokenizer-0-falcon
[ 99%] Built target batched
[ 99%] Built target test-tokenizer-1-bpe
[ 99%] Built target beam-search
[ 99%] Built target tokenize
[ 99%] Built target imatrix
[ 99%] Built target parallel
[ 99%] Built target lookahead
[ 99%] Built target perplexity
[ 99%] Built target batched-bench
[ 99%] Built target simple
[ 99%] Built target llama-bench
[ 99%] Built target save-load-state
[ 99%] Built target speculative
[ 99%] Built target passkey
[ 99%] Built target train-text-from-scratch
[ 99%] Built target main
[ 99%] Built target llava-cli
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	0m25.283s
user	0m40.942s
sys	0m6.628s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-quantize-fns
 1/21 Test  #1: test-quantize-fns ...................   Passed   15.85 sec
      Start  2: test-quantize-perf
 2/21 Test  #2: test-quantize-perf ..................   Passed   11.19 sec
      Start  3: test-sampling
 3/21 Test  #3: test-sampling .......................   Passed    0.07 sec
      Start  4: test-chat-template
 4/21 Test  #4: test-chat-template ..................   Passed    0.04 sec
      Start  5: test-tokenizer-0-llama
 5/21 Test  #5: test-tokenizer-0-llama ..............   Passed    0.73 sec
      Start  6: test-tokenizer-0-falcon
 6/21 Test  #6: test-tokenizer-0-falcon .............   Passed    1.90 sec
      Start  7: test-tokenizer-1-llama
 7/21 Test  #7: test-tokenizer-1-llama ..............   Passed    4.49 sec
      Start  8: test-tokenizer-1-baichuan
 8/21 Test  #8: test-tokenizer-1-baichuan ...........   Passed    5.08 sec
      Start  9: test-tokenizer-1-falcon
 9/21 Test  #9: test-tokenizer-1-falcon .............   Passed    8.46 sec
      Start 10: test-tokenizer-1-aquila
10/21 Test #10: test-tokenizer-1-aquila .............   Passed   12.06 sec
      Start 11: test-tokenizer-1-mpt
11/21 Test #11: test-tokenizer-1-mpt ................   Passed    7.62 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/21 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    6.88 sec
      Start 13: test-tokenizer-1-gpt-neox
13/21 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    7.37 sec
      Start 14: test-tokenizer-1-refact
14/21 Test #14: test-tokenizer-1-refact .............   Passed    7.16 sec
      Start 15: test-tokenizer-1-starcoder
15/21 Test #15: test-tokenizer-1-starcoder ..........   Passed    6.69 sec
      Start 16: test-tokenizer-1-gpt2
16/21 Test #16: test-tokenizer-1-gpt2 ...............   Passed    7.42 sec
      Start 17: test-grammar-parser
17/21 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/21 Test #18: test-llama-grammar ..................   Passed    0.04 sec
      Start 19: test-grad0
19/21 Test #19: test-grad0 ..........................   Passed    5.16 sec
      Start 20: test-backend-ops
20/21 Test #20: test-backend-ops ....................   Passed  193.19 sec
      Start 21: test-rope
21/21 Test #21: test-rope ...........................   Passed    0.61 sec

100% tests passed, 0 tests failed out of 21

Label Time Summary:
main    = 302.02 sec*proc (21 tests)

Total Test time (real) = 302.03 sec

real	5m2.064s
user	8m23.326s
sys	0m30.553s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ tee /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_release.log
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.1s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.300s
user	0m2.466s
sys	0m0.841s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_release-make.log
+ make -j
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  6%] Built target build_info
[  6%] Built target ggml
[  8%] Linking CUDA static library libggml_static.a
[  8%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[ 10%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 12%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 14%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 15%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 17%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 17%] Linking CXX executable ../../bin/benchmark
[ 17%] Built target gguf
[ 18%] Linking CXX executable ../../bin/quantize
[ 18%] Built target llava
[ 19%] Linking CXX executable ../bin/test-c
[ 20%] Linking CXX static library libcommon.a
[ 21%] Linking CXX static library libllava_static.a
[ 22%] Linking CXX executable ../../bin/quantize-stats
[ 22%] Built target test-c
[ 22%] Built target llava_static
[ 22%] Built target common
[ 22%] Built target benchmark
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 28%] Linking CXX executable ../bin/test-quantize-fns
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 29%] Built target quantize
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 31%] Built target quantize-stats
[ 31%] Linking CXX executable ../bin/test-quantize-perf
[ 32%] Linking CXX executable ../bin/test-chat-template
[ 33%] Linking CXX executable ../bin/test-sampling
[ 34%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 35%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grad0
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-rope
[ 53%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 54%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 58%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../../bin/baby-llama
[ 63%] Linking CXX executable ../../bin/batched
[ 63%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../../bin/batched-bench
[ 66%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/beam-search
[ 68%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 69%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 70%] Linking CXX executable ../../bin/finetune
[ 71%] Linking CXX executable ../../bin/embedding
[ 72%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 75%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 75%] Linking CXX executable ../../bin/infill
[ 75%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 76%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 77%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 78%] Linking CXX executable ../../bin/llava-cli
[ 78%] Linking CXX executable ../../bin/tokenize
[ 79%] Linking CXX executable ../../bin/llama-bench
[ 79%] Built target test-quantize-fns
[ 80%] Linking CXX executable ../../bin/parallel
[ 80%] Linking CXX executable ../../bin/perplexity
[ 81%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 82%] Linking CXX executable ../../bin/main
[ 83%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 84%] Linking CXX executable ../../bin/save-load-state
[ 85%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 86%] Linking CXX executable ../../bin/simple
[ 87%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 87%] Built target test-grammar-parser
[ 88%] Linking CXX executable ../../bin/speculative
[ 88%] Linking CXX executable ../../bin/passkey
[ 89%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 90%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 90%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 90%] Linking CXX executable ../../bin/lookahead
[ 91%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 92%] Linking CXX executable ../../bin/lookup
[ 93%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/train-text-from-scratch
[ 95%] Linking CXX executable ../../bin/imatrix
[ 95%] Built target test-sampling
[ 96%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 96%] Built target test-quantize-perf
[ 96%] Built target test-tokenizer-0-falcon
[ 97%] Linking CXX executable ../../bin/vdot
[ 98%] Linking CXX executable ../../bin/export-lora
[ 99%] Linking CXX executable ../../bin/q8dot
[ 99%] Built target test-chat-template
[ 99%] Built target test-tokenizer-0-llama
[ 99%] Built target test-grad0
[ 99%] Built target baby-llama
[ 99%] Built target test-tokenizer-1-llama
[ 99%] Built target test-rope
[ 99%] Built target test-backend-ops
[ 99%] Built target test-autorelease
[ 99%] Built target test-model-load-cancel
[ 99%] Built target test-llama-grammar
[ 99%] Built target test-tokenizer-1-bpe
[ 99%] Built target main
[ 99%] Built target embedding
[ 99%] Built target batched-bench
[ 99%] Built target save-load-state
[ 99%] Built target convert-llama2c-to-ggml
[ 99%] Built target batched
[ 99%] Built target finetune
[ 99%] Built target q8dot
[ 99%] Built target train-text-from-scratch
[ 99%] Built target llava-cli
[ 99%] Built target passkey
[ 99%] Built target export-lora
[ 99%] Built target imatrix
[ 99%] Built target beam-search
[ 99%] Built target simple
[ 99%] Built target infill
[ 99%] Built target lookup
[ 99%] Built target tokenize
[ 99%] Built target llama-bench
[ 99%] Built target vdot
[ 99%] Built target parallel
[ 99%] Built target perplexity
[ 99%] Built target speculative
[ 99%] Built target lookahead
[100%] Linking CXX executable ../../bin/server
[100%] Built target server

real	0m33.028s
user	0m38.239s
sys	0m3.531s
+ '[' -z ']'
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /home/ggml/work/llama.cpp/build-ci-release
      Start  1: test-quantize-fns
 1/21 Test  #1: test-quantize-fns ...................   Passed    8.90 sec
      Start  2: test-quantize-perf
 2/21 Test  #2: test-quantize-perf ..................   Passed    6.75 sec
      Start  3: test-sampling
 3/21 Test  #3: test-sampling .......................   Passed    0.05 sec
      Start  4: test-chat-template
 4/21 Test  #4: test-chat-template ..................   Passed    0.04 sec
      Start  5: test-tokenizer-0-llama
 5/21 Test  #5: test-tokenizer-0-llama ..............   Passed    0.59 sec
      Start  6: test-tokenizer-0-falcon
 6/21 Test  #6: test-tokenizer-0-falcon .............   Passed    0.85 sec
      Start  7: test-tokenizer-1-llama
 7/21 Test  #7: test-tokenizer-1-llama ..............   Passed    1.04 sec
      Start  8: test-tokenizer-1-baichuan
 8/21 Test  #8: test-tokenizer-1-baichuan ...........   Passed    1.20 sec
      Start  9: test-tokenizer-1-falcon
 9/21 Test  #9: test-tokenizer-1-falcon .............   Passed    1.62 sec
      Start 10: test-tokenizer-1-aquila
10/21 Test #10: test-tokenizer-1-aquila .............   Passed    2.41 sec
      Start 11: test-tokenizer-1-mpt
11/21 Test #11: test-tokenizer-1-mpt ................   Passed    1.55 sec
      Start 12: test-tokenizer-1-stablelm-3b-4e1t
12/21 Test #12: test-tokenizer-1-stablelm-3b-4e1t ...   Passed    1.42 sec
      Start 13: test-tokenizer-1-gpt-neox
13/21 Test #13: test-tokenizer-1-gpt-neox ...........   Passed    1.41 sec
      Start 14: test-tokenizer-1-refact
14/21 Test #14: test-tokenizer-1-refact .............   Passed    1.36 sec
      Start 15: test-tokenizer-1-starcoder
15/21 Test #15: test-tokenizer-1-starcoder ..........   Passed    1.38 sec
      Start 16: test-tokenizer-1-gpt2
16/21 Test #16: test-tokenizer-1-gpt2 ...............   Passed    1.41 sec
      Start 17: test-grammar-parser
17/21 Test #17: test-grammar-parser .................   Passed    0.00 sec
      Start 18: test-llama-grammar
18/21 Test #18: test-llama-grammar ..................   Passed    0.04 sec
      Start 19: test-grad0
19/21 Test #19: test-grad0 ..........................   Passed    4.81 sec
      Start 20: test-backend-ops
20/21 Test #20: test-backend-ops ....................   Passed   53.58 sec
      Start 21: test-rope
21/21 Test #21: test-rope ...........................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 21

Label Time Summary:
main    =  91.00 sec*proc (21 tests)

Total Test time (real) =  91.01 sec

real	1m31.045s
user	1m29.147s
sys	0m25.103s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/embd_bge_small.log
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:46 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model
https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/tokenizer.model:
2024-03-09 10:35:46 ERROR 404: Not Found.
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:46 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:46 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:47 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:47 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:48 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:48 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:48 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.3s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.518s
user	0m2.633s
sys	0m0.883s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/embd_bge_small-make.log
+ make -j
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  5%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  6%] Built target build_info
[  6%] Built target ggml
[  7%] Linking CUDA static library libggml_static.a
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  8%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[  9%] Built target gguf
[ 10%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 12%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 13%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 14%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 15%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 17%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 18%] Linking CXX executable ../bin/test-c
[ 18%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 18%] Built target llava
[ 18%] Linking CXX executable ../../bin/benchmark
[ 19%] Linking CXX static library libllava_static.a
[ 20%] Linking CXX executable ../../bin/quantize
[ 21%] Linking CXX static library libcommon.a
[ 22%] Linking CXX executable ../../bin/quantize-stats
[ 22%] Built target test-c
[ 22%] Built target llava_static
[ 22%] Built target common
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 29%] Built target benchmark
[ 30%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 32%] Linking CXX executable ../bin/test-quantize-perf
[ 33%] Linking CXX executable ../bin/test-quantize-fns
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 37%] Linking CXX executable ../bin/test-sampling
[ 38%] Linking CXX executable ../bin/test-chat-template
[ 39%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 39%] Built target quantize
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 40%] Built target quantize-stats
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-grad0
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Linking CXX executable ../bin/test-llama-grammar
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 56%] Linking CXX executable ../bin/test-rope
[ 57%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 61%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 61%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 62%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 63%] Linking CXX executable ../../bin/baby-llama
[ 64%] Linking CXX executable ../../bin/batched
[ 64%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 65%] Linking CXX executable ../../bin/batched-bench
[ 66%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 67%] Linking CXX executable ../../bin/beam-search
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 69%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 70%] Linking CXX executable ../../bin/embedding
[ 71%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 72%] Linking CXX executable ../../bin/infill
[ 73%] Linking CXX executable ../../bin/finetune
[ 73%] Built target test-grammar-parser
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 75%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 75%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 76%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llava-cli
[ 78%] Linking CXX executable ../../bin/main
[ 79%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 79%] Built target test-sampling
[ 80%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 81%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 81%] Linking CXX executable ../../bin/tokenize
[ 83%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 82%] Linking CXX executable ../../bin/parallel
[ 84%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 85%] Linking CXX executable ../../bin/perplexity
[ 86%] Linking CXX executable ../../bin/passkey
[ 87%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 88%] Linking CXX executable ../../bin/lookahead
[ 89%] Linking CXX executable ../../bin/save-load-state
[ 90%] Linking CXX executable ../../bin/lookup
[ 90%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 91%] Linking CXX executable ../../bin/simple
[ 91%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 91%] Linking CXX executable ../../bin/speculative
[ 92%] Linking CXX executable ../../bin/imatrix
[ 94%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/train-text-from-scratch
[ 97%] Linking CXX executable ../../bin/server
[ 98%] Linking CXX executable ../../bin/export-lora
[ 99%] Linking CXX executable ../../bin/q8dot
[ 99%] Built target test-llama-grammar
[ 99%] Built target test-quantize-perf
[100%] Linking CXX executable ../../bin/vdot
[100%] Built target test-tokenizer-1-bpe
[100%] Built target test-quantize-fns
[100%] Built target test-tokenizer-0-llama
[100%] Built target test-tokenizer-0-falcon
[100%] Built target test-tokenizer-1-llama
[100%] Built target baby-llama
[100%] Built target test-chat-template
[100%] Built target test-backend-ops
[100%] Built target test-rope
[100%] Built target test-model-load-cancel
[100%] Built target test-autorelease
[100%] Built target test-grad0
[100%] Built target llava-cli
[100%] Built target batched-bench
[100%] Built target convert-llama2c-to-ggml
[100%] Built target parallel
[100%] Built target save-load-state
[100%] Built target beam-search
[100%] Built target finetune
[100%] Built target batched
[100%] Built target lookup
[100%] Built target passkey
[100%] Built target infill
[100%] Built target export-lora
[100%] Built target speculative
[100%] Built target llama-bench
[100%] Built target q8dot
[100%] Built target simple
[100%] Built target main
[100%] Built target imatrix
[100%] Built target vdot
[100%] Built target train-text-from-scratch
[100%] Built target embedding
[100%] Built target tokenize
[100%] Built target perplexity
[100%] Built target lookahead
[100%] Built target server

real	0m2.087s
user	0m8.143s
sys	0m2.900s
+ python3 ../convert-hf-to-gguf.py ../models-mnt/bge-small
/mnt/llama.cpp/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading model: bge-small
gguf: This GGUF file is for Little Endian only
Set model parameters
Set model tokenizer
fname_tokenizer: ../models-mnt/bge-small
gguf: Setting special token type pad to 0
Exporting model to '../models-mnt/bge-small/ggml-model-f16.gguf'
gguf: loading model part 'pytorch_model.bin'
token_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
position_embd.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
token_types.weight, n_dims = 2, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
token_embd_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.0.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.0.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.1.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.1.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.2.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.2.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.3.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.3.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.4.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.4.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.5.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.5.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.6.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.6.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.7.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.7.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.8.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.8.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.9.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.9.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.10.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.10.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_q.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_q.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_k.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_k.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_v.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_v.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.attn_output.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.attn_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_up.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_up.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.ffn_down.weight, n_dims = 2, torch.float32 --> <class 'numpy.float16'>
blk.11.ffn_down.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.weight, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
blk.11.layer_output_norm.bias, n_dims = 1, torch.float32 --> <class 'numpy.float32'>
Model successfully exported to '../models-mnt/bge-small/ggml-model-f16.gguf'
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llama_model_quantize_internal ============ Strange model: n_attention_wv = 12, n_ffn_down = 24, hparams.n_layer = 12
llama_model_quantize_internal: meta size = 760800 bytes
[   1/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, quantizing to q8_0 .. size =    22.35 MiB ->    11.88 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[   2/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f16, size =    0.375 MB
[   3/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   6/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.031 0.046 0.066 0.086 0.108 0.230 0.109 0.088 0.066 0.046 0.031 0.020 0.027 
[   7/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   8/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.019 0.031 0.046 0.067 0.089 0.107 0.227 0.107 0.087 0.068 0.047 0.031 0.019 0.027 
[   9/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  10/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.018 0.030 0.045 0.066 0.086 0.109 0.237 0.109 0.088 0.065 0.046 0.030 0.018 0.026 
[  11/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.032 0.048 0.066 0.089 0.106 0.228 0.107 0.088 0.066 0.047 0.031 0.019 0.028 
[  13/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  14/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  16/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.030 0.021 0.033 0.049 0.069 0.091 0.110 0.232 0.105 0.084 0.063 0.044 0.028 0.017 0.024 
[  17/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  18/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.016 0.026 0.041 0.061 0.087 0.114 0.259 0.115 0.087 0.061 0.041 0.026 0.016 0.025 
[  19/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  20/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.019 0.031 0.047 0.067 0.089 0.108 0.230 0.108 0.087 0.066 0.046 0.030 0.019 0.026 
[  23/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  24/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.088 0.109 0.240 0.108 0.088 0.065 0.046 0.029 0.018 0.026 
[  25/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  26/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.029 0.045 0.064 0.087 0.109 0.239 0.109 0.087 0.064 0.045 0.030 0.019 0.027 
[  27/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.090 0.107 0.226 0.106 0.089 0.066 0.047 0.031 0.020 0.027 
[  29/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  30/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  32/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.028 0.019 0.030 0.046 0.066 0.089 0.111 0.241 0.108 0.085 0.062 0.043 0.028 0.017 0.025 
[  33/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  34/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.016 0.025 0.039 0.059 0.085 0.116 0.269 0.115 0.086 0.060 0.040 0.025 0.015 0.025 
[  35/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  36/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.089 0.107 0.231 0.107 0.087 0.065 0.046 0.031 0.019 0.027 
[  39/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  40/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.029 0.046 0.065 0.088 0.110 0.237 0.109 0.088 0.064 0.045 0.029 0.018 0.027 
[  41/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  42/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.066 0.087 0.109 0.237 0.110 0.087 0.066 0.046 0.029 0.019 0.027 
[  43/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.021 0.032 0.048 0.066 0.087 0.107 0.224 0.104 0.089 0.067 0.049 0.032 0.020 0.027 
[  45/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  46/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  48/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.109 0.240 0.108 0.087 0.064 0.044 0.029 0.018 0.026 
[  49/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  50/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.025 0.039 0.058 0.085 0.116 0.273 0.116 0.086 0.059 0.039 0.024 0.016 0.025 
[  51/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  52/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.045 0.066 0.088 0.109 0.234 0.108 0.089 0.066 0.046 0.029 0.019 0.027 
[  55/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  56/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.017 0.028 0.044 0.063 0.087 0.111 0.247 0.114 0.088 0.062 0.043 0.028 0.017 0.026 
[  57/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  58/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.064 0.088 0.110 0.243 0.111 0.088 0.064 0.044 0.029 0.018 0.026 
[  59/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.020 0.032 0.047 0.068 0.087 0.106 0.225 0.106 0.088 0.068 0.046 0.031 0.020 0.027 
[  61/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  62/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  64/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.088 0.108 0.239 0.108 0.087 0.065 0.045 0.029 0.018 0.027 
[  65/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  66/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.025 0.039 0.059 0.086 0.115 0.269 0.115 0.086 0.060 0.040 0.025 0.015 0.025 
[  67/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  68/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.029 0.046 0.064 0.088 0.109 0.235 0.110 0.088 0.065 0.045 0.031 0.018 0.027 
[  71/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  72/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.027 0.043 0.064 0.088 0.111 0.245 0.112 0.087 0.064 0.043 0.028 0.017 0.026 
[  73/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  74/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.027 0.042 0.062 0.085 0.113 0.256 0.112 0.087 0.061 0.042 0.027 0.017 0.026 
[  75/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.019 0.031 0.048 0.067 0.089 0.105 0.226 0.107 0.088 0.066 0.047 0.032 0.020 0.027 
[  77/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  78/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  80/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.018 0.029 0.044 0.063 0.087 0.108 0.242 0.110 0.088 0.065 0.045 0.030 0.019 0.027 
[  81/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  82/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.024 0.038 0.059 0.085 0.116 0.275 0.116 0.086 0.059 0.038 0.024 0.015 0.025 
[  83/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  84/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.089 0.107 0.233 0.107 0.087 0.066 0.046 0.030 0.019 0.027 
[  87/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  88/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.029 0.044 0.064 0.088 0.111 0.240 0.110 0.087 0.064 0.045 0.029 0.018 0.026 
[  89/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  90/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.017 0.027 0.040 0.061 0.085 0.112 0.262 0.114 0.086 0.061 0.042 0.027 0.016 0.026 
[  91/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.086 0.105 0.225 0.105 0.089 0.067 0.048 0.032 0.020 0.027 
[  93/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  94/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  96/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.017 0.028 0.042 0.062 0.086 0.109 0.243 0.111 0.089 0.065 0.046 0.030 0.019 0.028 
[  97/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  98/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.025 0.040 0.060 0.087 0.115 0.267 0.116 0.086 0.060 0.040 0.025 0.015 0.025 
[  99/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 100/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.030 0.046 0.067 0.088 0.106 0.236 0.108 0.087 0.066 0.046 0.030 0.018 0.028 
[ 103/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 104/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.017 0.029 0.044 0.064 0.088 0.110 0.244 0.112 0.087 0.064 0.045 0.028 0.018 0.026 
[ 105/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 106/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.016 0.026 0.040 0.059 0.085 0.115 0.267 0.114 0.085 0.060 0.041 0.025 0.016 0.025 
[ 107/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.020 0.032 0.047 0.067 0.088 0.105 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 109/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 110/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 112/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.018 0.028 0.044 0.064 0.087 0.110 0.240 0.110 0.088 0.065 0.045 0.029 0.019 0.027 
[ 113/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 114/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.017 0.027 0.042 0.062 0.086 0.113 0.255 0.113 0.087 0.062 0.042 0.027 0.017 0.026 
[ 115/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 116/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.047 0.064 0.088 0.110 0.235 0.109 0.089 0.065 0.045 0.029 0.018 0.027 
[ 119/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 120/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.086 0.107 0.236 0.111 0.088 0.065 0.045 0.029 0.019 0.027 
[ 121/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 122/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.016 0.026 0.041 0.060 0.086 0.113 0.263 0.113 0.086 0.061 0.041 0.027 0.017 0.025 
[ 123/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.021 0.031 0.047 0.067 0.088 0.105 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 125/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 126/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 128/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.088 0.110 0.242 0.109 0.088 0.064 0.045 0.029 0.018 0.027 
[ 129/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 130/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.016 0.027 0.041 0.062 0.087 0.113 0.256 0.113 0.087 0.062 0.042 0.027 0.016 0.025 
[ 131/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 132/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.018 0.028 0.044 0.065 0.088 0.112 0.239 0.111 0.087 0.065 0.044 0.028 0.018 0.026 
[ 135/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 136/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.062 0.089 0.113 0.247 0.113 0.087 0.063 0.042 0.027 0.017 0.026 
[ 137/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 138/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.016 0.026 0.041 0.059 0.086 0.113 0.265 0.113 0.086 0.061 0.041 0.026 0.016 0.026 
[ 139/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.089 0.104 0.225 0.105 0.087 0.067 0.049 0.032 0.020 0.027 
[ 141/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 142/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 144/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.028 0.019 0.030 0.046 0.066 0.088 0.109 0.238 0.109 0.087 0.064 0.044 0.029 0.018 0.025 
[ 145/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 146/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.017 0.027 0.042 0.062 0.087 0.112 0.253 0.113 0.087 0.063 0.042 0.027 0.017 0.025 
[ 147/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 148/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.030 0.044 0.065 0.088 0.108 0.236 0.109 0.087 0.066 0.047 0.030 0.019 0.027 
[ 151/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 152/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.029 0.044 0.065 0.088 0.109 0.237 0.111 0.087 0.065 0.045 0.029 0.019 0.026 
[ 153/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 154/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.044 0.064 0.086 0.110 0.242 0.110 0.087 0.064 0.045 0.029 0.017 0.027 
[ 155/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.021 0.031 0.047 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.031 0.020 0.028 
[ 157/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 158/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 160/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.019 0.030 0.045 0.065 0.087 0.108 0.234 0.110 0.088 0.066 0.046 0.030 0.019 0.027 
[ 161/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 162/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.026 0.017 0.028 0.044 0.064 0.088 0.111 0.245 0.112 0.087 0.064 0.043 0.028 0.017 0.026 
[ 163/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 164/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.029 0.045 0.065 0.088 0.109 0.233 0.111 0.088 0.065 0.047 0.031 0.019 0.027 
[ 167/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 168/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.019 0.030 0.046 0.066 0.086 0.108 0.233 0.109 0.088 0.066 0.046 0.030 0.019 0.027 
[ 169/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 170/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.030 0.045 0.065 0.087 0.109 0.244 0.107 0.086 0.064 0.044 0.029 0.019 0.027 
[ 171/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.068 0.088 0.107 0.223 0.105 0.089 0.068 0.047 0.032 0.020 0.027 
[ 173/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 174/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 176/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[ 177/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 178/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.015 0.025 0.039 0.060 0.086 0.117 0.267 0.117 0.087 0.060 0.039 0.024 0.015 0.025 
[ 179/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 180/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.029 0.047 0.066 0.089 0.108 0.233 0.108 0.089 0.065 0.046 0.030 0.018 0.027 
[ 183/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 184/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.026 0.018 0.030 0.046 0.066 0.088 0.109 0.233 0.109 0.087 0.066 0.045 0.031 0.019 0.026 
[ 185/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 186/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.025 0.017 0.028 0.043 0.064 0.088 0.111 0.245 0.110 0.087 0.064 0.044 0.027 0.018 0.027 
[ 187/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     0.28 MiB ->     0.15 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.068 0.088 0.105 0.225 0.106 0.086 0.067 0.048 0.032 0.020 0.028 
[ 189/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 190/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 192/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.025 0.016 0.026 0.041 0.061 0.085 0.112 0.257 0.115 0.088 0.063 0.043 0.027 0.017 0.026 
[ 193/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 194/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, quantizing to q8_0 .. size =     1.12 MiB ->     0.60 MiB | hist: 0.000 0.021 0.009 0.014 0.023 0.038 0.064 0.117 0.415 0.120 0.067 0.040 0.025 0.016 0.009 0.022 
[ 195/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 196/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_internal: model size  =    63.46 MB
llama_model_quantize_internal: quant size  =    34.00 MB
llama_model_quantize_internal: hist: 0.000 0.026 0.018 0.029 0.045 0.064 0.087 0.109 0.241 0.109 0.087 0.064 0.045 0.029 0.018 0.026 

main: quantize time =   171.88 ms
main:    total time =   171.88 ms
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/embd_bge_small-tg-f16.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is'
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709980557
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 19 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:   74 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 63.46 MiB (16.03 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/13 layers to GPU
llm_load_tensors:        CPU buffer size =    63.46 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     2.76 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    16.00 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1
embedding 0: -0.043980 -0.019912 0.007672 -0.000847 0.001357 -0.037027 0.109449 0.042568 0.092071 -0.015919 0.006763 -0.035700 -0.017880 0.015042 0.018139 0.015873 -0.011285 0.010411 -0.085250 -0.008474 0.091356 -0.017079 -0.060333 -0.024499 0.027517 0.076074 0.027980 -0.014591 0.017626 -0.033282 -0.037869 -0.018992 0.068690 -0.009838 -0.025042 0.072337 -0.046593 0.011029 -0.050265 0.047713 0.032379 -0.011764 0.022071 0.049636 0.010484 0.005801 -0.028863 0.008952 -0.018501 -0.051440 -0.046068 0.030500 -0.035428 0.054216 -0.069663 0.044206 0.029797 0.046287 0.073412 -0.042574 0.076089 0.038884 -0.181177 0.082531 0.042260 -0.064546 -0.060114 -0.017828 0.006472 0.005864 0.017187 -0.026625 0.064595 0.112581 0.035168 -0.067422 0.027059 -0.067307 -0.033451 -0.033223 0.033243 0.013542 -0.003368 -0.037499 -0.052080 0.055146 -0.001978 -0.038265 0.064486 0.028840 -0.043353 -0.029268 -0.039464 0.036323 0.008383 -0.015491 -0.036587 0.018145 0.028600 0.342800 -0.044476 0.056111 0.017655 -0.020872 -0.066827 0.000160 -0.037881 -0.030081 -0.008533 -0.021565 0.000552 -0.003196 0.004042 0.018897 -0.008551 0.025819 0.049436 0.000078 0.050953 -0.042478 -0.031875 0.023616 0.030699 -0.023155 -0.046279 -0.079284 0.115171 0.046716 0.027822 -0.040720 0.067771 -0.022930 0.010317 -0.032928 -0.018292 0.043866 0.024255 0.052401 0.007463 0.008894 0.011232 -0.074659 -0.065567 -0.026750 -0.041192 -0.023865 0.026715 0.006917 0.027764 0.052897 -0.036688 0.057705 -0.000202 0.031743 -0.019776 -0.022092 0.041055 -0.058905 0.019599 0.043131 0.043616 0.041597 -0.022521 0.027044 -0.021815 0.005445 -0.041320 -0.001241 0.024429 0.002087 0.044353 -0.022751 0.043664 0.064753 0.055475 0.037085 -0.000899 0.046110 0.045818 -0.008478 0.063057 -0.073266 -0.011961 0.032133 0.023960 0.014734 -0.033697 0.001105 -0.015820 -0.019001 0.047878 0.110849 0.028416 0.031340 -0.013306 -0.057499 0.006644 0.005149 -0.012237 -0.051460 -0.000969 -0.017693 -0.019416 -0.040905 0.009214 -0.057975 0.050946 0.052343 -0.009575 -0.040239 -0.014082 -0.024889 -0.017240 0.006319 0.006573 -0.026899 0.015624 0.030750 0.002538 0.023197 -0.022215 -0.098542 -0.051112 -0.278000 -0.014962 -0.061544 -0.027231 0.017668 -0.010942 -0.017095 0.035077 0.046984 -0.015432 0.015203 -0.025466 0.047837 -0.005920 -0.000713 -0.060997 -0.068917 -0.060335 -0.035942 0.043342 -0.055060 0.015081 0.000529 -0.058183 -0.010451 0.012614 0.151479 0.127119 -0.013602 0.042012 -0.025663 0.014014 -0.001043 -0.150475 0.044860 0.005318 -0.036279 -0.029824 -0.020180 -0.034897 0.010248 0.033528 -0.048167 -0.051813 -0.017450 -0.023504 0.047350 0.052051 -0.016789 -0.055461 0.025830 -0.005702 0.010714 0.038694 0.008197 -0.009728 -0.105782 -0.027424 -0.096107 0.025068 -0.011257 0.092328 0.056092 0.003724 0.027822 0.002057 -0.051083 -0.039902 -0.013536 -0.044918 -0.015356 0.002925 -0.043535 -0.077952 0.065224 -0.006834 -0.001631 -0.014626 0.071550 0.023699 -0.037172 0.009176 0.001566 -0.032266 0.015456 0.037870 0.000353 -0.053218 0.021342 -0.039816 0.000043 0.013418 0.019822 -0.057899 0.006468 -0.049556 -0.267871 0.039179 -0.067976 0.038277 -0.012316 0.041499 -0.016146 0.052380 -0.071357 0.011356 0.024716 -0.007242 0.082073 0.028525 -0.021525 0.040495 -0.004553 -0.074583 -0.014756 0.020065 0.002288 0.023130 0.197208 -0.043240 -0.026012 -0.004964 -0.019272 0.074255 0.001725 -0.031951 -0.036587 -0.045069 0.000554 -0.011543 0.018139 -0.029457 -0.008457 0.006393 0.050811 -0.014942 0.006183 0.026101 -0.030813 0.048039 0.114068 -0.040829 -0.011454 0.005410 -0.003583 0.025136 -0.059151 0.013749 -0.010404 0.038711 0.051439 0.035423 0.035030 -0.017037 0.026372 -0.014510 -0.050039 0.003235 0.054098 0.039754 -0.039142 



llama_print_timings:        load time =      31.55 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =      10.00 ms /     9 tokens (    1.11 ms per token,   900.36 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =     108.31 ms /    10 tokens

real	0m0.777s
user	0m0.286s
sys	0m0.510s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/embd_bge_small-tg-q8_0.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is'
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709980558
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 7
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  123 tensors
llama_model_loader: - type  f16:    1 tensors
llama_model_loader: - type q8_0:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 34.00 MiB (8.59 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/13 layers to GPU
llm_load_tensors:        CPU buffer size =    34.00 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =     2.76 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    16.00 MiB
llama_new_context_with_model: graph splits (measure): 1

system_info: n_threads = 3 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
batch_decode: n_tokens = 9, n_seq = 1
embedding 0: -0.044454 -0.019806 0.008425 -0.001658 0.001779 -0.036759 0.109019 0.043373 0.091165 -0.015181 0.006789 -0.035570 -0.019173 0.013641 0.017719 0.015106 -0.013268 0.011068 -0.083926 -0.007082 0.093548 -0.017052 -0.061701 -0.024357 0.028294 0.076383 0.026934 -0.014491 0.018342 -0.034793 -0.037537 -0.017987 0.068542 -0.010658 -0.023986 0.071772 -0.046301 0.010584 -0.050453 0.050388 0.033141 -0.012715 0.021456 0.050307 0.010165 0.005712 -0.028839 0.008599 -0.018906 -0.052632 -0.045646 0.029770 -0.035216 0.051594 -0.068929 0.043866 0.029205 0.045984 0.073591 -0.042048 0.075155 0.038411 -0.182611 0.082708 0.044309 -0.065399 -0.059829 -0.016909 0.005649 0.005957 0.017086 -0.027434 0.064515 0.111774 0.035061 -0.068578 0.027102 -0.067448 -0.034405 -0.034707 0.032971 0.014231 -0.004415 -0.037086 -0.052460 0.053751 -0.003201 -0.036324 0.063400 0.028793 -0.041630 -0.029495 -0.039325 0.036156 0.007287 -0.015820 -0.035803 0.017638 0.030548 0.346732 -0.044102 0.056144 0.016571 -0.021412 -0.063607 -0.000904 -0.037150 -0.029707 -0.009419 -0.020827 0.000708 -0.002746 0.004237 0.018166 -0.010456 0.024902 0.049130 -0.001884 0.051348 -0.041483 -0.030454 0.023481 0.030748 -0.022423 -0.044380 -0.079705 0.114010 0.047061 0.026691 -0.041594 0.067796 -0.022886 0.011182 -0.033732 -0.017395 0.043946 0.022508 0.051437 0.007903 0.007249 0.009941 -0.076188 -0.064218 -0.027048 -0.040565 -0.023684 0.027036 0.006132 0.026607 0.051503 -0.037247 0.058885 0.001360 0.031979 -0.020218 -0.022019 0.041428 -0.059317 0.018868 0.043380 0.042847 0.040733 -0.021502 0.028539 -0.023809 0.006640 -0.041287 0.001707 0.024675 0.000569 0.043791 -0.024013 0.043722 0.064947 0.055666 0.038235 -0.000724 0.047338 0.045326 -0.007942 0.061108 -0.072899 -0.011986 0.033168 0.023641 0.014780 -0.033558 0.000947 -0.015551 -0.018370 0.048707 0.110872 0.028607 0.030930 -0.011407 -0.055840 0.006044 0.005040 -0.011321 -0.051681 -0.002646 -0.017572 -0.020664 -0.040170 0.009202 -0.057681 0.050109 0.051904 -0.009844 -0.041472 -0.015863 -0.025166 -0.015681 0.004928 0.007403 -0.027498 0.016514 0.029875 0.001132 0.022691 -0.021976 -0.097322 -0.050516 -0.277597 -0.013000 -0.060898 -0.026051 0.017103 -0.009631 -0.016511 0.035074 0.048198 -0.016130 0.014956 -0.024193 0.049728 -0.006102 0.000276 -0.059688 -0.069145 -0.060383 -0.035510 0.044582 -0.054949 0.015589 0.000059 -0.058474 -0.011353 0.011102 0.150165 0.126349 -0.012129 0.042156 -0.025802 0.015186 -0.000960 -0.150463 0.043391 0.005722 -0.036747 -0.029880 -0.019002 -0.033883 0.010825 0.035010 -0.048887 -0.052897 -0.016389 -0.024895 0.048130 0.050769 -0.017301 -0.056308 0.023259 -0.006064 0.011419 0.039715 0.007303 -0.008586 -0.105995 -0.027139 -0.097807 0.023942 -0.010873 0.092480 0.056145 0.004337 0.027369 0.000976 -0.051116 -0.039264 -0.012851 -0.045764 -0.014922 0.003409 -0.043754 -0.077136 0.065538 -0.006276 -0.001407 -0.015571 0.069890 0.024463 -0.035971 0.007809 0.002233 -0.033726 0.016580 0.037806 -0.000158 -0.051953 0.021956 -0.039035 0.000098 0.012115 0.021044 -0.057901 0.005945 -0.049439 -0.269367 0.038631 -0.066569 0.037664 -0.012043 0.043172 -0.016206 0.051795 -0.072338 0.012282 0.024592 -0.007345 0.082538 0.030200 -0.021015 0.042242 -0.003036 -0.074485 -0.015457 0.019792 0.001831 0.023724 0.194983 -0.043327 -0.025375 -0.005301 -0.018949 0.074287 0.000796 -0.031120 -0.037353 -0.044217 0.000275 -0.010344 0.019217 -0.027160 -0.009570 0.006057 0.050269 -0.013845 0.006808 0.026766 -0.031702 0.048504 0.111706 -0.040231 -0.011500 0.004156 -0.003576 0.024720 -0.060324 0.014739 -0.010465 0.038206 0.049283 0.035079 0.037173 -0.016362 0.025752 -0.015522 -0.050739 0.004487 0.054170 0.040495 -0.039931 



llama_print_timings:        load time =      25.19 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       9.11 ms /     9 tokens (    1.01 ms per token,   988.36 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =     106.85 ms /    10 tokens

real	0m0.752s
user	0m0.235s
sys	0m0.537s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_open_llama_7b_v2
+ tee /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:59 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/config.json [502/502] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/tokenizer.model
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:35:59 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/tokenizer_config.json [593/593] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:36:00 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/special_tokens_map.json [330/330] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:36:00 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/pytorch_model.bin.index.json [26788/26788] -> "pytorch_model.bin.index.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00001-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/resolve/main/pytorch_model-00002-of-00002.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/7B-v2/ https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
+ local out=models-mnt/open-llama/7B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/7B-v2/
+ cd models-mnt/open-llama/7B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:36:00 URL:https://huggingface.co/openlm-research/open_llama_7b_v2/raw/main/generation_config.json [132/132] -> "generation_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /home/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ path_models=../models-mnt/open-llama/7B-v2
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.0s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m3.238s
user	0m2.384s
sys	0m0.861s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-make.log
+ make -j
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  6%] Built target build_info
[  6%] Built target ggml
[  7%] Linking CUDA static library libggml_static.a
[  7%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  8%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target gguf
[  9%] Built target llama
[ 10%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 11%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 12%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 13%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 13%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 14%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 16%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 17%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 18%] Linking CXX executable ../bin/test-c
[ 18%] Linking CXX executable ../../bin/benchmark
[ 19%] Linking CXX executable ../../bin/quantize
[ 19%] Built target llava
[ 20%] Linking CXX executable ../../bin/quantize-stats
[ 21%] Linking CXX static library libcommon.a
[ 22%] Linking CXX static library libllava_static.a
[ 22%] Built target test-c
[ 22%] Built target llava_static
[ 22%] Built target common
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 23%] Built target benchmark
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o
[ 30%] Linking CXX executable ../bin/test-quantize-fns
[ 29%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o
[ 33%] Linking CXX executable ../bin/test-quantize-perf
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-tokenizer-0-llama
[ 37%] Linking CXX executable ../bin/test-sampling
[ 37%] Built target quantize
[ 38%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 39%] Linking CXX executable ../bin/test-chat-template
[ 39%] Built target quantize-stats
[ 40%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-llama
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Linking CXX executable ../bin/test-tokenizer-0-falcon
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 51%] Linking CXX executable ../bin/test-backend-ops
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 54%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 56%] Linking CXX executable ../bin/test-grad0
[ 56%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 59%] Linking CXX executable ../bin/test-rope
[ 60%] Linking CXX executable ../bin/test-grammar-parser
[ 61%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 62%] Linking CXX executable ../../bin/baby-llama
[ 63%] Linking CXX executable ../bin/test-autorelease
[ 63%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 63%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 64%] Linking CXX executable ../../bin/beam-search
[ 65%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 66%] Linking CXX executable ../../bin/batched
[ 67%] Linking CXX executable ../../bin/batched-bench
[ 67%] Linking CXX executable ../bin/test-llama-grammar
[ 68%] Linking CXX executable ../../bin/embedding
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 71%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 72%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 73%] Linking CXX executable ../../bin/finetune
[ 74%] Linking CXX executable ../../bin/infill
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 76%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llava-cli
[ 78%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 78%] Linking CXX executable ../../bin/tokenize
[ 79%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 80%] Linking CXX executable ../../bin/parallel
[ 81%] Linking CXX executable ../../bin/main
[ 82%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 83%] Linking CXX executable ../../bin/perplexity
[ 84%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 85%] Linking CXX executable ../../bin/simple
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 87%] Linking CXX executable ../../bin/save-load-state
[ 88%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 88%] Linking CXX executable ../../bin/speculative
[ 89%] Linking CXX executable ../../bin/passkey
[ 90%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 90%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 92%] Linking CXX executable ../../bin/lookup
[ 92%] Linking CXX executable ../../bin/lookahead
[ 92%] Built target test-quantize-fns
[ 93%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/train-text-from-scratch
[ 95%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/imatrix
[ 97%] Linking CXX executable ../../bin/export-lora
[ 98%] Linking CXX executable ../../bin/vdot
[ 99%] Linking CXX executable ../../bin/server
[100%] Linking CXX executable ../../bin/q8dot
[100%] Built target test-grammar-parser
[100%] Built target test-quantize-perf
[100%] Built target test-tokenizer-1-bpe
[100%] Built target test-rope
[100%] Built target test-sampling
[100%] Built target test-tokenizer-0-llama
[100%] Built target convert-llama2c-to-ggml
[100%] Built target test-backend-ops
[100%] Built target test-chat-template
[100%] Built target test-grad0
[100%] Built target test-model-load-cancel
[100%] Built target test-llama-grammar
[100%] Built target parallel
[100%] Built target test-tokenizer-0-falcon
[100%] Built target test-tokenizer-1-llama
[100%] Built target embedding
[100%] Built target save-load-state
[100%] Built target batched
[100%] Built target test-autorelease
[100%] Built target beam-search
[100%] Built target vdot
[100%] Built target baby-llama
[100%] Built target batched-bench
[100%] Built target infill
[100%] Built target finetune
[100%] Built target tokenize
[100%] Built target llava-cli
[100%] Built target passkey
[100%] Built target llama-bench
[100%] Built target main
[100%] Built target perplexity
[100%] Built target speculative
[100%] Built target export-lora
[100%] Built target lookup
[100%] Built target simple
[100%] Built target lookahead
[100%] Built target server
[100%] Built target imatrix
[100%] Built target q8dot
[100%] Built target train-text-from-scratch

real	0m2.074s
user	0m8.098s
sys	0m2.921s
+ python3 ../convert.py ../models-mnt/open-llama/7B-v2
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00001-of-00002.bin
Loading model file ../models-mnt/open-llama/7B-v2/pytorch_model-00002-of-00002.bin
params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/7B-v2'))
Found vocab files: {'spm': PosixPath('../models-mnt/open-llama/7B-v2/tokenizer.model'), 'bpe': None, 'hfft': None}
Loading vocab file PosixPath('../models-mnt/open-llama/7B-v2/tokenizer.model'), type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
Permuting layer 26
Permuting layer 27
Permuting layer 28
Permuting layer 29
Permuting layer 30
Permuting layer 31
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.0.attn_rot_embd
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.1.attn_rot_embd
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.2.attn_rot_embd
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.3.attn_rot_embd
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.4.attn_rot_embd
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.5.attn_rot_embd
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.6.attn_rot_embd
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.7.attn_rot_embd
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.8.attn_rot_embd
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]
skipping tensor blk.9.attn_rot_embd
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.10.attn_rot_embd
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.11.attn_rot_embd
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.12.attn_rot_embd
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.13.attn_rot_embd
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.14.attn_rot_embd
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.15.attn_rot_embd
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.16.attn_rot_embd
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.17.attn_rot_embd
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.18.attn_rot_embd
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.19.attn_rot_embd
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.20.attn_rot_embd
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.21.attn_rot_embd
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.22.attn_rot_embd
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.23.attn_rot_embd
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.24.attn_rot_embd
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.25.attn_rot_embd
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]
model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.26.attn_rot_embd
model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]
model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]
model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.27.attn_rot_embd
model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]
model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]
model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.28.attn_rot_embd
model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]
model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]
model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.29.attn_rot_embd
model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]
model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]
model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.30.attn_rot_embd
model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]
model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]
model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]
skipping tensor blk.31.attn_rot_embd
model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]
model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]
model.norm.weight                                -> output_norm.weight                       | F16    | [4096]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]
Writing ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf, format 1
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   0
[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0
[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   0
[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   0
[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   0
[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   0
[  7/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   0
[  8/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   0
[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0
[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0
[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   0
[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 16/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 17/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 25/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 26/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   1
[ 34/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   1
[ 35/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   1
[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 43/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 44/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 52/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 53/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 61/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   2
[ 62/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   2
[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   2
[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   2
[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   2
[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2
[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   2
[ 70/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 71/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[ 79/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 80/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   3
[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3
[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   3
[ 88/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   3
[ 89/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   3
[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+   3
[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+   3
[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3
[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   3
[ 97/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[ 98/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   4
[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[106/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[107/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   4
[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[115/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   4
[116/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   4
[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   4
[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   4
[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   4
[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   4
[124/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[125/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   5
[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5
[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[133/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[134/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   5
[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5
[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[142/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   5
[143/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   5
[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   5
[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   5
[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   5
[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   5
[151/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[152/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   6
[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6
[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[160/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[161/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   6
[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6
[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[169/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   6
[170/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   6
[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   6
[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   6
[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   6
[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   6
[178/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[179/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7
[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   7
[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[187/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[188/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7
[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   7
[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[196/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   7
[197/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   7
[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   7
[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   7
[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7
[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   7
[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   7
[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7
[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   7
[205/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[206/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   8
[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[214/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[215/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+   8
[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[223/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   8
[224/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   8
[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+   8
[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   8
[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   8
[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   8
[232/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9
[233/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9
[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+   9
[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9
[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9
[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9
[241/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9
[242/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9
[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+   9
[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9
[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9
[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9
[250/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9
[251/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+   9
[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+   9
[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+   9
[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+   9
[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+   9
[259/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+   9
[260/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10
[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  10
[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10
[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10
[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10
[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10
[268/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10
[269/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10
[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  10
[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10
[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10
[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10
[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10
[277/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10
[278/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  10
[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  10
[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  10
[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  10
[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10
[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  10
[286/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  10
[287/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  11
[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  11
[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  11
[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  11
[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+  11
Wrote ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_f16=../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf
+ wiki_test=../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf q8_0
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q8_0 .. size =   250.00 MiB ->   132.81 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.016 0.026 0.041 0.060 0.085 0.113 0.267 0.113 0.085 0.060 0.041 0.026 0.016 0.025 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.024 0.014 0.023 0.037 0.057 0.084 0.116 0.287 0.117 0.084 0.057 0.037 0.023 0.014 0.024 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.065 0.087 0.107 0.240 0.107 0.087 0.065 0.046 0.030 0.019 0.027 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.026 0.017 0.028 0.043 0.063 0.088 0.112 0.246 0.111 0.088 0.063 0.043 0.028 0.017 0.026 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.049 0.067 0.088 0.105 0.223 0.105 0.088 0.067 0.049 0.032 0.020 0.028 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.015 0.025 0.039 0.059 0.086 0.116 0.272 0.116 0.086 0.059 0.039 0.024 0.015 0.025 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.016 0.025 0.040 0.060 0.086 0.115 0.265 0.115 0.087 0.060 0.040 0.025 0.016 0.025 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.233 0.107 0.087 0.066 0.047 0.031 0.019 0.027 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.025 0.016 0.026 0.040 0.060 0.086 0.115 0.263 0.115 0.086 0.060 0.040 0.026 0.016 0.025 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.087 0.106 0.231 0.106 0.087 0.066 0.047 0.031 0.020 0.027 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.231 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.230 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.107 0.228 0.107 0.088 0.066 0.047 0.031 0.019 0.027 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.087 0.066 0.048 0.031 0.020 0.027 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.087 0.106 0.229 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.087 0.066 0.047 0.031 0.020 0.027 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.229 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.087 0.066 0.048 0.031 0.020 0.027 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.227 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.066 0.088 0.106 0.228 0.106 0.088 0.066 0.047 0.031 0.020 0.027 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.066 0.088 0.106 0.227 0.106 0.088 0.066 0.048 0.031 0.020 0.027 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    32.00 MiB ->    17.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.019 0.030 0.046 0.066 0.088 0.108 0.232 0.108 0.088 0.066 0.047 0.030 0.019 0.027 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q8_0 .. size =    86.00 MiB ->    45.69 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q8_0 .. size =   250.00 MiB ->   132.81 MiB | hist: 0.000 0.027 0.021 0.033 0.050 0.069 0.089 0.105 0.218 0.103 0.087 0.067 0.049 0.033 0.021 0.028 
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  6828.64 MB
llama_model_quantize_internal: hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 

main: quantize time = 48740.63 ms
main:    total time = 48740.63 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf q4_0
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_0 .. size =   250.00 MiB ->    70.31 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.020 0.032 0.049 0.071 0.098 0.125 0.142 0.125 0.098 0.072 0.049 0.032 0.020 0.017 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.011 0.018 0.030 0.046 0.069 0.098 0.132 0.153 0.132 0.098 0.069 0.046 0.029 0.018 0.015 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.096 0.115 0.126 0.115 0.096 0.075 0.054 0.037 0.024 0.020 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.022 0.035 0.052 0.075 0.099 0.120 0.128 0.120 0.099 0.074 0.052 0.035 0.022 0.018 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.019 0.031 0.047 0.071 0.099 0.128 0.143 0.129 0.100 0.071 0.048 0.031 0.019 0.016 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.100 0.126 0.139 0.126 0.099 0.072 0.049 0.032 0.020 0.017 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.013 0.020 0.032 0.049 0.072 0.099 0.126 0.138 0.126 0.099 0.072 0.049 0.032 0.020 0.017 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.113 0.096 0.076 0.056 0.038 0.025 0.021 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.097 0.076 0.056 0.039 0.025 0.021 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    32.00 MiB ->     9.00 MiB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_0 .. size =    86.00 MiB ->    24.19 MiB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3647.87 MB
llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 

main: quantize time = 26716.33 ms
main:    total time = 26716.33 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf q4_1
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_1 .. size =   250.00 MiB ->    78.12 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.036 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.083 0.068 0.051 0.036 0.024 0.040 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.037 0.053 0.068 0.083 0.094 0.101 0.101 0.094 0.083 0.068 0.053 0.037 0.024 0.040 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.036 0.050 0.066 0.083 0.097 0.105 0.105 0.097 0.083 0.066 0.050 0.036 0.024 0.040 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.036 0.051 0.067 0.083 0.096 0.103 0.104 0.096 0.083 0.067 0.050 0.036 0.024 0.040 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.095 0.082 0.067 0.052 0.037 0.025 0.040 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.039 0.023 0.035 0.050 0.067 0.083 0.096 0.104 0.104 0.097 0.084 0.067 0.051 0.036 0.023 0.039 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.024 0.036 0.051 0.067 0.083 0.096 0.103 0.104 0.096 0.083 0.067 0.051 0.036 0.024 0.040 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.036 0.050 0.066 0.083 0.096 0.104 0.104 0.096 0.083 0.066 0.050 0.036 0.025 0.040 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.023 0.035 0.050 0.067 0.084 0.097 0.104 0.104 0.097 0.084 0.067 0.050 0.035 0.023 0.040 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.038 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.038 0.026 0.040 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.083 0.096 0.103 0.103 0.095 0.083 0.067 0.051 0.036 0.025 0.040 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.026 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.038 0.026 0.040 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.068 0.052 0.037 0.025 0.040 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.026 0.040 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.026 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.038 0.026 0.040 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.026 0.040 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.052 0.037 0.025 0.040 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.094 0.082 0.067 0.051 0.037 0.025 0.040 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.101 0.095 0.082 0.067 0.052 0.037 0.025 0.040 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.094 0.101 0.101 0.095 0.082 0.067 0.052 0.037 0.025 0.040 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.103 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.101 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.052 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.101 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.082 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.083 0.067 0.051 0.037 0.025 0.040 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    32.00 MiB ->    10.00 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.036 0.051 0.067 0.083 0.096 0.103 0.103 0.096 0.083 0.067 0.050 0.036 0.025 0.040 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_1 .. size =    86.00 MiB ->    26.88 MiB | hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4041.68 MB
llama_model_quantize_internal: hist: 0.040 0.025 0.037 0.051 0.067 0.083 0.095 0.102 0.102 0.095 0.082 0.067 0.051 0.037 0.025 0.040 

main: quantize time = 28536.07 ms
main:    total time = 28536.07 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf q5_0
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q5_0 .. size =   250.00 MiB ->    85.94 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.038 0.014 0.023 0.036 0.054 0.078 0.105 0.131 0.140 0.119 0.091 0.065 0.044 0.029 0.018 0.013 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.037 0.013 0.021 0.033 0.051 0.075 0.106 0.140 0.151 0.124 0.090 0.062 0.041 0.026 0.016 0.012 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.017 0.027 0.041 0.059 0.080 0.101 0.119 0.125 0.111 0.091 0.069 0.050 0.033 0.021 0.015 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.015 0.025 0.039 0.057 0.081 0.105 0.123 0.128 0.115 0.093 0.069 0.047 0.031 0.020 0.014 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.044 0.062 0.082 0.100 0.112 0.115 0.107 0.092 0.072 0.052 0.036 0.023 0.016 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.036 0.023 0.016 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.038 0.013 0.022 0.034 0.053 0.078 0.107 0.134 0.142 0.122 0.092 0.065 0.043 0.027 0.017 0.012 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.038 0.014 0.022 0.036 0.054 0.078 0.107 0.131 0.138 0.120 0.092 0.066 0.044 0.028 0.018 0.013 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.017 0.027 0.042 0.060 0.081 0.101 0.117 0.121 0.110 0.092 0.070 0.050 0.034 0.022 0.016 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.038 0.014 0.023 0.036 0.054 0.078 0.106 0.131 0.137 0.120 0.092 0.066 0.044 0.029 0.018 0.013 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.116 0.119 0.109 0.091 0.071 0.051 0.034 0.022 0.016 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.017 0.027 0.042 0.061 0.081 0.101 0.116 0.119 0.110 0.092 0.071 0.051 0.034 0.022 0.016 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.039 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.119 0.109 0.092 0.071 0.051 0.034 0.022 0.016 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.034 0.022 0.016 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.108 0.091 0.071 0.051 0.035 0.022 0.016 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.119 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.091 0.071 0.051 0.035 0.022 0.016 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.115 0.119 0.109 0.091 0.071 0.052 0.035 0.022 0.016 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.023 0.016 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.081 0.101 0.115 0.119 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.081 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.115 0.118 0.108 0.092 0.071 0.051 0.035 0.022 0.016 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.100 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.100 0.114 0.118 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.029 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.100 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.108 0.092 0.071 0.052 0.035 0.022 0.016 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.062 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.017 0.028 0.042 0.061 0.082 0.101 0.115 0.118 0.109 0.092 0.071 0.051 0.035 0.022 0.016 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.109 0.092 0.072 0.052 0.035 0.022 0.016 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.017 0.028 0.043 0.061 0.082 0.101 0.114 0.118 0.109 0.092 0.071 0.052 0.035 0.022 0.016 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    32.00 MiB ->    11.00 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.113 0.116 0.108 0.092 0.072 0.052 0.035 0.023 0.016 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.039 0.017 0.027 0.042 0.060 0.081 0.102 0.117 0.120 0.111 0.092 0.071 0.050 0.034 0.022 0.016 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_0 .. size =    86.00 MiB ->    29.56 MiB | hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.116 0.108 0.092 0.072 0.052 0.035 0.022 0.016 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4435.49 MB
llama_model_quantize_internal: hist: 0.040 0.018 0.028 0.043 0.061 0.082 0.101 0.114 0.117 0.108 0.092 0.072 0.052 0.035 0.022 0.016 

main: quantize time = 31659.04 ms
main:    total time = 31659.04 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf q5_1
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q5_1 .. size =   250.00 MiB ->    93.75 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.044 
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.026 0.039 0.053 0.068 0.081 0.092 0.097 0.098 0.092 0.081 0.068 0.053 0.039 0.026 0.044 
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.037 0.050 0.066 0.081 0.094 0.102 0.102 0.094 0.081 0.066 0.050 0.037 0.025 0.045 
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.025 0.037 0.051 0.067 0.082 0.094 0.100 0.100 0.094 0.082 0.067 0.051 0.037 0.025 0.044 
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.025 0.037 0.051 0.067 0.082 0.094 0.100 0.101 0.094 0.082 0.067 0.051 0.037 0.025 0.044 
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.025 0.037 0.051 0.067 0.082 0.094 0.100 0.100 0.094 0.082 0.067 0.051 0.037 0.025 0.044 
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.093 0.100 0.101 0.093 0.081 0.066 0.051 0.037 0.026 0.045 
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.044 0.025 0.036 0.051 0.067 0.082 0.094 0.101 0.101 0.094 0.082 0.067 0.051 0.036 0.025 0.044 
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.039 0.052 0.067 0.081 0.092 0.097 0.098 0.091 0.081 0.067 0.052 0.039 0.027 0.045 
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.100 0.100 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.039 0.027 0.045 
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.039 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.051 0.038 0.026 0.045 
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.051 0.038 0.026 0.045 
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.066 0.052 0.038 0.026 0.045 
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.051 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.098 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.051 0.038 0.026 0.045 
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.066 0.081 0.093 0.099 0.099 0.093 0.081 0.067 0.052 0.038 0.026 0.045 
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    32.00 MiB ->    12.00 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.093 0.099 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.027 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.037 0.051 0.066 0.081 0.093 0.100 0.100 0.093 0.081 0.066 0.051 0.037 0.026 0.045 
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_1 .. size =    86.00 MiB ->    32.25 MiB | hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.027 0.045 
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4829.30 MB
llama_model_quantize_internal: hist: 0.045 0.026 0.038 0.052 0.067 0.081 0.092 0.098 0.099 0.092 0.081 0.067 0.052 0.038 0.026 0.045 

main: quantize time = 33693.85 ms
main:    total time = 33693.85 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf q2_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q2_K .. size =   250.00 MiB ->    41.02 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q2_K .. size =    86.00 MiB ->    14.11 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  2414.82 MB

main: quantize time = 72305.67 ms
main:    total time = 72305.67 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf q3_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q3_K .. size =   250.00 MiB ->    53.71 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q3_K .. size =    86.00 MiB ->    18.48 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3144.52 MB

main: quantize time = 71110.02 ms
main:    total time = 71110.02 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf q4_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_K .. size =   250.00 MiB ->    70.31 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  3891.24 MB

main: quantize time = 126344.83 ms
main:    total time = 126344.83 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf q5_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q5_K .. size =   250.00 MiB ->    85.94 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q5_K .. size =    86.00 MiB ->    29.56 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  4560.87 MB

main: quantize time = 109299.28 ms
main:    total time = 109299.28 ms
+ ./bin/quantize ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf q6_k
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 753024 bytes
[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   7/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   8/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  16/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  17/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  25/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  34/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  35/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  43/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  44/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  52/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  53/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  61/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  62/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  70/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  71/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  79/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  80/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  88/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  89/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  97/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 107/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 115/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 116/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 124/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 125/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 133/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 134/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 142/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 143/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 151/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 152/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 161/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 169/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 170/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 178/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 179/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 187/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 188/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 196/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 205/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 206/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 215/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 223/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 224/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 232/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 233/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 241/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 242/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 250/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 259/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 260/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 268/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 269/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 277/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 278/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 286/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 287/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB
[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB
llama_model_quantize_internal: model size  = 12853.02 MB
llama_model_quantize_internal: quant size  =  5272.34 MB

main: quantize time = 60162.39 ms
main:    total time = 60162.39 ms
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to be happy and that there are many different ways in which we can achieve this.
I think it’s important not just to focus on yourself but also others as well, helping each other out wherever possible without expecting anything back from people for what they have done or helped you with (unless requested). This way everyone benefits equally! What would life be like if nobody did anything nice? We would all live in a world where only one person feels good about themselves at any given time.
My main aim is to help others as much possible while also making sure that my own well-being does not suffer too badly – sometimes it’s hard not feeling selfish when trying so desperately for other people but still having enough energy left over after everything else has been dealt with (or maybe even before). If everyone had this attitude, then surely no one would ever feel alone or neglected again?
I think I am a good person because of the way that my family raised me. They taught me about respect and kindness towards others without expecting anything in return; not only that but they also made sure we were always doing something fun together such as going on trips away with friends/family members where there would be plenty more opportunities for interacting with strangers too!
I think it’s
llama_print_timings:        load time =    2655.84 ms
llama_print_timings:      sample time =     120.88 ms /   256 runs   (    0.47 ms per token,  2117.87 tokens per second)
llama_print_timings: prompt eval time =      25.30 ms /     8 tokens (    3.16 ms per token,   316.24 tokens per second)
llama_print_timings:        eval time =    4924.11 ms /   255 runs   (   19.31 ms per token,    51.79 tokens per second)
llama_print_timings:       total time =    5159.30 ms /   263 tokens
Log end

real	0m8.426s
user	0m6.909s
sys	0m1.517s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   132.81 MiB
llm_load_tensors:      CUDA0 buffer size =  6695.84 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to be happy and that there are many different ways in which we can achieve this.
I think it’s important not just to focus on yourself but also your friends, family or anyone who surround you as they all play an essential role at achieving a successful persona for ourselves both inside and out! I believe happiness is something everyone strives towards no matter what obstacles life throws in our way however there are many ways we can achieve it. For example the key to being happy may just lie within us but sometimes external factors such as people, places or things could help influence this change too because these all play a big part on how individuals feel about themselves and others around them which leads me onto my second point…
The world is filled with many different types of cultures; some are more open minded than the next so it’s important we accept everyone for who they are. This means not judging people by looks, religion or race but instead taking time to get to know each other and understand what makes them happy which will lead you closer together as a friend/family member etc..
Another way in which one can achieve happiness through their life is by keeping fit because when we do this it releases endorphins into our bodies giving us that positive feeling inside. The
llama_print_timings:        load time =    1845.66 ms
llama_print_timings:      sample time =     124.54 ms /   256 runs   (    0.49 ms per token,  2055.60 tokens per second)
llama_print_timings: prompt eval time =      21.56 ms /     8 tokens (    2.69 ms per token,   371.13 tokens per second)
llama_print_timings:        eval time =    3199.04 ms /   255 runs   (   12.55 ms per token,    79.71 tokens per second)
llama_print_timings:       total time =    3432.31 ms /   263 tokens
Log end

real	0m5.911s
user	0m4.478s
sys	0m1.431s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to make others happy and help them, but sometimes it's hard for me.
I am a good student; my grades are very high at school (10), so they don't need much effort from mine part- this should be enough! But what about making people around you happier? I have been helping in the orphanage here since last year, and sometimes it feels like nothing.
It all started one day when our class went to visit some old houses; we were supposed to write down on a piece of paper how many persons lived there. We could not find any records from 1907-2008 because they had been destroyed during the war in Serbia, but I noticed that their families did not have enough money for rent and food (many people still don't).
I am a good student; my grades are very high at school. So why do those kids need help? What can one person do to make them happy or even save someone from hunger in the world? My teacher said it is something I will have to find out myself and that was probably true, because noone else could teach me how- maybe only God knows!
I am an average girl with a good education; my parents are
llama_print_timings:        load time =    1036.45 ms
llama_print_timings:      sample time =     126.51 ms /   256 runs   (    0.49 ms per token,  2023.56 tokens per second)
llama_print_timings: prompt eval time =      19.98 ms /     8 tokens (    2.50 ms per token,   400.48 tokens per second)
llama_print_timings:        eval time =    2280.82 ms /   255 runs   (    8.94 ms per token,   111.80 tokens per second)
llama_print_timings:       total time =    2521.98 ms /   263 tokens
Log end

real	0m4.182s
user	0m3.175s
sys	0m1.004s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 3
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.95 GiB (5.03 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    78.12 MiB
llm_load_tensors:      CUDA0 buffer size =  3963.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to enjoy it. It can be hard, and sometimes you have to work very hard for what you want in this world… but if your not having fun while doing that than why bother?
I am currently studying a Masters Degree (in Social Work) at uni as well as working full time with children who are on the autistic spectrum. I love my life and wouldnt change it even though there is always something new to challenge me each day! Some of these challenges can be really tough but i have learnt that if you believe in yourself, work hard enough & never give up…. anything is possible !
My blog will not only include stories about being a Social Worker… I’m also going to post my adventures as an aspiring author. The journey on which this has begun for me began at the end of last year when i found myself stuck in uni, working full time and trying desperately hard to find some inspiration towards writing that novel!
I am currently a member of several online groups who are dedicated to helping writers develop their stories… I’ve learnt so much from them already. One group is called ‘The Fab Five’. We meet regularly on Skype every week with five other aspiring authors and discuss our work & how we can help each
llama_print_timings:        load time =    1095.09 ms
llama_print_timings:      sample time =     133.13 ms /   256 runs   (    0.52 ms per token,  1922.99 tokens per second)
llama_print_timings: prompt eval time =      18.99 ms /     8 tokens (    2.37 ms per token,   421.34 tokens per second)
llama_print_timings:        eval time =    2383.59 ms /   255 runs   (    9.35 ms per token,   106.98 tokens per second)
llama_print_timings:       total time =    2631.11 ms /   263 tokens
Log end

real	0m4.323s
user	0m3.334s
sys	0m0.990s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 8
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.33 GiB (5.52 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4349.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to find and express our passions.
When we are passionate about something, it becomes a part of us in some way (whether that be through expression or experience). It drives who we are as people. Our passion affects everything else – relationships with others, work/life balance…it’s all connected! We can live happier lives when doing what makes us happy instead of what society tells us is expected out there “in real life.”
My passions include music and writing. I love to sing and play the guitar (and piano)! Writing gives me a place for my thoughts; it helps organize them before they take over my mind. And then, when others read or hear about these things that matter so deeply in their lives…well, what could be better than sharing yourself with others?!
I hope you enjoy reading this blog as much as I write it! 🙂
Love and Light,
Michelle xo
Advertisements
Hi Michelle. It’s nice to meet you here on your blog site at WordPress where we can share our passions of writing about topics that interest us – especially the things in life which have been a part of who we are as human beings! Thanks for sharing yourself with me and others via this
llama_print_timings:        load time =    1101.38 ms
llama_print_timings:      sample time =     124.92 ms /   256 runs   (    0.49 ms per token,  2049.23 tokens per second)
llama_print_timings: prompt eval time =      20.52 ms /     8 tokens (    2.57 ms per token,   389.79 tokens per second)
llama_print_timings:        eval time =    2522.23 ms /   255 runs   (    9.89 ms per token,   101.10 tokens per second)
llama_print_timings:       total time =    2751.68 ms /   263 tokens
Log end

real	0m4.452s
user	0m3.454s
sys	0m0.995s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 9
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.72 GiB (6.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    93.75 MiB
llm_load_tensors:      CUDA0 buffer size =  4735.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to find and create a good balance between our work, family, friends.
I’m not sure why that doesn’t happen for most people (including myself) or if it does then they aren’t happy doing so; because their definition of happiness might be different from mine… but I do know this much: there is something more to life than just working and surviving. There has got to be a sense of peace, freedom and contentment in our daily lives as well….
I believe that we all have the ability within us not only to make it happen (and live!) …but also…to truly enjoy ourselves while doing so! The key is having enough faith or hope for what you want out there. We needn’t fear failure, but rather embrace challenges and find solutions along the way….
If we can do this then I believe that our lives will not only be more fulfilling than they are now…but also: it would give us a better perspective on life in general! What kind of world would it really become? Would there still exist any wars or famine if every person could truly find what makes them happy (and the right balance between work and play)? I believe this is possible.
I think we are all capable not only
llama_print_timings:        load time =    1204.57 ms
llama_print_timings:      sample time =     119.09 ms /   256 runs   (    0.47 ms per token,  2149.65 tokens per second)
llama_print_timings: prompt eval time =      20.43 ms /     8 tokens (    2.55 ms per token,   391.62 tokens per second)
llama_print_timings:        eval time =    2601.83 ms /   255 runs   (   10.20 ms per token,    98.01 tokens per second)
llama_print_timings:       total time =    2826.38 ms /   263 tokens
Log end

real	0m4.639s
user	0m3.555s
sys	0m1.082s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q2_K:  129 tensors
llama_model_loader: - type q3_K:   96 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 2.36 GiB (3.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    41.02 MiB
llm_load_tensors:      CUDA0 buffer size =  2373.81 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to do things that are useful and meaningful, not necessarily in a materialistic way.
I think it's all right for people who want stuff from me but ultimately they can have my soul when push comes to shove — because most everybody on this planet has souls too! But if I don't get paid by anybody else then that money goes back into the business, or we go out and buy some of our friends something special.
I think life is about not only doing things for yourself but also having a sense of purpose with other people; it feels better to me as well when you can look around at your employees who are like family members too... that's why I feel the meaning in my day-to-day existence, and what keeps me going.
I think money is more meaningful than material things — but not necessarily so much because there certainly have been times where we need to buy stuff for ourselves or our families; it would be crazy otherwise! But if you do want something from anyone else on this planet then I believe they can just take your soul at any time... and nobody's going without anything anymore either.
I think life is about not only doing things with others but also having a sense of purpose too — that feels better
llama_print_timings:        load time =     718.03 ms
llama_print_timings:      sample time =     130.00 ms /   256 runs   (    0.51 ms per token,  1969.16 tokens per second)
llama_print_timings: prompt eval time =      33.54 ms /     8 tokens (    4.19 ms per token,   238.49 tokens per second)
llama_print_timings:        eval time =    2627.17 ms /   255 runs   (   10.30 ms per token,    97.06 tokens per second)
llama_print_timings:       total time =    2883.71 ms /   263 tokens
Log end

real	0m4.235s
user	0m3.411s
sys	0m0.822s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 12
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q3_K:  129 tensors
llama_model_loader: - type q4_K:   92 tensors
llama_model_loader: - type q5_K:    4 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.07 GiB (3.91 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    53.71 MiB
llm_load_tensors:      CUDA0 buffer size =  3090.81 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to be useful, to try and make this world a better place for those around you.
I have been in love with art from very early on: as long as i can remember (i'm 36 now). Painting has always attracted me above all else because it makes sense to my soul; the feeling of creating something new is just so incredible that nothing compares! I never tire of painting, and will not rest until we are left with no more beautiful creatures in this world.
I am a self-taught artist who started out by simply copying masterpieces as an adolescent (painting has always been my favourite subject at school). Over the years i have studied countless artists from every period of history, and feel myself to be very fortunate that I was able to do so in such great times. The masters are truly a gift for us all!
I am now focused on creating original works which can only come about through an understanding of art's past (my greatest passion). This is something i have been working toward since childhood, and my aim has always been that the work created by me would be recognised as being entirely unique to myself. I feel this goal was achieved long ago - in fact it happened when I began creating works which only
llama_print_timings:        load time =     818.91 ms
llama_print_timings:      sample time =     118.57 ms /   256 runs   (    0.46 ms per token,  2159.08 tokens per second)
llama_print_timings: prompt eval time =      30.29 ms /     8 tokens (    3.79 ms per token,   264.10 tokens per second)
llama_print_timings:        eval time =    2909.62 ms /   255 runs   (   11.41 ms per token,    87.64 tokens per second)
llama_print_timings:       total time =    3146.16 ms /   263 tokens
Log end

real	0m4.575s
user	0m3.714s
sys	0m0.860s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3820.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to find something you’re passionate about and then do it until you can't anymore.
I am a 20-something young professional living in San Francisco with my boyfriend, Greg (and our two cats), who works in social media & tech startups by day and loves spending time outdoors playing sports or exploring new places at night! I also love to read nonfiction books about business, culture/politics, history, art & design - as long as it's not too serious.
I have a passion for traveling (which you can see from my page) and am looking forward to the day when Greg will be able to accompany me on one of these trips! In the meantime I enjoy making new friends who share similar interests in exploring cities/nations around the world, experiencing local cuisine & culture with an open-mind.
I have a few rules: 1.) Be prepared for your stay - it's important that you know how to use the appliances and take care of my space as if it was yours! And while I want you to feel comfortable in our home please don't get too comfortable because this is still MY apartment. ;-)
2.) Respect other people around me- whether they are your roommates,
llama_print_timings:        load time =    1038.80 ms
llama_print_timings:      sample time =     132.71 ms /   256 runs   (    0.52 ms per token,  1929.00 tokens per second)
llama_print_timings: prompt eval time =      26.14 ms /     8 tokens (    3.27 ms per token,   306.03 tokens per second)
llama_print_timings:        eval time =    2416.88 ms /   255 runs   (    9.48 ms per token,   105.51 tokens per second)
llama_print_timings:       total time =    2669.29 ms /   263 tokens
Log end

real	0m4.367s
user	0m3.365s
sys	0m1.001s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 17
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4474.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to find and be loved, by someone or something.
I am a writer because it’s my passion; writing about all types of things that interest me such as family, relationships (particularly marriage), love, humor & faith.. And anything else anyone wants to know! LOL!! I have been married for 25 years and we are very blessed with our two sons. We live in the beautiful mountains west of Atlanta which is one of my favorite places on earth; it has everything you can imagine from lakes, rivers, parks, hiking trails & an abundance of wildlife like deer, turkey and more! I am so happy to be here at eNotes where there are plenty of writers who have something interesting or funny they want share with the world.
I believe in love – that being said if any one wants a book review on their work please let me know because we all need encouragement from time to time and it’s nice when someone takes an interest in what you do! I am always happy for new friends who just happen across my profile so feel free to say hello or ask questions, etc.
I like the following things: coffee, reading, writing (obviously), travel & shopping!! LOL!!! Also watching movies/tv shows and getting caught
llama_print_timings:        load time =    1128.05 ms
llama_print_timings:      sample time =     119.96 ms /   256 runs   (    0.47 ms per token,  2134.06 tokens per second)
llama_print_timings: prompt eval time =      27.30 ms /     8 tokens (    3.41 ms per token,   293.01 tokens per second)
llama_print_timings:        eval time =    2581.20 ms /   255 runs   (   10.12 ms per token,    98.79 tokens per second)
llama_print_timings:       total time =    2812.90 ms /   263 tokens
Log end

real	0m4.568s
user	0m3.543s
sys	0m1.025s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
+ ./bin/main --model ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf -t 1 -ngl 999 -s 1234 -n 256 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 18
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q6_K:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   102.54 MiB
llm_load_tensors:      CUDA0 buffer size =  5169.81 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    70.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 1


 I believe the meaning of life is to be happy and if you are not doing what makes your heart sing then it’s time for a change.
If at first, you don’t succeed – try again! This doesn’t mean that we should do something just because everybody else does; on contrary: Do things with passion or why bother? I believe in the power of “why” and finding my own way to be happy is what makes me feel alive.
To help make a better world for our children – by giving back, sharing knowledge & experience (both personal as well as work related). To get more involved within The Circle Of Life Foundation it would also mean that I can take care of the things which matter most in my life: family and friends! And finally to stay focused on what’s important.
I have been lucky enough so far, because all along they way there has always be someone who believed in me or gave a push when needed – thank you for believing in ME & giving back through your support of The Circle Of Life Foundation. Thank YOU from the bottom of my heart!
Julia Lundstedt is part of our board and works as an interior designer at Pelle, located on Birger Jarlsgatan 24a in central Stockholm
llama_print_timings:        load time =    1263.90 ms
llama_print_timings:      sample time =     116.80 ms /   256 runs   (    0.46 ms per token,  2191.78 tokens per second)
llama_print_timings: prompt eval time =      22.65 ms /     8 tokens (    2.83 ms per token,   353.17 tokens per second)
llama_print_timings:        eval time =    2872.42 ms /   255 runs   (   11.26 ms per token,    88.78 tokens per second)
llama_print_timings:       total time =    3096.48 ms /   263 tokens
Log end

real	0m4.964s
user	0m3.934s
sys	0m1.023s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981311
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1196.46 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.73 seconds per pass - ETA 0.03 minutes
[1]5.5314,[2]6.8315,[3]7.4349,[4]6.8779,
llama_print_timings:        load time =    2812.58 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2457.16 ms /  8192 tokens (    0.30 ms per token,  3333.93 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    3979.04 ms /  8193 tokens

Final estimate: PPL = 6.8779 +/- 0.25577

real	0m7.401s
user	0m6.078s
sys	0m1.972s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981318
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   132.81 MiB
llm_load_tensors:      CUDA0 buffer size =  6695.84 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1134.97 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.84 seconds per pass - ETA 0.05 minutes
[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,
llama_print_timings:        load time =    1422.33 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2872.01 ms /  8192 tokens (    0.35 ms per token,  2852.36 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4345.36 ms /  8193 tokens

Final estimate: PPL = 6.8806 +/- 0.25603

real	0m6.392s
user	0m5.572s
sys	0m1.494s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981324
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3577.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1113.4 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.95 seconds per pass - ETA 0.05 minutes
[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,
llama_print_timings:        load time =     796.12 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3306.90 ms /  8192 tokens (    0.40 ms per token,  2477.25 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4745.53 ms /  8193 tokens

Final estimate: PPL = 7.0138 +/- 0.26116

real	0m6.175s
user	0m5.540s
sys	0m1.288s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981331
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 3
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.95 GiB (5.03 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    78.12 MiB
llm_load_tensors:      CUDA0 buffer size =  3963.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1053.55 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.93 seconds per pass - ETA 0.05 minutes
[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,
llama_print_timings:        load time =     884.36 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3296.72 ms /  8192 tokens (    0.40 ms per token,  2484.89 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4661.71 ms /  8193 tokens

Final estimate: PPL = 7.0158 +/- 0.26045

real	0m6.146s
user	0m5.482s
sys	0m1.311s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981337
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 8
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.33 GiB (5.52 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4349.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1203.17 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.84 seconds per pass - ETA 0.05 minutes
[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,
llama_print_timings:        load time =    1001.87 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2864.54 ms /  8192 tokens (    0.35 ms per token,  2859.80 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4396.69 ms /  8193 tokens

Final estimate: PPL = 6.9230 +/- 0.25774

real	0m6.002s
user	0m5.395s
sys	0m1.271s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981343
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 9
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_1:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.72 GiB (6.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    93.75 MiB
llm_load_tensors:      CUDA0 buffer size =  4735.56 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1051.09 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.83 seconds per pass - ETA 0.05 minutes
[1]5.5659,[2]6.8475,[3]7.4558,[4]6.9074,
llama_print_timings:        load time =     991.44 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2877.11 ms /  8192 tokens (    0.35 ms per token,  2847.30 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4282.31 ms /  8193 tokens

Final estimate: PPL = 6.9074 +/- 0.25709

real	0m5.878s
user	0m5.207s
sys	0m1.351s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981349
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q2_K:  129 tensors
llama_model_loader: - type q3_K:   96 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 2.36 GiB (3.01 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    41.02 MiB
llm_load_tensors:      CUDA0 buffer size =  2373.81 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1174.33 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.90 seconds per pass - ETA 0.05 minutes
[1]6.5021,[2]7.9561,[3]8.7000,[4]8.2106,
llama_print_timings:        load time =     544.54 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3172.90 ms /  8192 tokens (    0.39 ms per token,  2581.87 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4661.97 ms /  8193 tokens

Final estimate: PPL = 8.2106 +/- 0.30950

real	0m5.818s
user	0m5.249s
sys	0m1.215s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981354
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 12
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q3_K:  129 tensors
llama_model_loader: - type q4_K:   92 tensors
llama_model_loader: - type q5_K:    4 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.07 GiB (3.91 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    53.71 MiB
llm_load_tensors:      CUDA0 buffer size =  3090.81 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1028.88 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.93 seconds per pass - ETA 0.05 minutes
[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,
llama_print_timings:        load time =     690.67 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3289.24 ms /  8192 tokens (    0.40 ms per token,  2490.55 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4628.78 ms /  8193 tokens

Final estimate: PPL = 7.1090 +/- 0.26581

real	0m5.919s
user	0m5.309s
sys	0m1.269s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981360
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    70.31 MiB
llm_load_tensors:      CUDA0 buffer size =  3820.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1069.73 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.93 seconds per pass - ETA 0.05 minutes
[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,
llama_print_timings:        load time =     829.13 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3284.64 ms /  8192 tokens (    0.40 ms per token,  2494.04 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4688.14 ms /  8193 tokens

Final estimate: PPL = 6.9337 +/- 0.25755

real	0m6.128s
user	0m5.520s
sys	0m1.294s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981367
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 17
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q5_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =    85.94 MiB
llm_load_tensors:      CUDA0 buffer size =  4474.94 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1046.56 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.88 seconds per pass - ETA 0.05 minutes
[1]5.5200,[2]6.8175,[3]7.4358,[4]6.8916,
llama_print_timings:        load time =     940.12 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3085.06 ms /  8192 tokens (    0.38 ms per token,  2655.38 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4445.66 ms /  8193 tokens

Final estimate: PPL = 6.8916 +/- 0.25650

real	0m5.996s
user	0m5.302s
sys	0m1.349s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981372
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 18
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q6_K:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   102.54 MiB
llm_load_tensors:      CUDA0 buffer size =  5169.81 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1196.09 ms
perplexity: calculating perplexity over 4 chunks, batch_size=512
perplexity: 0.89 seconds per pass - ETA 0.05 minutes
[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,
llama_print_timings:        load time =    1119.43 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3137.25 ms /  8192 tokens (    0.38 ms per token,  2611.21 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4654.12 ms /  8193 tokens

Final estimate: PPL = 6.8939 +/- 0.25650

real	0m6.375s
user	0m5.635s
sys	0m1.401s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-imatrix.log
+ ./bin/imatrix --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test.raw -t 1 -ngl 999 -c 2048 -b 512 --chunks 4
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981379
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
compute_imatrix: tokenizing the input ..
compute_imatrix: tokenization took 1026.12 ms
compute_imatrix: computing over 4 chunks with batch_size 512
compute_imatrix: 3.06 seconds per pass - ETA 0.20 minutes
[1]5.5314,[2]6.8315,
save_imatrix: stored collected data after 10 chunks in imatrix.dat
[3]7.4349,[4]6.8779,
save_imatrix: stored collected data after 16 chunks in imatrix.dat

llama_print_timings:        load time =    4354.37 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   11469.17 ms /  8192 tokens (    1.40 ms per token,   714.26 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   15984.59 ms /  8193 tokens

Final estimate: PPL = 6.8779 +/- 0.25577

real	0m16.604s
user	0m15.048s
sys	0m2.213s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-save-load-state.log
+ ./bin/save-load-state --model ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/33 layers to GPU
llm_load_tensors:        CPU buffer size =  3647.87 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    70.50 MiB
llama_new_context_with_model: graph splits (measure): 1
main : serialized state into 2756273 out of a maximum of 334045228 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    10.01 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    70.50 MiB
llama_new_context_with_model: graph splits (measure): 1
main : deserialized state from 2756273 out of a maximum of 334045228 bytes

main : success

first run: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.


second run: The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.

real	0m11.363s
user	0m29.158s
sys	0m1.404s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]5.5314,[2]6.8315,[3]7.4349,[4]6.8779,'
+ qnt=f16
++ echo '[1]5.5314,[2]6.8315,[3]7.4349,[4]6.8779,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8779
++ echo '6.8779 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 6.8779
+ return 0
  - f16 @ 6.8779 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,'
+ qnt=q8_0
++ echo '[1]5.5319,[2]6.8326,[3]7.4388,[4]6.8806,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8806
++ echo '6.8806 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 6.8806
+ return 0
  - q8_0 @ 6.8806 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,'
+ qnt=q4_0
++ echo '[1]5.6359,[2]6.9332,[3]7.5732,[4]7.0138,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.0138
++ echo '7.0138 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 7.0138
+ return 0
  - q4_0 @ 7.0138 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,'
+ qnt=q4_1
++ echo '[1]5.6920,[2]6.9505,[3]7.5628,[4]7.0158,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.0158
++ echo '7.0158 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 7.0158
+ return 0
  - q4_1 @ 7.0158 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_0.log
+ check_ppl q5_0 '[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,'
+ qnt=q5_0
++ echo '[1]5.5344,[2]6.8544,[3]7.4728,[4]6.9230,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.9230
++ echo '6.9230 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 6.9230
+ return 0
  - q5_0 @ 6.9230 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]5.5659,[2]6.8475,[3]7.4558,[4]6.9074,'
+ qnt=q5_1
++ echo '[1]5.5659,[2]6.8475,[3]7.4558,[4]6.9074,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.9074
++ echo '6.9074 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 6.9074
+ return 0
  - q5_1 @ 6.9074 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q2_k.log
+ check_ppl q2_k '[1]6.5021,[2]7.9561,[3]8.7000,[4]8.2106,'
+ qnt=q2_k
++ echo '[1]6.5021,[2]7.9561,[3]8.7000,[4]8.2106,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=8.2106
++ echo '8.2106 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q2_k 8.2106
+ return 0
  - q2_k @ 8.2106 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,'
+ qnt=q3_k
++ echo '[1]5.6684,[2]7.0210,[3]7.6408,[4]7.1090,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=7.1090
++ echo '7.1090 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 7.1090
+ return 0
  - q3_k @ 7.1090 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,'
+ qnt=q4_k
++ echo '[1]5.5834,[2]6.8660,[3]7.4748,[4]6.9337,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.9337
++ echo '6.9337 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 6.9337
+ return 0
  - q4_k @ 6.9337 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]5.5200,[2]6.8175,[3]7.4358,[4]6.8916,'
+ qnt=q5_k
++ echo '[1]5.5200,[2]6.8175,[3]7.4358,[4]6.8916,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8916
++ echo '6.8916 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 6.8916
+ return 0
  - q5_k @ 6.8916 OK
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,'
+ qnt=q6_k
++ echo '[1]5.5585,[2]6.8409,[3]7.4450,[4]6.8939,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=6.8939
++ echo '6.8939 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 6.8939
+ return 0
  - q6_k @ 6.8939 OK
+ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-imatrix.log
+ grep Final
+ path_lora=../models-mnt/open-llama/7B-v2/lora
+ path_shakespeare=../models-mnt/shakespeare
+ shakespeare=../models-mnt/shakespeare/shakespeare.txt
+ lora_shakespeare=../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin
+ gg_wget ../models-mnt/open-llama/7B-v2/lora https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
+ local out=../models-mnt/open-llama/7B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/7B-v2/lora
+ cd ../models-mnt/open-llama/7B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:50:07 URL:https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_config.json [457/457] -> "adapter_config.json" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/open-llama/7B-v2/lora https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ local out=../models-mnt/open-llama/7B-v2/lora
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/open-llama/7B-v2/lora
+ cd ../models-mnt/open-llama/7B-v2/lora
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/adapter_model.bin
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ gg_wget ../models-mnt/shakespeare https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
+ local out=../models-mnt/shakespeare
+ local url=https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp/build-ci-release
+ mkdir -p ../models-mnt/shakespeare
+ cd ../models-mnt/shakespeare
+ wget -nv -N https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt
Last-modified header missing -- time-stamps turned off.
2024-03-09 10:50:08 URL:https://huggingface.co/slaren/open_llama_7b_v2_shakespeare_lora/resolve/main/shakespeare.txt [94275/94275] -> "shakespeare.txt" [1]
+ cd /home/ggml/work/llama.cpp/build-ci-release
+ python3 ../convert-lora-to-ggml.py ../models-mnt/open-llama/7B-v2/lora
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraB (4096, 64) float32 1.00MB
model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraA (4096, 64) float32 1.00MB
model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraB (4096, 64) float32 1.00MB
model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraA (4096, 64) float32 1.00MB
model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraB (4096, 64) float32 1.00MB
Converted ../models-mnt/open-llama/7B-v2/lora/adapter_config.json and ../models-mnt/open-llama/7B-v2/lora/adapter_model.bin to ../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt -t 1 -ngl 999 -c 2048 -b 512 --chunks 3
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981411
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 73.518 ms
perplexity: calculating perplexity over 3 chunks, batch_size=512
perplexity: 0.74 seconds per pass - ETA 0.03 minutes
[1]14.1610,[2]11.8140,[3]11.2243,
llama_print_timings:        load time =    2763.07 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    1858.25 ms /  6144 tokens (    0.30 ms per token,  3306.34 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    2209.52 ms /  6145 tokens

Final estimate: PPL = 11.2243 +/- 0.52147

real	0m5.617s
user	0m4.263s
sys	0m1.852s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-lora-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf -f ../models-mnt/shakespeare/shakespeare.txt --lora ../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin -t 1 -ngl 999 -c 2048 -b 512 --chunks 3
main: build = 2369 (950ba1ab)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1709981416
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models-mnt/open-llama/7B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:  CUDA_Host buffer size =   250.00 MiB
llm_load_tensors:      CUDA0 buffer size = 12603.02 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB
llama_new_context_with_model: graph splits (measure): 2
llama_apply_lora_from_file_internal: applying lora adapter from '../models-mnt/open-llama/7B-v2/lora/ggml-adapter-model.bin' - please wait ...
llama_apply_lora_from_file_internal: r = 64, alpha = 128, scaling = 2.00
................ done (6782.59 ms)

system_info: n_threads = 1 / 6 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 62.16 ms
perplexity: calculating perplexity over 3 chunks, batch_size=512
perplexity: 0.72 seconds per pass - ETA 0.03 minutes
[1]11.5881,[2]9.1780,[3]8.7622,
llama_print_timings:        load time =   12372.34 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    1838.91 ms /  6144 tokens (    0.30 ms per token,  3341.11 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    2154.89 ms /  6145 tokens

Final estimate: PPL = 8.7622 +/- 0.39497

real	0m15.196s
user	0m7.844s
sys	0m7.844s
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-lora-ppl.log
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-f16.log
++ grep '^\[1\]'
++ cat /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/open_llama_7b_v2-ppl-shakespeare-lora-f16.log
++ grep '^\[1\]'
+ compare_ppl 'f16 shakespeare' '[1]14.1610,[2]11.8140,[3]11.2243,' '[1]11.5881,[2]9.1780,[3]8.7622,'
+ qnt='f16 shakespeare'
++ echo '[1]14.1610,[2]11.8140,[3]11.2243,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl1=11.2243
++ echo '[1]11.5881,[2]9.1780,[3]8.7622,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl2=8.7622
++ echo '11.2243 < 8.7622'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s %s OK\n' 'f16 shakespeare' 11.2243 8.7622
+ return 0
  - f16 shakespeare @ 11.2243 8.7622 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /home/ggml/work/llama.cpp
+ local model
+ tee /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_with_model_debug.log
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ [[ -s /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-debug
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    8.80 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    2.01 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =  10.81 sec*proc (2 tests)

Total Test time (real) =  10.82 sec
0.55user 10.28system 0:10.85elapsed 99%CPU (0avgtext+0avgdata 13769864maxresident)k
0inputs+40outputs (0major+3607394minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ tee /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_with_model_release.log
+ cd /home/ggml/work/llama.cpp
+ local model
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ [[ -s /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/0ba1ab84db199f0bbdecdb2bb911f35261b321/ggml-4-x86-cuda-v100/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-release
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    8.78 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    2.04 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =  10.82 sec*proc (2 tests)

Total Test time (real) =  10.83 sec
0.31user 10.53system 0:10.86elapsed 99%CPU (0avgtext+0avgdata 13771444maxresident)k
0inputs+40outputs (0major+3607567minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
