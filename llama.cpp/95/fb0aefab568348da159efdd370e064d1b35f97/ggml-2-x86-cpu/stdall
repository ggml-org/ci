Requirement already satisfied: numpy~=1.24.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.40.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.40.2)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.9.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.14.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.23.0)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.4)
Requirement already satisfied: tokenizers<0.20,>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.19.1)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.5.10)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (24.0)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.3)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)
Requirement already satisfied: triton==2.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.4)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.3.1)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.3)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: typing-extensions in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.11.0)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.4.127)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.7)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.40.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.1)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.9.0) (4.66.4)
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.9.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.9.0-py3-none-any.whl size=3289 sha256=701cc7c488d402c0e365bf43e84e57e5e97e1457a1f9d18440c8a81f13890b31
  Stored in directory: /tmp/pip-ephem-wheel-cache-bp5f3p0k/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.9.0
    Uninstalling gguf-0.9.0:
      Successfully uninstalled gguf-0.9.0
Successfully installed gguf-0.9.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.3s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.476s
user	0m0.370s
sys	0m0.110s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_debug-make.log
+ make -j
[  1%] Generating build details from Git
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building CXX object CMakeFiles/ggml.dir/sgemm.cpp.o
-- Found Git: /usr/bin/git (found version "2.34.1")
[  4%] Built target ggml
[  5%] Linking CXX static library libggml_static.a
[  6%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  8%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  9%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  8%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  9%] Linking CXX executable ../../bin/gguf
[  9%] Built target ggml_static
[  9%] Built target build_info
[  9%] Linking CXX static library libllama.a
[  9%] Built target gguf
[  9%] Built target llama
[  9%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[  9%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 10%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 12%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 13%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 15%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 17%] Linking CXX executable ../bin/test-c
[ 17%] Linking CXX executable ../../bin/benchmark
[ 17%] Linking CXX executable ../../bin/quantize-stats
[ 17%] Built target llava
[ 18%] Linking CXX static library libcommon.a
[ 19%] Linking CXX static library libllava_static.a
[ 19%] Built target test-c
[ 19%] Built target llava_static
[ 19%] Built target benchmark
[ 19%] Built target common
[ 20%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 34%] Linking CXX executable ../bin/test-tokenizer-0
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 35%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 35%] Linking CXX executable ../bin/test-sampling
[ 36%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 36%] Linking CXX executable ../bin/test-grammar-integration
[ 36%] Linking CXX executable ../bin/test-grammar-parser
[ 36%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-quantize-perf
[ 40%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 40%] Linking CXX executable ../bin/test-quantize-fns
[ 41%] Linking CXX executable ../bin/test-chat-template
[ 42%] Linking CXX executable ../bin/test-grad0
[ 42%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 47%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 48%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 49%] Linking CXX executable ../bin/test-autorelease
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 50%] Linking CXX executable ../bin/test-backend-ops
[ 51%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 51%] Linking CXX executable ../bin/test-rope
[ 51%] Linking CXX executable ../bin/test-model-load-cancel
[ 52%] Linking CXX executable ../../bin/baby-llama
[ 52%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 53%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 53%] Linking CXX executable ../../bin/batched
[ 53%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 54%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 55%] Linking CXX executable ../../bin/batched-bench
[ 55%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 55%] Built target test-grammar-parser
[ 55%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 56%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 57%] Linking CXX executable ../../bin/embedding
[ 58%] Linking CXX executable ../../bin/beam-search
[ 58%] Built target test-grad0
[ 59%] Linking CXX executable ../../bin/eval-callback
[ 59%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 60%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 61%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 62%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 63%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 64%] Linking CXX executable ../../bin/finetune
[ 65%] Linking CXX executable ../../bin/gritlm
[ 65%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 65%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llava-cli
[ 66%] Linking CXX executable ../../bin/infill
[ 67%] Linking CXX executable ../../bin/gguf-split
[ 68%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 70%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 69%] Linking CXX executable ../../bin/main
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Built target test-quantize-perf
[ 72%] Linking CXX executable ../../bin/tokenize
[ 73%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 73%] Built target test-rope
[ 73%] Linking CXX executable ../../bin/parallel
[ 73%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 74%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 74%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 75%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 75%] Linking CXX executable ../../bin/perplexity
[ 76%] Linking CXX executable ../../bin/quantize
[ 77%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 78%] Linking CXX executable ../../bin/retrieval
[ 79%] Linking CXX executable ../../bin/save-load-state
[ 79%] Linking CXX executable ../../bin/simple
[ 80%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 81%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 82%] Linking CXX executable ../../bin/passkey
[ 83%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 83%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 83%] Built target convert-llama2c-to-ggml
[ 83%] Built target test-backend-ops
[ 84%] Linking CXX executable ../../bin/speculative
[ 85%] Linking CXX executable ../../bin/lookup
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 87%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 87%] Linking CXX executable ../../bin/lookahead
[ 88%] Linking CXX executable ../../bin/lookup-create
[ 88%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 89%] Generating json-schema-to-grammar.mjs.hpp
[ 90%] Linking CXX executable ../../bin/lookup-stats
[ 91%] Linking CXX executable ../../bin/train-text-from-scratch
[ 92%] Generating index.html.hpp
[ 93%] Generating completion.js.hpp
[ 93%] Linking CXX executable ../../bin/lookup-merge
[ 93%] Generating index.js.hpp
[ 94%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 97%] Built target quantize-stats
[ 97%] Linking CXX executable ../../bin/imatrix
[ 98%] Linking CXX executable ../../bin/vdot
[ 99%] Linking CXX executable ../../bin/q8dot
[ 99%] Linking CXX executable ../../bin/export-lora
[ 99%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 99%] Built target lookup-merge
[ 99%] Built target q8dot
[ 99%] Built target export-lora
[ 99%] Built target vdot
[ 99%] Built target test-json-schema-to-grammar
[ 99%] Built target test-sampling
[ 99%] Built target test-chat-template
[ 99%] Built target test-autorelease
[ 99%] Built target test-grammar-integration
[ 99%] Built target test-llama-grammar
[ 99%] Built target test-model-load-cancel
[100%] Linking CXX executable ../../bin/server
[100%] Built target gguf-split
[100%] Built target batched-bench
[100%] Built target test-tokenizer-1-spm
[100%] Built target test-tokenizer-0
[100%] Built target baby-llama
[100%] Built target test-tokenizer-1-bpe
[100%] Built target passkey
[100%] Built target batched
[100%] Built target eval-callback
[100%] Built target gritlm
[100%] Built target tokenize
[100%] Built target beam-search
[100%] Built target retrieval
[100%] Built target embedding
[100%] Built target speculative
[100%] Built target lookup-stats
[100%] Built target infill
[100%] Built target quantize
[100%] Built target perplexity
[100%] Built target llama-bench
[100%] Built target train-text-from-scratch
[100%] Built target lookup-create
[100%] Built target finetune
[100%] Built target save-load-state
[100%] Built target lookahead
[100%] Built target imatrix
[100%] Built target main
[100%] Built target simple
[100%] Built target lookup
[100%] Built target parallel
[100%] Built target llava-cli
[100%] Built target server

real	0m6.584s
user	0m40.959s
sys	0m5.619s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /home/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-llama-spm
 1/22 Test  #1: test-tokenizer-0-llama-spm .......   Passed    0.14 sec
      Start  2: test-tokenizer-0-llama-bpe
 2/22 Test  #2: test-tokenizer-0-llama-bpe .......   Passed    1.97 sec
      Start  3: test-tokenizer-0-phi-3
 3/22 Test  #3: test-tokenizer-0-phi-3 ...........   Passed    0.14 sec
      Start  4: test-tokenizer-0-falcon
 4/22 Test  #4: test-tokenizer-0-falcon ..........   Passed    0.59 sec
      Start  5: test-tokenizer-0-bert-bge
 5/22 Test  #5: test-tokenizer-0-bert-bge ........   Passed    0.17 sec
      Start  6: test-tokenizer-0-starcoder
 6/22 Test  #6: test-tokenizer-0-starcoder .......   Passed    0.44 sec
      Start  7: test-tokenizer-0-gpt-2
 7/22 Test  #7: test-tokenizer-0-gpt-2 ...........   Passed    0.45 sec
      Start  8: test-tokenizer-0-refact
 8/22 Test  #8: test-tokenizer-0-refact ..........   Passed    0.44 sec
      Start  9: test-tokenizer-0-command-r
 9/22 Test  #9: test-tokenizer-0-command-r .......   Passed    2.78 sec
      Start 10: test-tokenizer-0-qwen2
10/22 Test #10: test-tokenizer-0-qwen2 ...........   Passed    1.55 sec
      Start 11: test-tokenizer-1-llama-spm
11/22 Test #11: test-tokenizer-1-llama-spm .......   Passed    3.35 sec
      Start 12: test-quantize-fns
12/22 Test #12: test-quantize-fns ................   Passed   31.29 sec
      Start 13: test-quantize-perf
13/22 Test #13: test-quantize-perf ...............   Passed    9.06 sec
      Start 14: test-sampling
14/22 Test #14: test-sampling ....................   Passed    0.03 sec
      Start 15: test-chat-template
15/22 Test #15: test-chat-template ...............   Passed    0.00 sec
      Start 16: test-grammar-parser
16/22 Test #16: test-grammar-parser ..............   Passed    0.00 sec
      Start 17: test-llama-grammar
17/22 Test #17: test-llama-grammar ...............   Passed    0.00 sec
      Start 18: test-grammar-integration
18/22 Test #18: test-grammar-integration .........   Passed    0.01 sec
      Start 19: test-grad0
19/22 Test #19: test-grad0 .......................   Passed    2.49 sec
      Start 20: test-backend-ops
20/22 Test #20: test-backend-ops .................   Passed    0.00 sec
      Start 21: test-rope
21/22 Test #21: test-rope ........................   Passed    0.06 sec
      Start 24: test-json-schema-to-grammar
22/22 Test #24: test-json-schema-to-grammar ......   Passed    1.16 sec

100% tests passed, 0 tests failed out of 22

Label Time Summary:
main    =  56.14 sec*proc (22 tests)

Total Test time (real) =  56.15 sec

real	0m56.213s
user	1m13.105s
sys	0m1.275s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.3s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.477s
user	0m0.341s
sys	0m0.139s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_release-make.log
+ make -j
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building CXX object CMakeFiles/ggml.dir/sgemm.cpp.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Built target ggml
[  5%] Linking CXX static library libggml_static.a
[  6%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  8%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[  9%] Linking CXX executable ../../bin/gguf
[  9%] Built target build_info
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[  9%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 10%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 10%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 12%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 15%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 16%] Linking CXX executable ../../bin/benchmark
[ 17%] Linking CXX executable ../bin/test-c
[ 17%] Built target llava
[ 17%] Linking CXX executable ../../bin/quantize-stats
[ 17%] Built target gguf
[ 18%] Linking CXX static library libllava_static.a
[ 19%] Linking CXX static library libcommon.a
[ 19%] Built target llava_static
[ 19%] Built target test-c
[ 19%] Built target common
[ 19%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 27%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 28%] Linking CXX executable ../bin/test-tokenizer-0
[ 28%] Linking CXX executable ../bin/test-quantize-fns
[ 28%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 29%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 29%] Linking CXX executable ../bin/test-quantize-perf
[ 30%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 31%] Linking CXX executable ../bin/test-sampling
[ 32%] Built target benchmark
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Linking CXX executable ../bin/test-grammar-parser
[ 35%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-chat-template
[ 40%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 40%] Linking CXX executable ../bin/test-llama-grammar
[ 41%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 46%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 46%] Built target quantize-stats
[ 47%] Linking CXX executable ../bin/test-backend-ops
[ 48%] Linking CXX executable ../../bin/baby-llama
[ 48%] Linking CXX executable ../bin/test-rope
[ 49%] Linking CXX executable ../bin/test-autorelease
[ 50%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 50%] Linking CXX executable ../bin/test-model-load-cancel
[ 50%] Built target test-quantize-perf
[ 50%] Built target test-quantize-fns
[ 51%] Linking CXX executable ../bin/test-grad0
[ 51%] Built target test-grammar-parser
[ 51%] Linking CXX executable ../../bin/batched
[ 51%] Built target test-tokenizer-0
[ 52%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 52%] Built target test-sampling
[ 53%] Linking CXX executable ../../bin/batched-bench
[ 54%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 54%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 55%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 55%] Built target test-llama-grammar
[ 55%] Built target test-tokenizer-1-spm
[ 55%] Built target test-rope
[ 55%] Built target test-json-schema-to-grammar
[ 56%] Linking CXX executable ../../bin/beam-search
[ 56%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 56%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 57%] Linking CXX executable ../../bin/embedding
[ 57%] Built target test-tokenizer-1-bpe
[ 57%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 57%] Built target test-grad0
[ 58%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 59%] Linking CXX executable ../../bin/eval-callback
[ 59%] Built target test-grammar-integration
[ 60%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 60%] Built target test-backend-ops
[ 61%] Linking CXX executable ../../bin/gritlm
[ 62%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 63%] Linking CXX executable ../../bin/infill
[ 64%] Linking CXX executable ../../bin/gguf-split
[ 64%] Built target test-chat-template
[ 64%] Built target batched
[ 65%] Linking CXX executable ../../bin/finetune
[ 66%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 66%] Built target test-model-load-cancel
[ 66%] Built target test-autorelease
[ 66%] Built target batched-bench
[ 67%] Linking CXX executable ../../bin/llama-bench
[ 67%] Built target convert-llama2c-to-ggml
[ 67%] Built target baby-llama
[ 68%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 68%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 68%] Built target beam-search
[ 69%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 70%] Linking CXX executable ../../bin/main
[ 71%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 71%] Built target embedding
[ 72%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 72%] Linking CXX executable ../../bin/perplexity
[ 72%] Linking CXX executable ../../bin/llava-cli
[ 72%] Built target gguf-split
[ 72%] Linking CXX executable ../../bin/parallel
[ 73%] Linking CXX executable ../../bin/tokenize
[ 73%] Built target eval-callback
[ 73%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 73%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 73%] Built target gritlm
[ 74%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 74%] Built target infill
[ 75%] Linking CXX executable ../../bin/retrieval
[ 76%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 77%] Linking CXX executable ../../bin/save-load-state
[ 78%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/quantize
[ 79%] Linking CXX executable ../../bin/simple
[ 80%] Linking CXX executable ../../bin/passkey
[ 80%] Built target finetune
[ 80%] Built target llama-bench
[ 81%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 81%] Built target main
[ 81%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 82%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 83%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 83%] Built target llava-cli
[ 83%] Built target perplexity
[ 84%] Linking CXX executable ../../bin/speculative
[ 85%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 86%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 86%] Linking CXX executable ../../bin/lookahead
[ 87%] Linking CXX executable ../../bin/lookup-create
[ 87%] Built target tokenize
[ 87%] Built target save-load-state
[ 88%] Generating json-schema-to-grammar.mjs.hpp
[ 88%] Built target parallel
[ 88%] Linking CXX executable ../../bin/lookup-merge
[ 89%] Linking CXX executable ../../bin/lookup-stats
[ 89%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 90%] Linking CXX executable ../../bin/lookup
[ 90%] Built target retrieval
[ 91%] Generating completion.js.hpp
[ 92%] Generating index.html.hpp
[ 93%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 94%] Linking CXX executable ../../bin/train-text-from-scratch
[ 94%] Generating index.js.hpp
[ 95%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 95%] Built target quantize
[ 96%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 96%] Linking CXX executable ../../bin/imatrix
[ 97%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/vdot
[ 98%] Built target lookup-merge
[ 98%] Built target passkey
[ 98%] Linking CXX executable ../../bin/export-lora
[ 99%] Linking CXX executable ../../bin/q8dot
[ 99%] Built target speculative
[ 99%] Built target simple
[ 99%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 99%] Built target lookahead
[ 99%] Built target lookup-create
[100%] Linking CXX executable ../../bin/server
[100%] Built target lookup-stats
[100%] Built target lookup
[100%] Built target export-lora
[100%] Built target train-text-from-scratch
[100%] Built target vdot
[100%] Built target imatrix
[100%] Built target q8dot
[100%] Built target server

real	0m1.262s
user	0m6.934s
sys	0m1.810s
+ '[' -z ']'
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /home/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-llama-spm
 1/22 Test  #1: test-tokenizer-0-llama-spm .......   Passed    0.04 sec
      Start  2: test-tokenizer-0-llama-bpe
 2/22 Test  #2: test-tokenizer-0-llama-bpe .......   Passed    0.40 sec
      Start  3: test-tokenizer-0-phi-3
 3/22 Test  #3: test-tokenizer-0-phi-3 ...........   Passed    0.04 sec
      Start  4: test-tokenizer-0-falcon
 4/22 Test  #4: test-tokenizer-0-falcon ..........   Passed    0.11 sec
      Start  5: test-tokenizer-0-bert-bge
 5/22 Test  #5: test-tokenizer-0-bert-bge ........   Passed    0.03 sec
      Start  6: test-tokenizer-0-starcoder
 6/22 Test  #6: test-tokenizer-0-starcoder .......   Passed    0.08 sec
      Start  7: test-tokenizer-0-gpt-2
 7/22 Test  #7: test-tokenizer-0-gpt-2 ...........   Passed    0.08 sec
      Start  8: test-tokenizer-0-refact
 8/22 Test  #8: test-tokenizer-0-refact ..........   Passed    0.08 sec
      Start  9: test-tokenizer-0-command-r
 9/22 Test  #9: test-tokenizer-0-command-r .......   Passed    0.64 sec
      Start 10: test-tokenizer-0-qwen2
10/22 Test #10: test-tokenizer-0-qwen2 ...........   Passed    0.30 sec
      Start 11: test-tokenizer-1-llama-spm
11/22 Test #11: test-tokenizer-1-llama-spm .......   Passed    0.45 sec
      Start 12: test-quantize-fns
12/22 Test #12: test-quantize-fns ................   Passed   16.01 sec
      Start 13: test-quantize-perf
13/22 Test #13: test-quantize-perf ...............   Passed    4.47 sec
      Start 14: test-sampling
14/22 Test #14: test-sampling ....................   Passed    0.01 sec
      Start 15: test-chat-template
15/22 Test #15: test-chat-template ...............   Passed    0.00 sec
      Start 16: test-grammar-parser
16/22 Test #16: test-grammar-parser ..............   Passed    0.00 sec
      Start 17: test-llama-grammar
17/22 Test #17: test-llama-grammar ...............   Passed    0.00 sec
      Start 18: test-grammar-integration
18/22 Test #18: test-grammar-integration .........   Passed    0.00 sec
      Start 19: test-grad0
19/22 Test #19: test-grad0 .......................   Passed    2.34 sec
      Start 20: test-backend-ops
20/22 Test #20: test-backend-ops .................   Passed    0.00 sec
      Start 21: test-rope
21/22 Test #21: test-rope ........................   Passed    0.05 sec
      Start 24: test-json-schema-to-grammar
22/22 Test #24: test-json-schema-to-grammar ......   Passed    1.51 sec

100% tests passed, 0 tests failed out of 22

Label Time Summary:
main    =  26.64 sec*proc (22 tests)

Total Test time (real) =  26.65 sec

real	0m26.718s
user	0m26.834s
sys	0m1.425s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/embd_bge_small.log
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:11 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:11 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json [711396/711396] -> "tokenizer.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:11 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:11 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:12 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:12 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:12 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:12 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:39:12 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.3s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.477s
user	0m0.366s
sys	0m0.114s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/embd_bge_small-make.log
+ make -j
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building CXX object CMakeFiles/ggml.dir/sgemm.cpp.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  5%] Built target build_info
[  5%] Built target ggml
[  5%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  7%] Linking CXX static library libggml_static.a
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[  9%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  9%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[  9%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[  9%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 10%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 11%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 11%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 13%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 14%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 17%] Linking CXX executable ../bin/test-c
[ 17%] Linking CXX executable ../../bin/quantize-stats
[ 17%] Linking CXX executable ../../bin/benchmark
[ 17%] Built target llava
[ 17%] Built target gguf
[ 18%] Linking CXX static library libcommon.a
[ 19%] Linking CXX static library libllava_static.a
[ 19%] Built target test-c
[ 19%] Built target llava_static
[ 19%] Built target common
[ 19%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 20%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 28%] Linking CXX executable ../bin/test-tokenizer-0
[ 29%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 30%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 30%] Linking CXX executable ../bin/test-quantize-fns
[ 31%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 31%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 32%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 34%] Linking CXX executable ../bin/test-sampling
[ 34%] Built target benchmark
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 35%] Linking CXX executable ../bin/test-quantize-perf
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-grammar-parser
[ 37%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 38%] Linking CXX executable ../bin/test-chat-template
[ 39%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 42%] Linking CXX executable ../bin/test-llama-grammar
[ 42%] Linking CXX executable ../bin/test-grammar-integration
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 46%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grad0
[ 48%] Linking CXX executable ../bin/test-backend-ops
[ 48%] Linking CXX executable ../bin/test-rope
[ 48%] Built target quantize-stats
[ 49%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 50%] Linking CXX executable ../bin/test-autorelease
[ 50%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 50%] Linking CXX executable ../bin/test-model-load-cancel
[ 51%] Linking CXX executable ../../bin/baby-llama
[ 52%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 52%] Linking CXX executable ../../bin/batched
[ 52%] Built target test-grammar-parser
[ 52%] Built target test-quantize-perf
[ 53%] Linking CXX executable ../../bin/batched-bench
[ 53%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 53%] Built target test-quantize-fns
[ 54%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 54%] Built target test-backend-ops
[ 55%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 55%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 56%] Linking CXX executable ../../bin/beam-search
[ 56%] Built target test-sampling
[ 57%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 58%] Linking CXX executable ../../bin/embedding
[ 58%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 58%] Built target test-tokenizer-0
[ 58%] Built target test-grad0
[ 58%] Built target test-chat-template
[ 59%] Linking CXX executable ../../bin/finetune
[ 60%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 60%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 60%] Built target test-tokenizer-1-bpe
[ 60%] Built target test-autorelease
[ 61%] Linking CXX executable ../../bin/gguf-split
[ 61%] Built target test-grammar-integration
[ 61%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../../bin/eval-callback
[ 62%] Built target test-tokenizer-1-spm
[ 63%] Linking CXX executable ../../bin/gritlm
[ 64%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 64%] Built target test-rope
[ 65%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 65%] Built target convert-llama2c-to-ggml
[ 66%] Linking CXX executable ../../bin/llama-bench
[ 67%] Linking CXX executable ../../bin/infill
[ 67%] Built target test-json-schema-to-grammar
[ 67%] Built target test-llama-grammar
[ 67%] Built target baby-llama
[ 68%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 68%] Linking CXX executable ../../bin/llava-cli
[ 68%] Built target gguf-split
[ 68%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 68%] Built target batched
[ 69%] Linking CXX executable ../../bin/main
[ 70%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 70%] Built target beam-search
[ 70%] Built target eval-callback
[ 70%] Built target finetune
[ 71%] Linking CXX executable ../../bin/tokenize
[ 72%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 73%] Generating json-schema-to-grammar.mjs.hpp
[ 74%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 75%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 76%] Generating completion.js.hpp
[ 76%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 76%] Linking CXX executable ../../bin/parallel
[ 77%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 78%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 79%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 79%] Generating index.js.hpp
[ 80%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 81%] Generating index.html.hpp
[ 81%] Built target batched-bench
[ 81%] Linking CXX executable ../../bin/perplexity
[ 82%] Linking CXX executable ../../bin/speculative
[ 83%] Linking CXX executable ../../bin/vdot
[ 83%] Built target embedding
[ 84%] Linking CXX executable ../../bin/q8dot
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 85%] Linking CXX executable ../../bin/quantize
[ 85%] Built target infill
[ 86%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 87%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 87%] Linking CXX executable ../../bin/imatrix
[ 88%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 89%] Linking CXX executable ../../bin/retrieval
[ 90%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 90%] Built target gritlm
[ 91%] Linking CXX executable ../../bin/save-load-state
[ 92%] Linking CXX executable ../../bin/passkey
[ 92%] Linking CXX executable ../../bin/simple
[ 93%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 93%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 93%] Linking CXX executable ../../bin/lookahead
[ 94%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 94%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 96%] Linking CXX executable ../../bin/lookup
[ 96%] Built target main
[ 97%] Linking CXX executable ../../bin/lookup-create
[ 97%] Built target llama-bench
[ 98%] Linking CXX executable ../../bin/lookup-stats
[ 98%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 98%] Linking CXX executable ../../bin/lookup-merge
[ 99%] Linking CXX executable ../../bin/train-text-from-scratch
[ 99%] Built target vdot
[ 99%] Linking CXX executable ../../bin/export-lora
[ 99%] Built target tokenize
[100%] Linking CXX executable ../../bin/server
[100%] Built target llava-cli
[100%] Built target speculative
[100%] Built target q8dot
[100%] Built target parallel
[100%] Built target passkey
[100%] Built target export-lora
[100%] Built target lookahead
[100%] Built target lookup-merge
[100%] Built target lookup
[100%] Built target perplexity
[100%] Built target quantize
[100%] Built target save-load-state
[100%] Built target imatrix
[100%] Built target simple
[100%] Built target retrieval
[100%] Built target lookup-create
[100%] Built target train-text-from-scratch
[100%] Built target lookup-stats
[100%] Built target server

real	0m1.240s
user	0m7.231s
sys	0m1.675s
+ python3 ../convert-hf-to-gguf.py ../models-mnt/bge-small
INFO:hf-to-gguf:Loading model: bge-small
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 512
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Setting special token type unk to 100
INFO:gguf.vocab:Setting special token type sep to 102
INFO:gguf.vocab:Setting special token type pad to 0
INFO:gguf.vocab:Setting special token type cls to 101
INFO:gguf.vocab:Setting special token type mask to 103
INFO:hf-to-gguf:Exporting model to '../models-mnt/bge-small/ggml-model-f16.gguf'
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
/mnt/llama.cpp/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
INFO:hf-to-gguf:token_embd.weight,               torch.float32 --> F16, shape = {384, 30522}
INFO:hf-to-gguf:position_embd.weight,            torch.float32 --> F32, shape = {384, 512}
INFO:hf-to-gguf:token_types.weight,              torch.float32 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.4.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.4.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.4.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.5.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.5.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.5.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.6.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.6.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.6.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.7.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.7.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.7.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.8.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.8.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.8.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.9.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.9.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.9.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.10.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.10.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.10.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.11.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.11.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.11.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
Writing:   0%|          | 0.00/66.9M [00:00<?, ?byte/s]Writing:  72%|  | 48.0M/66.9M [00:00<00:00, 480Mbyte/s]Writing: 100%|| 66.9M/66.9M [00:00<00:00, 479Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to '../models-mnt/bge-small/ggml-model-f16.gguf'
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 22 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = jina-v2-en
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  17:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  20:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  124 tensors
llama_model_loader: - type  f16:   73 tensors
[   1/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   2/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f32, size =    0.750 MB
[   3/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   6/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   7/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   8/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   9/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  10/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  11/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  13/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  14/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  16/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  17/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  18/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  19/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  20/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  23/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  24/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  25/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  26/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  27/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  29/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  30/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  32/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  33/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  34/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  35/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  36/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  39/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  40/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  41/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  42/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  43/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  45/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  46/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  48/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  49/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  50/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  51/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  52/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  55/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  56/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  57/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  58/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  59/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  61/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  62/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  64/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  65/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  66/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  67/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  68/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  71/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  72/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  73/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  74/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  75/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  77/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  78/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  80/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  81/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  82/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  83/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  84/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  87/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  88/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  89/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  90/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  91/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  93/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  94/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  96/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  97/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  98/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  99/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 100/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 103/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 104/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 105/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 106/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 107/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 109/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 110/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 112/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 113/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 114/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 115/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 116/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 119/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 120/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 121/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 122/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 123/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 125/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 126/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 128/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 129/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 130/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 131/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 132/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 135/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 136/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 137/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 138/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 139/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 141/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 142/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 144/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 145/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 146/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 147/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 148/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 151/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 152/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 153/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 154/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 155/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 157/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 158/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 160/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 161/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 162/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 163/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 164/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 167/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 168/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 169/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 170/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 171/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 173/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 174/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 176/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 177/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 178/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 179/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 180/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 183/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 184/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 185/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 186/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 187/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 189/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 190/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 192/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 193/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 194/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 195/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 196/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_internal: model size  =    63.84 MB
llama_model_quantize_internal: quant size  =    34.38 MB

main: quantize time =   122.62 ms
main:    total time =   122.62 ms
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/embd_bge_small-tg-f16.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is'
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367155
llama_model_loader: loaded meta data with 22 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = jina-v2-en
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  17:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  20:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  124 tensors
llama_model_loader: - type  f16:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =    63.84 MiB
...............................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 2048
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB
llama_new_context_with_model: graph nodes  = 429
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
batch_decode: n_tokens = 9, n_seq = 1

llama_print_timings:        load time =      12.22 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       3.74 ms /     9 tokens (    0.42 ms per token,  2407.70 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =       4.82 ms /    10 tokens

embedding 0: -0.043973 -0.019884  0.007663 -0.000832  0.001384 -0.037043  0.109429  0.042574  0.092058 -0.015914  0.006789 -0.035678 -0.017887  0.015052  0.018120  0.015864 -0.011303  0.010416 -0.085217 -0.008459  0.091368 -0.017066 -0.060342 -0.024485  0.027517  0.076064  0.027980 -0.014553  0.017654 -0.033284 -0.037870 -0.019001  0.068665 -0.009841 -0.025040  0.072343 -0.046558  0.011024 -0.050261  0.047714  0.032397 -0.011754  0.022052  0.049640  0.010463  0.005795 -0.028866  0.008928 -0.018515 -0.051481 -0.046042  0.030482 -0.035420  0.054201 -0.069654  0.044251  0.029795  0.046303  0.073405 -0.042586  0.076105  0.038853 -0.181174  0.082502  0.042270 -0.064541 -0.060107 -0.017848  0.006475  0.005892  0.017173 -0.026633  0.064562  0.112599  0.035151 -0.067415  0.027091 -0.067278 -0.033467 -0.033234  0.033242  0.013524 -0.003332 -0.037479 -0.052062  0.055152 -0.001983 -0.038292  0.064450  0.028822 -0.043335 -0.029236 -0.039466  0.036323  0.008385 -0.015454 -0.036588  0.018146  0.028599  0.342831 -0.044468  0.056102  0.017633 -0.020863 -0.066805  0.000153 -0.037913 -0.030070 -0.008535 -0.021581  0.000536 -0.003216  0.004011  0.018916 -0.008553  0.025823  0.049441  0.000086  0.050940 -0.042480 -0.031904  0.023602  0.030694 -0.023159 -0.046273 -0.079267  0.115187  0.046755  0.027834 -0.040731  0.067789 -0.022965  0.010318 -0.032958 -0.018309  0.043840  0.024264  0.052404  0.007477  0.008893  0.011243 -0.074647 -0.065566 -0.026746 -0.041198 -0.023884  0.026735  0.006897  0.027740  0.052873 -0.036658  0.057698 -0.000190  0.031754 -0.019768 -0.022072  0.041040 -0.058903  0.019613  0.043147  0.043596  0.041578 -0.022522  0.027046 -0.021826  0.005442 -0.041315 -0.001239  0.024448  0.002091  0.044333 -0.022737  0.043669  0.064759  0.055422  0.037072 -0.000922  0.046112  0.045813 -0.008493  0.063045 -0.073248 -0.011937  0.032114  0.023951  0.014719 -0.033687  0.001091 -0.015830 -0.019010  0.047874  0.110820  0.028442  0.031366 -0.013284 -0.057521  0.006649  0.005148 -0.012254 -0.051442 -0.000977 -0.017648 -0.019446 -0.040925  0.009197 -0.057945  0.050966  0.052339 -0.009609 -0.040256 -0.014084 -0.024882 -0.017266  0.006298  0.006585 -0.026933  0.015609  0.030761  0.002575  0.023214 -0.022196 -0.098554 -0.051096 -0.278019 -0.014999 -0.061565 -0.027230  0.017666 -0.010951 -0.017081  0.035055  0.046988 -0.015427  0.015238 -0.025470  0.047851 -0.005957 -0.000740 -0.061026 -0.068936 -0.060387 -0.035954  0.043319 -0.055047  0.015081  0.000537 -0.058199 -0.010453  0.012636  0.151510  0.127100 -0.013604  0.042001 -0.025672  0.014032 -0.001047 -0.150463  0.044849  0.005319 -0.036276 -0.029802 -0.020196 -0.034877  0.010222  0.033545 -0.048174 -0.051790 -0.017458 -0.023487  0.047366  0.052076 -0.016779 -0.055451  0.025834 -0.005708  0.010712  0.038702  0.008203 -0.009765 -0.105785 -0.027435 -0.096103  0.025061 -0.011241  0.092367  0.056101  0.003778  0.027793  0.002078 -0.051088 -0.039881 -0.013534 -0.044976 -0.015329  0.002925 -0.043513 -0.077945  0.065223 -0.006822 -0.001606 -0.014654  0.071549  0.023720 -0.037171  0.009171  0.001546 -0.032265  0.015457  0.037872  0.000355 -0.053208  0.021320 -0.039827  0.000033  0.013409  0.019808 -0.057879  0.006475 -0.049533 -0.267845  0.039154 -0.067969  0.038242 -0.012330  0.041493 -0.016116  0.052391 -0.071351  0.011368  0.024714 -0.007231  0.082101  0.028538 -0.021506  0.040491 -0.004553 -0.074596 -0.014756  0.020032  0.002299  0.023152  0.197216 -0.043224 -0.025986 -0.004957 -0.019280  0.074267  0.001719 -0.031988 -0.036599 -0.045077  0.000546 -0.011566  0.018121 -0.029470 -0.008458  0.006416  0.050807 -0.014958  0.006174  0.026088 -0.030801  0.048051  0.114089 -0.040818 -0.011478  0.005393 -0.003589  0.025162 -0.059140  0.013761 -0.010407  0.038699  0.051454  0.035408  0.035042 -0.017040  0.026373 -0.014498 -0.050023  0.003218  0.054131  0.039731 -0.039132 

real	0m0.051s
user	0m0.046s
sys	0m0.021s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/embd_bge_small-tg-q8_0.log
+ ./bin/embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is'
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367155
llama_model_loader: loaded meta data with 22 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = bge-small
llama_model_loader: - kv   2:                           bert.block_count u32              = 12
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 7
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  13:                         tokenizer.ggml.pre str              = jina-v2-en
llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  17:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  20:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  124 tensors
llama_model_loader: - type q8_0:   73 tensors
llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 384
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_embd_head_k    = 32
llm_load_print_meta: n_embd_head_v    = 32
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 384
llm_load_print_meta: n_embd_v_gqa     = 384
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 1536
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 33M
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 33.21 M
llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
llm_load_print_meta: general.name     = bge-small
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_tensors: ggml ctx size =    0.09 MiB
llm_load_tensors:        CPU buffer size =    34.38 MiB
.................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 2048
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:        CPU compute buffer size =    16.01 MiB
llama_new_context_with_model: graph nodes  = 429
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
batch_decode: n_tokens = 9, n_seq = 1

llama_print_timings:        load time =      10.68 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =       3.54 ms /     9 tokens (    0.39 ms per token,  2543.81 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =       4.73 ms /    10 tokens

embedding 0: -0.044922 -0.019413  0.009515 -0.002102  0.002819 -0.037346  0.108884  0.042320  0.092050 -0.015802  0.006203 -0.037483 -0.019315  0.014838  0.017312  0.014293 -0.014406  0.012199 -0.084337 -0.007969  0.092391 -0.017228 -0.062058 -0.024418  0.027417  0.077134  0.028219 -0.014623  0.017895 -0.035334 -0.038133 -0.018044  0.068934 -0.010808 -0.024000  0.071910 -0.045750  0.011734 -0.050866  0.050023  0.032255 -0.012384  0.022736  0.051037  0.009457  0.005324 -0.028037  0.007784 -0.019140 -0.054168 -0.047119  0.029088 -0.036035  0.053016 -0.067690  0.044014  0.029616  0.047229  0.073648 -0.043168  0.075918  0.037900 -0.183180  0.081508  0.042762 -0.066165 -0.059699 -0.017308  0.007110  0.005047  0.016962 -0.027730  0.064833  0.111964  0.034996 -0.068446  0.026829 -0.066310 -0.034038 -0.035547  0.032486  0.014914 -0.004339 -0.037040 -0.051606  0.053873 -0.002577 -0.037109  0.062467  0.029008 -0.041969 -0.029640 -0.039698  0.037257  0.007866 -0.014732 -0.036933  0.018711  0.029345  0.345662 -0.043828  0.055518  0.015965 -0.021585 -0.062967  0.000030 -0.037829 -0.031117 -0.008600 -0.019632  0.000917 -0.003949  0.004752  0.017646 -0.010475  0.024866  0.048859 -0.001073  0.050841 -0.042711 -0.029936  0.023056  0.031158 -0.023216 -0.044477 -0.079865  0.114434  0.046968  0.027606 -0.040895  0.067793 -0.022368  0.009769 -0.034681 -0.016154  0.044193  0.022540  0.051639  0.007797  0.007391  0.010083 -0.074316 -0.064372 -0.025101 -0.041340 -0.024550  0.027224  0.005440  0.026617  0.051940 -0.036700  0.058742  0.001461  0.032451 -0.020921 -0.021366  0.040976 -0.059863  0.019722  0.043085  0.042850  0.040582 -0.022107  0.029685 -0.022324  0.007497 -0.040081  0.000397  0.023739  0.002122  0.044315 -0.022987  0.043469  0.064731  0.056117  0.038352  0.000367  0.048953  0.045241 -0.009339  0.060821 -0.073344 -0.011166  0.032674  0.022647  0.014999 -0.033428  0.000347 -0.015448 -0.018743  0.048620  0.110470  0.029597  0.030962 -0.011328 -0.056829  0.006497  0.004694 -0.012985 -0.051779 -0.003150 -0.017702 -0.019727 -0.040388  0.009924 -0.059078  0.050095  0.052471 -0.010575 -0.039471 -0.015563 -0.023734 -0.016019  0.005706  0.007087 -0.027253  0.016754  0.030656  0.001635  0.023381 -0.021934 -0.097271 -0.050370 -0.277306 -0.014223 -0.061428 -0.027445  0.017050 -0.009608 -0.017319  0.033824  0.048471 -0.016368  0.016177 -0.022949  0.049435 -0.005335  0.000750 -0.060800 -0.068838 -0.059804 -0.036057  0.042588 -0.054961  0.014508 -0.000423 -0.059416 -0.009816  0.010768  0.150426  0.126644 -0.011118  0.042668 -0.025554  0.015019 -0.000210 -0.150502  0.042745  0.005784 -0.036606 -0.029014 -0.019236 -0.034275  0.009913  0.034990 -0.049935 -0.053754 -0.017105 -0.024232  0.048749  0.050670 -0.016684 -0.056609  0.023764 -0.005639  0.011582  0.038581  0.006578 -0.008041 -0.106921 -0.027615 -0.097465  0.025000 -0.011210  0.092479  0.055639  0.005293  0.026939  0.001806 -0.051768 -0.038913 -0.013483 -0.046246 -0.014757  0.001830 -0.044614 -0.077737  0.066446 -0.006428 -0.000018 -0.014871  0.071112  0.024387 -0.036434  0.008250  0.001929 -0.033247  0.017147  0.038480  0.001671 -0.051957  0.021081 -0.038685  0.000496  0.012745  0.021241 -0.057647  0.004903 -0.049491 -0.267161  0.038997 -0.067915  0.037074 -0.011075  0.043293 -0.015534  0.050443 -0.070772  0.012405  0.024955 -0.006946  0.082643  0.029026 -0.022356  0.042367 -0.003229 -0.074061 -0.015830  0.020035  0.002637  0.023772  0.196120 -0.044049 -0.024944 -0.004666 -0.017908  0.073222  0.001813 -0.031931 -0.035815 -0.044377 -0.000573 -0.011445  0.018698 -0.027002 -0.010099  0.005813  0.048950 -0.014805  0.006814  0.027005 -0.030880  0.048526  0.111968 -0.039582 -0.012955  0.004347 -0.002677  0.025424 -0.060709  0.014993 -0.008847  0.037792  0.050273  0.035482  0.036962 -0.017363  0.025554 -0.016189 -0.050841  0.003358  0.054623  0.039363 -0.039516 

real	0m0.048s
user	0m0.051s
sys	0m0.013s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_test_scripts_debug
+ tee /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/test_scripts_debug.log
+ cd /home/ggml/work/llama.cpp
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/test_scripts_debug-scripts.log
+ cd ./examples/gguf-split
+ bash tests.sh /home/ggml/work/llama.cpp/build-ci-debug/bin /mnt/llama.cpp/models
+ SPLIT=/home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split
+ MAIN=/home/ggml/work/llama.cpp/build-ci-debug/bin/main
+ WORK_PATH=/mnt/llama.cpp/models/gguf-split
+++ dirname tests.sh
++ realpath ./../../
+ ROOT_DIR=/home/ggml/work/llama.cpp
+ mkdir -p /mnt/llama.cpp/models/gguf-split
+ rm -f '/mnt/llama.cpp/models/gguf-split/ggml-model-split*.gguf' '/mnt/llama.cpp/models/gguf-split/ggml-model-merge*.gguf'
+ cd /mnt/llama.cpp/models/gguf-split
+ /home/ggml/work/llama.cpp/scripts/hf.sh --repo ggml-org/gemma-1.1-2b-it-Q8_0-GGUF --file gemma-1.1-2b-it.Q8_0.gguf
[+] attempting to download gemma-1.1-2b-it.Q8_0.gguf
[+] wget -q --show-progress -c -O ./gemma-1.1-2b-it.Q8_0.gguf https://huggingface.co/ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/resolve/main/gemma-1.1-2b-it.Q8_0.gguf
./gemma-1.1-2b-it.Q8_0.gguf
+ echo PASS
PASS
+ /home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split --split-max-tensors 28 /mnt/llama.cpp/models/gguf-split/gemma-1.1-2b-it.Q8_0.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split
n_split: 6
split 00001: n_tensors = 28, total_size = 871M
split 00002: n_tensors = 28, total_size = 334M
split 00003: n_tensors = 28, total_size = 402M
split 00004: n_tensors = 28, total_size = 335M
split 00005: n_tensors = 28, total_size = 338M
split 00006: n_tensors = 24, total_size = 261M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ... gguf_split: 6 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367158
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =   865.98 MiB
llm_load_tensors:        CPU buffer size =   334.74 MiB
llm_load_tensors:        CPU buffer size =   402.73 MiB
llm_load_tensors:        CPU buffer size =   335.27 MiB
llm_load_tensors:        CPU buffer size =   338.98 MiB
llm_load_tensors:        CPU buffer size =   261.96 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CPU buffer from size 0.00 MiB to 504.00 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Soire is a French company that specializes in tableware and decorative objects. Founded in 1765, the company has a rich history of creating elegant and timeless
llama_print_timings:        load time =    1235.64 ms
llama_print_timings:      sample time =      47.54 ms /    32 runs   (    1.49 ms per token,   673.19 tokens per second)
llama_print_timings: prompt eval time =    1145.01 ms /     2 tokens (  572.50 ms per token,     1.75 tokens per second)
llama_print_timings:        eval time =   20870.29 ms /    31 runs   (  673.24 ms per token,     1.49 tokens per second)
llama_print_timings:       total time =   22195.98 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split --merge /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf
gguf_merge: /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf -> /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ...[3Ddone
gguf_merge: /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf merged from 6 split with 164 tensors.
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367184
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 0
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =  2539.66 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CPU buffer from size 0.00 MiB to 504.00 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Heirs, a British crime drama series, has garnered widespread acclaim for its gripping plot, exceptional performances, and intricate character development.

**Critical Acclaim:**


llama_print_timings:        load time =    1235.02 ms
llama_print_timings:      sample time =      47.74 ms /    32 runs   (    1.49 ms per token,   670.31 tokens per second)
llama_print_timings: prompt eval time =    1146.21 ms /     2 tokens (  573.11 ms per token,     1.74 tokens per second)
llama_print_timings:        eval time =   20851.41 ms /    31 runs   (  672.63 ms per token,     1.49 tokens per second)
llama_print_timings:       total time =   22177.70 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split --split-max-tensors 32 --no-tensor-first-split /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors
n_split: 7
split 00001: n_tensors = 0, total_size = 5M
split 00002: n_tensors = 32, total_size = 967M
split 00003: n_tensors = 32, total_size = 344M
split 00004: n_tensors = 32, total_size = 411M
split 00005: n_tensors = 32, total_size = 437M
split 00006: n_tensors = 32, total_size = 344M
split 00007: n_tensors = 4, total_size = 34M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00001-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00002-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00003-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00004-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00005-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00006-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00007-of-00007.gguf ... gguf_split: 7 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00001-of-00007.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367211
llama_model_loader: additional 6 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00001-of-00007.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 7
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =   967.99 MiB
llm_load_tensors:        CPU buffer size =   344.30 MiB
llm_load_tensors:        CPU buffer size =   411.77 MiB
llm_load_tensors:        CPU buffer size =   437.27 MiB
llm_load_tensors:        CPU buffer size =   344.30 MiB
llm_load_tensors:        CPU buffer size =    34.02 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CPU buffer from size 0.00 MiB to 504.00 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> The Witcher season 2 has just concluded, and fans are eagerly anticipating Season 3. Here are some reasons why you think The Witcher season 2 was a success
llama_print_timings:        load time =    1233.14 ms
llama_print_timings:      sample time =      47.67 ms /    32 runs   (    1.49 ms per token,   671.24 tokens per second)
llama_print_timings: prompt eval time =    1146.31 ms /     2 tokens (  573.15 ms per token,     1.74 tokens per second)
llama_print_timings:        eval time =   21022.53 ms /    31 runs   (  678.15 ms per token,     1.47 tokens per second)
llama_print_timings:       total time =   22350.29 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split --split-max-size 2G /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G
n_split: 2
split 00001: n_tensors = 118, total_size = 2021M
split 00002: n_tensors = 46, total_size = 523M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00002-of-00002.gguf ... gguf_split: 2 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367238
llama_model_loader: additional 1 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 2
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =  2015.76 MiB
llm_load_tensors:        CPU buffer size =   523.91 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CPU buffer from size 0.00 MiB to 504.00 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> If you have a choice, what type of career would you choose and why?

As a large language model, I do not have personal preferences or the ability to
llama_print_timings:        load time =    1331.83 ms
llama_print_timings:      sample time =      47.80 ms /    32 runs   (    1.49 ms per token,   669.39 tokens per second)
llama_print_timings: prompt eval time =    1202.29 ms /     2 tokens (  601.15 ms per token,     1.66 tokens per second)
llama_print_timings:        eval time =   20818.05 ms /    31 runs   (  671.55 ms per token,     1.49 tokens per second)
llama_print_timings:       total time =   22201.94 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ rm -f /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00002-of-00002.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00001-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00002-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00003-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00004-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00005-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00006-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00007-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf

real	1m47.846s
user	6m15.570s
sys	0m9.484s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/test_scripts_debug-scripts.log
+ cd ./examples/quantize
+ bash tests.sh /home/ggml/work/llama.cpp/build-ci-debug/bin /mnt/llama.cpp/models
+ SPLIT=/home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split
+ QUANTIZE=/home/ggml/work/llama.cpp/build-ci-debug/bin/quantize
+ MAIN=/home/ggml/work/llama.cpp/build-ci-debug/bin/main
+ WORK_PATH=/mnt/llama.cpp/models/quantize
+++ dirname tests.sh
++ realpath ./../../
+ ROOT_DIR=/home/ggml/work/llama.cpp
+ mkdir -p /mnt/llama.cpp/models/quantize
+ rm -f '/mnt/llama.cpp/models/quantize/ggml-model-split*.gguf' '/mnt/llama.cpp/models/quantize/ggml-model-requant*.gguf'
+ cd /mnt/llama.cpp/models/quantize
+ /home/ggml/work/llama.cpp/scripts/hf.sh --repo ggml-org/gemma-1.1-2b-it-Q8_0-GGUF --file gemma-1.1-2b-it.Q8_0.gguf
[+] attempting to download gemma-1.1-2b-it.Q8_0.gguf
[+] wget -q --show-progress -c -O ./gemma-1.1-2b-it.Q8_0.gguf https://huggingface.co/ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/resolve/main/gemma-1.1-2b-it.Q8_0.gguf
./gemma-1.1-2b-it.Q8_0.gguf
+ echo PASS
PASS
+ /home/ggml/work/llama.cpp/build-ci-debug/bin/gguf-split --split-max-tensors 28 /mnt/llama.cpp/models/quantize/gemma-1.1-2b-it.Q8_0.gguf /mnt/llama.cpp/models/quantize/ggml-model-split
n_split: 6
split 00001: n_tensors = 28, total_size = 871M
split 00002: n_tensors = 28, total_size = 334M
split 00003: n_tensors = 28, total_size = 402M
split 00004: n_tensors = 28, total_size = 335M
split 00005: n_tensors = 28, total_size = 338M
split 00006: n_tensors = 24, total_size = 261M
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00002-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00003-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00004-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00005-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00006-of-00006.gguf ... gguf_split: 6 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/quantize --allow-requantize --keep-split /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant.gguf Q4_K
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '/mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf' to '/mnt/llama.cpp/models/quantize/ggml-model-requant' as Q4_K
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
[   1/ 164]                    token_embd.weight - [ 2048, 256000,     1,     1], type =   q8_0, converting to q6_K .. size =   531.25 MiB ->   410.16 MiB
[   2/ 164]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 164]                blk.0.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[   4/ 164]                blk.0.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[   5/ 164]                  blk.0.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[   6/ 164]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 164]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[   8/ 164]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[   9/ 164]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  10/ 164]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  11/ 164]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 164]                blk.1.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  13/ 164]                blk.1.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  14/ 164]                  blk.1.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  15/ 164]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  16/ 164]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  17/ 164]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  18/ 164]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  19/ 164]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  20/ 164]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  21/ 164]               blk.10.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  22/ 164]               blk.10.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  23/ 164]                 blk.10.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  24/ 164]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  25/ 164]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  26/ 164]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  27/ 164]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  28/ 164]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  29/ 164]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 164]               blk.11.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  31/ 164]               blk.11.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  32/ 164]                 blk.11.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  33/ 164]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  34/ 164]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  35/ 164]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  36/ 164]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  37/ 164]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  38/ 164]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 164]               blk.12.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  40/ 164]               blk.12.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  41/ 164]                 blk.12.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  42/ 164]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 164]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  44/ 164]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  45/ 164]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  46/ 164]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  47/ 164]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 164]               blk.13.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  49/ 164]               blk.13.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  50/ 164]                 blk.13.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  51/ 164]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  52/ 164]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  53/ 164]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  54/ 164]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  55/ 164]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  56/ 164]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  57/ 164]               blk.14.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  58/ 164]               blk.14.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  59/ 164]                 blk.14.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  60/ 164]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  61/ 164]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  62/ 164]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  63/ 164]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  64/ 164]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  65/ 164]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 164]               blk.15.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  67/ 164]               blk.15.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  68/ 164]                 blk.15.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  69/ 164]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  70/ 164]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  71/ 164]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  72/ 164]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  73/ 164]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  74/ 164]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 164]               blk.16.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  76/ 164]               blk.16.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  77/ 164]                 blk.16.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  78/ 164]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 164]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  80/ 164]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  81/ 164]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  82/ 164]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  83/ 164]               blk.17.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  84/ 164]                 blk.17.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  85/ 164]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  86/ 164]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  87/ 164]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  88/ 164]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  89/ 164]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 164]                blk.2.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  91/ 164]                blk.2.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  92/ 164]                  blk.2.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  93/ 164]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  94/ 164]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  95/ 164]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  96/ 164]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  97/ 164]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  98/ 164]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 164]                blk.3.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 100/ 164]                blk.3.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 101/ 164]                  blk.3.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 102/ 164]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 164]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 104/ 164]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 105/ 164]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 106/ 164]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 107/ 164]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 164]                blk.4.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 109/ 164]                blk.4.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 110/ 164]                  blk.4.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 111/ 164]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 112/ 164]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 113/ 164]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 114/ 164]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 115/ 164]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 116/ 164]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 117/ 164]                blk.5.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 118/ 164]                blk.5.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 119/ 164]                  blk.5.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 120/ 164]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 121/ 164]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 122/ 164]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 123/ 164]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 124/ 164]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 125/ 164]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 164]                blk.6.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 127/ 164]                blk.6.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 128/ 164]                  blk.6.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 129/ 164]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 130/ 164]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 131/ 164]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 132/ 164]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 133/ 164]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 134/ 164]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 164]                blk.7.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 136/ 164]                blk.7.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 137/ 164]                  blk.7.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 138/ 164]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 164]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 140/ 164]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 141/ 164]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 142/ 164]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 143/ 164]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 164]                blk.8.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 145/ 164]                blk.8.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 146/ 164]                  blk.8.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 147/ 164]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 148/ 164]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 149/ 164]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 150/ 164]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 151/ 164]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 152/ 164]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 153/ 164]                blk.9.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 154/ 164]                blk.9.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 155/ 164]                  blk.9.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 156/ 164]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 157/ 164]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 158/ 164]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 159/ 164]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 160/ 164]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 161/ 164]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 164]               blk.17.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 163/ 164]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 164]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
llama_model_quantize_internal: model size  =  2539.66 MB
llama_model_quantize_internal: quant size  =  1548.98 MB

main: quantize time = 198122.47 ms
main:    total time = 198122.47 ms
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/quantize/ggml-model-requant-00001-of-00006.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367463
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/quantize/ggml-model-requant-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q4_K:  108 tensors
llama_model_loader: - type q6_K:   19 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 1.51 GiB (5.18 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =   604.15 MiB
llm_load_tensors:        CPU buffer size =   185.62 MiB
llm_load_tensors:        CPU buffer size =   221.61 MiB
llm_load_tensors:        CPU buffer size =   185.89 MiB
llm_load_tensors:        CPU buffer size =   187.86 MiB
llm_load_tensors:        CPU buffer size =   163.85 MiB
........................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CPU buffer from size 0.00 MiB to 504.00 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Helian Company, which is domiciled in the United States, has been accused of anti-competitive practices by the Federal Trade Commission (FTC).

**Questions:**
llama_print_timings:        load time =     632.65 ms
llama_print_timings:      sample time =      47.41 ms /    32 runs   (    1.48 ms per token,   674.91 tokens per second)
llama_print_timings: prompt eval time =     569.82 ms /     2 tokens (  284.91 ms per token,     3.51 tokens per second)
llama_print_timings:        eval time =   10487.00 ms /    31 runs   (  338.29 ms per token,     2.96 tokens per second)
llama_print_timings:       total time =   11237.22 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/quantize --allow-requantize /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf Q4_K
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '/mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf' to '/mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf' as Q4_K
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
[   1/ 164]                    token_embd.weight - [ 2048, 256000,     1,     1], type =   q8_0, converting to q6_K .. size =   531.25 MiB ->   410.16 MiB
[   2/ 164]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 164]                blk.0.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[   4/ 164]                blk.0.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[   5/ 164]                  blk.0.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[   6/ 164]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 164]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[   8/ 164]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[   9/ 164]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  10/ 164]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  11/ 164]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 164]                blk.1.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  13/ 164]                blk.1.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  14/ 164]                  blk.1.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  15/ 164]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  16/ 164]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  17/ 164]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  18/ 164]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  19/ 164]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  20/ 164]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  21/ 164]               blk.10.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  22/ 164]               blk.10.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  23/ 164]                 blk.10.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  24/ 164]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  25/ 164]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  26/ 164]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  27/ 164]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  28/ 164]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  29/ 164]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 164]               blk.11.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  31/ 164]               blk.11.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  32/ 164]                 blk.11.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  33/ 164]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  34/ 164]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  35/ 164]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  36/ 164]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  37/ 164]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  38/ 164]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 164]               blk.12.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  40/ 164]               blk.12.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  41/ 164]                 blk.12.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  42/ 164]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 164]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  44/ 164]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  45/ 164]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  46/ 164]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  47/ 164]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 164]               blk.13.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  49/ 164]               blk.13.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  50/ 164]                 blk.13.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  51/ 164]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  52/ 164]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  53/ 164]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  54/ 164]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  55/ 164]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  56/ 164]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  57/ 164]               blk.14.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  58/ 164]               blk.14.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  59/ 164]                 blk.14.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  60/ 164]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  61/ 164]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  62/ 164]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  63/ 164]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  64/ 164]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  65/ 164]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 164]               blk.15.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  67/ 164]               blk.15.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  68/ 164]                 blk.15.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  69/ 164]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  70/ 164]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  71/ 164]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  72/ 164]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  73/ 164]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  74/ 164]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 164]               blk.16.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  76/ 164]               blk.16.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  77/ 164]                 blk.16.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  78/ 164]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 164]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  80/ 164]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  81/ 164]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  82/ 164]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  83/ 164]               blk.17.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  84/ 164]                 blk.17.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  85/ 164]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  86/ 164]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  87/ 164]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  88/ 164]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  89/ 164]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 164]                blk.2.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  91/ 164]                blk.2.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  92/ 164]                  blk.2.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  93/ 164]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  94/ 164]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  95/ 164]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  96/ 164]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  97/ 164]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  98/ 164]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 164]                blk.3.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 100/ 164]                blk.3.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 101/ 164]                  blk.3.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 102/ 164]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 164]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 104/ 164]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 105/ 164]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 106/ 164]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 107/ 164]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 164]                blk.4.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 109/ 164]                blk.4.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 110/ 164]                  blk.4.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 111/ 164]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 112/ 164]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 113/ 164]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 114/ 164]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 115/ 164]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 116/ 164]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 117/ 164]                blk.5.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 118/ 164]                blk.5.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 119/ 164]                  blk.5.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 120/ 164]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 121/ 164]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 122/ 164]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 123/ 164]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 124/ 164]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 125/ 164]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 164]                blk.6.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 127/ 164]                blk.6.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 128/ 164]                  blk.6.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 129/ 164]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 130/ 164]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 131/ 164]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 132/ 164]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 133/ 164]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 134/ 164]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 164]                blk.7.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 136/ 164]                blk.7.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 137/ 164]                  blk.7.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 138/ 164]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 164]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 140/ 164]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 141/ 164]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 142/ 164]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 143/ 164]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 164]                blk.8.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 145/ 164]                blk.8.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 146/ 164]                  blk.8.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 147/ 164]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 148/ 164]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 149/ 164]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 150/ 164]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 151/ 164]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 152/ 164]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 153/ 164]                blk.9.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 154/ 164]                blk.9.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 155/ 164]                  blk.9.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 156/ 164]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 157/ 164]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 158/ 164]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 159/ 164]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 160/ 164]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 161/ 164]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 164]               blk.17.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 163/ 164]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 164]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
llama_model_quantize_internal: model size  =  2539.66 MB
llama_model_quantize_internal: quant size  =  1548.98 MB

main: quantize time = 198059.73 ms
main:    total time = 198059.73 ms
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-debug/bin/main --model /mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367675
llama_model_loader: loaded meta data with 24 key-value pairs and 164 tensors from /mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q4_K:  108 tensors
llama_model_loader: - type q6_K:   19 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 1.51 GiB (5.18 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =  1548.98 MiB
........................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
ggml_gallocr_reserve_n: reallocating CPU buffer from size 0.00 MiB to 504.00 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Once upon a time, there was a little girl named Lily who lived in a small village. Lily had a very curious mind and always wanted to explore the world beyond her village.
llama_print_timings:        load time =     624.26 ms
llama_print_timings:      sample time =      47.36 ms /    32 runs   (    1.48 ms per token,   675.69 tokens per second)
llama_print_timings: prompt eval time =    1273.83 ms /     5 tokens (  254.77 ms per token,     3.93 tokens per second)
llama_print_timings:        eval time =   10508.47 ms /    31 runs   (  338.98 ms per token,     2.95 tokens per second)
llama_print_timings:       total time =   11964.23 ms /    36 tokens
Log end
+ echo PASS
PASS
+ echo

+ rm -f /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00002-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00003-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00004-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00005-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00006-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00001-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00002-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00003-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00004-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00005-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00006-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf

real	7m5.657s
user	49m55.615s
sys	0m7.243s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_test_scripts_release
+ cd /home/ggml/work/llama.cpp
+ set -e
+ tee /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/test_scripts_release.log
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/test_scripts_release-scripts.log
+ cd ./examples/gguf-split
+ bash tests.sh /home/ggml/work/llama.cpp/build-ci-release/bin /mnt/llama.cpp/models
+ SPLIT=/home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split
+ MAIN=/home/ggml/work/llama.cpp/build-ci-release/bin/main
+ WORK_PATH=/mnt/llama.cpp/models/gguf-split
+++ dirname tests.sh
++ realpath ./../../
+ ROOT_DIR=/home/ggml/work/llama.cpp
+ mkdir -p /mnt/llama.cpp/models/gguf-split
+ rm -f '/mnt/llama.cpp/models/gguf-split/ggml-model-split*.gguf' '/mnt/llama.cpp/models/gguf-split/ggml-model-merge*.gguf'
+ cd /mnt/llama.cpp/models/gguf-split
+ /home/ggml/work/llama.cpp/scripts/hf.sh --repo ggml-org/gemma-1.1-2b-it-Q8_0-GGUF --file gemma-1.1-2b-it.Q8_0.gguf
[+] attempting to download gemma-1.1-2b-it.Q8_0.gguf
[+] wget -q --show-progress -c -O ./gemma-1.1-2b-it.Q8_0.gguf https://huggingface.co/ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/resolve/main/gemma-1.1-2b-it.Q8_0.gguf
./gemma-1.1-2b-it.Q8_0.gguf
+ echo PASS
PASS
+ /home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split --split-max-tensors 28 /mnt/llama.cpp/models/gguf-split/gemma-1.1-2b-it.Q8_0.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split
n_split: 6
split 00001: n_tensors = 28, total_size = 871M
split 00002: n_tensors = 28, total_size = 334M
split 00003: n_tensors = 28, total_size = 402M
split 00004: n_tensors = 28, total_size = 335M
split 00005: n_tensors = 28, total_size = 338M
split 00006: n_tensors = 24, total_size = 261M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ... gguf_split: 6 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367691
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =   865.98 MiB
llm_load_tensors:        CPU buffer size =   334.74 MiB
llm_load_tensors:        CPU buffer size =   402.73 MiB
llm_load_tensors:        CPU buffer size =   335.27 MiB
llm_load_tensors:        CPU buffer size =   338.98 MiB
llm_load_tensors:        CPU buffer size =   261.96 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Once upon a time, in a world filled with whimsical creatures and magical landscapes, lived a curious little girl named Anya.

Anya possessed an insatiable curiosity for the unknown, a
llama_print_timings:        load time =     190.93 ms
llama_print_timings:      sample time =       4.62 ms /    32 runs   (    0.14 ms per token,  6929.41 tokens per second)
llama_print_timings: prompt eval time =     133.82 ms /     5 tokens (   26.76 ms per token,    37.36 tokens per second)
llama_print_timings:        eval time =    2155.79 ms /    31 runs   (   69.54 ms per token,    14.38 tokens per second)
llama_print_timings:       total time =    2350.45 ms /    36 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split --merge /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf
gguf_merge: /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf -> /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ...[3Ddone
gguf_merge: reading metadata /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf ...[3Ddone
gguf_merge: writing tensors /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf ...[3Ddone
gguf_merge: /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf merged from 6 split with 164 tensors.
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367696
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 0
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =  2539.66 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> The Good Place.

The Good Place is a comedy-drama series created by Mike Schur. It stars Kristen Bell, Jason Sudeikis, and Ted Danson
llama_print_timings:        load time =     158.61 ms
llama_print_timings:      sample time =       5.20 ms /    32 runs   (    0.16 ms per token,  6159.77 tokens per second)
llama_print_timings: prompt eval time =      72.68 ms /     2 tokens (   36.34 ms per token,    27.52 tokens per second)
llama_print_timings:        eval time =    2008.07 ms /    31 runs   (   64.78 ms per token,    15.44 tokens per second)
llama_print_timings:       total time =    2142.51 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split --split-max-tensors 32 --no-tensor-first-split /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors
n_split: 7
split 00001: n_tensors = 0, total_size = 5M
split 00002: n_tensors = 32, total_size = 967M
split 00003: n_tensors = 32, total_size = 344M
split 00004: n_tensors = 32, total_size = 411M
split 00005: n_tensors = 32, total_size = 437M
split 00006: n_tensors = 32, total_size = 344M
split 00007: n_tensors = 4, total_size = 34M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00001-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00002-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00003-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00004-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00005-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00006-of-00007.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00007-of-00007.gguf ... gguf_split: 7 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00001-of-00007.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367701
llama_model_loader: additional 6 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00001-of-00007.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 7
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =   967.99 MiB
llm_load_tensors:        CPU buffer size =   344.30 MiB
llm_load_tensors:        CPU buffer size =   411.77 MiB
llm_load_tensors:        CPU buffer size =   437.27 MiB
llm_load_tensors:        CPU buffer size =   344.30 MiB
llm_load_tensors:        CPU buffer size =    34.02 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Once upon a time, in a magical kingdom nestled between rolling hills, there lived a courageous princess named Anya.

**Anya's Abilities:**
- Strong and resilient

llama_print_timings:        load time =     158.66 ms
llama_print_timings:      sample time =       5.05 ms /    32 runs   (    0.16 ms per token,  6332.87 tokens per second)
llama_print_timings: prompt eval time =     134.56 ms /     5 tokens (   26.91 ms per token,    37.16 tokens per second)
llama_print_timings:        eval time =    2113.94 ms /    31 runs   (   68.19 ms per token,    14.66 tokens per second)
llama_print_timings:       total time =    2311.61 ms /    36 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split --split-max-size 2G /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G
n_split: 2
split 00001: n_tensors = 118, total_size = 2021M
split 00002: n_tensors = 46, total_size = 523M
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf ... done
Writing file /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00002-of-00002.gguf ... gguf_split: 2 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367705
llama_model_loader: additional 1 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 2
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =  2015.76 MiB
llm_load_tensors:        CPU buffer size =   523.91 MiB
.............................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Shelli, and Poppy, a trio of young adventurers, embark on a perilous journey to find the legendary Emerald Eye.

**Chapter 1: The Whispering
llama_print_timings:        load time =     158.78 ms
llama_print_timings:      sample time =       5.12 ms /    32 runs   (    0.16 ms per token,  6243.90 tokens per second)
llama_print_timings: prompt eval time =      78.78 ms /     2 tokens (   39.39 ms per token,    25.39 tokens per second)
llama_print_timings:        eval time =    2114.75 ms /    31 runs   (   68.22 ms per token,    14.66 tokens per second)
llama_print_timings:       total time =    2256.36 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ rm -f /mnt/llama.cpp/models/gguf-split/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00002-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00003-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00004-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00005-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-00006-of-00006.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00001-of-00002.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-2G-00002-of-00002.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00001-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00002-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00003-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00004-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00005-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00006-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-split-32-tensors-00007-of-00007.gguf /mnt/llama.cpp/models/gguf-split/ggml-model-merge.gguf

real	0m20.143s
user	0m37.038s
sys	0m9.212s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/test_scripts_release-scripts.log
+ cd ./examples/quantize
+ bash tests.sh /home/ggml/work/llama.cpp/build-ci-release/bin /mnt/llama.cpp/models
+ SPLIT=/home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split
+ QUANTIZE=/home/ggml/work/llama.cpp/build-ci-release/bin/quantize
+ MAIN=/home/ggml/work/llama.cpp/build-ci-release/bin/main
+ WORK_PATH=/mnt/llama.cpp/models/quantize
+++ dirname tests.sh
++ realpath ./../../
+ ROOT_DIR=/home/ggml/work/llama.cpp
+ mkdir -p /mnt/llama.cpp/models/quantize
+ rm -f '/mnt/llama.cpp/models/quantize/ggml-model-split*.gguf' '/mnt/llama.cpp/models/quantize/ggml-model-requant*.gguf'
+ cd /mnt/llama.cpp/models/quantize
+ /home/ggml/work/llama.cpp/scripts/hf.sh --repo ggml-org/gemma-1.1-2b-it-Q8_0-GGUF --file gemma-1.1-2b-it.Q8_0.gguf
[+] attempting to download gemma-1.1-2b-it.Q8_0.gguf
[+] wget -q --show-progress -c -O ./gemma-1.1-2b-it.Q8_0.gguf https://huggingface.co/ggml-org/gemma-1.1-2b-it-Q8_0-GGUF/resolve/main/gemma-1.1-2b-it.Q8_0.gguf
./gemma-1.1-2b-it.Q8_0.gguf
+ echo PASS
PASS
+ /home/ggml/work/llama.cpp/build-ci-release/bin/gguf-split --split-max-tensors 28 /mnt/llama.cpp/models/quantize/gemma-1.1-2b-it.Q8_0.gguf /mnt/llama.cpp/models/quantize/ggml-model-split
n_split: 6
split 00001: n_tensors = 28, total_size = 871M
split 00002: n_tensors = 28, total_size = 334M
split 00003: n_tensors = 28, total_size = 402M
split 00004: n_tensors = 28, total_size = 335M
split 00005: n_tensors = 28, total_size = 338M
split 00006: n_tensors = 24, total_size = 261M
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00002-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00003-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00004-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00005-of-00006.gguf ... done
Writing file /mnt/llama.cpp/models/quantize/ggml-model-split-00006-of-00006.gguf ... gguf_split: 6 gguf split written with a total of 164 tensors.
done
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/quantize --allow-requantize --keep-split /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant.gguf Q4_K
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '/mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf' to '/mnt/llama.cpp/models/quantize/ggml-model-requant' as Q4_K
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
[   1/ 164]                    token_embd.weight - [ 2048, 256000,     1,     1], type =   q8_0, converting to q6_K .. size =   531.25 MiB ->   410.16 MiB
[   2/ 164]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 164]                blk.0.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[   4/ 164]                blk.0.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[   5/ 164]                  blk.0.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[   6/ 164]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 164]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[   8/ 164]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[   9/ 164]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  10/ 164]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  11/ 164]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 164]                blk.1.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  13/ 164]                blk.1.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  14/ 164]                  blk.1.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  15/ 164]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  16/ 164]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  17/ 164]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  18/ 164]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  19/ 164]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  20/ 164]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  21/ 164]               blk.10.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  22/ 164]               blk.10.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  23/ 164]                 blk.10.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  24/ 164]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  25/ 164]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  26/ 164]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  27/ 164]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  28/ 164]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  29/ 164]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 164]               blk.11.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  31/ 164]               blk.11.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  32/ 164]                 blk.11.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  33/ 164]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  34/ 164]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  35/ 164]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  36/ 164]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  37/ 164]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  38/ 164]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 164]               blk.12.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  40/ 164]               blk.12.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  41/ 164]                 blk.12.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  42/ 164]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 164]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  44/ 164]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  45/ 164]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  46/ 164]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  47/ 164]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 164]               blk.13.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  49/ 164]               blk.13.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  50/ 164]                 blk.13.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  51/ 164]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  52/ 164]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  53/ 164]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  54/ 164]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  55/ 164]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  56/ 164]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  57/ 164]               blk.14.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  58/ 164]               blk.14.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  59/ 164]                 blk.14.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  60/ 164]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  61/ 164]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  62/ 164]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  63/ 164]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  64/ 164]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  65/ 164]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 164]               blk.15.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  67/ 164]               blk.15.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  68/ 164]                 blk.15.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  69/ 164]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  70/ 164]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  71/ 164]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  72/ 164]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  73/ 164]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  74/ 164]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 164]               blk.16.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  76/ 164]               blk.16.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  77/ 164]                 blk.16.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  78/ 164]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 164]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  80/ 164]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  81/ 164]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  82/ 164]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  83/ 164]               blk.17.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  84/ 164]                 blk.17.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  85/ 164]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  86/ 164]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  87/ 164]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  88/ 164]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  89/ 164]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 164]                blk.2.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  91/ 164]                blk.2.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  92/ 164]                  blk.2.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  93/ 164]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  94/ 164]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  95/ 164]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  96/ 164]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  97/ 164]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  98/ 164]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 164]                blk.3.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 100/ 164]                blk.3.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 101/ 164]                  blk.3.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 102/ 164]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 164]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 104/ 164]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 105/ 164]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 106/ 164]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 107/ 164]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 164]                blk.4.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 109/ 164]                blk.4.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 110/ 164]                  blk.4.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 111/ 164]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 112/ 164]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 113/ 164]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 114/ 164]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 115/ 164]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 116/ 164]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 117/ 164]                blk.5.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 118/ 164]                blk.5.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 119/ 164]                  blk.5.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 120/ 164]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 121/ 164]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 122/ 164]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 123/ 164]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 124/ 164]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 125/ 164]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 164]                blk.6.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 127/ 164]                blk.6.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 128/ 164]                  blk.6.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 129/ 164]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 130/ 164]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 131/ 164]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 132/ 164]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 133/ 164]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 134/ 164]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 164]                blk.7.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 136/ 164]                blk.7.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 137/ 164]                  blk.7.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 138/ 164]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 164]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 140/ 164]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 141/ 164]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 142/ 164]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 143/ 164]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 164]                blk.8.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 145/ 164]                blk.8.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 146/ 164]                  blk.8.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 147/ 164]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 148/ 164]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 149/ 164]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 150/ 164]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 151/ 164]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 152/ 164]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 153/ 164]                blk.9.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 154/ 164]                blk.9.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 155/ 164]                  blk.9.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 156/ 164]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 157/ 164]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 158/ 164]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 159/ 164]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 160/ 164]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 161/ 164]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 164]               blk.17.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 163/ 164]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 164]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
llama_model_quantize_internal: model size  =  2539.66 MB
llama_model_quantize_internal: quant size  =  1548.98 MB

main: quantize time = 31999.12 ms
main:    total time = 31999.12 ms
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/quantize/ggml-model-requant-00001-of-00006.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367743
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/quantize/ggml-model-requant-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q4_K:  108 tensors
llama_model_loader: - type q6_K:   19 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 1.51 GiB (5.18 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =   604.15 MiB
llm_load_tensors:        CPU buffer size =   185.62 MiB
llm_load_tensors:        CPU buffer size =   221.61 MiB
llm_load_tensors:        CPU buffer size =   185.89 MiB
llm_load_tensors:        CPU buffer size =   187.86 MiB
llm_load_tensors:        CPU buffer size =   163.85 MiB
........................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Heuristics:

**1. Availability heuristic:**
- Judging the likelihood of an event based on how easily examples come to mind.
- Useful for quickly
llama_print_timings:        load time =     123.64 ms
llama_print_timings:      sample time =       4.78 ms /    32 runs   (    0.15 ms per token,  6701.57 tokens per second)
llama_print_timings: prompt eval time =      67.29 ms /     2 tokens (   33.65 ms per token,    29.72 tokens per second)
llama_print_timings:        eval time =    1514.25 ms /    31 runs   (   48.85 ms per token,    20.47 tokens per second)
llama_print_timings:       total time =    1643.03 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/quantize --allow-requantize /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf Q4_K
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '/mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf' to '/mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf' as Q4_K
llama_model_loader: additional 5 GGUFs metadata loaded.
llama_model_loader: loaded meta data with 27 key-value pairs and 164 tensors from /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - kv  24:                                   split.no u16              = 0
llama_model_loader: - kv  25:                                split.count u16              = 6
llama_model_loader: - kv  26:                        split.tensors.count i32              = 164
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
[   1/ 164]                    token_embd.weight - [ 2048, 256000,     1,     1], type =   q8_0, converting to q6_K .. size =   531.25 MiB ->   410.16 MiB
[   2/ 164]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 164]                blk.0.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[   4/ 164]                blk.0.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[   5/ 164]                  blk.0.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[   6/ 164]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 164]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[   8/ 164]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[   9/ 164]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  10/ 164]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  11/ 164]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 164]                blk.1.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  13/ 164]                blk.1.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  14/ 164]                  blk.1.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  15/ 164]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  16/ 164]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  17/ 164]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  18/ 164]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  19/ 164]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  20/ 164]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  21/ 164]               blk.10.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  22/ 164]               blk.10.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  23/ 164]                 blk.10.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  24/ 164]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  25/ 164]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  26/ 164]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  27/ 164]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  28/ 164]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  29/ 164]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 164]               blk.11.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  31/ 164]               blk.11.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  32/ 164]                 blk.11.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  33/ 164]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  34/ 164]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  35/ 164]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  36/ 164]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  37/ 164]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  38/ 164]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 164]               blk.12.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  40/ 164]               blk.12.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  41/ 164]                 blk.12.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  42/ 164]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 164]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  44/ 164]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  45/ 164]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  46/ 164]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  47/ 164]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 164]               blk.13.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  49/ 164]               blk.13.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  50/ 164]                 blk.13.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  51/ 164]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  52/ 164]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  53/ 164]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  54/ 164]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  55/ 164]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  56/ 164]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  57/ 164]               blk.14.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  58/ 164]               blk.14.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  59/ 164]                 blk.14.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  60/ 164]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  61/ 164]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  62/ 164]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  63/ 164]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  64/ 164]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  65/ 164]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 164]               blk.15.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[  67/ 164]               blk.15.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  68/ 164]                 blk.15.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  69/ 164]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  70/ 164]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  71/ 164]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  72/ 164]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  73/ 164]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  74/ 164]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 164]               blk.16.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  76/ 164]               blk.16.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  77/ 164]                 blk.16.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  78/ 164]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 164]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  80/ 164]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  81/ 164]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  82/ 164]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  83/ 164]               blk.17.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  84/ 164]                 blk.17.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  85/ 164]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  86/ 164]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  87/ 164]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  88/ 164]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  89/ 164]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 164]                blk.2.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  91/ 164]                blk.2.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  92/ 164]                  blk.2.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[  93/ 164]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  94/ 164]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[  95/ 164]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  96/ 164]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[  97/ 164]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[  98/ 164]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 164]                blk.3.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 100/ 164]                blk.3.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 101/ 164]                  blk.3.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 102/ 164]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 164]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 104/ 164]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 105/ 164]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 106/ 164]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 107/ 164]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 164]                blk.4.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 109/ 164]                blk.4.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 110/ 164]                  blk.4.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 111/ 164]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 112/ 164]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 113/ 164]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 114/ 164]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 115/ 164]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 116/ 164]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 117/ 164]                blk.5.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 118/ 164]                blk.5.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 119/ 164]                  blk.5.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 120/ 164]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 121/ 164]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 122/ 164]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 123/ 164]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 124/ 164]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 125/ 164]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 164]                blk.6.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 127/ 164]                blk.6.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 128/ 164]                  blk.6.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 129/ 164]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 130/ 164]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 131/ 164]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 132/ 164]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 133/ 164]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 134/ 164]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 164]                blk.7.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 136/ 164]                blk.7.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 137/ 164]                  blk.7.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 138/ 164]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 164]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 140/ 164]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 141/ 164]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 142/ 164]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 143/ 164]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 164]                blk.8.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 145/ 164]                blk.8.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 146/ 164]                  blk.8.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 147/ 164]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 148/ 164]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 149/ 164]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 150/ 164]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 151/ 164]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 152/ 164]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 153/ 164]                blk.9.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 154/ 164]                blk.9.ffn_gate.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 155/ 164]                  blk.9.ffn_up.weight - [ 2048, 16384,     1,     1], type =   q8_0, converting to q4_K .. size =    34.00 MiB ->    18.00 MiB
[ 156/ 164]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 157/ 164]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q4_K .. size =     0.53 MiB ->     0.28 MiB
[ 158/ 164]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 159/ 164]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =   q8_0, converting to q4_K .. size =     4.25 MiB ->     2.25 MiB
[ 160/ 164]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =   q8_0, converting to q6_K .. size =     0.53 MiB ->     0.41 MiB
[ 161/ 164]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 164]               blk.17.ffn_down.weight - [16384,  2048,     1,     1], type =   q8_0, converting to q6_K .. size =    34.00 MiB ->    26.25 MiB
[ 163/ 164]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 164]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
llama_model_quantize_internal: model size  =  2539.66 MB
llama_model_quantize_internal: quant size  =  1548.98 MB

main: quantize time = 32032.62 ms
main:    total time = 32032.62 ms
+ echo PASS
PASS
+ echo

+ /home/ggml/work/llama.cpp/build-ci-release/bin/main --model /mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf --random-prompt --n-predict 32
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716367777
llama_model_loader: loaded meta data with 24 key-value pairs and 164 tensors from /mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.name str              = gemma-1.1-2b-it
llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   3:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          gemma.block_count u32              = 18
llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv   8:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv   9:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  10:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q4_K:  108 tensors
llama_model_loader: - type q6_K:   19 tensors
llm_load_vocab: mismatch in special tokens definition ( 416/256000 vs 260/256000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gemma
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 256000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 8
llm_load_print_meta: n_head_kv        = 1
llm_load_print_meta: n_layer          = 18
llm_load_print_meta: n_rot            = 256
llm_load_print_meta: n_embd_head_k    = 256
llm_load_print_meta: n_embd_head_v    = 256
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 256
llm_load_print_meta: n_embd_v_gqa     = 256
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 16384
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 2B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 2.51 B
llm_load_print_meta: model size       = 1.51 GiB (5.18 BPW) 
llm_load_print_meta: general.name     = gemma-1.1-2b-it
llm_load_print_meta: BOS token        = 2 '<bos>'
llm_load_print_meta: EOS token        = 1 '<eos>'
llm_load_print_meta: UNK token        = 3 '<unk>'
llm_load_print_meta: PAD token        = 0 '<pad>'
llm_load_print_meta: LF token         = 227 '<0x0A>'
llm_load_print_meta: EOT token        = 107 '<end_of_turn>'
llm_load_tensors: ggml ctx size =    0.08 MiB
llm_load_tensors:        CPU buffer size =  1548.98 MiB
........................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB
llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB
llama_new_context_with_model:        CPU compute buffer size =   504.00 MiB
llama_new_context_with_model: graph nodes  = 601
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 32, n_keep = 1


<bos> Soar is a revolutionary AI-powered personal growth and transformation platform that uses cutting-edge algorithms and personalized guidance to help individuals achieve their full potential.

**Features
llama_print_timings:        load time =     122.89 ms
llama_print_timings:      sample time =       4.69 ms /    32 runs   (    0.15 ms per token,  6818.67 tokens per second)
llama_print_timings: prompt eval time =      72.19 ms /     2 tokens (   36.10 ms per token,    27.70 tokens per second)
llama_print_timings:        eval time =    1460.26 ms /    31 runs   (   47.11 ms per token,    21.23 tokens per second)
llama_print_timings:       total time =    1596.15 ms /    33 tokens
Log end
+ echo PASS
PASS
+ echo

+ rm -f /mnt/llama.cpp/models/quantize/ggml-model-split-00001-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00002-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00003-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00004-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00005-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-split-00006-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00001-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00002-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00003-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00004-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00005-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-00006-of-00006.gguf /mnt/llama.cpp/models/quantize/ggml-model-requant-merge.gguf

real	1m10.791s
user	8m13.200s
sys	0m6.818s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_open_llama_3b_v2
+ tee /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2.log
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:49:40 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/config.json [506/506] -> "config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/tokenizer.model
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:49:40 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/tokenizer_config.json [593/593] -> "tokenizer_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:49:40 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/special_tokens_map.json [330/330] -> "special_tokens_map.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/resolve/main/pytorch_model.bin
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/open-llama/3B-v2/ https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
+ local out=models-mnt/open-llama/3B-v2/
+ local url=https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/open-llama/3B-v2/
+ cd models-mnt/open-llama/3B-v2/
+ wget -nv -N https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json
Last-modified header missing -- time-stamps turned off.
2024-05-22 08:49:40 URL:https://huggingface.co/openlm-research/open_llama_3b_v2/raw/main/generation_config.json [137/137] -> "generation_config.json" [1]
+ cd /home/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/home/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /home/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ head -n 60 models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ path_models=../models-mnt/open-llama/3B-v2
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_QKK_64=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.3s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-release

real	0m0.470s
user	0m0.374s
sys	0m0.100s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-make.log
+ make -j
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building CXX object CMakeFiles/ggml.dir/sgemm.cpp.o
[  4%] Built target ggml
[  5%] Built target build_info
[  6%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  9%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[  9%] Linking CXX static library libggml_static.a
[  9%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  9%] Linking CXX executable ../../bin/gguf
[  9%] Linking CXX static library libllama.a
[  9%] Built target ggml_static
[  9%] Built target llama
[ 10%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 10%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 13%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 13%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 14%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 15%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 16%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 16%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 16%] Linking CXX executable ../../bin/quantize-stats
[ 17%] Linking CXX executable ../bin/test-c
[ 17%] Built target llava
[ 17%] Built target gguf
[ 17%] Linking CXX executable ../../bin/benchmark
[ 18%] Linking CXX static library libcommon.a
[ 19%] Linking CXX static library libllava_static.a
[ 19%] Built target llava_static
[ 19%] Built target test-c
[ 19%] Built target common
[ 20%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 22%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 24%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 26%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 28%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 30%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 32%] Linking CXX executable ../bin/test-quantize-fns
[ 33%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 34%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 35%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 36%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 37%] Linking CXX executable ../bin/test-chat-template
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 40%] Linking CXX executable ../bin/test-grammar-parser
[ 40%] Linking CXX executable ../bin/test-llama-grammar
[ 41%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-grammar-integration
[ 41%] Built target benchmark
[ 41%] Linking CXX executable ../bin/test-quantize-perf
[ 41%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-rope
[ 44%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 45%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 48%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Linking CXX executable ../bin/test-backend-ops
[ 49%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 49%] Linking CXX executable ../../bin/batched
[ 51%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 50%] Linking CXX executable ../../bin/beam-search
[ 52%] Linking CXX executable ../../bin/batched-bench
[ 53%] Linking CXX executable ../bin/test-grad0
[ 53%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 54%] Linking CXX executable ../../bin/baby-llama
[ 55%] Linking CXX executable ../bin/test-autorelease
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Built target quantize-stats
[ 55%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 56%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 56%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 57%] Linking CXX executable ../../bin/embedding
[ 57%] Built target test-grammar-parser
[ 58%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 59%] Linking CXX executable ../../bin/eval-callback
[ 59%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 59%] Built target test-rope
[ 60%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 60%] Built target test-backend-ops
[ 60%] Built target convert-llama2c-to-ggml
[ 60%] Built target test-quantize-fns
[ 60%] Built target test-tokenizer-1-spm
[ 61%] Linking CXX executable ../../bin/gritlm
[ 62%] Linking CXX executable ../../bin/finetune
[ 63%] Linking CXX executable ../../bin/gguf-split
[ 63%] Built target test-sampling
[ 63%] Built target test-quantize-perf
[ 63%] Built target batched
[ 63%] Built target test-grad0
[ 63%] Built target embedding
[ 63%] Built target test-tokenizer-1-bpe
[ 63%] Built target test-chat-template
[ 63%] Built target test-grammar-integration
[ 63%] Built target eval-callback
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-autorelease
[ 63%] Built target test-llama-grammar
[ 63%] Built target test-json-schema-to-grammar
[ 64%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 64%] Built target test-tokenizer-0
[ 65%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 66%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 66%] Built target baby-llama
[ 66%] Built target beam-search
[ 66%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 66%] Linking CXX executable ../../bin/llava-cli
[ 67%] Linking CXX executable ../../bin/infill
[ 68%] Linking CXX executable ../../bin/llama-bench
[ 69%] Linking CXX executable ../../bin/main
[ 70%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 71%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 72%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 72%] Built target batched-bench
[ 73%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 73%] Linking CXX executable ../../bin/parallel
[ 73%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 73%] Linking CXX executable ../../bin/perplexity
[ 73%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 74%] Linking CXX executable ../../bin/tokenize
[ 75%] Linking CXX executable ../../bin/quantize
[ 76%] Built target gguf-split
[ 76%] Linking CXX executable ../../bin/retrieval
[ 77%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 78%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 79%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 80%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 81%] Linking CXX executable ../../bin/save-load-state
[ 81%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 82%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 83%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 84%] Generating index.html.hpp
[ 85%] Generating json-schema-to-grammar.mjs.hpp
[ 86%] Generating completion.js.hpp
[ 86%] Linking CXX executable ../../bin/lookup-merge
[ 86%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 87%] Linking CXX executable ../../bin/lookup
[ 88%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 90%] Linking CXX executable ../../bin/speculative
[ 90%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 90%] Generating index.js.hpp
[ 90%] Built target finetune
[ 90%] Built target gritlm
[ 90%] Linking CXX executable ../../bin/simple
[ 91%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 91%] Linking CXX executable ../../bin/lookahead
[ 92%] Linking CXX executable ../../bin/passkey
[ 93%] Linking CXX executable ../../bin/lookup-create
[ 94%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/lookup-stats
[ 96%] Linking CXX executable ../../bin/train-text-from-scratch
[ 96%] Linking CXX executable ../../bin/export-lora
[ 96%] Linking CXX executable ../../bin/imatrix
[ 97%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/q8dot
[ 98%] Built target infill
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Built target llama-bench
[ 99%] Built target main
[ 99%] Built target llava-cli
[ 99%] Built target export-lora
[ 99%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 99%] Built target parallel
[100%] Linking CXX executable ../../bin/server
[100%] Built target lookup-merge
[100%] Built target perplexity
[100%] Built target simple
[100%] Built target save-load-state
[100%] Built target train-text-from-scratch
[100%] Built target retrieval
[100%] Built target tokenize
[100%] Built target quantize
[100%] Built target vdot
[100%] Built target lookup-stats
[100%] Built target q8dot
[100%] Built target lookup-create
[100%] Built target lookup
[100%] Built target passkey
[100%] Built target speculative
[100%] Built target imatrix
[100%] Built target lookahead
[100%] Built target server

real	0m1.256s
user	0m7.359s
sys	0m1.693s
+ python3 ../convert.py ../models-mnt/open-llama/3B-v2
INFO:convert:Loading model file ../models-mnt/open-llama/3B-v2/pytorch_model.bin
INFO:convert:model parameters count : 3426474900 (3B)
INFO:convert:params = Params(n_vocab=32000, n_embd=3200, n_layer=26, n_ctx=2048, n_ff=8640, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models-mnt/open-llama/3B-v2'))
INFO:convert:Loaded vocab file PosixPath('../models-mnt/open-llama/3B-v2/tokenizer.model'), type 'spm'
INFO:convert:Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
INFO:convert:Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
INFO:convert:Writing ../models-mnt/open-llama/3B-v2/3B-v2-3B-F16.gguf, format 1
WARNING:convert:Ignoring added_tokens.json since model matches vocab size without it.
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:gguf.vocab:Setting special token type bos to 1
INFO:gguf.vocab:Setting special token type eos to 2
INFO:gguf.vocab:Setting special token type pad to 0
INFO:gguf.vocab:Setting add_bos_token to True
INFO:gguf.vocab:Setting add_eos_token to False
INFO:convert:[  1/237] Writing tensor token_embd.weight                      | size  32000 x   3200  | type F16  | T+   0
INFO:convert:[  2/237] Writing tensor blk.0.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[  3/237] Writing tensor blk.0.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[  4/237] Writing tensor blk.0.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[  5/237] Writing tensor blk.0.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[  6/237] Writing tensor blk.0.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
INFO:convert:[  7/237] Writing tensor blk.0.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
INFO:convert:[  8/237] Writing tensor blk.0.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
INFO:convert:[  9/237] Writing tensor blk.0.attn_norm.weight                 | size   3200           | type F32  | T+   0
INFO:convert:[ 10/237] Writing tensor blk.0.ffn_norm.weight                  | size   3200           | type F32  | T+   0
INFO:convert:[ 11/237] Writing tensor blk.1.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 12/237] Writing tensor blk.1.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 13/237] Writing tensor blk.1.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 14/237] Writing tensor blk.1.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 15/237] Writing tensor blk.1.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
INFO:convert:[ 16/237] Writing tensor blk.1.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
INFO:convert:[ 17/237] Writing tensor blk.1.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
INFO:convert:[ 18/237] Writing tensor blk.1.attn_norm.weight                 | size   3200           | type F32  | T+   0
INFO:convert:[ 19/237] Writing tensor blk.1.ffn_norm.weight                  | size   3200           | type F32  | T+   0
INFO:convert:[ 20/237] Writing tensor blk.2.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 21/237] Writing tensor blk.2.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 22/237] Writing tensor blk.2.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 23/237] Writing tensor blk.2.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 24/237] Writing tensor blk.2.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
INFO:convert:[ 25/237] Writing tensor blk.2.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
INFO:convert:[ 26/237] Writing tensor blk.2.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
INFO:convert:[ 27/237] Writing tensor blk.2.attn_norm.weight                 | size   3200           | type F32  | T+   0
INFO:convert:[ 28/237] Writing tensor blk.2.ffn_norm.weight                  | size   3200           | type F32  | T+   0
INFO:convert:[ 29/237] Writing tensor blk.3.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 30/237] Writing tensor blk.3.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 31/237] Writing tensor blk.3.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 32/237] Writing tensor blk.3.attn_output.weight               | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 33/237] Writing tensor blk.3.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   0
INFO:convert:[ 34/237] Writing tensor blk.3.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   0
INFO:convert:[ 35/237] Writing tensor blk.3.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   0
INFO:convert:[ 36/237] Writing tensor blk.3.attn_norm.weight                 | size   3200           | type F32  | T+   0
INFO:convert:[ 37/237] Writing tensor blk.3.ffn_norm.weight                  | size   3200           | type F32  | T+   0
INFO:convert:[ 38/237] Writing tensor blk.4.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 39/237] Writing tensor blk.4.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 40/237] Writing tensor blk.4.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   0
INFO:convert:[ 41/237] Writing tensor blk.4.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 42/237] Writing tensor blk.4.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 43/237] Writing tensor blk.4.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
INFO:convert:[ 44/237] Writing tensor blk.4.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 45/237] Writing tensor blk.4.attn_norm.weight                 | size   3200           | type F32  | T+   1
INFO:convert:[ 46/237] Writing tensor blk.4.ffn_norm.weight                  | size   3200           | type F32  | T+   1
INFO:convert:[ 47/237] Writing tensor blk.5.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 48/237] Writing tensor blk.5.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 49/237] Writing tensor blk.5.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 50/237] Writing tensor blk.5.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 51/237] Writing tensor blk.5.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 52/237] Writing tensor blk.5.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
INFO:convert:[ 53/237] Writing tensor blk.5.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 54/237] Writing tensor blk.5.attn_norm.weight                 | size   3200           | type F32  | T+   1
INFO:convert:[ 55/237] Writing tensor blk.5.ffn_norm.weight                  | size   3200           | type F32  | T+   1
INFO:convert:[ 56/237] Writing tensor blk.6.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 57/237] Writing tensor blk.6.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 58/237] Writing tensor blk.6.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 59/237] Writing tensor blk.6.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 60/237] Writing tensor blk.6.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 61/237] Writing tensor blk.6.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
INFO:convert:[ 62/237] Writing tensor blk.6.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 63/237] Writing tensor blk.6.attn_norm.weight                 | size   3200           | type F32  | T+   1
INFO:convert:[ 64/237] Writing tensor blk.6.ffn_norm.weight                  | size   3200           | type F32  | T+   1
INFO:convert:[ 65/237] Writing tensor blk.7.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 66/237] Writing tensor blk.7.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 67/237] Writing tensor blk.7.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 68/237] Writing tensor blk.7.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 69/237] Writing tensor blk.7.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 70/237] Writing tensor blk.7.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
INFO:convert:[ 71/237] Writing tensor blk.7.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 72/237] Writing tensor blk.7.attn_norm.weight                 | size   3200           | type F32  | T+   1
INFO:convert:[ 73/237] Writing tensor blk.7.ffn_norm.weight                  | size   3200           | type F32  | T+   1
INFO:convert:[ 74/237] Writing tensor blk.8.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 75/237] Writing tensor blk.8.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 76/237] Writing tensor blk.8.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 77/237] Writing tensor blk.8.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 78/237] Writing tensor blk.8.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 79/237] Writing tensor blk.8.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
INFO:convert:[ 80/237] Writing tensor blk.8.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 81/237] Writing tensor blk.8.attn_norm.weight                 | size   3200           | type F32  | T+   1
INFO:convert:[ 82/237] Writing tensor blk.8.ffn_norm.weight                  | size   3200           | type F32  | T+   1
INFO:convert:[ 83/237] Writing tensor blk.9.attn_q.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 84/237] Writing tensor blk.9.attn_k.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 85/237] Writing tensor blk.9.attn_v.weight                    | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 86/237] Writing tensor blk.9.attn_output.weight               | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 87/237] Writing tensor blk.9.ffn_gate.weight                  | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 88/237] Writing tensor blk.9.ffn_down.weight                  | size   3200 x   8640  | type F16  | T+   1
INFO:convert:[ 89/237] Writing tensor blk.9.ffn_up.weight                    | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 90/237] Writing tensor blk.9.attn_norm.weight                 | size   3200           | type F32  | T+   1
INFO:convert:[ 91/237] Writing tensor blk.9.ffn_norm.weight                  | size   3200           | type F32  | T+   1
INFO:convert:[ 92/237] Writing tensor blk.10.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 93/237] Writing tensor blk.10.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 94/237] Writing tensor blk.10.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 95/237] Writing tensor blk.10.attn_output.weight              | size   3200 x   3200  | type F16  | T+   1
INFO:convert:[ 96/237] Writing tensor blk.10.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 97/237] Writing tensor blk.10.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   1
INFO:convert:[ 98/237] Writing tensor blk.10.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   1
INFO:convert:[ 99/237] Writing tensor blk.10.attn_norm.weight                | size   3200           | type F32  | T+   2
INFO:convert:[100/237] Writing tensor blk.10.ffn_norm.weight                 | size   3200           | type F32  | T+   2
INFO:convert:[101/237] Writing tensor blk.11.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[102/237] Writing tensor blk.11.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[103/237] Writing tensor blk.11.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[104/237] Writing tensor blk.11.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[105/237] Writing tensor blk.11.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[106/237] Writing tensor blk.11.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
INFO:convert:[107/237] Writing tensor blk.11.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[108/237] Writing tensor blk.11.attn_norm.weight                | size   3200           | type F32  | T+   2
INFO:convert:[109/237] Writing tensor blk.11.ffn_norm.weight                 | size   3200           | type F32  | T+   2
INFO:convert:[110/237] Writing tensor blk.12.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[111/237] Writing tensor blk.12.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[112/237] Writing tensor blk.12.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[113/237] Writing tensor blk.12.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[114/237] Writing tensor blk.12.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[115/237] Writing tensor blk.12.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
INFO:convert:[116/237] Writing tensor blk.12.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[117/237] Writing tensor blk.12.attn_norm.weight                | size   3200           | type F32  | T+   2
INFO:convert:[118/237] Writing tensor blk.12.ffn_norm.weight                 | size   3200           | type F32  | T+   2
INFO:convert:[119/237] Writing tensor blk.13.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[120/237] Writing tensor blk.13.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[121/237] Writing tensor blk.13.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[122/237] Writing tensor blk.13.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[123/237] Writing tensor blk.13.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[124/237] Writing tensor blk.13.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
INFO:convert:[125/237] Writing tensor blk.13.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[126/237] Writing tensor blk.13.attn_norm.weight                | size   3200           | type F32  | T+   2
INFO:convert:[127/237] Writing tensor blk.13.ffn_norm.weight                 | size   3200           | type F32  | T+   2
INFO:convert:[128/237] Writing tensor blk.14.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[129/237] Writing tensor blk.14.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[130/237] Writing tensor blk.14.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[131/237] Writing tensor blk.14.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[132/237] Writing tensor blk.14.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[133/237] Writing tensor blk.14.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
INFO:convert:[134/237] Writing tensor blk.14.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[135/237] Writing tensor blk.14.attn_norm.weight                | size   3200           | type F32  | T+   2
INFO:convert:[136/237] Writing tensor blk.14.ffn_norm.weight                 | size   3200           | type F32  | T+   2
INFO:convert:[137/237] Writing tensor blk.15.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[138/237] Writing tensor blk.15.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[139/237] Writing tensor blk.15.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[140/237] Writing tensor blk.15.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[141/237] Writing tensor blk.15.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[142/237] Writing tensor blk.15.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
INFO:convert:[143/237] Writing tensor blk.15.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[144/237] Writing tensor blk.15.attn_norm.weight                | size   3200           | type F32  | T+   2
INFO:convert:[145/237] Writing tensor blk.15.ffn_norm.weight                 | size   3200           | type F32  | T+   2
INFO:convert:[146/237] Writing tensor blk.16.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[147/237] Writing tensor blk.16.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[148/237] Writing tensor blk.16.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[149/237] Writing tensor blk.16.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[150/237] Writing tensor blk.16.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[151/237] Writing tensor blk.16.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
INFO:convert:[152/237] Writing tensor blk.16.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[153/237] Writing tensor blk.16.attn_norm.weight                | size   3200           | type F32  | T+   2
INFO:convert:[154/237] Writing tensor blk.16.ffn_norm.weight                 | size   3200           | type F32  | T+   2
INFO:convert:[155/237] Writing tensor blk.17.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[156/237] Writing tensor blk.17.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[157/237] Writing tensor blk.17.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[158/237] Writing tensor blk.17.attn_output.weight              | size   3200 x   3200  | type F16  | T+   2
INFO:convert:[159/237] Writing tensor blk.17.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[160/237] Writing tensor blk.17.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   2
INFO:convert:[161/237] Writing tensor blk.17.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   2
INFO:convert:[162/237] Writing tensor blk.17.attn_norm.weight                | size   3200           | type F32  | T+   3
INFO:convert:[163/237] Writing tensor blk.17.ffn_norm.weight                 | size   3200           | type F32  | T+   3
INFO:convert:[164/237] Writing tensor blk.18.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[165/237] Writing tensor blk.18.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[166/237] Writing tensor blk.18.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[167/237] Writing tensor blk.18.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[168/237] Writing tensor blk.18.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[169/237] Writing tensor blk.18.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
INFO:convert:[170/237] Writing tensor blk.18.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[171/237] Writing tensor blk.18.attn_norm.weight                | size   3200           | type F32  | T+   3
INFO:convert:[172/237] Writing tensor blk.18.ffn_norm.weight                 | size   3200           | type F32  | T+   3
INFO:convert:[173/237] Writing tensor blk.19.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[174/237] Writing tensor blk.19.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[175/237] Writing tensor blk.19.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[176/237] Writing tensor blk.19.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[177/237] Writing tensor blk.19.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[178/237] Writing tensor blk.19.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
INFO:convert:[179/237] Writing tensor blk.19.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[180/237] Writing tensor blk.19.attn_norm.weight                | size   3200           | type F32  | T+   3
INFO:convert:[181/237] Writing tensor blk.19.ffn_norm.weight                 | size   3200           | type F32  | T+   3
INFO:convert:[182/237] Writing tensor blk.20.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[183/237] Writing tensor blk.20.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[184/237] Writing tensor blk.20.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[185/237] Writing tensor blk.20.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[186/237] Writing tensor blk.20.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[187/237] Writing tensor blk.20.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
INFO:convert:[188/237] Writing tensor blk.20.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[189/237] Writing tensor blk.20.attn_norm.weight                | size   3200           | type F32  | T+   3
INFO:convert:[190/237] Writing tensor blk.20.ffn_norm.weight                 | size   3200           | type F32  | T+   3
INFO:convert:[191/237] Writing tensor blk.21.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[192/237] Writing tensor blk.21.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[193/237] Writing tensor blk.21.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[194/237] Writing tensor blk.21.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[195/237] Writing tensor blk.21.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[196/237] Writing tensor blk.21.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
INFO:convert:[197/237] Writing tensor blk.21.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[198/237] Writing tensor blk.21.attn_norm.weight                | size   3200           | type F32  | T+   3
INFO:convert:[199/237] Writing tensor blk.21.ffn_norm.weight                 | size   3200           | type F32  | T+   3
INFO:convert:[200/237] Writing tensor blk.22.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[201/237] Writing tensor blk.22.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[202/237] Writing tensor blk.22.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[203/237] Writing tensor blk.22.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[204/237] Writing tensor blk.22.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[205/237] Writing tensor blk.22.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
INFO:convert:[206/237] Writing tensor blk.22.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[207/237] Writing tensor blk.22.attn_norm.weight                | size   3200           | type F32  | T+   3
INFO:convert:[208/237] Writing tensor blk.22.ffn_norm.weight                 | size   3200           | type F32  | T+   3
INFO:convert:[209/237] Writing tensor blk.23.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[210/237] Writing tensor blk.23.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[211/237] Writing tensor blk.23.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[212/237] Writing tensor blk.23.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[213/237] Writing tensor blk.23.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[214/237] Writing tensor blk.23.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   3
INFO:convert:[215/237] Writing tensor blk.23.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[216/237] Writing tensor blk.23.attn_norm.weight                | size   3200           | type F32  | T+   3
INFO:convert:[217/237] Writing tensor blk.23.ffn_norm.weight                 | size   3200           | type F32  | T+   3
INFO:convert:[218/237] Writing tensor blk.24.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[219/237] Writing tensor blk.24.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[220/237] Writing tensor blk.24.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[221/237] Writing tensor blk.24.attn_output.weight              | size   3200 x   3200  | type F16  | T+   3
INFO:convert:[222/237] Writing tensor blk.24.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   3
INFO:convert:[223/237] Writing tensor blk.24.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
INFO:convert:[224/237] Writing tensor blk.24.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
INFO:convert:[225/237] Writing tensor blk.24.attn_norm.weight                | size   3200           | type F32  | T+   4
INFO:convert:[226/237] Writing tensor blk.24.ffn_norm.weight                 | size   3200           | type F32  | T+   4
INFO:convert:[227/237] Writing tensor blk.25.attn_q.weight                   | size   3200 x   3200  | type F16  | T+   4
INFO:convert:[228/237] Writing tensor blk.25.attn_k.weight                   | size   3200 x   3200  | type F16  | T+   4
INFO:convert:[229/237] Writing tensor blk.25.attn_v.weight                   | size   3200 x   3200  | type F16  | T+   4
INFO:convert:[230/237] Writing tensor blk.25.attn_output.weight              | size   3200 x   3200  | type F16  | T+   4
INFO:convert:[231/237] Writing tensor blk.25.ffn_gate.weight                 | size   8640 x   3200  | type F16  | T+   4
INFO:convert:[232/237] Writing tensor blk.25.ffn_down.weight                 | size   3200 x   8640  | type F16  | T+   4
INFO:convert:[233/237] Writing tensor blk.25.ffn_up.weight                   | size   8640 x   3200  | type F16  | T+   4
INFO:convert:[234/237] Writing tensor blk.25.attn_norm.weight                | size   3200           | type F32  | T+   4
INFO:convert:[235/237] Writing tensor blk.25.ffn_norm.weight                 | size   3200           | type F32  | T+   4
INFO:convert:[236/237] Writing tensor output_norm.weight                     | size   3200           | type F32  | T+   4
INFO:convert:[237/237] Writing tensor output.weight                          | size  32000 x   3200  | type F16  | T+   4
INFO:convert:Wrote ../models-mnt/open-llama/3B-v2/3B-v2-3B-F16.gguf
+ model_f16=../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf
+ wiki_test_60=../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf q8_0
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    19.53 MiB ->    10.38 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q8_0 .. size =    52.73 MiB ->    28.02 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   195.31 MiB ->   103.76 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  3472.45 MB

main: quantize time = 11098.11 ms
main:    total time = 11098.11 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf q4_0
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q4_0 .. size =   195.31 MiB ->    54.93 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    19.53 MiB ->     5.49 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_0 .. size =    52.73 MiB ->    14.83 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1866.13 MB

main: quantize time =  6495.67 ms
main:    total time =  6495.67 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf q4_1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q4_1 .. size =   195.31 MiB ->    61.04 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    19.53 MiB ->     6.10 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_1 .. size =    52.73 MiB ->    16.48 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2064.25 MB

main: quantize time =  6500.84 ms
main:    total time =  6500.84 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf q5_0
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q5_0 .. size =   195.31 MiB ->    67.14 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    19.53 MiB ->     6.71 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_0 .. size =    52.73 MiB ->    18.13 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2262.37 MB

main: quantize time =  7146.36 ms
main:    total time =  7146.36 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf q5_1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q5_1 .. size =   195.31 MiB ->    73.24 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    19.53 MiB ->     7.32 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_1 .. size =    52.73 MiB ->    19.78 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2460.49 MB

main: quantize time =  7437.56 ms
main:    total time =  7437.56 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf q2_k
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q2_K .. size =   195.31 MiB ->    36.62 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q2_K .. size =    19.53 MiB ->     3.66 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q2_K .. size =    52.73 MiB ->     9.89 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1346.35 MB

main: quantize time = 32653.41 ms
main:    total time = 32653.41 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf q3_k
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q3_K .. size =   195.31 MiB ->    42.72 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q3_K .. size =    19.53 MiB ->     4.27 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q3_K .. size =    52.73 MiB ->    11.54 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  1662.08 MB

main: quantize time = 29708.16 ms
main:    total time = 29708.16 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf q4_k
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q4_K .. size =   195.31 MiB ->    57.98 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q4_K .. size =    19.53 MiB ->     5.80 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q4_K .. size =    52.73 MiB ->    15.66 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2082.62 MB

main: quantize time = 53508.68 ms
main:    total time = 53508.68 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf q5_k
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q5_K .. size =   195.31 MiB ->    70.19 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q5_K .. size =    19.53 MiB ->     7.02 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q5_K .. size =    52.73 MiB ->    18.95 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2420.14 MB

main: quantize time = 33915.64 ms
main:    total time = 33915.64 ms
+ ./bin/quantize ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf q6_k
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: quantizing '../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf' to '../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
[   1/ 237]                    token_embd.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
[   2/ 237]                  blk.0.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   3/ 237]                  blk.0.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   4/ 237]                  blk.0.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   5/ 237]             blk.0.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[   6/ 237]                blk.0.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   7/ 237]                blk.0.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   8/ 237]                  blk.0.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[   9/ 237]               blk.0.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  10/ 237]                blk.0.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  11/ 237]                  blk.1.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  12/ 237]                  blk.1.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  13/ 237]                  blk.1.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  14/ 237]             blk.1.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  15/ 237]                blk.1.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  16/ 237]                blk.1.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  17/ 237]                  blk.1.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  18/ 237]               blk.1.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  19/ 237]                blk.1.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  20/ 237]                  blk.2.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  21/ 237]                  blk.2.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  22/ 237]                  blk.2.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  23/ 237]             blk.2.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  24/ 237]                blk.2.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  25/ 237]                blk.2.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  26/ 237]                  blk.2.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  27/ 237]               blk.2.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  28/ 237]                blk.2.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  29/ 237]                  blk.3.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  30/ 237]                  blk.3.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  31/ 237]                  blk.3.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  32/ 237]             blk.3.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  33/ 237]                blk.3.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  34/ 237]                blk.3.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  35/ 237]                  blk.3.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  36/ 237]               blk.3.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  37/ 237]                blk.3.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  38/ 237]                  blk.4.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  39/ 237]                  blk.4.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  40/ 237]                  blk.4.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  41/ 237]             blk.4.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  42/ 237]                blk.4.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  43/ 237]                blk.4.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  44/ 237]                  blk.4.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  45/ 237]               blk.4.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  46/ 237]                blk.4.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  47/ 237]                  blk.5.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  48/ 237]                  blk.5.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  49/ 237]                  blk.5.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  50/ 237]             blk.5.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  51/ 237]                blk.5.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  52/ 237]                blk.5.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  53/ 237]                  blk.5.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  54/ 237]               blk.5.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  55/ 237]                blk.5.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  56/ 237]                  blk.6.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  57/ 237]                  blk.6.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  58/ 237]                  blk.6.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  59/ 237]             blk.6.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  60/ 237]                blk.6.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  61/ 237]                blk.6.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  62/ 237]                  blk.6.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  63/ 237]               blk.6.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  64/ 237]                blk.6.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  65/ 237]                  blk.7.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  66/ 237]                  blk.7.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  67/ 237]                  blk.7.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  68/ 237]             blk.7.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  69/ 237]                blk.7.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  70/ 237]                blk.7.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  71/ 237]                  blk.7.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  72/ 237]               blk.7.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  73/ 237]                blk.7.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  74/ 237]                  blk.8.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  75/ 237]                  blk.8.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  76/ 237]                  blk.8.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  77/ 237]             blk.8.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  78/ 237]                blk.8.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  79/ 237]                blk.8.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  80/ 237]                  blk.8.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  81/ 237]               blk.8.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  82/ 237]                blk.8.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  83/ 237]                  blk.9.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  84/ 237]                  blk.9.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  85/ 237]                  blk.9.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  86/ 237]             blk.9.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  87/ 237]                blk.9.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  88/ 237]                blk.9.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  89/ 237]                  blk.9.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  90/ 237]               blk.9.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  91/ 237]                blk.9.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[  92/ 237]                 blk.10.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  93/ 237]                 blk.10.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  94/ 237]                 blk.10.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  95/ 237]            blk.10.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[  96/ 237]               blk.10.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  97/ 237]               blk.10.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  98/ 237]                 blk.10.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[  99/ 237]              blk.10.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 100/ 237]               blk.10.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 101/ 237]                 blk.11.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 102/ 237]                 blk.11.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 103/ 237]                 blk.11.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 104/ 237]            blk.11.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 105/ 237]               blk.11.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 106/ 237]               blk.11.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 107/ 237]                 blk.11.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 108/ 237]              blk.11.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 109/ 237]               blk.11.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 110/ 237]                 blk.12.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 111/ 237]                 blk.12.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 112/ 237]                 blk.12.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 113/ 237]            blk.12.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 114/ 237]               blk.12.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 115/ 237]               blk.12.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 116/ 237]                 blk.12.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 117/ 237]              blk.12.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 118/ 237]               blk.12.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 119/ 237]                 blk.13.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 120/ 237]                 blk.13.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 121/ 237]                 blk.13.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 122/ 237]            blk.13.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 123/ 237]               blk.13.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 124/ 237]               blk.13.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 125/ 237]                 blk.13.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 126/ 237]              blk.13.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 127/ 237]               blk.13.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 128/ 237]                 blk.14.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 129/ 237]                 blk.14.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 130/ 237]                 blk.14.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 131/ 237]            blk.14.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 132/ 237]               blk.14.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 133/ 237]               blk.14.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 134/ 237]                 blk.14.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 135/ 237]              blk.14.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 136/ 237]               blk.14.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 137/ 237]                 blk.15.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 138/ 237]                 blk.15.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 139/ 237]                 blk.15.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 140/ 237]            blk.15.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 141/ 237]               blk.15.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 142/ 237]               blk.15.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 143/ 237]                 blk.15.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 144/ 237]              blk.15.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 145/ 237]               blk.15.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 146/ 237]                 blk.16.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 147/ 237]                 blk.16.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 148/ 237]                 blk.16.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 149/ 237]            blk.16.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 150/ 237]               blk.16.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 151/ 237]               blk.16.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 152/ 237]                 blk.16.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 153/ 237]              blk.16.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 154/ 237]               blk.16.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 155/ 237]                 blk.17.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 156/ 237]                 blk.17.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 157/ 237]                 blk.17.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 158/ 237]            blk.17.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 159/ 237]               blk.17.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 160/ 237]               blk.17.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 161/ 237]                 blk.17.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 162/ 237]              blk.17.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 163/ 237]               blk.17.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 164/ 237]                 blk.18.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 165/ 237]                 blk.18.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 166/ 237]                 blk.18.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 167/ 237]            blk.18.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 168/ 237]               blk.18.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 169/ 237]               blk.18.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 170/ 237]                 blk.18.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 171/ 237]              blk.18.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 172/ 237]               blk.18.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 173/ 237]                 blk.19.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 174/ 237]                 blk.19.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 175/ 237]                 blk.19.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 176/ 237]            blk.19.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 177/ 237]               blk.19.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 178/ 237]               blk.19.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 179/ 237]                 blk.19.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 180/ 237]              blk.19.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 181/ 237]               blk.19.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 182/ 237]                 blk.20.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 183/ 237]                 blk.20.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 184/ 237]                 blk.20.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 185/ 237]            blk.20.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 186/ 237]               blk.20.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 187/ 237]               blk.20.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 188/ 237]                 blk.20.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 189/ 237]              blk.20.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 190/ 237]               blk.20.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 191/ 237]                 blk.21.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 192/ 237]                 blk.21.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 193/ 237]                 blk.21.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 194/ 237]            blk.21.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 195/ 237]               blk.21.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 196/ 237]               blk.21.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 197/ 237]                 blk.21.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 198/ 237]              blk.21.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 199/ 237]               blk.21.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 200/ 237]                 blk.22.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 201/ 237]                 blk.22.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 202/ 237]                 blk.22.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 203/ 237]            blk.22.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 204/ 237]               blk.22.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 205/ 237]               blk.22.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 206/ 237]                 blk.22.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 207/ 237]              blk.22.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 208/ 237]               blk.22.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 209/ 237]                 blk.23.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 210/ 237]                 blk.23.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 211/ 237]                 blk.23.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 212/ 237]            blk.23.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 213/ 237]               blk.23.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 214/ 237]               blk.23.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 215/ 237]                 blk.23.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 216/ 237]              blk.23.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 217/ 237]               blk.23.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 218/ 237]                 blk.24.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 219/ 237]                 blk.24.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 220/ 237]                 blk.24.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 221/ 237]            blk.24.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 222/ 237]               blk.24.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 223/ 237]               blk.24.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 224/ 237]                 blk.24.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 225/ 237]              blk.24.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 226/ 237]               blk.24.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 227/ 237]                 blk.25.attn_q.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 228/ 237]                 blk.25.attn_k.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 229/ 237]                 blk.25.attn_v.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 230/ 237]            blk.25.attn_output.weight - [ 3200,  3200,     1,     1], type =    f16, converting to q6_K .. size =    19.53 MiB ->     8.24 MiB
[ 231/ 237]               blk.25.ffn_gate.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 232/ 237]               blk.25.ffn_down.weight - [ 8640,  3200,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 233/ 237]                 blk.25.ffn_up.weight - [ 3200,  8640,     1,     1], type =    f16, converting to q6_K .. size =    52.73 MiB ->    22.25 MiB
[ 234/ 237]              blk.25.attn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 235/ 237]               blk.25.ffn_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 236/ 237]                   output_norm.weight - [ 3200,     1,     1,     1], type =    f32, size =    0.012 MB
[ 237/ 237]                        output.weight - [ 3200, 32000,     1,     1], type =    f16, converting to q6_K .. size =   195.31 MiB ->    82.40 MiB
llama_model_quantize_internal: model size  =  6535.80 MB
llama_model_quantize_internal: quant size  =  2757.67 MB

main: quantize time = 33951.67 ms
main:    total time = 33951.67 ms
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-f16.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to love. To love life, to love yourself, to love others. To be able to love others, you must first be able to love yourself, and to love yourself you must first love.
The world is a complex place, the universe is even more. The world is not perfect, nor is the universe
llama_print_timings:        load time =     376.85 ms
llama_print_timings:      sample time =       2.35 ms /    64 runs   (    0.04 ms per token, 27199.32 tokens per second)
llama_print_timings: prompt eval time =     294.64 ms /     8 tokens (   36.83 ms per token,    27.15 tokens per second)
llama_print_timings:        eval time =    7189.66 ms /    63 runs   (  114.12 ms per token,     8.76 tokens per second)
llama_print_timings:       total time =    7505.70 ms /    71 tokens
Log end

real	0m8.015s
user	0m29.924s
sys	0m0.355s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q8_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to love. To love life, to love each other, to love our world. We are all connected. We are all interwoven. We are all connected to each other and our world. We are all interwoven. If we could each accept this, then the world would be a better place.

llama_print_timings:        load time =     309.33 ms
llama_print_timings:      sample time =       2.06 ms /    64 runs   (    0.03 ms per token, 31037.83 tokens per second)
llama_print_timings: prompt eval time =     209.35 ms /     8 tokens (   26.17 ms per token,    38.21 tokens per second)
llama_print_timings:        eval time =    5338.88 ms /    63 runs   (   84.74 ms per token,    11.80 tokens per second)
llama_print_timings:       total time =    5567.84 ms /    71 tokens
Log end

real	0m6.018s
user	0m22.104s
sys	0m0.371s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to experience it, not to have it explained to you.
So, I am going to share with you the reasons behind my belief in the magic that is life.
1. It is better to see a movie than to read a book.
The movie is a visual experience. You don
llama_print_timings:        load time =     213.98 ms
llama_print_timings:      sample time =       2.06 ms /    64 runs   (    0.03 ms per token, 31037.83 tokens per second)
llama_print_timings: prompt eval time =     201.53 ms /     8 tokens (   25.19 ms per token,    39.70 tokens per second)
llama_print_timings:        eval time =    4309.39 ms /    63 runs   (   68.40 ms per token,    14.62 tokens per second)
llama_print_timings:       total time =    4529.82 ms /    71 tokens
Log end

real	0m4.830s
user	0m17.976s
sys	0m0.208s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_1.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 3
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.02 GiB (5.05 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2064.25 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to help others, but I also believe that we are all here for a reason. I believe that everyone on earth has a purpose and we are all here for a reason. I believe that there is always something in each life that is greater than us, and we are all here to help each other.
The meaning
llama_print_timings:        load time =     251.77 ms
llama_print_timings:      sample time =       2.07 ms /    64 runs   (    0.03 ms per token, 30962.75 tokens per second)
llama_print_timings: prompt eval time =     375.39 ms /     8 tokens (   46.92 ms per token,    21.31 tokens per second)
llama_print_timings:        eval time =    4704.61 ms /    63 runs   (   74.68 ms per token,    13.39 tokens per second)
llama_print_timings:       total time =    5098.97 ms /    71 tokens
Log end

real	0m5.439s
user	0m20.294s
sys	0m0.267s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_0.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 8
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.21 GiB (5.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2262.37 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to find our purpose, live it, and inspire others to do the same.
I believe the meaning of life is to find our purpose, live it, and inspire others to do the same.
I believe the meaning of life is to find our purpose, live it, and inspire others to do the same.
llama_print_timings:        load time =     264.69 ms
llama_print_timings:      sample time =       2.00 ms /    64 runs   (    0.03 ms per token, 32032.03 tokens per second)
llama_print_timings: prompt eval time =     396.62 ms /     8 tokens (   49.58 ms per token,    20.17 tokens per second)
llama_print_timings:        eval time =    4957.09 ms /    63 runs   (   78.68 ms per token,    12.71 tokens per second)
llama_print_timings:       total time =    5373.03 ms /    71 tokens
Log end

real	0m5.728s
user	0m21.409s
sys	0m0.252s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_1.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 9
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.40 GiB (6.02 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2460.49 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to find and create happiness for yourself and those around you. I am a huge believer in the power of the mind. My experience has taught me that you are the only person standing in your way of achieving the happiness you are seeking. There are two things that will hold you back: fear and doubt.
I
llama_print_timings:        load time =     276.22 ms
llama_print_timings:      sample time =       2.10 ms /    64 runs   (    0.03 ms per token, 30432.72 tokens per second)
llama_print_timings: prompt eval time =     422.72 ms /     8 tokens (   52.84 ms per token,    18.93 tokens per second)
llama_print_timings:        eval time =    5206.24 ms /    63 runs   (   82.64 ms per token,    12.10 tokens per second)
llama_print_timings:       total time =    5647.72 ms /    71 tokens
Log end

real	0m6.017s
user	0m22.513s
sys	0m0.276s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q2_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 10
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q2_K:  105 tensors
llama_model_loader: - type q3_K:   78 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.31 GiB (3.30 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  1346.35 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to make your own, and to create your own meaning.
I am a person who is not afraid of what to do.
I believe in the power of one.
I am a person who wants to be a star in his career and to be a person who is loved by the world.
I am
llama_print_timings:        load time =     272.48 ms
llama_print_timings:      sample time =       2.07 ms /    64 runs   (    0.03 ms per token, 30947.78 tokens per second)
llama_print_timings: prompt eval time =     612.57 ms /     8 tokens (   76.57 ms per token,    13.06 tokens per second)
llama_print_timings:        eval time =    5760.70 ms /    63 runs   (   91.44 ms per token,    10.94 tokens per second)
llama_print_timings:       total time =    6391.90 ms /    71 tokens
Log end

real	0m6.736s
user	0m25.440s
sys	0m0.450s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q3_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 12
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q3_K:  105 tensors
llama_model_loader: - type q4_K:   75 tensors
llama_model_loader: - type q5_K:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.62 GiB (4.07 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  1662.08 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to help others and not to be helped.
I believe that love is the greatest thing in the world and is not to be taken for granted.
I believe that it is better to have had a short life than to have a long, painful death.
I believe in karma.
I believe that you
llama_print_timings:        load time =     253.40 ms
llama_print_timings:      sample time =       2.08 ms /    64 runs   (    0.03 ms per token, 30695.44 tokens per second)
llama_print_timings: prompt eval time =     465.10 ms /     8 tokens (   58.14 ms per token,    17.20 tokens per second)
llama_print_timings:        eval time =    4937.02 ms /    63 runs   (   78.37 ms per token,    12.76 tokens per second)
llama_print_timings:       total time =    5421.19 ms /    71 tokens
Log end

real	0m5.751s
user	0m21.540s
sys	0m0.315s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.03 GiB (5.10 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2082.62 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to love, and I think that love can be found in all of our relationships. I believe that love can be a verb, not just a noun. I also believe that love is not limited to romantic partnerships, and can include family, friends, pets, and even strangers.
I believe that love can be found
llama_print_timings:        load time =     258.25 ms
llama_print_timings:      sample time =       2.07 ms /    64 runs   (    0.03 ms per token, 30858.24 tokens per second)
llama_print_timings: prompt eval time =     392.00 ms /     8 tokens (   49.00 ms per token,    20.41 tokens per second)
llama_print_timings:        eval time =    4787.92 ms /    63 runs   (   76.00 ms per token,    13.16 tokens per second)
llama_print_timings:       total time =    5199.76 ms /    71 tokens
Log end

real	0m5.543s
user	0m20.483s
sys	0m0.507s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 17
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.36 GiB (5.92 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2420.14 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to find something you love to do and then to do it so well that it becomes your lifes work.
- Michael J. Fox
Selling is not about closing. Its about setting up the sale.
- Brian Tracy
In business, like in life, timing is everything.
-
llama_print_timings:        load time =     284.97 ms
llama_print_timings:      sample time =       2.11 ms /    64 runs   (    0.03 ms per token, 30274.36 tokens per second)
llama_print_timings: prompt eval time =     469.19 ms /     8 tokens (   58.65 ms per token,    17.05 tokens per second)
llama_print_timings:        eval time =    5528.25 ms /    63 runs   (   87.75 ms per token,    11.40 tokens per second)
llama_print_timings:       total time =    6016.15 ms /    71 tokens
Log end

real	0m6.393s
user	0m23.893s
sys	0m0.431s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q6_k.log
+ ./bin/main --model ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
Log start
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1234
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 18
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q6_K:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.69 GiB (6.75 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2757.67 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
generate: n_ctx = 512, n_batch = 2048, n_predict = 64, n_keep = 1


<s> I believe the meaning of life is to love. To be a loving person and to love others. I believe the meaning of life is to love. To be a loving person and to love others. I believe in the power of love. Love is the only reason I wake up in the morning. Love is the only reason Im on the other
llama_print_timings:        load time =     317.36 ms
llama_print_timings:      sample time =       2.05 ms /    64 runs   (    0.03 ms per token, 31173.89 tokens per second)
llama_print_timings: prompt eval time =     520.05 ms /     8 tokens (   65.01 ms per token,    15.38 tokens per second)
llama_print_timings:        eval time =    6169.99 ms /    63 runs   (   97.94 ms per token,    10.21 tokens per second)
llama_print_timings:       total time =    6708.54 ms /    71 tokens
Log end

real	0m7.125s
user	0m26.769s
sys	0m0.423s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-f16.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368085
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.169 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 4.59 seconds per pass - ETA 0.07 minutes
[1]4.2447,
llama_print_timings:        load time =     338.94 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    4584.74 ms /   128 tokens (   35.82 ms per token,    27.92 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4598.07 ms /   129 tokens

Final estimate: PPL = 4.2447 +/- 1.01604

real	0m5.068s
user	0m18.826s
sys	0m0.352s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q8_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368090
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 7
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q8_0:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 3.39 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  3472.45 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.087 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 2.83 seconds per pass - ETA 0.03 minutes
[1]4.2465,
llama_print_timings:        load time =     277.19 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    2831.16 ms /   128 tokens (   22.12 ms per token,    45.21 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    2844.55 ms /   129 tokens

Final estimate: PPL = 4.2465 +/- 1.01661

real	0m3.248s
user	0m11.653s
sys	0m0.371s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368094
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.028 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 3.12 seconds per pass - ETA 0.05 minutes
[1]4.1642,
llama_print_timings:        load time =     180.67 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    3113.99 ms /   128 tokens (   24.33 ms per token,    41.10 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    3127.62 ms /   129 tokens

Final estimate: PPL = 4.1642 +/- 0.97601

real	0m3.387s
user	0m12.686s
sys	0m0.303s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368097
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 3
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.02 GiB (5.05 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2064.25 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.095 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 5.86 seconds per pass - ETA 0.08 minutes
[1]4.3918,
llama_print_timings:        load time =     222.18 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    5860.33 ms /   128 tokens (   45.78 ms per token,    21.84 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    5873.73 ms /   129 tokens

Final estimate: PPL = 4.3918 +/- 1.05200

real	0m6.177s
user	0m23.784s
sys	0m0.316s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_0.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368103
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 8
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.21 GiB (5.54 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2262.37 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.154 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 6.17 seconds per pass - ETA 0.10 minutes
[1]4.3207,
llama_print_timings:        load time =     235.46 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    6166.48 ms /   128 tokens (   48.18 ms per token,    20.76 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    6179.72 ms /   129 tokens

Final estimate: PPL = 4.3207 +/- 1.03297

real	0m6.498s
user	0m25.165s
sys	0m0.220s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_1.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368110
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_1.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 9
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_1:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_1
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.40 GiB (6.02 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2460.49 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.032 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 6.59 seconds per pass - ETA 0.10 minutes
[1]4.2617,
llama_print_timings:        load time =     246.99 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    6583.95 ms /   128 tokens (   51.44 ms per token,    19.44 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    6597.15 ms /   129 tokens

Final estimate: PPL = 4.2617 +/- 1.02386

real	0m6.929s
user	0m26.798s
sys	0m0.264s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q2_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368116
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 10
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q2_K:  105 tensors
llama_model_loader: - type q3_K:   78 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.31 GiB (3.30 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  1346.35 MiB
..............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.035 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 9.87 seconds per pass - ETA 0.15 minutes
[1]5.6214,
llama_print_timings:        load time =     241.50 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    9870.32 ms /   128 tokens (   77.11 ms per token,    12.97 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    9883.71 ms /   129 tokens

Final estimate: PPL = 5.6214 +/- 1.37196

real	0m10.190s
user	0m39.759s
sys	0m0.535s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q3_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368127
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q3_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 12
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q3_K:  105 tensors
llama_model_loader: - type q4_K:   75 tensors
llama_model_loader: - type q5_K:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q3_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.62 GiB (4.07 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  1662.08 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.089 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 7.40 seconds per pass - ETA 0.12 minutes
[1]4.4095,
llama_print_timings:        load time =     219.83 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7393.35 ms /   128 tokens (   57.76 ms per token,    17.31 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7405.84 ms /   129 tokens

Final estimate: PPL = 4.4095 +/- 1.04982

real	0m7.696s
user	0m29.762s
sys	0m0.503s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368134
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.03 GiB (5.10 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2082.62 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.099 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 6.13 seconds per pass - ETA 0.10 minutes
[1]4.1887,
llama_print_timings:        load time =     221.46 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    6126.33 ms /   128 tokens (   47.86 ms per token,    20.89 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    6139.72 ms /   129 tokens

Final estimate: PPL = 4.1887 +/- 0.99693

real	0m6.438s
user	0m24.545s
sys	0m0.639s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368141
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q5_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 17
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q5_K:  157 tensors
llama_model_loader: - type q6_K:   27 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.36 GiB (5.92 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2420.14 MiB
................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.081 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 7.28 seconds per pass - ETA 0.12 minutes
[1]4.2379,
llama_print_timings:        load time =     255.16 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    7279.11 ms /   128 tokens (   56.87 ms per token,    17.58 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    7292.46 ms /   129 tokens

Final estimate: PPL = 4.2379 +/- 1.00631

real	0m7.631s
user	0m29.296s
sys	0m0.588s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q6_k.log
+ ./bin/perplexity --model ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368148
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q6_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 18
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q6_K:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q6_K
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 2.69 GiB (6.75 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  2757.67 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 8.042 ms
perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
perplexity: 8.11 seconds per pass - ETA 0.13 minutes
[1]4.2523,
llama_print_timings:        load time =     284.43 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    8105.88 ms /   128 tokens (   63.33 ms per token,    15.79 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    8118.59 ms /   129 tokens

Final estimate: PPL = 4.2523 +/- 1.02398

real	0m8.493s
user	0m32.714s
sys	0m0.567s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-imatrix.log
+ ./bin/imatrix --model ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -c 128 -b 128 --chunks 1
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: seed  = 1716368157
llama_model_loader: loaded meta data with 21 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type  f16:  184 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 6.38 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  6535.80 MiB
.................................................................................................
llama_new_context_with_model: n_ctx      = 128
llama_new_context_with_model: n_batch    = 128
llama_new_context_with_model: n_ubatch   = 128
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    40.63 MiB
llama_new_context_with_model: KV self size  =   40.62 MiB, K (f16):   20.31 MiB, V (f16):   20.31 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    17.19 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1

system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
compute_imatrix: tokenizing the input ..
compute_imatrix: tokenization took 8.028 ms
compute_imatrix: computing over 1 chunks with batch_size 128
compute_imatrix: 4.72 seconds per pass - ETA 0.07 minutes
[1]4.2447,
save_imatrix: stored collected data after 1 chunks in imatrix.dat

llama_print_timings:        load time =    4944.06 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =    4717.86 ms /   128 tokens (   36.86 ms per token,    27.13 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =    4947.91 ms /   129 tokens

Final estimate: PPL = 4.2447 +/- 1.01604

real	0m5.079s
user	0m17.704s
sys	0m0.324s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-save-load-state.log
+ ./bin/save-load-state --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1
main : serialized state into 1798851 out of a maximum of 236005464 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1
main : deserialized state from 1798851 out of a maximum of 236005464 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 838
llama_new_context_with_model: graph splits = 1
main : deserialized state from 1798851 out of a maximum of 236005464 bytes
main : seq 0 copied, 1664660 bytes
main : kv cache cleared
main : seq 1 restored, 1664660 bytes

main : success

first run: The quick brown fox jump over the lazy dog
July 31, 2007


second run: The quick brown fox jump over the lazy dog
July 31, 2007


single seq run: The quick brown fox jump over the lazy dog
July 31, 2007

real	0m4.092s
user	0m13.830s
sys	0m0.423s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-save-load-state.log
+ ./bin/save-load-state -fa --model ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf
main: build = 2963 (95fb0aef)
main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
llama_model_loader: loaded meta data with 22 key-value pairs and 237 tensors from ../models-mnt/open-llama/3B-v2/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = open-llama
llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000
llama_model_loader: - kv   3:                       llama.context_length u32              = 2048
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3200
llama_model_loader: - kv   5:                          llama.block_count u32              = 26
llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8640
llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 100
llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   53 tensors
llama_model_loader: - type q4_0:  183 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 3200
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 26
llm_load_print_meta: n_rot            = 100
llm_load_print_meta: n_embd_head_k    = 100
llm_load_print_meta: n_embd_head_v    = 100
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 3200
llm_load_print_meta: n_embd_v_gqa     = 3200
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8640
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 3.43 B
llm_load_print_meta: model size       = 1.82 GiB (4.57 BPW) 
llm_load_print_meta: general.name     = open-llama
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MiB
llm_load_tensors:        CPU buffer size =  1866.13 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 1
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 735
llama_new_context_with_model: graph splits = 1
main : serialized state into 1798829 out of a maximum of 236005464 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 1
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 735
llama_new_context_with_model: graph splits = 1
main : deserialized state from 1798829 out of a maximum of 236005464 bytes
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 1
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =   162.50 MiB
llama_new_context_with_model: KV self size  =  162.50 MiB, K (f16):   81.25 MiB, V (f16):   81.25 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    68.75 MiB
llama_new_context_with_model: graph nodes  = 735
llama_new_context_with_model: graph splits = 1
main : deserialized state from 1798829 out of a maximum of 236005464 bytes
main : seq 0 copied, 1664660 bytes
main : kv cache cleared
main : seq 1 restored, 1664660 bytes

main : success

first run: The quick brown fox jumps over the last lamppost.
Written by Tom Waits.


second run: The quick brown fox jumps over the last lamppost.
Written by Tom Waits.


single seq run: The quick brown fox jumps over the last lamppost.
Written by Tom Waits.

real	0m4.033s
user	0m13.662s
sys	0m0.357s
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]4.2447,'
+ qnt=f16
++ echo '[1]4.2447,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2447
++ echo '4.2447 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 4.2447
+ return 0
  - f16 @ 4.2447 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]4.2465,'
+ qnt=q8_0
++ echo '[1]4.2465,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2465
++ echo '4.2465 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 4.2465
+ return 0
  - q8_0 @ 4.2465 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]4.1642,'
+ qnt=q4_0
++ echo '[1]4.1642,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.1642
++ echo '4.1642 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 4.1642
+ return 0
  - q4_0 @ 4.1642 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]4.3918,'
+ qnt=q4_1
++ grep -oE '[0-9]+\.[0-9]+'
++ echo '[1]4.3918,'
++ tail -n 1
+ ppl=4.3918
++ echo '4.3918 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 4.3918
+ return 0
  - q4_1 @ 4.3918 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]4.3207,'
+ qnt=q5_0
++ echo '[1]4.3207,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.3207
++ echo '4.3207 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 4.3207
+ return 0
  - q5_0 @ 4.3207 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]4.2617,'
+ qnt=q5_1
++ echo '[1]4.2617,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2617
++ echo '4.2617 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 4.2617
+ return 0
  - q5_1 @ 4.2617 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q2_k.log
++ grep '^\[1\]'
+ check_ppl q2_k '[1]5.6214,'
+ qnt=q2_k
++ echo '[1]5.6214,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=5.6214
++ echo '5.6214 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q2_k 5.6214
+ return 0
  - q2_k @ 5.6214 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]4.4095,'
+ qnt=q3_k
++ echo '[1]4.4095,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.4095
++ echo '4.4095 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 4.4095
+ return 0
  - q3_k @ 4.4095 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]4.1887,'
+ qnt=q4_k
++ echo '[1]4.1887,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.1887
++ echo '4.1887 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 4.1887
+ return 0
  - q4_k @ 4.1887 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]4.2379,'
+ qnt=q5_k
++ echo '[1]4.2379,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2379
++ echo '4.2379 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 4.2379
+ return 0
  - q5_k @ 4.2379 OK
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-ppl.log
++ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]4.2523,'
+ qnt=q6_k
++ echo '[1]4.2523,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=4.2523
++ echo '4.2523 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 4.2523
+ return 0
  - q6_k @ 4.2523 OK
+ cat /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/open_llama_3b_v2-imatrix.log
+ grep Final
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /home/ggml/work/llama.cpp
+ local model
+ tee /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_with_model_debug.log
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-debug
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    0.14 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    0.48 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.62 sec*proc (2 tests)

Total Test time (real) =   0.62 sec
0.35user 0.33system 0:00.68elapsed 99%CPU (0avgtext+0avgdata 6880508maxresident)k
0inputs+40outputs (0major+115035minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ tee /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_with_model_release.log
+ cd /home/ggml/work/llama.cpp
+ local model
++ gg_get_model
++ local gguf_3b=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
++ local gguf_7b=/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf ]]
++ echo -n /mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ model=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /home/ggml/results/llama.cpp/95/fb0aefab568348da159efdd370e064d1b35f97/ggml-2-x86-cpu/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/mnt/llama.cpp/models/open-llama/3B-v2/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /home/ggml/work/llama.cpp/build-ci-release
    Start 22: test-model-load-cancel
1/2 Test #22: test-model-load-cancel ...........   Passed    0.04 sec
    Start 23: test-autorelease
2/2 Test #23: test-autorelease .................   Passed    0.37 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.41 sec*proc (2 tests)

Total Test time (real) =   0.41 sec
0.11user 0.35system 0:00.47elapsed 99%CPU (0avgtext+0avgdata 6879708maxresident)k
0inputs+40outputs (0major+115009minor)pagefaults 0swaps
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
