### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.06 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.88 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.88 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  191.48 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.91 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.95 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 251.82 sec*proc (29 tests)

Total Test time (real) = 251.83 sec

real	4m11.865s
user	8m32.254s
sys	0m7.240s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.83 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.37 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.20 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.46 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.47 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.52 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.47 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.68 sec*proc (29 tests)

Total Test time (real) =  54.69 sec

real	0m54.700s
user	1m17.123s
sys	0m6.072s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.196 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.098 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.684 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.691 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.694 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.695 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.696 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.697 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.698 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.699 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.700 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.704 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.705 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.708 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.709 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.710 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.710 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.711 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.712 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.712 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.455 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.711 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.713 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.714 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.714 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.715 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.715 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.033.716 I llama_model_loader: - type  f32:  124 tensors
0.00.033.716 I llama_model_loader: - type  f16:   73 tensors
0.00.033.717 I print_info: file format = GGUF V3 (latest)
0.00.033.718 I print_info: file type   = F16
0.00.033.719 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.038.263 I load: special tokens cache size = 5
0.00.040.653 I load: token to piece cache size = 0.2032 MB
0.00.040.671 I print_info: arch             = bert
0.00.040.673 I print_info: vocab_only       = 0
0.00.040.673 I print_info: n_ctx_train      = 512
0.00.040.674 I print_info: n_embd           = 384
0.00.040.674 I print_info: n_layer          = 12
0.00.040.678 I print_info: n_head           = 12
0.00.040.685 I print_info: n_head_kv        = 12
0.00.040.685 I print_info: n_rot            = 32
0.00.040.685 I print_info: n_swa            = 0
0.00.040.685 I print_info: n_embd_head_k    = 32
0.00.040.686 I print_info: n_embd_head_v    = 32
0.00.040.687 I print_info: n_gqa            = 1
0.00.040.687 I print_info: n_embd_k_gqa     = 384
0.00.040.688 I print_info: n_embd_v_gqa     = 384
0.00.040.689 I print_info: f_norm_eps       = 1.0e-12
0.00.040.690 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.690 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.690 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.693 I print_info: f_logit_scale    = 0.0e+00
0.00.040.694 I print_info: n_ff             = 1536
0.00.040.694 I print_info: n_expert         = 0
0.00.040.694 I print_info: n_expert_used    = 0
0.00.040.694 I print_info: causal attn      = 0
0.00.040.694 I print_info: pooling type     = 2
0.00.040.695 I print_info: rope type        = 2
0.00.040.695 I print_info: rope scaling     = linear
0.00.040.695 I print_info: freq_base_train  = 10000.0
0.00.040.696 I print_info: freq_scale_train = 1
0.00.040.696 I print_info: n_ctx_orig_yarn  = 512
0.00.040.696 I print_info: rope_finetuned   = unknown
0.00.040.697 I print_info: ssm_d_conv       = 0
0.00.040.697 I print_info: ssm_d_inner      = 0
0.00.040.697 I print_info: ssm_d_state      = 0
0.00.040.699 I print_info: ssm_dt_rank      = 0
0.00.040.700 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.700 I print_info: model type       = 33M
0.00.040.700 I print_info: model params     = 33.21 M
0.00.040.701 I print_info: general.name     = Bge Small
0.00.040.702 I print_info: vocab type       = WPM
0.00.040.702 I print_info: n_vocab          = 30522
0.00.040.702 I print_info: n_merges         = 0
0.00.040.702 I print_info: BOS token        = 101 '[CLS]'
0.00.040.703 I print_info: UNK token        = 100 '[UNK]'
0.00.040.703 I print_info: SEP token        = 102 '[SEP]'
0.00.040.703 I print_info: PAD token        = 0 '[PAD]'
0.00.040.703 I print_info: MASK token       = 103 '[MASK]'
0.00.040.704 I print_info: LF token         = 0 '[PAD]'
0.00.040.704 I print_info: max token length = 21
0.00.040.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.044.084 I load_tensors: offloading 12 repeating layers to GPU
0.00.044.086 I load_tensors: offloading output layer to GPU
0.00.044.086 I load_tensors: offloaded 13/13 layers to GPU
0.00.044.113 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.044.114 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.406 I llama_context_unified: n_seq_max     = 1
0.00.044.408 I llama_context_unified: n_ctx         = 512
0.00.044.408 I llama_context_unified: n_ctx_per_seq = 512
0.00.044.408 I llama_context_unified: n_batch       = 2048
0.00.044.408 I llama_context_unified: n_ubatch      = 2048
0.00.044.409 I llama_context_unified: flash_attn    = 0
0.00.044.409 I llama_context_unified: freq_base     = 10000.0
0.00.044.410 I llama_context_unified: freq_scale    = 1
0.00.044.411 I ggml_metal_init: allocating
0.00.044.416 I ggml_metal_init: found device: Apple M4
0.00.044.421 I ggml_metal_init: picking default device: Apple M4
0.00.045.186 I ggml_metal_init: using embedded metal library
0.00.049.307 I ggml_metal_init: GPU name:   Apple M4
0.00.049.310 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.049.311 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.049.311 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.049.312 I ggml_metal_init: simdgroup reduction   = true
0.00.049.312 I ggml_metal_init: simdgroup matrix mul. = true
0.00.049.312 I ggml_metal_init: has residency sets    = true
0.00.049.312 I ggml_metal_init: has bfloat            = true
0.00.049.312 I ggml_metal_init: use bfloat            = true
0.00.049.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.049.314 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.388 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.062.059 I init:      Metal KV buffer size =     9.00 MiB
0.00.062.061 I llama_context_unified: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.062.062 I llama_context_unified:        CPU  output buffer size =     0.00 MiB
0.00.063.275 I llama_context_unified:      Metal compute buffer size =    16.00 MiB
0.00.063.276 I llama_context_unified:        CPU compute buffer size =     2.51 MiB
0.00.063.276 I llama_context_unified: graph nodes  = 429
0.00.063.277 I llama_context_unified: graph splits = 2
0.00.063.278 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.063.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.068.736 I 
0.00.068.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.069.393 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.074.494 I llama_perf_context_print:        load time =      46.63 ms
0.00.074.495 I llama_perf_context_print: prompt eval time =       4.94 ms /     9 tokens (    0.55 ms per token,  1823.34 tokens per second)
0.00.074.496 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.074.496 I llama_perf_context_print:       total time =       5.76 ms /    10 tokens
0.00.074.687 I ggml_metal_free: deallocating

real	0m0.268s
user	0m0.051s
sys	0m0.036s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.295 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.065 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.070 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.075 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.075 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.076 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.079 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.080 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.080 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.080 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.081 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.084 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.086 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.086 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.087 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.087 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.087 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.088 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.551 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.228 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.229 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.230 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.230 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.230 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.231 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.231 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.231 I llama_model_loader: - type  f32:  124 tensors
0.00.015.232 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.232 I print_info: file format = GGUF V3 (latest)
0.00.015.233 I print_info: file type   = Q8_0
0.00.015.233 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.735 I load: special tokens cache size = 5
0.00.019.038 I load: token to piece cache size = 0.2032 MB
0.00.019.047 I print_info: arch             = bert
0.00.019.048 I print_info: vocab_only       = 0
0.00.019.048 I print_info: n_ctx_train      = 512
0.00.019.048 I print_info: n_embd           = 384
0.00.019.048 I print_info: n_layer          = 12
0.00.019.052 I print_info: n_head           = 12
0.00.019.052 I print_info: n_head_kv        = 12
0.00.019.052 I print_info: n_rot            = 32
0.00.019.052 I print_info: n_swa            = 0
0.00.019.052 I print_info: n_embd_head_k    = 32
0.00.019.053 I print_info: n_embd_head_v    = 32
0.00.019.053 I print_info: n_gqa            = 1
0.00.019.054 I print_info: n_embd_k_gqa     = 384
0.00.019.054 I print_info: n_embd_v_gqa     = 384
0.00.019.055 I print_info: f_norm_eps       = 1.0e-12
0.00.019.055 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.055 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.056 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.056 I print_info: f_logit_scale    = 0.0e+00
0.00.019.056 I print_info: n_ff             = 1536
0.00.019.056 I print_info: n_expert         = 0
0.00.019.057 I print_info: n_expert_used    = 0
0.00.019.057 I print_info: causal attn      = 0
0.00.019.057 I print_info: pooling type     = 2
0.00.019.057 I print_info: rope type        = 2
0.00.019.057 I print_info: rope scaling     = linear
0.00.019.058 I print_info: freq_base_train  = 10000.0
0.00.019.058 I print_info: freq_scale_train = 1
0.00.019.058 I print_info: n_ctx_orig_yarn  = 512
0.00.019.058 I print_info: rope_finetuned   = unknown
0.00.019.058 I print_info: ssm_d_conv       = 0
0.00.019.058 I print_info: ssm_d_inner      = 0
0.00.019.061 I print_info: ssm_d_state      = 0
0.00.019.061 I print_info: ssm_dt_rank      = 0
0.00.019.061 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.062 I print_info: model type       = 33M
0.00.019.062 I print_info: model params     = 33.21 M
0.00.019.062 I print_info: general.name     = Bge Small
0.00.019.063 I print_info: vocab type       = WPM
0.00.019.063 I print_info: n_vocab          = 30522
0.00.019.063 I print_info: n_merges         = 0
0.00.019.063 I print_info: BOS token        = 101 '[CLS]'
0.00.019.064 I print_info: UNK token        = 100 '[UNK]'
0.00.019.064 I print_info: SEP token        = 102 '[SEP]'
0.00.019.064 I print_info: PAD token        = 0 '[PAD]'
0.00.019.064 I print_info: MASK token       = 103 '[MASK]'
0.00.019.064 I print_info: LF token         = 0 '[PAD]'
0.00.019.065 I print_info: max token length = 21
0.00.019.065 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.779 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.780 I load_tensors: offloading output layer to GPU
0.00.020.780 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.786 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.786 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.951 I llama_context_unified: n_seq_max     = 1
0.00.020.952 I llama_context_unified: n_ctx         = 512
0.00.020.952 I llama_context_unified: n_ctx_per_seq = 512
0.00.020.952 I llama_context_unified: n_batch       = 2048
0.00.020.952 I llama_context_unified: n_ubatch      = 2048
0.00.020.952 I llama_context_unified: flash_attn    = 0
0.00.020.953 I llama_context_unified: freq_base     = 10000.0
0.00.020.953 I llama_context_unified: freq_scale    = 1
0.00.020.954 I ggml_metal_init: allocating
0.00.020.957 I ggml_metal_init: found device: Apple M4
0.00.020.960 I ggml_metal_init: picking default device: Apple M4
0.00.021.467 I ggml_metal_init: using embedded metal library
0.00.023.961 I ggml_metal_init: GPU name:   Apple M4
0.00.023.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.964 I ggml_metal_init: simdgroup reduction   = true
0.00.023.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.965 I ggml_metal_init: has residency sets    = true
0.00.023.965 I ggml_metal_init: has bfloat            = true
0.00.023.965 I ggml_metal_init: use bfloat            = true
0.00.023.966 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.966 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.105 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.690 I init:      Metal KV buffer size =     9.00 MiB
0.00.034.692 I llama_context_unified: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.694 I llama_context_unified:        CPU  output buffer size =     0.00 MiB
0.00.035.646 I llama_context_unified:      Metal compute buffer size =    16.00 MiB
0.00.035.647 I llama_context_unified:        CPU compute buffer size =     2.51 MiB
0.00.035.647 I llama_context_unified: graph nodes  = 429
0.00.035.647 I llama_context_unified: graph splits = 2
0.00.035.649 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.780 I 
0.00.039.805 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.326 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.748 I llama_perf_context_print:        load time =      30.48 ms
0.00.044.749 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2097.41 tokens per second)
0.00.044.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.750 I llama_perf_context_print:       total time =       4.97 ms /    10 tokens
0.00.045.008 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.275 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.201 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.332 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.340 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.341 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.342 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.342 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.343 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.345 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.346 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.346 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.347 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.351 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.354 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.355 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.356 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.357 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.963 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.977 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.475 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.476 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.476 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.476 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.477 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.477 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.477 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.052.478 I llama_model_loader: - type  f32:   40 tensors
0.00.052.480 I llama_model_loader: - type  f16:   30 tensors
0.00.052.481 I print_info: file format = GGUF V3 (latest)
0.00.052.482 I print_info: file type   = F16
0.00.052.485 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.056.984 W load: empty token at index 5
0.00.062.550 W load: model vocab missing newline token, using special_pad_id instead
0.00.064.182 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.064.220 I load: special tokens cache size = 5
0.00.329.127 I load: token to piece cache size = 1.5060 MB
0.00.329.134 I print_info: arch             = jina-bert-v2
0.00.329.134 I print_info: vocab_only       = 0
0.00.329.134 I print_info: n_ctx_train      = 8192
0.00.329.135 I print_info: n_embd           = 384
0.00.329.135 I print_info: n_layer          = 4
0.00.329.140 I print_info: n_head           = 12
0.00.329.141 I print_info: n_head_kv        = 12
0.00.329.141 I print_info: n_rot            = 32
0.00.329.144 I print_info: n_swa            = 0
0.00.329.144 I print_info: n_embd_head_k    = 32
0.00.329.144 I print_info: n_embd_head_v    = 32
0.00.329.147 I print_info: n_gqa            = 1
0.00.329.147 I print_info: n_embd_k_gqa     = 384
0.00.329.152 I print_info: n_embd_v_gqa     = 384
0.00.329.153 I print_info: f_norm_eps       = 1.0e-12
0.00.329.155 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.329.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.329.156 I print_info: f_max_alibi_bias = 8.0e+00
0.00.329.156 I print_info: f_logit_scale    = 0.0e+00
0.00.329.157 I print_info: n_ff             = 1536
0.00.329.157 I print_info: n_expert         = 0
0.00.329.157 I print_info: n_expert_used    = 0
0.00.329.157 I print_info: causal attn      = 0
0.00.329.158 I print_info: pooling type     = -1
0.00.329.158 I print_info: rope type        = -1
0.00.329.158 I print_info: rope scaling     = linear
0.00.329.158 I print_info: freq_base_train  = 10000.0
0.00.329.158 I print_info: freq_scale_train = 1
0.00.329.161 I print_info: n_ctx_orig_yarn  = 8192
0.00.329.162 I print_info: rope_finetuned   = unknown
0.00.329.162 I print_info: ssm_d_conv       = 0
0.00.329.162 I print_info: ssm_d_inner      = 0
0.00.329.162 I print_info: ssm_d_state      = 0
0.00.329.162 I print_info: ssm_dt_rank      = 0
0.00.329.162 I print_info: ssm_dt_b_c_rms   = 0
0.00.329.162 I print_info: model type       = 33M
0.00.329.163 I print_info: model params     = 32.90 M
0.00.329.163 I print_info: general.name     = Jina Bert Implementation
0.00.329.164 I print_info: vocab type       = BPE
0.00.329.164 I print_info: n_vocab          = 61056
0.00.329.164 I print_info: n_merges         = 39382
0.00.329.165 I print_info: BOS token        = 0 '<s>'
0.00.329.165 I print_info: EOS token        = 2 '</s>'
0.00.329.165 I print_info: UNK token        = 3 '<unk>'
0.00.329.165 I print_info: SEP token        = 2 '</s>'
0.00.329.165 I print_info: PAD token        = 1 '<pad>'
0.00.329.166 I print_info: MASK token       = 4 '<mask>'
0.00.329.166 I print_info: EOG token        = 2 '</s>'
0.00.329.166 I print_info: max token length = 45
0.00.329.167 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.331.286 I load_tensors: offloading 4 repeating layers to GPU
0.00.331.287 I load_tensors: offloading output layer to GPU
0.00.331.287 I load_tensors: offloaded 5/5 layers to GPU
0.00.331.313 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.314 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.331.621 I llama_context_unified: n_seq_max     = 1
0.00.331.622 I llama_context_unified: n_ctx         = 8192
0.00.331.622 I llama_context_unified: n_ctx_per_seq = 8192
0.00.331.622 I llama_context_unified: n_batch       = 2048
0.00.331.623 I llama_context_unified: n_ubatch      = 2048
0.00.331.623 I llama_context_unified: flash_attn    = 0
0.00.331.623 I llama_context_unified: freq_base     = 10000.0
0.00.331.623 I llama_context_unified: freq_scale    = 1
0.00.331.624 I ggml_metal_init: allocating
0.00.331.627 I ggml_metal_init: found device: Apple M4
0.00.331.631 I ggml_metal_init: picking default device: Apple M4
0.00.332.487 I ggml_metal_init: using embedded metal library
0.00.335.074 I ggml_metal_init: GPU name:   Apple M4
0.00.335.076 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.077 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.077 I ggml_metal_init: simdgroup reduction   = true
0.00.335.077 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.077 I ggml_metal_init: has residency sets    = true
0.00.335.077 I ggml_metal_init: has bfloat            = true
0.00.335.077 I ggml_metal_init: use bfloat            = true
0.00.335.078 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.079 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.344.601 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.347.539 I init:      Metal KV buffer size =    48.00 MiB
0.00.347.541 I llama_context_unified: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.542 I llama_context_unified:        CPU  output buffer size =     0.00 MiB
0.00.353.515 I llama_context_unified:      Metal compute buffer size =   220.01 MiB
0.00.353.516 I llama_context_unified:        CPU compute buffer size =    22.02 MiB
0.00.353.516 I llama_context_unified: graph nodes  = 154
0.00.353.516 I llama_context_unified: graph splits = 2
0.00.353.518 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.755 I 
0.00.360.789 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.360.884 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.360.885 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.360.887 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.360.888 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.360.891 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.360.891 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.417 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.066 I llama_perf_context_print:        load time =     336.55 ms
0.00.365.067 I llama_perf_context_print: prompt eval time =       3.64 ms /    62 tokens (    0.06 ms per token, 17032.97 tokens per second)
0.00.365.068 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.070 I llama_perf_context_print:       total time =       4.31 ms /    63 tokens
0.00.365.505 I ggml_metal_free: deallocating

real	0m1.086s
user	0m0.333s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.232 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.418 I main: llama backend init
0.00.000.425 I main: load the model and apply lora adapter, if any
0.00.064.596 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.077.225 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.077.239 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.077.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.077.248 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.077.249 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.077.250 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.077.250 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.077.253 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.077.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.077.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.077.256 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.077.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.077.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.077.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.077.262 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.077.263 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.077.263 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.084.281 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.086.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.094.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.094.544 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.094.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.094.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.094.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.094.548 I llama_model_loader: - type  f32:  194 tensors
0.00.094.549 I llama_model_loader: - type  f16:   98 tensors
0.00.094.554 I print_info: file format = GGUF V3 (latest)
0.00.094.556 I print_info: file type   = all F32 (guessed)
0.00.094.558 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.111.785 I load: special tokens cache size = 25
0.00.121.907 I load: token to piece cache size = 0.2984 MB
0.00.121.912 I print_info: arch             = gptneox
0.00.121.912 I print_info: vocab_only       = 0
0.00.121.912 I print_info: n_ctx_train      = 2048
0.00.121.913 I print_info: n_embd           = 2048
0.00.121.913 I print_info: n_layer          = 24
0.00.121.921 I print_info: n_head           = 16
0.00.121.922 I print_info: n_head_kv        = 16
0.00.121.922 I print_info: n_rot            = 32
0.00.121.922 I print_info: n_swa            = 0
0.00.121.922 I print_info: n_embd_head_k    = 128
0.00.121.922 I print_info: n_embd_head_v    = 128
0.00.121.923 I print_info: n_gqa            = 1
0.00.121.924 I print_info: n_embd_k_gqa     = 2048
0.00.121.931 I print_info: n_embd_v_gqa     = 2048
0.00.121.932 I print_info: f_norm_eps       = 1.0e-05
0.00.121.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.121.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.121.933 I print_info: f_max_alibi_bias = 0.0e+00
0.00.121.933 I print_info: f_logit_scale    = 0.0e+00
0.00.121.934 I print_info: n_ff             = 8192
0.00.121.934 I print_info: n_expert         = 0
0.00.121.935 I print_info: n_expert_used    = 0
0.00.121.935 I print_info: causal attn      = 1
0.00.121.935 I print_info: pooling type     = 0
0.00.121.935 I print_info: rope type        = 2
0.00.121.935 I print_info: rope scaling     = linear
0.00.121.936 I print_info: freq_base_train  = 10000.0
0.00.121.936 I print_info: freq_scale_train = 1
0.00.121.936 I print_info: n_ctx_orig_yarn  = 2048
0.00.121.937 I print_info: rope_finetuned   = unknown
0.00.121.937 I print_info: ssm_d_conv       = 0
0.00.121.937 I print_info: ssm_d_inner      = 0
0.00.121.937 I print_info: ssm_d_state      = 0
0.00.121.937 I print_info: ssm_dt_rank      = 0
0.00.121.938 I print_info: ssm_dt_b_c_rms   = 0
0.00.121.938 I print_info: model type       = 1.4B
0.00.121.939 I print_info: model params     = 1.41 B
0.00.121.939 I print_info: general.name     = 1.4B
0.00.121.940 I print_info: vocab type       = BPE
0.00.121.941 I print_info: n_vocab          = 50304
0.00.121.941 I print_info: n_merges         = 50009
0.00.121.941 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.121.941 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.121.941 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.121.942 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.121.942 I print_info: LF token         = 187 ''
0.00.121.943 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.121.943 I print_info: max token length = 1024
0.00.121.944 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.183.905 I load_tensors: offloading 24 repeating layers to GPU
0.00.183.908 I load_tensors: offloading output layer to GPU
0.00.183.909 I load_tensors: offloaded 25/25 layers to GPU
0.00.183.930 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.183.931 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.184.480 I llama_context_unified: n_seq_max     = 1
0.00.184.481 I llama_context_unified: n_ctx         = 2048
0.00.184.481 I llama_context_unified: n_ctx_per_seq = 2048
0.00.184.481 I llama_context_unified: n_batch       = 2048
0.00.184.481 I llama_context_unified: n_ubatch      = 512
0.00.184.482 I llama_context_unified: flash_attn    = 0
0.00.184.482 I llama_context_unified: freq_base     = 10000.0
0.00.184.482 I llama_context_unified: freq_scale    = 1
0.00.184.483 I ggml_metal_init: allocating
0.00.184.503 I ggml_metal_init: found device: Apple M4
0.00.184.508 I ggml_metal_init: picking default device: Apple M4
0.00.185.101 I ggml_metal_init: using embedded metal library
0.00.248.160 I ggml_metal_init: GPU name:   Apple M4
0.00.248.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.248.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.248.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.248.167 I ggml_metal_init: simdgroup reduction   = true
0.00.248.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.248.168 I ggml_metal_init: has residency sets    = true
0.00.248.168 I ggml_metal_init: has bfloat            = true
0.00.248.168 I ggml_metal_init: use bfloat            = true
0.00.248.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.248.170 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.275.472 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.309.407 I init:      Metal KV buffer size =   384.00 MiB
0.00.309.413 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.309.438 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.313.444 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.313.446 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.313.447 I llama_context_unified: graph nodes  = 967
0.00.313.447 I llama_context_unified: graph splits = 2
0.00.313.454 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.313.573 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.313.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.382.779 I main: llama threadpool init, n_threads = 4
0.00.382.822 I 
0.00.382.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.382.854 I 
0.00.383.033 I sampler seed: 1234
0.00.383.037 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.383.062 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.383.063 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.383.063 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.210.609 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.02.210.609 I llama_perf_context_print:        load time =     317.28 ms
0.02.210.610 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.44 tokens per second)
0.02.210.612 I llama_perf_context_print:        eval time =    1781.02 ms /    63 runs   (   28.27 ms per token,    35.37 tokens per second)
0.02.210.612 I llama_perf_context_print:       total time =    1828.72 ms /    70 tokens
0.02.214.664 I ggml_metal_free: deallocating

real	0m2.510s
user	0m0.138s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.716 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.223 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.199 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.203 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.205 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.206 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.207 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.211 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.212 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.213 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.980 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.620 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.622 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.623 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.624 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.624 I llama_model_loader: - type  f32:  194 tensors
0.00.050.625 I llama_model_loader: - type  f16:   98 tensors
0.00.050.625 I print_info: file format = GGUF V3 (latest)
0.00.050.626 I print_info: file type   = all F32 (guessed)
0.00.050.627 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.461 I load: special tokens cache size = 25
0.00.070.265 I load: token to piece cache size = 0.2984 MB
0.00.070.268 I print_info: arch             = gptneox
0.00.070.268 I print_info: vocab_only       = 0
0.00.070.268 I print_info: n_ctx_train      = 2048
0.00.070.268 I print_info: n_embd           = 2048
0.00.070.269 I print_info: n_layer          = 24
0.00.070.271 I print_info: n_head           = 16
0.00.070.272 I print_info: n_head_kv        = 16
0.00.070.272 I print_info: n_rot            = 32
0.00.070.273 I print_info: n_swa            = 0
0.00.070.273 I print_info: n_embd_head_k    = 128
0.00.070.273 I print_info: n_embd_head_v    = 128
0.00.070.274 I print_info: n_gqa            = 1
0.00.070.274 I print_info: n_embd_k_gqa     = 2048
0.00.070.275 I print_info: n_embd_v_gqa     = 2048
0.00.070.276 I print_info: f_norm_eps       = 1.0e-05
0.00.070.276 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.276 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.276 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.277 I print_info: f_logit_scale    = 0.0e+00
0.00.070.277 I print_info: n_ff             = 8192
0.00.070.277 I print_info: n_expert         = 0
0.00.070.278 I print_info: n_expert_used    = 0
0.00.070.278 I print_info: causal attn      = 1
0.00.070.278 I print_info: pooling type     = 0
0.00.070.278 I print_info: rope type        = 2
0.00.070.279 I print_info: rope scaling     = linear
0.00.070.281 I print_info: freq_base_train  = 10000.0
0.00.070.281 I print_info: freq_scale_train = 1
0.00.070.282 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.282 I print_info: rope_finetuned   = unknown
0.00.070.282 I print_info: ssm_d_conv       = 0
0.00.070.282 I print_info: ssm_d_inner      = 0
0.00.070.282 I print_info: ssm_d_state      = 0
0.00.070.282 I print_info: ssm_dt_rank      = 0
0.00.070.283 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.283 I print_info: model type       = 1.4B
0.00.070.283 I print_info: model params     = 1.41 B
0.00.070.283 I print_info: general.name     = 1.4B
0.00.070.284 I print_info: vocab type       = BPE
0.00.070.284 I print_info: n_vocab          = 50304
0.00.070.284 I print_info: n_merges         = 50009
0.00.070.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.288 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.289 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.290 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.290 I print_info: LF token         = 187 ''
0.00.070.291 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.291 I print_info: max token length = 1024
0.00.070.291 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.469.769 I load_tensors: offloading 24 repeating layers to GPU
0.01.469.775 I load_tensors: offloading output layer to GPU
0.01.469.776 I load_tensors: offloaded 25/25 layers to GPU
0.01.469.804 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.469.807 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.470.857 I llama_context_unified: n_seq_max     = 1
0.01.470.859 I llama_context_unified: n_ctx         = 128
0.01.470.859 I llama_context_unified: n_ctx_per_seq = 128
0.01.470.859 I llama_context_unified: n_batch       = 128
0.01.470.859 I llama_context_unified: n_ubatch      = 128
0.01.470.860 I llama_context_unified: flash_attn    = 0
0.01.470.860 I llama_context_unified: freq_base     = 10000.0
0.01.470.861 I llama_context_unified: freq_scale    = 1
0.01.470.861 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.470.863 I ggml_metal_init: allocating
0.01.470.965 I ggml_metal_init: found device: Apple M4
0.01.470.973 I ggml_metal_init: picking default device: Apple M4
0.01.472.206 I ggml_metal_init: using embedded metal library
0.01.476.374 I ggml_metal_init: GPU name:   Apple M4
0.01.476.376 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.476.376 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.476.377 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.476.377 I ggml_metal_init: simdgroup reduction   = true
0.01.476.378 I ggml_metal_init: simdgroup matrix mul. = true
0.01.476.378 I ggml_metal_init: has residency sets    = true
0.01.476.378 I ggml_metal_init: has bfloat            = true
0.01.476.378 I ggml_metal_init: use bfloat            = true
0.01.476.379 I ggml_metal_init: hasUnifiedMemory      = true
0.01.476.380 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.487.682 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.489.498 I init:      Metal KV buffer size =    24.00 MiB
0.01.489.500 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.489.517 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.491.262 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.01.491.263 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.01.491.264 I llama_context_unified: graph nodes  = 967
0.01.491.264 I llama_context_unified: graph splits = 2
0.01.491.265 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.491.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.525.549 I 
0.01.525.584 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.525.601 I perplexity: tokenizing the input ..
0.01.529.401 I perplexity: tokenization took 3.798 ms
0.01.529.421 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.647.699 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.648.947 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.648.963 I llama_perf_context_print:        load time =    1504.32 ms
0.01.648.964 I llama_perf_context_print: prompt eval time =     118.05 ms /   128 tokens (    0.92 ms per token,  1084.30 tokens per second)
0.01.648.965 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.648.965 I llama_perf_context_print:       total time =     123.41 ms /   129 tokens
0.01.649.477 I ggml_metal_free: deallocating

real	0m1.843s
user	0m0.094s
sys	0m0.268s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.552 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.553 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.555 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.555 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.556 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.560 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.361 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.125 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.127 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.127 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.128 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.129 I llama_model_loader: - type  f32:  194 tensors
0.00.034.129 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.130 I print_info: file format = GGUF V3 (latest)
0.00.034.131 I print_info: file type   = Q8_0
0.00.034.132 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.824 I load: special tokens cache size = 25
0.00.049.458 I load: token to piece cache size = 0.2984 MB
0.00.049.461 I print_info: arch             = gptneox
0.00.049.462 I print_info: vocab_only       = 0
0.00.049.462 I print_info: n_ctx_train      = 2048
0.00.049.462 I print_info: n_embd           = 2048
0.00.049.462 I print_info: n_layer          = 24
0.00.049.467 I print_info: n_head           = 16
0.00.049.468 I print_info: n_head_kv        = 16
0.00.049.468 I print_info: n_rot            = 32
0.00.049.470 I print_info: n_swa            = 0
0.00.049.470 I print_info: n_embd_head_k    = 128
0.00.049.470 I print_info: n_embd_head_v    = 128
0.00.049.471 I print_info: n_gqa            = 1
0.00.049.471 I print_info: n_embd_k_gqa     = 2048
0.00.049.472 I print_info: n_embd_v_gqa     = 2048
0.00.049.473 I print_info: f_norm_eps       = 1.0e-05
0.00.049.473 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.473 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.473 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.473 I print_info: f_logit_scale    = 0.0e+00
0.00.049.474 I print_info: n_ff             = 8192
0.00.049.474 I print_info: n_expert         = 0
0.00.049.474 I print_info: n_expert_used    = 0
0.00.049.475 I print_info: causal attn      = 1
0.00.049.475 I print_info: pooling type     = 0
0.00.049.475 I print_info: rope type        = 2
0.00.049.475 I print_info: rope scaling     = linear
0.00.049.475 I print_info: freq_base_train  = 10000.0
0.00.049.476 I print_info: freq_scale_train = 1
0.00.049.476 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.476 I print_info: rope_finetuned   = unknown
0.00.049.476 I print_info: ssm_d_conv       = 0
0.00.049.477 I print_info: ssm_d_inner      = 0
0.00.049.477 I print_info: ssm_d_state      = 0
0.00.049.477 I print_info: ssm_dt_rank      = 0
0.00.049.477 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.477 I print_info: model type       = 1.4B
0.00.049.478 I print_info: model params     = 1.41 B
0.00.049.478 I print_info: general.name     = 1.4B
0.00.049.479 I print_info: vocab type       = BPE
0.00.049.479 I print_info: n_vocab          = 50304
0.00.049.479 I print_info: n_merges         = 50009
0.00.049.479 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.479 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.480 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.480 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.481 I print_info: LF token         = 187 ''
0.00.049.481 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.481 I print_info: max token length = 1024
0.00.049.481 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.154.623 I load_tensors: offloading 24 repeating layers to GPU
0.01.154.629 I load_tensors: offloading output layer to GPU
0.01.154.630 I load_tensors: offloaded 25/25 layers to GPU
0.01.154.654 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.154.656 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.155.709 I llama_context_unified: n_seq_max     = 1
0.01.155.710 I llama_context_unified: n_ctx         = 2048
0.01.155.711 I llama_context_unified: n_ctx_per_seq = 2048
0.01.155.711 I llama_context_unified: n_batch       = 2048
0.01.155.711 I llama_context_unified: n_ubatch      = 512
0.01.155.712 I llama_context_unified: flash_attn    = 0
0.01.155.712 I llama_context_unified: freq_base     = 10000.0
0.01.155.713 I llama_context_unified: freq_scale    = 1
0.01.155.714 I ggml_metal_init: allocating
0.01.155.722 I ggml_metal_init: found device: Apple M4
0.01.155.729 I ggml_metal_init: picking default device: Apple M4
0.01.156.930 I ggml_metal_init: using embedded metal library
0.01.162.066 I ggml_metal_init: GPU name:   Apple M4
0.01.162.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.162.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.162.071 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.162.071 I ggml_metal_init: simdgroup reduction   = true
0.01.162.072 I ggml_metal_init: simdgroup matrix mul. = true
0.01.162.072 I ggml_metal_init: has residency sets    = true
0.01.162.072 I ggml_metal_init: has bfloat            = true
0.01.162.072 I ggml_metal_init: use bfloat            = true
0.01.162.073 I ggml_metal_init: hasUnifiedMemory      = true
0.01.162.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.177.466 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.231.064 I init:      Metal KV buffer size =   384.00 MiB
0.01.231.072 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.231.097 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.235.333 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.01.235.334 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.01.235.335 I llama_context_unified: graph nodes  = 967
0.01.235.335 I llama_context_unified: graph splits = 2
0.01.235.345 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.235.468 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.235.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.290.223 I main: llama threadpool init, n_threads = 4
0.01.290.271 I 
0.01.290.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.290.297 I 
0.01.290.475 I sampler seed: 1234
0.01.290.479 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.290.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.290.492 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.290.492 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.385.691 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.02.385.691 I llama_perf_context_print:        load time =    1279.65 ms
0.02.385.692 I llama_perf_context_print: prompt eval time =      49.19 ms /     7 tokens (    7.03 ms per token,   142.32 tokens per second)
0.02.385.695 I llama_perf_context_print:        eval time =    1043.04 ms /    63 runs   (   16.56 ms per token,    60.40 tokens per second)
0.02.385.695 I llama_perf_context_print:       total time =    1096.17 ms /    70 tokens
0.02.389.622 I ggml_metal_free: deallocating

real	0m2.409s
user	0m0.107s
sys	0m0.263s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.200 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.724 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.996 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.998 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.998 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.001 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.001 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.002 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.002 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.003 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.003 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.005 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.005 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.005 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.819 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.863 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.662 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.666 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.667 I llama_model_loader: - type  f32:  194 tensors
0.00.026.672 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.673 I print_info: file format = GGUF V3 (latest)
0.00.026.674 I print_info: file type   = Q8_0
0.00.026.677 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.836 I load: special tokens cache size = 25
0.00.041.015 I load: token to piece cache size = 0.2984 MB
0.00.041.019 I print_info: arch             = gptneox
0.00.041.019 I print_info: vocab_only       = 0
0.00.041.019 I print_info: n_ctx_train      = 2048
0.00.041.019 I print_info: n_embd           = 2048
0.00.041.020 I print_info: n_layer          = 24
0.00.041.023 I print_info: n_head           = 16
0.00.041.026 I print_info: n_head_kv        = 16
0.00.041.027 I print_info: n_rot            = 32
0.00.041.027 I print_info: n_swa            = 0
0.00.041.027 I print_info: n_embd_head_k    = 128
0.00.041.027 I print_info: n_embd_head_v    = 128
0.00.041.028 I print_info: n_gqa            = 1
0.00.041.029 I print_info: n_embd_k_gqa     = 2048
0.00.041.029 I print_info: n_embd_v_gqa     = 2048
0.00.041.030 I print_info: f_norm_eps       = 1.0e-05
0.00.041.030 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.030 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.030 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.030 I print_info: f_logit_scale    = 0.0e+00
0.00.041.031 I print_info: n_ff             = 8192
0.00.041.032 I print_info: n_expert         = 0
0.00.041.032 I print_info: n_expert_used    = 0
0.00.041.032 I print_info: causal attn      = 1
0.00.041.032 I print_info: pooling type     = 0
0.00.041.032 I print_info: rope type        = 2
0.00.041.032 I print_info: rope scaling     = linear
0.00.041.033 I print_info: freq_base_train  = 10000.0
0.00.041.033 I print_info: freq_scale_train = 1
0.00.041.033 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.033 I print_info: rope_finetuned   = unknown
0.00.041.033 I print_info: ssm_d_conv       = 0
0.00.041.034 I print_info: ssm_d_inner      = 0
0.00.041.034 I print_info: ssm_d_state      = 0
0.00.041.035 I print_info: ssm_dt_rank      = 0
0.00.041.035 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.035 I print_info: model type       = 1.4B
0.00.041.036 I print_info: model params     = 1.41 B
0.00.041.036 I print_info: general.name     = 1.4B
0.00.041.036 I print_info: vocab type       = BPE
0.00.041.037 I print_info: n_vocab          = 50304
0.00.041.037 I print_info: n_merges         = 50009
0.00.041.037 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.037 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.037 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.037 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.038 I print_info: LF token         = 187 ''
0.00.041.038 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.038 I print_info: max token length = 1024
0.00.041.038 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.949.447 I load_tensors: offloading 24 repeating layers to GPU
0.00.949.452 I load_tensors: offloading output layer to GPU
0.00.949.453 I load_tensors: offloaded 25/25 layers to GPU
0.00.949.474 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.949.475 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.950.325 I llama_context_unified: n_seq_max     = 1
0.00.950.328 I llama_context_unified: n_ctx         = 128
0.00.950.328 I llama_context_unified: n_ctx_per_seq = 128
0.00.950.328 I llama_context_unified: n_batch       = 128
0.00.950.329 I llama_context_unified: n_ubatch      = 128
0.00.950.329 I llama_context_unified: flash_attn    = 0
0.00.950.330 I llama_context_unified: freq_base     = 10000.0
0.00.950.331 I llama_context_unified: freq_scale    = 1
0.00.950.332 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.950.333 I ggml_metal_init: allocating
0.00.950.372 I ggml_metal_init: found device: Apple M4
0.00.950.382 I ggml_metal_init: picking default device: Apple M4
0.00.951.446 I ggml_metal_init: using embedded metal library
0.00.955.556 I ggml_metal_init: GPU name:   Apple M4
0.00.955.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.955.560 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.955.560 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.955.561 I ggml_metal_init: simdgroup reduction   = true
0.00.955.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.955.561 I ggml_metal_init: has residency sets    = true
0.00.955.561 I ggml_metal_init: has bfloat            = true
0.00.955.562 I ggml_metal_init: use bfloat            = true
0.00.955.562 I ggml_metal_init: hasUnifiedMemory      = true
0.00.955.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.966.262 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.967.941 I init:      Metal KV buffer size =    24.00 MiB
0.00.967.943 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.967.959 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.969.593 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.969.595 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.969.595 I llama_context_unified: graph nodes  = 967
0.00.969.595 I llama_context_unified: graph splits = 2
0.00.969.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.969.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.995.028 I 
0.00.995.066 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.995.075 I perplexity: tokenizing the input ..
0.00.998.909 I perplexity: tokenization took 3.832 ms
0.00.998.918 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.136.442 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.137.872 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.137.882 I llama_perf_context_print:        load time =     985.30 ms
0.01.137.883 I llama_perf_context_print: prompt eval time =     137.29 ms /   128 tokens (    1.07 ms per token,   932.33 tokens per second)
0.01.137.884 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.137.884 I llama_perf_context_print:       total time =     142.85 ms /   129 tokens
0.01.138.423 I ggml_metal_free: deallocating

real	0m1.161s
user	0m0.066s
sys	0m0.138s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.100 I main: llama backend init
0.00.000.102 I main: load the model and apply lora adapter, if any
0.00.016.306 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.057 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.058 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.067 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.067 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.068 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.068 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.071 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.072 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.933 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.366 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.368 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.369 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.369 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.051.370 I llama_model_loader: - type  f32:  194 tensors
0.00.051.371 I llama_model_loader: - type q4_0:   97 tensors
0.00.051.371 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.372 I print_info: file format = GGUF V3 (latest)
0.00.051.373 I print_info: file type   = Q4_0
0.00.051.377 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.067.822 I load: special tokens cache size = 25
0.00.080.519 I load: token to piece cache size = 0.2984 MB
0.00.080.524 I print_info: arch             = gptneox
0.00.080.524 I print_info: vocab_only       = 0
0.00.080.524 I print_info: n_ctx_train      = 2048
0.00.080.524 I print_info: n_embd           = 2048
0.00.080.525 I print_info: n_layer          = 24
0.00.080.528 I print_info: n_head           = 16
0.00.080.530 I print_info: n_head_kv        = 16
0.00.080.530 I print_info: n_rot            = 32
0.00.080.530 I print_info: n_swa            = 0
0.00.080.530 I print_info: n_embd_head_k    = 128
0.00.080.531 I print_info: n_embd_head_v    = 128
0.00.080.532 I print_info: n_gqa            = 1
0.00.080.533 I print_info: n_embd_k_gqa     = 2048
0.00.080.534 I print_info: n_embd_v_gqa     = 2048
0.00.080.534 I print_info: f_norm_eps       = 1.0e-05
0.00.080.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.080.536 I print_info: f_clamp_kqv      = 0.0e+00
0.00.080.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.080.536 I print_info: f_logit_scale    = 0.0e+00
0.00.080.537 I print_info: n_ff             = 8192
0.00.080.537 I print_info: n_expert         = 0
0.00.080.537 I print_info: n_expert_used    = 0
0.00.080.538 I print_info: causal attn      = 1
0.00.080.538 I print_info: pooling type     = 0
0.00.080.538 I print_info: rope type        = 2
0.00.080.538 I print_info: rope scaling     = linear
0.00.080.541 I print_info: freq_base_train  = 10000.0
0.00.080.541 I print_info: freq_scale_train = 1
0.00.080.542 I print_info: n_ctx_orig_yarn  = 2048
0.00.080.542 I print_info: rope_finetuned   = unknown
0.00.080.542 I print_info: ssm_d_conv       = 0
0.00.080.543 I print_info: ssm_d_inner      = 0
0.00.080.543 I print_info: ssm_d_state      = 0
0.00.080.543 I print_info: ssm_dt_rank      = 0
0.00.080.543 I print_info: ssm_dt_b_c_rms   = 0
0.00.080.543 I print_info: model type       = 1.4B
0.00.080.544 I print_info: model params     = 1.41 B
0.00.080.544 I print_info: general.name     = 1.4B
0.00.080.545 I print_info: vocab type       = BPE
0.00.080.545 I print_info: n_vocab          = 50304
0.00.080.545 I print_info: n_merges         = 50009
0.00.080.546 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.080.546 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.080.546 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.080.546 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.080.547 I print_info: LF token         = 187 ''
0.00.080.547 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.080.547 I print_info: max token length = 1024
0.00.080.553 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.674.483 I load_tensors: offloading 24 repeating layers to GPU
0.00.674.499 I load_tensors: offloading output layer to GPU
0.00.674.500 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.535 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.674.546 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.676.224 I llama_context_unified: n_seq_max     = 1
0.00.676.227 I llama_context_unified: n_ctx         = 2048
0.00.676.227 I llama_context_unified: n_ctx_per_seq = 2048
0.00.676.228 I llama_context_unified: n_batch       = 2048
0.00.676.228 I llama_context_unified: n_ubatch      = 512
0.00.676.229 I llama_context_unified: flash_attn    = 0
0.00.676.231 I llama_context_unified: freq_base     = 10000.0
0.00.676.231 I llama_context_unified: freq_scale    = 1
0.00.676.234 I ggml_metal_init: allocating
0.00.676.308 I ggml_metal_init: found device: Apple M4
0.00.676.323 I ggml_metal_init: picking default device: Apple M4
0.00.678.203 I ggml_metal_init: using embedded metal library
0.00.684.775 I ggml_metal_init: GPU name:   Apple M4
0.00.684.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.684.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.684.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.684.782 I ggml_metal_init: simdgroup reduction   = true
0.00.684.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.684.783 I ggml_metal_init: has residency sets    = true
0.00.684.783 I ggml_metal_init: has bfloat            = true
0.00.684.784 I ggml_metal_init: use bfloat            = true
0.00.684.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.684.786 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.994 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.338 I init:      Metal KV buffer size =   384.00 MiB
0.00.758.347 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.758.371 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.762.766 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.762.768 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.762.769 I llama_context_unified: graph nodes  = 967
0.00.762.769 I llama_context_unified: graph splits = 2
0.00.762.776 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.762.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.762.901 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.483 I main: llama threadpool init, n_threads = 4
0.00.817.523 I 
0.00.817.545 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.545 I 
0.00.817.672 I sampler seed: 1234
0.00.817.677 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.817.696 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.817.697 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.817.697 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.510.227 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50176.68 tokens per second)
0.01.510.228 I llama_perf_context_print:        load time =     800.50 ms
0.01.510.229 I llama_perf_context_print: prompt eval time =      49.59 ms /     7 tokens (    7.08 ms per token,   141.14 tokens per second)
0.01.510.229 I llama_perf_context_print:        eval time =     640.05 ms /    63 runs   (   10.16 ms per token,    98.43 tokens per second)
0.01.510.230 I llama_perf_context_print:       total time =     693.42 ms /    70 tokens
0.01.514.184 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.131s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.247 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.095 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.071 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.082 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.084 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.088 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.088 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.914 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.916 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.916 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.917 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.917 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.918 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.918 I llama_model_loader: - type  f32:  194 tensors
0.00.029.919 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.919 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.919 I print_info: file format = GGUF V3 (latest)
0.00.029.924 I print_info: file type   = Q4_0
0.00.029.926 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.038.258 I load: special tokens cache size = 25
0.00.044.294 I load: token to piece cache size = 0.2984 MB
0.00.044.299 I print_info: arch             = gptneox
0.00.044.299 I print_info: vocab_only       = 0
0.00.044.299 I print_info: n_ctx_train      = 2048
0.00.044.300 I print_info: n_embd           = 2048
0.00.044.300 I print_info: n_layer          = 24
0.00.044.304 I print_info: n_head           = 16
0.00.044.305 I print_info: n_head_kv        = 16
0.00.044.305 I print_info: n_rot            = 32
0.00.044.305 I print_info: n_swa            = 0
0.00.044.305 I print_info: n_embd_head_k    = 128
0.00.044.305 I print_info: n_embd_head_v    = 128
0.00.044.308 I print_info: n_gqa            = 1
0.00.044.308 I print_info: n_embd_k_gqa     = 2048
0.00.044.309 I print_info: n_embd_v_gqa     = 2048
0.00.044.311 I print_info: f_norm_eps       = 1.0e-05
0.00.044.311 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.311 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.311 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.312 I print_info: f_logit_scale    = 0.0e+00
0.00.044.312 I print_info: n_ff             = 8192
0.00.044.313 I print_info: n_expert         = 0
0.00.044.313 I print_info: n_expert_used    = 0
0.00.044.313 I print_info: causal attn      = 1
0.00.044.313 I print_info: pooling type     = 0
0.00.044.314 I print_info: rope type        = 2
0.00.044.314 I print_info: rope scaling     = linear
0.00.044.315 I print_info: freq_base_train  = 10000.0
0.00.044.315 I print_info: freq_scale_train = 1
0.00.044.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.315 I print_info: rope_finetuned   = unknown
0.00.044.316 I print_info: ssm_d_conv       = 0
0.00.044.316 I print_info: ssm_d_inner      = 0
0.00.044.316 I print_info: ssm_d_state      = 0
0.00.044.316 I print_info: ssm_dt_rank      = 0
0.00.044.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.316 I print_info: model type       = 1.4B
0.00.044.317 I print_info: model params     = 1.41 B
0.00.044.317 I print_info: general.name     = 1.4B
0.00.044.317 I print_info: vocab type       = BPE
0.00.044.318 I print_info: n_vocab          = 50304
0.00.044.318 I print_info: n_merges         = 50009
0.00.044.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.318 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.318 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.318 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.320 I print_info: LF token         = 187 ''
0.00.044.320 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.320 I print_info: max token length = 1024
0.00.044.321 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.663.594 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.609 I load_tensors: offloading output layer to GPU
0.00.663.609 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.642 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.663.643 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.665.031 I llama_context_unified: n_seq_max     = 1
0.00.665.039 I llama_context_unified: n_ctx         = 128
0.00.665.040 I llama_context_unified: n_ctx_per_seq = 128
0.00.665.040 I llama_context_unified: n_batch       = 128
0.00.665.040 I llama_context_unified: n_ubatch      = 128
0.00.665.041 I llama_context_unified: flash_attn    = 0
0.00.665.043 I llama_context_unified: freq_base     = 10000.0
0.00.665.044 I llama_context_unified: freq_scale    = 1
0.00.665.044 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.046 I ggml_metal_init: allocating
0.00.665.156 I ggml_metal_init: found device: Apple M4
0.00.665.173 I ggml_metal_init: picking default device: Apple M4
0.00.667.063 I ggml_metal_init: using embedded metal library
0.00.672.780 I ggml_metal_init: GPU name:   Apple M4
0.00.672.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.672.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.672.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.672.791 I ggml_metal_init: simdgroup reduction   = true
0.00.672.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.672.792 I ggml_metal_init: has residency sets    = true
0.00.672.792 I ggml_metal_init: has bfloat            = true
0.00.672.792 I ggml_metal_init: use bfloat            = true
0.00.672.794 I ggml_metal_init: hasUnifiedMemory      = true
0.00.672.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.487 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.004 I init:      Metal KV buffer size =    24.00 MiB
0.00.696.007 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.696.034 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.699.424 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.699.426 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.699.426 I llama_context_unified: graph nodes  = 967
0.00.699.426 I llama_context_unified: graph splits = 2
0.00.699.430 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.699.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.725.005 I 
0.00.725.079 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.725.100 I perplexity: tokenizing the input ..
0.00.732.456 I perplexity: tokenization took 7.352 ms
0.00.732.478 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.866.512 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.867.851 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.867.864 I llama_perf_context_print:        load time =     714.90 ms
0.00.867.865 I llama_perf_context_print: prompt eval time =     133.10 ms /   128 tokens (    1.04 ms per token,   961.70 tokens per second)
0.00.867.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.867.866 I llama_perf_context_print:       total time =     142.86 ms /   129 tokens
0.00.868.453 I ggml_metal_free: deallocating

real	0m0.886s
user	0m0.081s
sys	0m0.117s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.654 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.026 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.030 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.032 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.032 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.033 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.039 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.045 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.045 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.737 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.739 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.740 I llama_model_loader: - type  f32:  194 tensors
0.00.034.740 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.740 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.741 I print_info: file format = GGUF V3 (latest)
0.00.034.741 I print_info: file type   = Q4_1
0.00.034.742 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.505 I load: special tokens cache size = 25
0.00.050.164 I load: token to piece cache size = 0.2984 MB
0.00.050.167 I print_info: arch             = gptneox
0.00.050.167 I print_info: vocab_only       = 0
0.00.050.167 I print_info: n_ctx_train      = 2048
0.00.050.168 I print_info: n_embd           = 2048
0.00.050.168 I print_info: n_layer          = 24
0.00.050.171 I print_info: n_head           = 16
0.00.050.172 I print_info: n_head_kv        = 16
0.00.050.172 I print_info: n_rot            = 32
0.00.050.172 I print_info: n_swa            = 0
0.00.050.172 I print_info: n_embd_head_k    = 128
0.00.050.172 I print_info: n_embd_head_v    = 128
0.00.050.173 I print_info: n_gqa            = 1
0.00.050.174 I print_info: n_embd_k_gqa     = 2048
0.00.050.175 I print_info: n_embd_v_gqa     = 2048
0.00.050.175 I print_info: f_norm_eps       = 1.0e-05
0.00.050.176 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.176 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.176 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.176 I print_info: f_logit_scale    = 0.0e+00
0.00.050.177 I print_info: n_ff             = 8192
0.00.050.177 I print_info: n_expert         = 0
0.00.050.177 I print_info: n_expert_used    = 0
0.00.050.177 I print_info: causal attn      = 1
0.00.050.177 I print_info: pooling type     = 0
0.00.050.180 I print_info: rope type        = 2
0.00.050.181 I print_info: rope scaling     = linear
0.00.050.182 I print_info: freq_base_train  = 10000.0
0.00.050.182 I print_info: freq_scale_train = 1
0.00.050.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.182 I print_info: rope_finetuned   = unknown
0.00.050.183 I print_info: ssm_d_conv       = 0
0.00.050.183 I print_info: ssm_d_inner      = 0
0.00.050.183 I print_info: ssm_d_state      = 0
0.00.050.183 I print_info: ssm_dt_rank      = 0
0.00.050.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.183 I print_info: model type       = 1.4B
0.00.050.184 I print_info: model params     = 1.41 B
0.00.050.184 I print_info: general.name     = 1.4B
0.00.050.184 I print_info: vocab type       = BPE
0.00.050.184 I print_info: n_vocab          = 50304
0.00.050.190 I print_info: n_merges         = 50009
0.00.050.192 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.192 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.192 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.192 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.192 I print_info: LF token         = 187 ''
0.00.050.194 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.194 I print_info: max token length = 1024
0.00.050.195 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.685.698 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.712 I load_tensors: offloading output layer to GPU
0.00.685.713 I load_tensors: offloaded 25/25 layers to GPU
0.00.685.747 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.685.749 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.687.449 I llama_context_unified: n_seq_max     = 1
0.00.687.452 I llama_context_unified: n_ctx         = 2048
0.00.687.453 I llama_context_unified: n_ctx_per_seq = 2048
0.00.687.454 I llama_context_unified: n_batch       = 2048
0.00.687.454 I llama_context_unified: n_ubatch      = 512
0.00.687.455 I llama_context_unified: flash_attn    = 0
0.00.687.457 I llama_context_unified: freq_base     = 10000.0
0.00.687.457 I llama_context_unified: freq_scale    = 1
0.00.687.460 I ggml_metal_init: allocating
0.00.687.528 I ggml_metal_init: found device: Apple M4
0.00.687.541 I ggml_metal_init: picking default device: Apple M4
0.00.689.426 I ggml_metal_init: using embedded metal library
0.00.696.309 I ggml_metal_init: GPU name:   Apple M4
0.00.696.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.315 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.316 I ggml_metal_init: simdgroup reduction   = true
0.00.696.317 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.317 I ggml_metal_init: has residency sets    = true
0.00.696.317 I ggml_metal_init: has bfloat            = true
0.00.696.317 I ggml_metal_init: use bfloat            = true
0.00.696.318 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.320 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.279 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.772.715 I init:      Metal KV buffer size =   384.00 MiB
0.00.772.722 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.772.746 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.777.415 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.777.417 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.777.417 I llama_context_unified: graph nodes  = 967
0.00.777.418 I llama_context_unified: graph splits = 2
0.00.777.423 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.777.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.777.537 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.831.439 I main: llama threadpool init, n_threads = 4
0.00.831.483 I 
0.00.831.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.831.507 I 
0.00.831.678 I sampler seed: 1234
0.00.831.683 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.831.694 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.831.694 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.831.694 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.571.228 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.571.228 I llama_perf_context_print:        load time =     822.09 ms
0.01.571.229 I llama_perf_context_print: prompt eval time =      49.17 ms /     7 tokens (    7.02 ms per token,   142.37 tokens per second)
0.01.571.230 I llama_perf_context_print:        eval time =     687.49 ms /    63 runs   (   10.91 ms per token,    91.64 tokens per second)
0.01.571.230 I llama_perf_context_print:       total time =     740.48 ms /    70 tokens
0.01.575.236 I ggml_metal_free: deallocating

real	0m1.590s
user	0m0.111s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.208 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.168 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.175 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.182 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.183 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.185 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.186 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.188 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.190 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.191 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.042 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.086 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.009 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.012 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.012 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.013 I llama_model_loader: - type  f32:  194 tensors
0.00.033.013 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.014 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.014 I print_info: file format = GGUF V3 (latest)
0.00.033.015 I print_info: file type   = Q4_1
0.00.033.016 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.041.443 I load: special tokens cache size = 25
0.00.047.549 I load: token to piece cache size = 0.2984 MB
0.00.047.554 I print_info: arch             = gptneox
0.00.047.554 I print_info: vocab_only       = 0
0.00.047.555 I print_info: n_ctx_train      = 2048
0.00.047.555 I print_info: n_embd           = 2048
0.00.047.555 I print_info: n_layer          = 24
0.00.047.559 I print_info: n_head           = 16
0.00.047.560 I print_info: n_head_kv        = 16
0.00.047.560 I print_info: n_rot            = 32
0.00.047.562 I print_info: n_swa            = 0
0.00.047.562 I print_info: n_embd_head_k    = 128
0.00.047.562 I print_info: n_embd_head_v    = 128
0.00.047.563 I print_info: n_gqa            = 1
0.00.047.564 I print_info: n_embd_k_gqa     = 2048
0.00.047.565 I print_info: n_embd_v_gqa     = 2048
0.00.047.565 I print_info: f_norm_eps       = 1.0e-05
0.00.047.566 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.566 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.566 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.566 I print_info: f_logit_scale    = 0.0e+00
0.00.047.567 I print_info: n_ff             = 8192
0.00.047.567 I print_info: n_expert         = 0
0.00.047.567 I print_info: n_expert_used    = 0
0.00.047.567 I print_info: causal attn      = 1
0.00.047.568 I print_info: pooling type     = 0
0.00.047.572 I print_info: rope type        = 2
0.00.047.572 I print_info: rope scaling     = linear
0.00.047.573 I print_info: freq_base_train  = 10000.0
0.00.047.573 I print_info: freq_scale_train = 1
0.00.047.573 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.573 I print_info: rope_finetuned   = unknown
0.00.047.573 I print_info: ssm_d_conv       = 0
0.00.047.574 I print_info: ssm_d_inner      = 0
0.00.047.574 I print_info: ssm_d_state      = 0
0.00.047.574 I print_info: ssm_dt_rank      = 0
0.00.047.574 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.574 I print_info: model type       = 1.4B
0.00.047.576 I print_info: model params     = 1.41 B
0.00.047.576 I print_info: general.name     = 1.4B
0.00.047.576 I print_info: vocab type       = BPE
0.00.047.576 I print_info: n_vocab          = 50304
0.00.047.576 I print_info: n_merges         = 50009
0.00.047.577 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.577 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.577 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.577 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.577 I print_info: LF token         = 187 ''
0.00.047.578 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.578 I print_info: max token length = 1024
0.00.047.578 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.761.956 I load_tensors: offloading 24 repeating layers to GPU
0.00.761.977 I load_tensors: offloading output layer to GPU
0.00.761.978 I load_tensors: offloaded 25/25 layers to GPU
0.00.762.013 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.762.015 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.763.382 I llama_context_unified: n_seq_max     = 1
0.00.763.385 I llama_context_unified: n_ctx         = 128
0.00.763.386 I llama_context_unified: n_ctx_per_seq = 128
0.00.763.386 I llama_context_unified: n_batch       = 128
0.00.763.387 I llama_context_unified: n_ubatch      = 128
0.00.763.387 I llama_context_unified: flash_attn    = 0
0.00.763.389 I llama_context_unified: freq_base     = 10000.0
0.00.763.390 I llama_context_unified: freq_scale    = 1
0.00.763.391 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.763.393 I ggml_metal_init: allocating
0.00.763.464 I ggml_metal_init: found device: Apple M4
0.00.763.477 I ggml_metal_init: picking default device: Apple M4
0.00.765.548 I ggml_metal_init: using embedded metal library
0.00.771.431 I ggml_metal_init: GPU name:   Apple M4
0.00.771.459 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.771.460 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.771.461 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.771.462 I ggml_metal_init: simdgroup reduction   = true
0.00.771.462 I ggml_metal_init: simdgroup matrix mul. = true
0.00.771.462 I ggml_metal_init: has residency sets    = true
0.00.771.463 I ggml_metal_init: has bfloat            = true
0.00.771.463 I ggml_metal_init: use bfloat            = true
0.00.771.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.771.470 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.791.429 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.794.805 I init:      Metal KV buffer size =    24.00 MiB
0.00.794.812 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.794.844 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.798.154 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.798.156 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.798.157 I llama_context_unified: graph nodes  = 967
0.00.798.157 I llama_context_unified: graph splits = 2
0.00.798.161 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.798.161 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.823.766 I 
0.00.823.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.823.837 I perplexity: tokenizing the input ..
0.00.831.397 I perplexity: tokenization took 7.555 ms
0.00.831.418 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.968.681 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.969.992 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.970.006 I llama_perf_context_print:        load time =     814.55 ms
0.00.970.007 I llama_perf_context_print: prompt eval time =     136.32 ms /   128 tokens (    1.06 ms per token,   938.97 tokens per second)
0.00.970.008 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.970.009 I llama_perf_context_print:       total time =     146.24 ms /   129 tokens
0.00.970.581 I ggml_metal_free: deallocating

real	0m0.984s
user	0m0.082s
sys	0m0.141s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.021.116 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.117 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.044.124 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.133 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.134 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.134 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.139 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.142 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.144 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.144 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.437 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.856 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.857 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.859 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.860 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.059.862 I llama_model_loader: - type  f32:  194 tensors
0.00.059.863 I llama_model_loader: - type q5_0:   97 tensors
0.00.059.864 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.868 I print_info: file format = GGUF V3 (latest)
0.00.059.876 I print_info: file type   = Q5_0
0.00.059.904 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.078.027 I load: special tokens cache size = 25
0.00.090.029 I load: token to piece cache size = 0.2984 MB
0.00.090.034 I print_info: arch             = gptneox
0.00.090.035 I print_info: vocab_only       = 0
0.00.090.035 I print_info: n_ctx_train      = 2048
0.00.090.035 I print_info: n_embd           = 2048
0.00.090.036 I print_info: n_layer          = 24
0.00.090.039 I print_info: n_head           = 16
0.00.090.041 I print_info: n_head_kv        = 16
0.00.090.043 I print_info: n_rot            = 32
0.00.090.043 I print_info: n_swa            = 0
0.00.090.044 I print_info: n_embd_head_k    = 128
0.00.090.044 I print_info: n_embd_head_v    = 128
0.00.090.045 I print_info: n_gqa            = 1
0.00.090.046 I print_info: n_embd_k_gqa     = 2048
0.00.090.047 I print_info: n_embd_v_gqa     = 2048
0.00.090.048 I print_info: f_norm_eps       = 1.0e-05
0.00.090.048 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.049 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.049 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.049 I print_info: f_logit_scale    = 0.0e+00
0.00.090.050 I print_info: n_ff             = 8192
0.00.090.050 I print_info: n_expert         = 0
0.00.090.051 I print_info: n_expert_used    = 0
0.00.090.051 I print_info: causal attn      = 1
0.00.090.051 I print_info: pooling type     = 0
0.00.090.051 I print_info: rope type        = 2
0.00.090.052 I print_info: rope scaling     = linear
0.00.090.052 I print_info: freq_base_train  = 10000.0
0.00.090.053 I print_info: freq_scale_train = 1
0.00.090.054 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.054 I print_info: rope_finetuned   = unknown
0.00.090.054 I print_info: ssm_d_conv       = 0
0.00.090.057 I print_info: ssm_d_inner      = 0
0.00.090.057 I print_info: ssm_d_state      = 0
0.00.090.057 I print_info: ssm_dt_rank      = 0
0.00.090.057 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.057 I print_info: model type       = 1.4B
0.00.090.058 I print_info: model params     = 1.41 B
0.00.090.058 I print_info: general.name     = 1.4B
0.00.090.065 I print_info: vocab type       = BPE
0.00.090.065 I print_info: n_vocab          = 50304
0.00.090.065 I print_info: n_merges         = 50009
0.00.090.066 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.068 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.068 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.068 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.069 I print_info: LF token         = 187 ''
0.00.090.069 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.069 I print_info: max token length = 1024
0.00.090.070 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.786.019 I load_tensors: offloading 24 repeating layers to GPU
0.00.786.035 I load_tensors: offloading output layer to GPU
0.00.786.036 I load_tensors: offloaded 25/25 layers to GPU
0.00.786.071 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.786.078 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.787.518 I llama_context_unified: n_seq_max     = 1
0.00.787.521 I llama_context_unified: n_ctx         = 2048
0.00.787.522 I llama_context_unified: n_ctx_per_seq = 2048
0.00.787.522 I llama_context_unified: n_batch       = 2048
0.00.787.523 I llama_context_unified: n_ubatch      = 512
0.00.787.523 I llama_context_unified: flash_attn    = 0
0.00.787.526 I llama_context_unified: freq_base     = 10000.0
0.00.787.526 I llama_context_unified: freq_scale    = 1
0.00.787.528 I ggml_metal_init: allocating
0.00.787.612 I ggml_metal_init: found device: Apple M4
0.00.787.625 I ggml_metal_init: picking default device: Apple M4
0.00.789.542 I ggml_metal_init: using embedded metal library
0.00.796.290 I ggml_metal_init: GPU name:   Apple M4
0.00.796.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.796.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.796.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.796.296 I ggml_metal_init: simdgroup reduction   = true
0.00.796.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.796.297 I ggml_metal_init: has residency sets    = true
0.00.796.297 I ggml_metal_init: has bfloat            = true
0.00.796.297 I ggml_metal_init: use bfloat            = true
0.00.796.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.796.300 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.813.737 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.872.177 I init:      Metal KV buffer size =   384.00 MiB
0.00.872.183 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.872.205 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.876.900 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.876.902 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.876.902 I llama_context_unified: graph nodes  = 967
0.00.876.902 I llama_context_unified: graph splits = 2
0.00.876.909 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.877.032 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.877.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.934.463 I main: llama threadpool init, n_threads = 4
0.00.934.506 I 
0.00.934.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.934.530 I 
0.00.934.682 I sampler seed: 1234
0.00.934.687 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.934.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.934.698 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.934.698 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.727.467 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.727.468 I llama_perf_context_print:        load time =     912.64 ms
0.01.727.468 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.32 tokens per second)
0.01.727.469 I llama_perf_context_print:        eval time =     746.72 ms /    63 runs   (   11.85 ms per token,    84.37 tokens per second)
0.01.727.469 I llama_perf_context_print:       total time =     793.71 ms /    70 tokens
0.01.731.674 I ggml_metal_free: deallocating

real	0m1.773s
user	0m0.136s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.935 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.566 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.318 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.034.799 I llama_model_loader: - type  f32:  194 tensors
0.00.034.799 I llama_model_loader: - type q5_0:   97 tensors
0.00.034.800 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.800 I print_info: file format = GGUF V3 (latest)
0.00.034.801 I print_info: file type   = Q5_0
0.00.034.802 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.764 I load: special tokens cache size = 25
0.00.052.637 I load: token to piece cache size = 0.2984 MB
0.00.052.641 I print_info: arch             = gptneox
0.00.052.642 I print_info: vocab_only       = 0
0.00.052.642 I print_info: n_ctx_train      = 2048
0.00.052.642 I print_info: n_embd           = 2048
0.00.052.643 I print_info: n_layer          = 24
0.00.052.647 I print_info: n_head           = 16
0.00.052.650 I print_info: n_head_kv        = 16
0.00.052.651 I print_info: n_rot            = 32
0.00.052.651 I print_info: n_swa            = 0
0.00.052.651 I print_info: n_embd_head_k    = 128
0.00.052.651 I print_info: n_embd_head_v    = 128
0.00.052.652 I print_info: n_gqa            = 1
0.00.052.653 I print_info: n_embd_k_gqa     = 2048
0.00.052.655 I print_info: n_embd_v_gqa     = 2048
0.00.052.656 I print_info: f_norm_eps       = 1.0e-05
0.00.052.657 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.657 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.657 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.657 I print_info: f_logit_scale    = 0.0e+00
0.00.052.658 I print_info: n_ff             = 8192
0.00.052.658 I print_info: n_expert         = 0
0.00.052.659 I print_info: n_expert_used    = 0
0.00.052.659 I print_info: causal attn      = 1
0.00.052.659 I print_info: pooling type     = 0
0.00.052.659 I print_info: rope type        = 2
0.00.052.660 I print_info: rope scaling     = linear
0.00.052.662 I print_info: freq_base_train  = 10000.0
0.00.052.662 I print_info: freq_scale_train = 1
0.00.052.662 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.663 I print_info: rope_finetuned   = unknown
0.00.052.663 I print_info: ssm_d_conv       = 0
0.00.052.663 I print_info: ssm_d_inner      = 0
0.00.052.664 I print_info: ssm_d_state      = 0
0.00.052.665 I print_info: ssm_dt_rank      = 0
0.00.052.666 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.666 I print_info: model type       = 1.4B
0.00.052.666 I print_info: model params     = 1.41 B
0.00.052.667 I print_info: general.name     = 1.4B
0.00.052.667 I print_info: vocab type       = BPE
0.00.052.668 I print_info: n_vocab          = 50304
0.00.052.668 I print_info: n_merges         = 50009
0.00.052.668 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.669 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.669 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.669 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.670 I print_info: LF token         = 187 ''
0.00.052.672 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.672 I print_info: max token length = 1024
0.00.052.673 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.763.969 I load_tensors: offloading 24 repeating layers to GPU
0.00.763.989 I load_tensors: offloading output layer to GPU
0.00.763.990 I load_tensors: offloaded 25/25 layers to GPU
0.00.764.022 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.764.024 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.765.325 I llama_context_unified: n_seq_max     = 1
0.00.765.329 I llama_context_unified: n_ctx         = 128
0.00.765.329 I llama_context_unified: n_ctx_per_seq = 128
0.00.765.330 I llama_context_unified: n_batch       = 128
0.00.765.330 I llama_context_unified: n_ubatch      = 128
0.00.765.330 I llama_context_unified: flash_attn    = 0
0.00.765.332 I llama_context_unified: freq_base     = 10000.0
0.00.765.333 I llama_context_unified: freq_scale    = 1
0.00.765.333 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.765.336 I ggml_metal_init: allocating
0.00.765.420 I ggml_metal_init: found device: Apple M4
0.00.765.434 I ggml_metal_init: picking default device: Apple M4
0.00.767.423 I ggml_metal_init: using embedded metal library
0.00.773.568 I ggml_metal_init: GPU name:   Apple M4
0.00.773.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.773.574 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.773.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.773.576 I ggml_metal_init: simdgroup reduction   = true
0.00.773.576 I ggml_metal_init: simdgroup matrix mul. = true
0.00.773.576 I ggml_metal_init: has residency sets    = true
0.00.773.577 I ggml_metal_init: has bfloat            = true
0.00.773.577 I ggml_metal_init: use bfloat            = true
0.00.773.578 I ggml_metal_init: hasUnifiedMemory      = true
0.00.773.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.792.319 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.795.876 I init:      Metal KV buffer size =    24.00 MiB
0.00.795.879 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.795.904 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.799.104 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.799.106 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.799.106 I llama_context_unified: graph nodes  = 967
0.00.799.107 I llama_context_unified: graph splits = 2
0.00.799.111 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.799.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.312 I 
0.00.827.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.393 I perplexity: tokenizing the input ..
0.00.834.894 I perplexity: tokenization took 7.497 ms
0.00.834.918 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.982.681 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.984.009 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.984.026 I llama_perf_context_print:        load time =     812.37 ms
0.00.984.027 I llama_perf_context_print: prompt eval time =     146.80 ms /   128 tokens (    1.15 ms per token,   871.95 tokens per second)
0.00.984.028 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.984.028 I llama_perf_context_print:       total time =     156.72 ms /   129 tokens
0.00.984.646 I ggml_metal_free: deallocating

real	0m1.014s
user	0m0.086s
sys	0m0.155s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.624 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.625 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.625 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.625 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.626 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.626 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.627 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.631 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.362 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.151 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.152 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.152 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.152 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.153 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.153 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.153 I llama_model_loader: - type  f32:  194 tensors
0.00.033.154 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.154 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.154 I print_info: file format = GGUF V3 (latest)
0.00.033.155 I print_info: file type   = Q5_1
0.00.033.155 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.041.054 I load: special tokens cache size = 25
0.00.046.802 I load: token to piece cache size = 0.2984 MB
0.00.046.805 I print_info: arch             = gptneox
0.00.046.805 I print_info: vocab_only       = 0
0.00.046.805 I print_info: n_ctx_train      = 2048
0.00.046.805 I print_info: n_embd           = 2048
0.00.046.806 I print_info: n_layer          = 24
0.00.046.808 I print_info: n_head           = 16
0.00.046.809 I print_info: n_head_kv        = 16
0.00.046.809 I print_info: n_rot            = 32
0.00.046.809 I print_info: n_swa            = 0
0.00.046.810 I print_info: n_embd_head_k    = 128
0.00.046.810 I print_info: n_embd_head_v    = 128
0.00.046.811 I print_info: n_gqa            = 1
0.00.046.812 I print_info: n_embd_k_gqa     = 2048
0.00.046.812 I print_info: n_embd_v_gqa     = 2048
0.00.046.813 I print_info: f_norm_eps       = 1.0e-05
0.00.046.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.816 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.816 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.816 I print_info: f_logit_scale    = 0.0e+00
0.00.046.817 I print_info: n_ff             = 8192
0.00.046.817 I print_info: n_expert         = 0
0.00.046.817 I print_info: n_expert_used    = 0
0.00.046.817 I print_info: causal attn      = 1
0.00.046.817 I print_info: pooling type     = 0
0.00.046.819 I print_info: rope type        = 2
0.00.046.820 I print_info: rope scaling     = linear
0.00.046.820 I print_info: freq_base_train  = 10000.0
0.00.046.821 I print_info: freq_scale_train = 1
0.00.046.821 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.821 I print_info: rope_finetuned   = unknown
0.00.046.821 I print_info: ssm_d_conv       = 0
0.00.046.821 I print_info: ssm_d_inner      = 0
0.00.046.821 I print_info: ssm_d_state      = 0
0.00.046.822 I print_info: ssm_dt_rank      = 0
0.00.046.822 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.822 I print_info: model type       = 1.4B
0.00.046.822 I print_info: model params     = 1.41 B
0.00.046.822 I print_info: general.name     = 1.4B
0.00.046.823 I print_info: vocab type       = BPE
0.00.046.823 I print_info: n_vocab          = 50304
0.00.046.823 I print_info: n_merges         = 50009
0.00.046.824 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.824 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.824 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.824 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.828 I print_info: LF token         = 187 ''
0.00.046.828 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.830 I print_info: max token length = 1024
0.00.046.830 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.766.171 I load_tensors: offloading 24 repeating layers to GPU
0.00.766.175 I load_tensors: offloading output layer to GPU
0.00.766.176 I load_tensors: offloaded 25/25 layers to GPU
0.00.766.194 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.766.195 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.767.189 I llama_context_unified: n_seq_max     = 1
0.00.767.195 I llama_context_unified: n_ctx         = 2048
0.00.767.196 I llama_context_unified: n_ctx_per_seq = 2048
0.00.767.196 I llama_context_unified: n_batch       = 2048
0.00.767.196 I llama_context_unified: n_ubatch      = 512
0.00.767.197 I llama_context_unified: flash_attn    = 0
0.00.767.198 I llama_context_unified: freq_base     = 10000.0
0.00.767.198 I llama_context_unified: freq_scale    = 1
0.00.767.199 I ggml_metal_init: allocating
0.00.767.238 I ggml_metal_init: found device: Apple M4
0.00.767.249 I ggml_metal_init: picking default device: Apple M4
0.00.768.439 I ggml_metal_init: using embedded metal library
0.00.772.743 I ggml_metal_init: GPU name:   Apple M4
0.00.772.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.772.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.772.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.772.748 I ggml_metal_init: simdgroup reduction   = true
0.00.772.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.772.748 I ggml_metal_init: has residency sets    = true
0.00.772.748 I ggml_metal_init: has bfloat            = true
0.00.772.748 I ggml_metal_init: use bfloat            = true
0.00.772.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.772.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.783.209 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.815.293 I init:      Metal KV buffer size =   384.00 MiB
0.00.815.299 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.815.323 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.820.688 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.820.690 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.820.690 I llama_context_unified: graph nodes  = 967
0.00.820.690 I llama_context_unified: graph splits = 2
0.00.820.697 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.820.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.820.822 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.881.554 I main: llama threadpool init, n_threads = 4
0.00.881.595 I 
0.00.881.614 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.881.616 I 
0.00.881.791 I sampler seed: 1234
0.00.881.795 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.881.806 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.881.806 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.881.806 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.731.370 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.731.370 I llama_perf_context_print:        load time =     872.06 ms
0.01.731.371 I llama_perf_context_print: prompt eval time =      51.94 ms /     7 tokens (    7.42 ms per token,   134.77 tokens per second)
0.01.731.372 I llama_perf_context_print:        eval time =     794.65 ms /    63 runs   (   12.61 ms per token,    79.28 tokens per second)
0.01.731.372 I llama_perf_context_print:       total time =     850.51 ms /    70 tokens
0.01.735.059 I ggml_metal_free: deallocating

real	0m1.750s
user	0m0.099s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.254 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.646 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.655 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.656 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.657 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.657 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.661 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.661 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.662 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.662 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.662 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.664 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.665 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.351 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.352 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.352 I llama_model_loader: - type  f32:  194 tensors
0.00.025.353 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.353 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.353 I print_info: file format = GGUF V3 (latest)
0.00.025.356 I print_info: file type   = Q5_1
0.00.025.357 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.615 I load: special tokens cache size = 25
0.00.039.335 I load: token to piece cache size = 0.2984 MB
0.00.039.339 I print_info: arch             = gptneox
0.00.039.339 I print_info: vocab_only       = 0
0.00.039.339 I print_info: n_ctx_train      = 2048
0.00.039.339 I print_info: n_embd           = 2048
0.00.039.340 I print_info: n_layer          = 24
0.00.039.344 I print_info: n_head           = 16
0.00.039.347 I print_info: n_head_kv        = 16
0.00.039.347 I print_info: n_rot            = 32
0.00.039.348 I print_info: n_swa            = 0
0.00.039.348 I print_info: n_embd_head_k    = 128
0.00.039.348 I print_info: n_embd_head_v    = 128
0.00.039.349 I print_info: n_gqa            = 1
0.00.039.350 I print_info: n_embd_k_gqa     = 2048
0.00.039.350 I print_info: n_embd_v_gqa     = 2048
0.00.039.351 I print_info: f_norm_eps       = 1.0e-05
0.00.039.351 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.352 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.352 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.352 I print_info: f_logit_scale    = 0.0e+00
0.00.039.353 I print_info: n_ff             = 8192
0.00.039.353 I print_info: n_expert         = 0
0.00.039.353 I print_info: n_expert_used    = 0
0.00.039.353 I print_info: causal attn      = 1
0.00.039.353 I print_info: pooling type     = 0
0.00.039.353 I print_info: rope type        = 2
0.00.039.354 I print_info: rope scaling     = linear
0.00.039.354 I print_info: freq_base_train  = 10000.0
0.00.039.355 I print_info: freq_scale_train = 1
0.00.039.355 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.357 I print_info: rope_finetuned   = unknown
0.00.039.357 I print_info: ssm_d_conv       = 0
0.00.039.357 I print_info: ssm_d_inner      = 0
0.00.039.357 I print_info: ssm_d_state      = 0
0.00.039.357 I print_info: ssm_dt_rank      = 0
0.00.039.357 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.357 I print_info: model type       = 1.4B
0.00.039.358 I print_info: model params     = 1.41 B
0.00.039.358 I print_info: general.name     = 1.4B
0.00.039.359 I print_info: vocab type       = BPE
0.00.039.359 I print_info: n_vocab          = 50304
0.00.039.359 I print_info: n_merges         = 50009
0.00.039.359 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.360 I print_info: LF token         = 187 ''
0.00.039.361 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.361 I print_info: max token length = 1024
0.00.039.361 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.746.260 I load_tensors: offloading 24 repeating layers to GPU
0.00.746.278 I load_tensors: offloading output layer to GPU
0.00.746.279 I load_tensors: offloaded 25/25 layers to GPU
0.00.746.310 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.746.312 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.747.717 I llama_context_unified: n_seq_max     = 1
0.00.747.721 I llama_context_unified: n_ctx         = 128
0.00.747.721 I llama_context_unified: n_ctx_per_seq = 128
0.00.747.722 I llama_context_unified: n_batch       = 128
0.00.747.722 I llama_context_unified: n_ubatch      = 128
0.00.747.723 I llama_context_unified: flash_attn    = 0
0.00.747.725 I llama_context_unified: freq_base     = 10000.0
0.00.747.726 I llama_context_unified: freq_scale    = 1
0.00.747.726 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.747.729 I ggml_metal_init: allocating
0.00.747.804 I ggml_metal_init: found device: Apple M4
0.00.747.818 I ggml_metal_init: picking default device: Apple M4
0.00.749.701 I ggml_metal_init: using embedded metal library
0.00.756.559 I ggml_metal_init: GPU name:   Apple M4
0.00.756.565 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.756.566 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.756.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.756.567 I ggml_metal_init: simdgroup reduction   = true
0.00.756.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.756.567 I ggml_metal_init: has residency sets    = true
0.00.756.568 I ggml_metal_init: has bfloat            = true
0.00.756.568 I ggml_metal_init: use bfloat            = true
0.00.756.569 I ggml_metal_init: hasUnifiedMemory      = true
0.00.756.572 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.774.449 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.777.954 I init:      Metal KV buffer size =    24.00 MiB
0.00.777.958 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.777.982 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.781.211 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.781.213 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.781.213 I llama_context_unified: graph nodes  = 967
0.00.781.214 I llama_context_unified: graph splits = 2
0.00.781.218 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.781.220 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.810.472 I 
0.00.810.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.542 I perplexity: tokenizing the input ..
0.00.817.576 I perplexity: tokenization took 7.031 ms
0.00.817.594 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.966.121 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.967.410 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.967.422 I llama_perf_context_print:        load time =     801.21 ms
0.00.967.423 I llama_perf_context_print: prompt eval time =     147.52 ms /   128 tokens (    1.15 ms per token,   867.69 tokens per second)
0.00.967.423 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.967.424 I llama_perf_context_print:       total time =     156.95 ms /   129 tokens
0.00.967.961 I ggml_metal_free: deallocating

real	0m0.981s
user	0m0.081s
sys	0m0.150s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.695 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.176 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.181 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.184 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.185 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.186 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.186 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.187 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.188 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.190 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.193 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.193 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.194 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.500 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.501 I llama_model_loader: - type  f32:  194 tensors
0.00.024.502 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.502 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.502 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.503 I print_info: file format = GGUF V3 (latest)
0.00.024.503 I print_info: file type   = Q2_K - Medium
0.00.024.504 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.327 I load: special tokens cache size = 25
0.00.038.012 I load: token to piece cache size = 0.2984 MB
0.00.038.014 I print_info: arch             = gptneox
0.00.038.014 I print_info: vocab_only       = 0
0.00.038.015 I print_info: n_ctx_train      = 2048
0.00.038.015 I print_info: n_embd           = 2048
0.00.038.015 I print_info: n_layer          = 24
0.00.038.017 I print_info: n_head           = 16
0.00.038.018 I print_info: n_head_kv        = 16
0.00.038.018 I print_info: n_rot            = 32
0.00.038.018 I print_info: n_swa            = 0
0.00.038.019 I print_info: n_embd_head_k    = 128
0.00.038.019 I print_info: n_embd_head_v    = 128
0.00.038.020 I print_info: n_gqa            = 1
0.00.038.020 I print_info: n_embd_k_gqa     = 2048
0.00.038.023 I print_info: n_embd_v_gqa     = 2048
0.00.038.023 I print_info: f_norm_eps       = 1.0e-05
0.00.038.023 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.024 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.024 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.024 I print_info: f_logit_scale    = 0.0e+00
0.00.038.025 I print_info: n_ff             = 8192
0.00.038.025 I print_info: n_expert         = 0
0.00.038.026 I print_info: n_expert_used    = 0
0.00.038.026 I print_info: causal attn      = 1
0.00.038.032 I print_info: pooling type     = 0
0.00.038.032 I print_info: rope type        = 2
0.00.038.033 I print_info: rope scaling     = linear
0.00.038.033 I print_info: freq_base_train  = 10000.0
0.00.038.035 I print_info: freq_scale_train = 1
0.00.038.036 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.036 I print_info: rope_finetuned   = unknown
0.00.038.036 I print_info: ssm_d_conv       = 0
0.00.038.036 I print_info: ssm_d_inner      = 0
0.00.038.036 I print_info: ssm_d_state      = 0
0.00.038.036 I print_info: ssm_dt_rank      = 0
0.00.038.039 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.039 I print_info: model type       = 1.4B
0.00.038.040 I print_info: model params     = 1.41 B
0.00.038.041 I print_info: general.name     = 1.4B
0.00.038.041 I print_info: vocab type       = BPE
0.00.038.042 I print_info: n_vocab          = 50304
0.00.038.042 I print_info: n_merges         = 50009
0.00.038.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.042 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.042 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.043 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.043 I print_info: LF token         = 187 ''
0.00.038.043 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.043 I print_info: max token length = 1024
0.00.038.044 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.345.390 I load_tensors: offloading 24 repeating layers to GPU
0.00.345.398 I load_tensors: offloading output layer to GPU
0.00.345.399 I load_tensors: offloaded 25/25 layers to GPU
0.00.345.430 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.345.436 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.346.871 I llama_context_unified: n_seq_max     = 1
0.00.346.876 I llama_context_unified: n_ctx         = 2048
0.00.346.877 I llama_context_unified: n_ctx_per_seq = 2048
0.00.346.877 I llama_context_unified: n_batch       = 2048
0.00.346.877 I llama_context_unified: n_ubatch      = 512
0.00.346.878 I llama_context_unified: flash_attn    = 0
0.00.346.880 I llama_context_unified: freq_base     = 10000.0
0.00.346.880 I llama_context_unified: freq_scale    = 1
0.00.346.882 I ggml_metal_init: allocating
0.00.346.986 I ggml_metal_init: found device: Apple M4
0.00.347.000 I ggml_metal_init: picking default device: Apple M4
0.00.348.888 I ggml_metal_init: using embedded metal library
0.00.354.359 I ggml_metal_init: GPU name:   Apple M4
0.00.354.369 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.354.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.354.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.354.371 I ggml_metal_init: simdgroup reduction   = true
0.00.354.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.354.372 I ggml_metal_init: has residency sets    = true
0.00.354.372 I ggml_metal_init: has bfloat            = true
0.00.354.372 I ggml_metal_init: use bfloat            = true
0.00.354.374 I ggml_metal_init: hasUnifiedMemory      = true
0.00.354.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.702 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.438.181 I init:      Metal KV buffer size =   384.00 MiB
0.00.438.188 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.438.212 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.443.656 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.443.658 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.443.659 I llama_context_unified: graph nodes  = 967
0.00.443.659 I llama_context_unified: graph splits = 2
0.00.443.670 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.443.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.443.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.805 I main: llama threadpool init, n_threads = 4
0.00.500.848 I 
0.00.500.870 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.870 I 
0.00.501.049 I sampler seed: 1234
0.00.501.054 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.064 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.066 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.066 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.175.406 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.175.407 I llama_perf_context_print:        load time =     490.41 ms
0.01.175.408 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.80 tokens per second)
0.01.175.409 I llama_perf_context_print:        eval time =     635.67 ms /    63 runs   (   10.09 ms per token,    99.11 tokens per second)
0.01.175.409 I llama_perf_context_print:       total time =     675.30 ms /    70 tokens
0.01.179.331 I ggml_metal_free: deallocating

real	0m1.199s
user	0m0.111s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.501 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.189 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.192 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.193 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.195 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.197 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.294 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.620 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.621 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.622 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.622 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.623 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.623 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.623 I llama_model_loader: - type  f32:  194 tensors
0.00.033.624 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.624 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.624 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.625 I print_info: file format = GGUF V3 (latest)
0.00.033.625 I print_info: file type   = Q2_K - Medium
0.00.033.626 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.042.903 I load: special tokens cache size = 25
0.00.050.738 I load: token to piece cache size = 0.2984 MB
0.00.050.741 I print_info: arch             = gptneox
0.00.050.741 I print_info: vocab_only       = 0
0.00.050.742 I print_info: n_ctx_train      = 2048
0.00.050.742 I print_info: n_embd           = 2048
0.00.050.742 I print_info: n_layer          = 24
0.00.050.746 I print_info: n_head           = 16
0.00.050.746 I print_info: n_head_kv        = 16
0.00.050.747 I print_info: n_rot            = 32
0.00.050.747 I print_info: n_swa            = 0
0.00.050.747 I print_info: n_embd_head_k    = 128
0.00.050.747 I print_info: n_embd_head_v    = 128
0.00.050.748 I print_info: n_gqa            = 1
0.00.050.749 I print_info: n_embd_k_gqa     = 2048
0.00.050.750 I print_info: n_embd_v_gqa     = 2048
0.00.050.750 I print_info: f_norm_eps       = 1.0e-05
0.00.050.751 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.751 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.751 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.751 I print_info: f_logit_scale    = 0.0e+00
0.00.050.754 I print_info: n_ff             = 8192
0.00.050.754 I print_info: n_expert         = 0
0.00.050.754 I print_info: n_expert_used    = 0
0.00.050.754 I print_info: causal attn      = 1
0.00.050.754 I print_info: pooling type     = 0
0.00.050.756 I print_info: rope type        = 2
0.00.050.757 I print_info: rope scaling     = linear
0.00.050.757 I print_info: freq_base_train  = 10000.0
0.00.050.757 I print_info: freq_scale_train = 1
0.00.050.758 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.758 I print_info: rope_finetuned   = unknown
0.00.050.758 I print_info: ssm_d_conv       = 0
0.00.050.758 I print_info: ssm_d_inner      = 0
0.00.050.758 I print_info: ssm_d_state      = 0
0.00.050.759 I print_info: ssm_dt_rank      = 0
0.00.050.759 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.759 I print_info: model type       = 1.4B
0.00.050.764 I print_info: model params     = 1.41 B
0.00.050.764 I print_info: general.name     = 1.4B
0.00.050.764 I print_info: vocab type       = BPE
0.00.050.765 I print_info: n_vocab          = 50304
0.00.050.765 I print_info: n_merges         = 50009
0.00.050.765 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.766 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.766 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.766 I print_info: LF token         = 187 ''
0.00.050.766 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.767 I print_info: max token length = 1024
0.00.050.768 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.369.214 I load_tensors: offloading 24 repeating layers to GPU
0.00.369.233 I load_tensors: offloading output layer to GPU
0.00.369.234 I load_tensors: offloaded 25/25 layers to GPU
0.00.369.266 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.369.267 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.370.741 I llama_context_unified: n_seq_max     = 1
0.00.370.746 I llama_context_unified: n_ctx         = 128
0.00.370.746 I llama_context_unified: n_ctx_per_seq = 128
0.00.370.746 I llama_context_unified: n_batch       = 128
0.00.370.747 I llama_context_unified: n_ubatch      = 128
0.00.370.747 I llama_context_unified: flash_attn    = 0
0.00.370.749 I llama_context_unified: freq_base     = 10000.0
0.00.370.749 I llama_context_unified: freq_scale    = 1
0.00.370.750 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.370.753 I ggml_metal_init: allocating
0.00.370.833 I ggml_metal_init: found device: Apple M4
0.00.370.848 I ggml_metal_init: picking default device: Apple M4
0.00.372.771 I ggml_metal_init: using embedded metal library
0.00.378.512 I ggml_metal_init: GPU name:   Apple M4
0.00.378.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.378.536 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.378.536 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.378.537 I ggml_metal_init: simdgroup reduction   = true
0.00.378.538 I ggml_metal_init: simdgroup matrix mul. = true
0.00.378.538 I ggml_metal_init: has residency sets    = true
0.00.378.538 I ggml_metal_init: has bfloat            = true
0.00.378.538 I ggml_metal_init: use bfloat            = true
0.00.378.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.378.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.400.414 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.404.377 I init:      Metal KV buffer size =    24.00 MiB
0.00.404.384 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.404.431 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.407.945 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.407.948 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.407.948 I llama_context_unified: graph nodes  = 967
0.00.407.949 I llama_context_unified: graph splits = 2
0.00.407.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.407.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.436.751 I 
0.00.436.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.436.830 I perplexity: tokenizing the input ..
0.00.442.453 I perplexity: tokenization took 5.62 ms
0.00.442.464 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.573.997 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.575.323 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.575.338 I llama_perf_context_print:        load time =     421.24 ms
0.00.575.338 I llama_perf_context_print: prompt eval time =     131.27 ms /   128 tokens (    1.03 ms per token,   975.08 tokens per second)
0.00.575.339 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.575.340 I llama_perf_context_print:       total time =     138.59 ms /   129 tokens
0.00.575.920 I ggml_metal_free: deallocating

real	0m0.599s
user	0m0.086s
sys	0m0.101s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.659 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.147 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.153 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.160 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.162 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.162 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.166 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.167 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.923 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.946 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.619 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.619 I llama_model_loader: - type  f32:  194 tensors
0.00.024.620 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.620 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.620 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.620 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.621 I print_info: file format = GGUF V3 (latest)
0.00.024.624 I print_info: file type   = Q3_K - Medium
0.00.024.624 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.281 I load: special tokens cache size = 25
0.00.038.123 I load: token to piece cache size = 0.2984 MB
0.00.038.125 I print_info: arch             = gptneox
0.00.038.125 I print_info: vocab_only       = 0
0.00.038.126 I print_info: n_ctx_train      = 2048
0.00.038.126 I print_info: n_embd           = 2048
0.00.038.126 I print_info: n_layer          = 24
0.00.038.129 I print_info: n_head           = 16
0.00.038.130 I print_info: n_head_kv        = 16
0.00.038.130 I print_info: n_rot            = 32
0.00.038.130 I print_info: n_swa            = 0
0.00.038.130 I print_info: n_embd_head_k    = 128
0.00.038.130 I print_info: n_embd_head_v    = 128
0.00.038.131 I print_info: n_gqa            = 1
0.00.038.132 I print_info: n_embd_k_gqa     = 2048
0.00.038.132 I print_info: n_embd_v_gqa     = 2048
0.00.038.133 I print_info: f_norm_eps       = 1.0e-05
0.00.038.134 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.134 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.134 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.134 I print_info: f_logit_scale    = 0.0e+00
0.00.038.135 I print_info: n_ff             = 8192
0.00.038.137 I print_info: n_expert         = 0
0.00.038.137 I print_info: n_expert_used    = 0
0.00.038.138 I print_info: causal attn      = 1
0.00.038.139 I print_info: pooling type     = 0
0.00.038.139 I print_info: rope type        = 2
0.00.038.140 I print_info: rope scaling     = linear
0.00.038.140 I print_info: freq_base_train  = 10000.0
0.00.038.140 I print_info: freq_scale_train = 1
0.00.038.141 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.141 I print_info: rope_finetuned   = unknown
0.00.038.141 I print_info: ssm_d_conv       = 0
0.00.038.141 I print_info: ssm_d_inner      = 0
0.00.038.141 I print_info: ssm_d_state      = 0
0.00.038.141 I print_info: ssm_dt_rank      = 0
0.00.038.141 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.142 I print_info: model type       = 1.4B
0.00.038.142 I print_info: model params     = 1.41 B
0.00.038.142 I print_info: general.name     = 1.4B
0.00.038.143 I print_info: vocab type       = BPE
0.00.038.143 I print_info: n_vocab          = 50304
0.00.038.143 I print_info: n_merges         = 50009
0.00.038.143 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.144 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.144 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.144 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.144 I print_info: LF token         = 187 ''
0.00.038.145 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.145 I print_info: max token length = 1024
0.00.038.145 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.465.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.465.624 I load_tensors: offloading output layer to GPU
0.00.465.625 I load_tensors: offloaded 25/25 layers to GPU
0.00.465.662 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.465.663 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.467.245 I llama_context_unified: n_seq_max     = 1
0.00.467.250 I llama_context_unified: n_ctx         = 2048
0.00.467.251 I llama_context_unified: n_ctx_per_seq = 2048
0.00.467.252 I llama_context_unified: n_batch       = 2048
0.00.467.252 I llama_context_unified: n_ubatch      = 512
0.00.467.252 I llama_context_unified: flash_attn    = 0
0.00.467.255 I llama_context_unified: freq_base     = 10000.0
0.00.467.255 I llama_context_unified: freq_scale    = 1
0.00.467.258 I ggml_metal_init: allocating
0.00.467.336 I ggml_metal_init: found device: Apple M4
0.00.467.350 I ggml_metal_init: picking default device: Apple M4
0.00.469.338 I ggml_metal_init: using embedded metal library
0.00.475.072 I ggml_metal_init: GPU name:   Apple M4
0.00.475.095 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.475.095 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.475.096 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.475.097 I ggml_metal_init: simdgroup reduction   = true
0.00.475.097 I ggml_metal_init: simdgroup matrix mul. = true
0.00.475.097 I ggml_metal_init: has residency sets    = true
0.00.475.097 I ggml_metal_init: has bfloat            = true
0.00.475.098 I ggml_metal_init: use bfloat            = true
0.00.475.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.475.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.495.905 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.562.304 I init:      Metal KV buffer size =   384.00 MiB
0.00.562.313 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.562.335 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.567.299 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.567.303 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.567.304 I llama_context_unified: graph nodes  = 967
0.00.567.304 I llama_context_unified: graph splits = 2
0.00.567.310 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.567.439 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.567.440 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.686 I main: llama threadpool init, n_threads = 4
0.00.626.729 I 
0.00.626.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.753 I 
0.00.626.922 I sampler seed: 1234
0.00.626.927 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.626.947 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.626.948 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.626.948 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.370.006 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.370.007 I llama_perf_context_print:        load time =     617.32 ms
0.01.370.008 I llama_perf_context_print: prompt eval time =      48.54 ms /     7 tokens (    6.93 ms per token,   144.21 tokens per second)
0.01.370.008 I llama_perf_context_print:        eval time =     691.69 ms /    63 runs   (   10.98 ms per token,    91.08 tokens per second)
0.01.370.009 I llama_perf_context_print:       total time =     744.02 ms /    70 tokens
0.01.373.737 I ggml_metal_free: deallocating

real	0m1.389s
user	0m0.110s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.258 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.625 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.635 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.638 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.640 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.641 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.642 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.384 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.144 I llama_model_loader: - type  f32:  194 tensors
0.00.027.144 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.144 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.145 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.145 I print_info: file format = GGUF V3 (latest)
0.00.027.151 I print_info: file type   = Q3_K - Medium
0.00.027.154 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.035.516 I load: special tokens cache size = 25
0.00.041.435 I load: token to piece cache size = 0.2984 MB
0.00.041.439 I print_info: arch             = gptneox
0.00.041.440 I print_info: vocab_only       = 0
0.00.041.440 I print_info: n_ctx_train      = 2048
0.00.041.440 I print_info: n_embd           = 2048
0.00.041.440 I print_info: n_layer          = 24
0.00.041.445 I print_info: n_head           = 16
0.00.041.446 I print_info: n_head_kv        = 16
0.00.041.446 I print_info: n_rot            = 32
0.00.041.446 I print_info: n_swa            = 0
0.00.041.446 I print_info: n_embd_head_k    = 128
0.00.041.446 I print_info: n_embd_head_v    = 128
0.00.041.449 I print_info: n_gqa            = 1
0.00.041.449 I print_info: n_embd_k_gqa     = 2048
0.00.041.450 I print_info: n_embd_v_gqa     = 2048
0.00.041.451 I print_info: f_norm_eps       = 1.0e-05
0.00.041.451 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.451 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.451 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.452 I print_info: f_logit_scale    = 0.0e+00
0.00.041.454 I print_info: n_ff             = 8192
0.00.041.454 I print_info: n_expert         = 0
0.00.041.454 I print_info: n_expert_used    = 0
0.00.041.455 I print_info: causal attn      = 1
0.00.041.455 I print_info: pooling type     = 0
0.00.041.455 I print_info: rope type        = 2
0.00.041.455 I print_info: rope scaling     = linear
0.00.041.456 I print_info: freq_base_train  = 10000.0
0.00.041.456 I print_info: freq_scale_train = 1
0.00.041.456 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.457 I print_info: rope_finetuned   = unknown
0.00.041.457 I print_info: ssm_d_conv       = 0
0.00.041.457 I print_info: ssm_d_inner      = 0
0.00.041.457 I print_info: ssm_d_state      = 0
0.00.041.457 I print_info: ssm_dt_rank      = 0
0.00.041.458 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.458 I print_info: model type       = 1.4B
0.00.041.458 I print_info: model params     = 1.41 B
0.00.041.458 I print_info: general.name     = 1.4B
0.00.041.459 I print_info: vocab type       = BPE
0.00.041.459 I print_info: n_vocab          = 50304
0.00.041.459 I print_info: n_merges         = 50009
0.00.041.460 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.460 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.460 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.460 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.460 I print_info: LF token         = 187 ''
0.00.041.461 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.461 I print_info: max token length = 1024
0.00.041.461 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.462.405 I load_tensors: offloading 24 repeating layers to GPU
0.00.462.424 I load_tensors: offloading output layer to GPU
0.00.462.425 I load_tensors: offloaded 25/25 layers to GPU
0.00.462.458 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.462.460 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.463.822 I llama_context_unified: n_seq_max     = 1
0.00.463.826 I llama_context_unified: n_ctx         = 128
0.00.463.827 I llama_context_unified: n_ctx_per_seq = 128
0.00.463.827 I llama_context_unified: n_batch       = 128
0.00.463.828 I llama_context_unified: n_ubatch      = 128
0.00.463.828 I llama_context_unified: flash_attn    = 0
0.00.463.831 I llama_context_unified: freq_base     = 10000.0
0.00.463.831 I llama_context_unified: freq_scale    = 1
0.00.463.832 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.463.834 I ggml_metal_init: allocating
0.00.463.910 I ggml_metal_init: found device: Apple M4
0.00.463.925 I ggml_metal_init: picking default device: Apple M4
0.00.465.838 I ggml_metal_init: using embedded metal library
0.00.471.567 I ggml_metal_init: GPU name:   Apple M4
0.00.471.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.471.586 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.471.587 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.471.588 I ggml_metal_init: simdgroup reduction   = true
0.00.471.588 I ggml_metal_init: simdgroup matrix mul. = true
0.00.471.588 I ggml_metal_init: has residency sets    = true
0.00.471.589 I ggml_metal_init: has bfloat            = true
0.00.471.589 I ggml_metal_init: use bfloat            = true
0.00.471.591 I ggml_metal_init: hasUnifiedMemory      = true
0.00.471.597 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.492.463 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.496.225 I init:      Metal KV buffer size =    24.00 MiB
0.00.496.232 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.496.285 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.499.662 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.499.664 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.499.664 I llama_context_unified: graph nodes  = 967
0.00.499.665 I llama_context_unified: graph splits = 2
0.00.499.671 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.499.672 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.527.593 I 
0.00.527.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.527.673 I perplexity: tokenizing the input ..
0.00.534.475 I perplexity: tokenization took 6.799 ms
0.00.534.496 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.680.832 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.682.135 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.682.148 I llama_perf_context_print:        load time =     518.33 ms
0.00.682.149 I llama_perf_context_print: prompt eval time =     145.30 ms /   128 tokens (    1.14 ms per token,   880.95 tokens per second)
0.00.682.150 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.682.151 I llama_perf_context_print:       total time =     154.56 ms /   129 tokens
0.00.682.709 I ggml_metal_free: deallocating

real	0m0.696s
user	0m0.082s
sys	0m0.119s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.009.452 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.287 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.297 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.298 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.298 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.299 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.299 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.300 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.301 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.301 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.303 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.303 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.304 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.304 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.306 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.306 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.306 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.075 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.769 I llama_model_loader: - type  f32:  194 tensors
0.00.025.769 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.770 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.770 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.770 I print_info: file format = GGUF V3 (latest)
0.00.025.771 I print_info: file type   = Q4_K - Medium
0.00.025.772 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.592 I load: special tokens cache size = 25
0.00.039.508 I load: token to piece cache size = 0.2984 MB
0.00.039.511 I print_info: arch             = gptneox
0.00.039.511 I print_info: vocab_only       = 0
0.00.039.512 I print_info: n_ctx_train      = 2048
0.00.039.512 I print_info: n_embd           = 2048
0.00.039.512 I print_info: n_layer          = 24
0.00.039.515 I print_info: n_head           = 16
0.00.039.516 I print_info: n_head_kv        = 16
0.00.039.516 I print_info: n_rot            = 32
0.00.039.516 I print_info: n_swa            = 0
0.00.039.516 I print_info: n_embd_head_k    = 128
0.00.039.517 I print_info: n_embd_head_v    = 128
0.00.039.517 I print_info: n_gqa            = 1
0.00.039.518 I print_info: n_embd_k_gqa     = 2048
0.00.039.521 I print_info: n_embd_v_gqa     = 2048
0.00.039.522 I print_info: f_norm_eps       = 1.0e-05
0.00.039.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.524 I print_info: f_logit_scale    = 0.0e+00
0.00.039.525 I print_info: n_ff             = 8192
0.00.039.525 I print_info: n_expert         = 0
0.00.039.525 I print_info: n_expert_used    = 0
0.00.039.526 I print_info: causal attn      = 1
0.00.039.526 I print_info: pooling type     = 0
0.00.039.526 I print_info: rope type        = 2
0.00.039.526 I print_info: rope scaling     = linear
0.00.039.527 I print_info: freq_base_train  = 10000.0
0.00.039.528 I print_info: freq_scale_train = 1
0.00.039.529 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.529 I print_info: rope_finetuned   = unknown
0.00.039.529 I print_info: ssm_d_conv       = 0
0.00.039.529 I print_info: ssm_d_inner      = 0
0.00.039.529 I print_info: ssm_d_state      = 0
0.00.039.529 I print_info: ssm_dt_rank      = 0
0.00.039.530 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.530 I print_info: model type       = 1.4B
0.00.039.530 I print_info: model params     = 1.41 B
0.00.039.530 I print_info: general.name     = 1.4B
0.00.039.531 I print_info: vocab type       = BPE
0.00.039.531 I print_info: n_vocab          = 50304
0.00.039.531 I print_info: n_merges         = 50009
0.00.039.535 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.535 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.535 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.535 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.536 I print_info: LF token         = 187 ''
0.00.039.536 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.536 I print_info: max token length = 1024
0.00.039.537 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.534.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.534.908 I load_tensors: offloading output layer to GPU
0.00.534.909 I load_tensors: offloaded 25/25 layers to GPU
0.00.534.940 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.534.942 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.536.387 I llama_context_unified: n_seq_max     = 1
0.00.536.391 I llama_context_unified: n_ctx         = 2048
0.00.536.392 I llama_context_unified: n_ctx_per_seq = 2048
0.00.536.392 I llama_context_unified: n_batch       = 2048
0.00.536.393 I llama_context_unified: n_ubatch      = 512
0.00.536.393 I llama_context_unified: flash_attn    = 0
0.00.536.395 I llama_context_unified: freq_base     = 10000.0
0.00.536.396 I llama_context_unified: freq_scale    = 1
0.00.536.398 I ggml_metal_init: allocating
0.00.536.474 I ggml_metal_init: found device: Apple M4
0.00.536.487 I ggml_metal_init: picking default device: Apple M4
0.00.538.362 I ggml_metal_init: using embedded metal library
0.00.545.065 I ggml_metal_init: GPU name:   Apple M4
0.00.545.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.545.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.545.072 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.545.073 I ggml_metal_init: simdgroup reduction   = true
0.00.545.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.545.074 I ggml_metal_init: has residency sets    = true
0.00.545.074 I ggml_metal_init: has bfloat            = true
0.00.545.074 I ggml_metal_init: use bfloat            = true
0.00.545.075 I ggml_metal_init: hasUnifiedMemory      = true
0.00.545.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.563.519 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.681 I init:      Metal KV buffer size =   384.00 MiB
0.00.617.688 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.617.712 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.622.525 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.622.529 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.622.529 I llama_context_unified: graph nodes  = 967
0.00.622.529 I llama_context_unified: graph splits = 2
0.00.622.534 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.622.663 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.622.664 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.409 I main: llama threadpool init, n_threads = 4
0.00.679.456 I 
0.00.679.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.479 I 
0.00.679.628 I sampler seed: 1234
0.00.679.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.644 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.644 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.644 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.429.744 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.429.745 I llama_perf_context_print:        load time =     669.26 ms
0.01.429.745 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.76 tokens per second)
0.01.429.746 I llama_perf_context_print:        eval time =     700.14 ms /    63 runs   (   11.11 ms per token,    89.98 tokens per second)
0.01.429.746 I llama_perf_context_print:       total time =     751.03 ms /    70 tokens
0.01.433.523 I ggml_metal_free: deallocating

real	0m1.449s
user	0m0.108s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.180 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.063 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.071 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.073 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.073 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.075 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.075 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.076 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.076 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.076 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.077 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.077 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.667 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.668 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.668 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.669 I llama_model_loader: - type  f32:  194 tensors
0.00.027.670 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.670 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.670 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.671 I print_info: file format = GGUF V3 (latest)
0.00.027.677 I print_info: file type   = Q4_K - Medium
0.00.027.678 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.680 I load: special tokens cache size = 25
0.00.041.700 I load: token to piece cache size = 0.2984 MB
0.00.041.703 I print_info: arch             = gptneox
0.00.041.704 I print_info: vocab_only       = 0
0.00.041.704 I print_info: n_ctx_train      = 2048
0.00.041.704 I print_info: n_embd           = 2048
0.00.041.704 I print_info: n_layer          = 24
0.00.041.709 I print_info: n_head           = 16
0.00.041.710 I print_info: n_head_kv        = 16
0.00.041.710 I print_info: n_rot            = 32
0.00.041.711 I print_info: n_swa            = 0
0.00.041.711 I print_info: n_embd_head_k    = 128
0.00.041.711 I print_info: n_embd_head_v    = 128
0.00.041.712 I print_info: n_gqa            = 1
0.00.041.712 I print_info: n_embd_k_gqa     = 2048
0.00.041.713 I print_info: n_embd_v_gqa     = 2048
0.00.041.714 I print_info: f_norm_eps       = 1.0e-05
0.00.041.714 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.714 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.714 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.715 I print_info: f_logit_scale    = 0.0e+00
0.00.041.717 I print_info: n_ff             = 8192
0.00.041.717 I print_info: n_expert         = 0
0.00.041.717 I print_info: n_expert_used    = 0
0.00.041.717 I print_info: causal attn      = 1
0.00.041.718 I print_info: pooling type     = 0
0.00.041.718 I print_info: rope type        = 2
0.00.041.719 I print_info: rope scaling     = linear
0.00.041.719 I print_info: freq_base_train  = 10000.0
0.00.041.720 I print_info: freq_scale_train = 1
0.00.041.720 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.720 I print_info: rope_finetuned   = unknown
0.00.041.720 I print_info: ssm_d_conv       = 0
0.00.041.721 I print_info: ssm_d_inner      = 0
0.00.041.721 I print_info: ssm_d_state      = 0
0.00.041.721 I print_info: ssm_dt_rank      = 0
0.00.041.721 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.723 I print_info: model type       = 1.4B
0.00.041.723 I print_info: model params     = 1.41 B
0.00.041.723 I print_info: general.name     = 1.4B
0.00.041.724 I print_info: vocab type       = BPE
0.00.041.724 I print_info: n_vocab          = 50304
0.00.041.728 I print_info: n_merges         = 50009
0.00.041.728 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.728 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.729 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.729 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.729 I print_info: LF token         = 187 ''
0.00.041.729 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.729 I print_info: max token length = 1024
0.00.041.730 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.159 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.179 I load_tensors: offloading output layer to GPU
0.00.589.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.212 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.589.213 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.590.395 I llama_context_unified: n_seq_max     = 1
0.00.590.404 I llama_context_unified: n_ctx         = 128
0.00.590.405 I llama_context_unified: n_ctx_per_seq = 128
0.00.590.405 I llama_context_unified: n_batch       = 128
0.00.590.406 I llama_context_unified: n_ubatch      = 128
0.00.590.406 I llama_context_unified: flash_attn    = 0
0.00.590.408 I llama_context_unified: freq_base     = 10000.0
0.00.590.409 I llama_context_unified: freq_scale    = 1
0.00.590.410 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.412 I ggml_metal_init: allocating
0.00.590.508 I ggml_metal_init: found device: Apple M4
0.00.590.525 I ggml_metal_init: picking default device: Apple M4
0.00.592.500 I ggml_metal_init: using embedded metal library
0.00.598.245 I ggml_metal_init: GPU name:   Apple M4
0.00.598.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.254 I ggml_metal_init: simdgroup reduction   = true
0.00.598.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.255 I ggml_metal_init: has residency sets    = true
0.00.598.255 I ggml_metal_init: has bfloat            = true
0.00.598.255 I ggml_metal_init: use bfloat            = true
0.00.598.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.499 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.966 I init:      Metal KV buffer size =    24.00 MiB
0.00.620.969 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.993 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.624.164 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.624.166 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.624.167 I llama_context_unified: graph nodes  = 967
0.00.624.167 I llama_context_unified: graph splits = 2
0.00.624.171 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.173 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.743 I 
0.00.650.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.851 I perplexity: tokenizing the input ..
0.00.657.891 I perplexity: tokenization took 7.036 ms
0.00.657.912 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.978 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.794.287 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.794.300 I llama_perf_context_print:        load time =     641.55 ms
0.00.794.301 I llama_perf_context_print: prompt eval time =     134.09 ms /   128 tokens (    1.05 ms per token,   954.59 tokens per second)
0.00.794.301 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.302 I llama_perf_context_print:       total time =     143.56 ms /   129 tokens
0.00.794.912 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.080s
sys	0m0.138s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.609 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.458 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.468 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.469 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.470 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.470 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.470 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.471 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.471 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.472 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.472 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.472 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.474 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.223 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.229 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.883 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.884 I llama_model_loader: - type  f32:  194 tensors
0.00.025.884 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.885 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.885 I print_info: file format = GGUF V3 (latest)
0.00.025.886 I print_info: file type   = Q5_K - Medium
0.00.025.887 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.667 I load: special tokens cache size = 25
0.00.039.823 I load: token to piece cache size = 0.2984 MB
0.00.039.826 I print_info: arch             = gptneox
0.00.039.826 I print_info: vocab_only       = 0
0.00.039.826 I print_info: n_ctx_train      = 2048
0.00.039.826 I print_info: n_embd           = 2048
0.00.039.827 I print_info: n_layer          = 24
0.00.039.829 I print_info: n_head           = 16
0.00.039.830 I print_info: n_head_kv        = 16
0.00.039.830 I print_info: n_rot            = 32
0.00.039.830 I print_info: n_swa            = 0
0.00.039.831 I print_info: n_embd_head_k    = 128
0.00.039.831 I print_info: n_embd_head_v    = 128
0.00.039.832 I print_info: n_gqa            = 1
0.00.039.832 I print_info: n_embd_k_gqa     = 2048
0.00.039.835 I print_info: n_embd_v_gqa     = 2048
0.00.039.835 I print_info: f_norm_eps       = 1.0e-05
0.00.039.836 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.836 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.836 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.836 I print_info: f_logit_scale    = 0.0e+00
0.00.039.837 I print_info: n_ff             = 8192
0.00.039.837 I print_info: n_expert         = 0
0.00.039.837 I print_info: n_expert_used    = 0
0.00.039.838 I print_info: causal attn      = 1
0.00.039.838 I print_info: pooling type     = 0
0.00.039.838 I print_info: rope type        = 2
0.00.039.838 I print_info: rope scaling     = linear
0.00.039.840 I print_info: freq_base_train  = 10000.0
0.00.039.840 I print_info: freq_scale_train = 1
0.00.039.840 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.841 I print_info: rope_finetuned   = unknown
0.00.039.841 I print_info: ssm_d_conv       = 0
0.00.039.841 I print_info: ssm_d_inner      = 0
0.00.039.841 I print_info: ssm_d_state      = 0
0.00.039.841 I print_info: ssm_dt_rank      = 0
0.00.039.841 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.841 I print_info: model type       = 1.4B
0.00.039.842 I print_info: model params     = 1.41 B
0.00.039.842 I print_info: general.name     = 1.4B
0.00.039.843 I print_info: vocab type       = BPE
0.00.039.843 I print_info: n_vocab          = 50304
0.00.039.843 I print_info: n_merges         = 50009
0.00.039.843 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.843 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.844 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.844 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.848 I print_info: LF token         = 187 ''
0.00.039.849 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.849 I print_info: max token length = 1024
0.00.039.850 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.757 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.772 I load_tensors: offloading output layer to GPU
0.00.619.773 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.807 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.619.809 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.621.201 I llama_context_unified: n_seq_max     = 1
0.00.621.204 I llama_context_unified: n_ctx         = 2048
0.00.621.205 I llama_context_unified: n_ctx_per_seq = 2048
0.00.621.205 I llama_context_unified: n_batch       = 2048
0.00.621.206 I llama_context_unified: n_ubatch      = 512
0.00.621.206 I llama_context_unified: flash_attn    = 0
0.00.621.207 I llama_context_unified: freq_base     = 10000.0
0.00.621.208 I llama_context_unified: freq_scale    = 1
0.00.621.209 I ggml_metal_init: allocating
0.00.621.227 I ggml_metal_init: found device: Apple M4
0.00.621.237 I ggml_metal_init: picking default device: Apple M4
0.00.622.721 I ggml_metal_init: using embedded metal library
0.00.629.058 I ggml_metal_init: GPU name:   Apple M4
0.00.629.061 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.062 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.063 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.064 I ggml_metal_init: simdgroup reduction   = true
0.00.629.064 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.064 I ggml_metal_init: has residency sets    = true
0.00.629.065 I ggml_metal_init: has bfloat            = true
0.00.629.065 I ggml_metal_init: use bfloat            = true
0.00.629.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.104 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.704.490 I init:      Metal KV buffer size =   384.00 MiB
0.00.704.497 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.704.524 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.709.448 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.709.449 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.709.450 I llama_context_unified: graph nodes  = 967
0.00.709.450 I llama_context_unified: graph splits = 2
0.00.709.456 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.709.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.027 I main: llama threadpool init, n_threads = 4
0.00.776.074 I 
0.00.776.099 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.101 I 
0.00.776.253 I sampler seed: 1234
0.00.776.257 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.281 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.282 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.283 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.620.700 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.620.701 I llama_perf_context_print:        load time =     765.70 ms
0.01.620.702 I llama_perf_context_print: prompt eval time =      55.85 ms /     7 tokens (    7.98 ms per token,   125.34 tokens per second)
0.01.620.703 I llama_perf_context_print:        eval time =     785.62 ms /    63 runs   (   12.47 ms per token,    80.19 tokens per second)
0.01.620.705 I llama_perf_context_print:       total time =     845.39 ms /    70 tokens
0.01.624.646 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.107s
sys	0m0.238s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.473 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.403 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.410 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.414 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.419 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.419 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.032 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.709 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.711 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.713 I llama_model_loader: - type  f32:  194 tensors
0.00.028.713 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.713 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.714 I print_info: file format = GGUF V3 (latest)
0.00.028.714 I print_info: file type   = Q5_K - Medium
0.00.028.715 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.036.702 I load: special tokens cache size = 25
0.00.042.755 I load: token to piece cache size = 0.2984 MB
0.00.042.759 I print_info: arch             = gptneox
0.00.042.760 I print_info: vocab_only       = 0
0.00.042.760 I print_info: n_ctx_train      = 2048
0.00.042.760 I print_info: n_embd           = 2048
0.00.042.760 I print_info: n_layer          = 24
0.00.042.765 I print_info: n_head           = 16
0.00.042.766 I print_info: n_head_kv        = 16
0.00.042.766 I print_info: n_rot            = 32
0.00.042.766 I print_info: n_swa            = 0
0.00.042.766 I print_info: n_embd_head_k    = 128
0.00.042.768 I print_info: n_embd_head_v    = 128
0.00.042.769 I print_info: n_gqa            = 1
0.00.042.769 I print_info: n_embd_k_gqa     = 2048
0.00.042.770 I print_info: n_embd_v_gqa     = 2048
0.00.042.771 I print_info: f_norm_eps       = 1.0e-05
0.00.042.771 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.771 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.771 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.771 I print_info: f_logit_scale    = 0.0e+00
0.00.042.772 I print_info: n_ff             = 8192
0.00.042.772 I print_info: n_expert         = 0
0.00.042.772 I print_info: n_expert_used    = 0
0.00.042.772 I print_info: causal attn      = 1
0.00.042.773 I print_info: pooling type     = 0
0.00.042.773 I print_info: rope type        = 2
0.00.042.773 I print_info: rope scaling     = linear
0.00.042.773 I print_info: freq_base_train  = 10000.0
0.00.042.774 I print_info: freq_scale_train = 1
0.00.042.774 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.774 I print_info: rope_finetuned   = unknown
0.00.042.778 I print_info: ssm_d_conv       = 0
0.00.042.778 I print_info: ssm_d_inner      = 0
0.00.042.778 I print_info: ssm_d_state      = 0
0.00.042.778 I print_info: ssm_dt_rank      = 0
0.00.042.779 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.779 I print_info: model type       = 1.4B
0.00.042.779 I print_info: model params     = 1.41 B
0.00.042.780 I print_info: general.name     = 1.4B
0.00.042.780 I print_info: vocab type       = BPE
0.00.042.780 I print_info: n_vocab          = 50304
0.00.042.780 I print_info: n_merges         = 50009
0.00.042.780 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.781 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.781 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.781 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.781 I print_info: LF token         = 187 ''
0.00.042.781 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.782 I print_info: max token length = 1024
0.00.042.782 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.669.812 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.834 I load_tensors: offloading output layer to GPU
0.00.669.835 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.888 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.669.890 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.671.273 I llama_context_unified: n_seq_max     = 1
0.00.671.277 I llama_context_unified: n_ctx         = 128
0.00.671.278 I llama_context_unified: n_ctx_per_seq = 128
0.00.671.278 I llama_context_unified: n_batch       = 128
0.00.671.278 I llama_context_unified: n_ubatch      = 128
0.00.671.279 I llama_context_unified: flash_attn    = 0
0.00.671.281 I llama_context_unified: freq_base     = 10000.0
0.00.671.281 I llama_context_unified: freq_scale    = 1
0.00.671.282 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.671.285 I ggml_metal_init: allocating
0.00.671.360 I ggml_metal_init: found device: Apple M4
0.00.671.373 I ggml_metal_init: picking default device: Apple M4
0.00.673.256 I ggml_metal_init: using embedded metal library
0.00.679.811 I ggml_metal_init: GPU name:   Apple M4
0.00.679.815 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.679.816 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.679.817 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.679.818 I ggml_metal_init: simdgroup reduction   = true
0.00.679.818 I ggml_metal_init: simdgroup matrix mul. = true
0.00.679.818 I ggml_metal_init: has residency sets    = true
0.00.679.818 I ggml_metal_init: has bfloat            = true
0.00.679.819 I ggml_metal_init: use bfloat            = true
0.00.679.820 I ggml_metal_init: hasUnifiedMemory      = true
0.00.679.822 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.346 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.700.853 I init:      Metal KV buffer size =    24.00 MiB
0.00.700.856 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.700.878 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.704.174 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.704.176 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.704.176 I llama_context_unified: graph nodes  = 967
0.00.704.176 I llama_context_unified: graph splits = 2
0.00.704.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.704.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.356 I 
0.00.737.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.440 I perplexity: tokenizing the input ..
0.00.744.132 I perplexity: tokenization took 6.688 ms
0.00.744.150 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.885.810 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.887.166 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.887.180 I llama_perf_context_print:        load time =     725.88 ms
0.00.887.182 I llama_perf_context_print: prompt eval time =     141.00 ms /   128 tokens (    1.10 ms per token,   907.77 tokens per second)
0.00.887.182 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.887.183 I llama_perf_context_print:       total time =     149.83 ms /   129 tokens
0.00.887.765 I ggml_metal_free: deallocating

real	0m0.906s
user	0m0.079s
sys	0m0.157s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.586 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.252 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.258 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.259 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.259 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.260 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.260 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.263 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.263 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.263 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.264 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.264 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.267 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.267 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.939 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.537 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.538 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.539 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.540 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.540 I llama_model_loader: - type  f32:  194 tensors
0.00.024.540 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.541 I print_info: file format = GGUF V3 (latest)
0.00.024.541 I print_info: file type   = Q6_K
0.00.024.542 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.386 I load: special tokens cache size = 25
0.00.038.436 I load: token to piece cache size = 0.2984 MB
0.00.038.438 I print_info: arch             = gptneox
0.00.038.439 I print_info: vocab_only       = 0
0.00.038.439 I print_info: n_ctx_train      = 2048
0.00.038.439 I print_info: n_embd           = 2048
0.00.038.439 I print_info: n_layer          = 24
0.00.038.442 I print_info: n_head           = 16
0.00.038.443 I print_info: n_head_kv        = 16
0.00.038.443 I print_info: n_rot            = 32
0.00.038.443 I print_info: n_swa            = 0
0.00.038.443 I print_info: n_embd_head_k    = 128
0.00.038.443 I print_info: n_embd_head_v    = 128
0.00.038.446 I print_info: n_gqa            = 1
0.00.038.447 I print_info: n_embd_k_gqa     = 2048
0.00.038.447 I print_info: n_embd_v_gqa     = 2048
0.00.038.448 I print_info: f_norm_eps       = 1.0e-05
0.00.038.453 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.453 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.453 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.453 I print_info: f_logit_scale    = 0.0e+00
0.00.038.454 I print_info: n_ff             = 8192
0.00.038.454 I print_info: n_expert         = 0
0.00.038.455 I print_info: n_expert_used    = 0
0.00.038.455 I print_info: causal attn      = 1
0.00.038.458 I print_info: pooling type     = 0
0.00.038.459 I print_info: rope type        = 2
0.00.038.459 I print_info: rope scaling     = linear
0.00.038.460 I print_info: freq_base_train  = 10000.0
0.00.038.460 I print_info: freq_scale_train = 1
0.00.038.460 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.460 I print_info: rope_finetuned   = unknown
0.00.038.461 I print_info: ssm_d_conv       = 0
0.00.038.461 I print_info: ssm_d_inner      = 0
0.00.038.461 I print_info: ssm_d_state      = 0
0.00.038.461 I print_info: ssm_dt_rank      = 0
0.00.038.461 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.461 I print_info: model type       = 1.4B
0.00.038.462 I print_info: model params     = 1.41 B
0.00.038.462 I print_info: general.name     = 1.4B
0.00.038.462 I print_info: vocab type       = BPE
0.00.038.463 I print_info: n_vocab          = 50304
0.00.038.463 I print_info: n_merges         = 50009
0.00.038.463 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.463 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.463 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.463 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.464 I print_info: LF token         = 187 ''
0.00.038.464 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.464 I print_info: max token length = 1024
0.00.038.465 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.674.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.674.069 I load_tensors: offloading output layer to GPU
0.00.674.069 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.100 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.674.104 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.675.635 I llama_context_unified: n_seq_max     = 1
0.00.675.637 I llama_context_unified: n_ctx         = 2048
0.00.675.638 I llama_context_unified: n_ctx_per_seq = 2048
0.00.675.638 I llama_context_unified: n_batch       = 2048
0.00.675.639 I llama_context_unified: n_ubatch      = 512
0.00.675.639 I llama_context_unified: flash_attn    = 0
0.00.675.640 I llama_context_unified: freq_base     = 10000.0
0.00.675.641 I llama_context_unified: freq_scale    = 1
0.00.675.642 I ggml_metal_init: allocating
0.00.675.658 I ggml_metal_init: found device: Apple M4
0.00.675.668 I ggml_metal_init: picking default device: Apple M4
0.00.677.164 I ggml_metal_init: using embedded metal library
0.00.683.566 I ggml_metal_init: GPU name:   Apple M4
0.00.683.569 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.683.570 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.683.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.683.575 I ggml_metal_init: simdgroup reduction   = true
0.00.683.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.683.575 I ggml_metal_init: has residency sets    = true
0.00.683.575 I ggml_metal_init: has bfloat            = true
0.00.683.576 I ggml_metal_init: use bfloat            = true
0.00.683.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.683.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.701.290 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.761.279 I init:      Metal KV buffer size =   384.00 MiB
0.00.761.286 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.761.312 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.766.162 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.766.164 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.766.164 I llama_context_unified: graph nodes  = 967
0.00.766.164 I llama_context_unified: graph splits = 2
0.00.766.169 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.766.304 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.766.304 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.714 I main: llama threadpool init, n_threads = 4
0.00.828.757 I 
0.00.828.778 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.780 I 
0.00.828.932 I sampler seed: 1234
0.00.828.937 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.828.947 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.828.947 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.828.948 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.697.536 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.697.537 I llama_perf_context_print:        load time =     819.40 ms
0.01.697.537 I llama_perf_context_print: prompt eval time =      54.10 ms /     7 tokens (    7.73 ms per token,   129.39 tokens per second)
0.01.697.538 I llama_perf_context_print:        eval time =     811.69 ms /    63 runs   (   12.88 ms per token,    77.62 tokens per second)
0.01.697.538 I llama_perf_context_print:       total time =     869.55 ms /    70 tokens
0.01.701.383 I ggml_metal_free: deallocating

real	0m1.718s
user	0m0.108s
sys	0m0.249s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4719 (956e0697) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.161 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.944 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.950 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.952 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.954 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.959 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.959 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.960 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.961 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.963 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.963 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.964 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.965 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.966 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.966 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.677 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.678 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.679 I llama_model_loader: - type  f32:  194 tensors
0.00.025.679 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.680 I print_info: file format = GGUF V3 (latest)
0.00.025.681 I print_info: file type   = Q6_K
0.00.025.682 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.645 I load: special tokens cache size = 25
0.00.039.598 I load: token to piece cache size = 0.2984 MB
0.00.039.601 I print_info: arch             = gptneox
0.00.039.601 I print_info: vocab_only       = 0
0.00.039.601 I print_info: n_ctx_train      = 2048
0.00.039.602 I print_info: n_embd           = 2048
0.00.039.602 I print_info: n_layer          = 24
0.00.039.605 I print_info: n_head           = 16
0.00.039.606 I print_info: n_head_kv        = 16
0.00.039.606 I print_info: n_rot            = 32
0.00.039.607 I print_info: n_swa            = 0
0.00.039.609 I print_info: n_embd_head_k    = 128
0.00.039.610 I print_info: n_embd_head_v    = 128
0.00.039.610 I print_info: n_gqa            = 1
0.00.039.611 I print_info: n_embd_k_gqa     = 2048
0.00.039.612 I print_info: n_embd_v_gqa     = 2048
0.00.039.613 I print_info: f_norm_eps       = 1.0e-05
0.00.039.613 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.613 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.614 I print_info: f_logit_scale    = 0.0e+00
0.00.039.615 I print_info: n_ff             = 8192
0.00.039.615 I print_info: n_expert         = 0
0.00.039.616 I print_info: n_expert_used    = 0
0.00.039.616 I print_info: causal attn      = 1
0.00.039.616 I print_info: pooling type     = 0
0.00.039.616 I print_info: rope type        = 2
0.00.039.616 I print_info: rope scaling     = linear
0.00.039.617 I print_info: freq_base_train  = 10000.0
0.00.039.617 I print_info: freq_scale_train = 1
0.00.039.617 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.618 I print_info: rope_finetuned   = unknown
0.00.039.618 I print_info: ssm_d_conv       = 0
0.00.039.618 I print_info: ssm_d_inner      = 0
0.00.039.619 I print_info: ssm_d_state      = 0
0.00.039.619 I print_info: ssm_dt_rank      = 0
0.00.039.620 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.620 I print_info: model type       = 1.4B
0.00.039.620 I print_info: model params     = 1.41 B
0.00.039.620 I print_info: general.name     = 1.4B
0.00.039.621 I print_info: vocab type       = BPE
0.00.039.621 I print_info: n_vocab          = 50304
0.00.039.621 I print_info: n_merges         = 50009
0.00.039.622 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.622 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.622 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.622 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.623 I print_info: LF token         = 187 ''
0.00.039.623 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.624 I print_info: max token length = 1024
0.00.039.624 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.734.365 I load_tensors: offloading 24 repeating layers to GPU
0.00.734.385 I load_tensors: offloading output layer to GPU
0.00.734.386 I load_tensors: offloaded 25/25 layers to GPU
0.00.734.420 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.734.421 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.735.838 I llama_context_unified: n_seq_max     = 1
0.00.735.842 I llama_context_unified: n_ctx         = 128
0.00.735.842 I llama_context_unified: n_ctx_per_seq = 128
0.00.735.843 I llama_context_unified: n_batch       = 128
0.00.735.843 I llama_context_unified: n_ubatch      = 128
0.00.735.843 I llama_context_unified: flash_attn    = 0
0.00.735.846 I llama_context_unified: freq_base     = 10000.0
0.00.735.847 I llama_context_unified: freq_scale    = 1
0.00.735.847 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.735.850 I ggml_metal_init: allocating
0.00.735.923 I ggml_metal_init: found device: Apple M4
0.00.735.936 I ggml_metal_init: picking default device: Apple M4
0.00.737.882 I ggml_metal_init: using embedded metal library
0.00.744.308 I ggml_metal_init: GPU name:   Apple M4
0.00.744.313 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.744.313 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.744.314 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.744.315 I ggml_metal_init: simdgroup reduction   = true
0.00.744.315 I ggml_metal_init: simdgroup matrix mul. = true
0.00.744.315 I ggml_metal_init: has residency sets    = true
0.00.744.315 I ggml_metal_init: has bfloat            = true
0.00.744.316 I ggml_metal_init: use bfloat            = true
0.00.744.316 I ggml_metal_init: hasUnifiedMemory      = true
0.00.744.319 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.762.128 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.765.922 I init:      Metal KV buffer size =    24.00 MiB
0.00.765.930 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.765.958 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.769.175 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.769.177 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.769.178 I llama_context_unified: graph nodes  = 967
0.00.769.178 I llama_context_unified: graph splits = 2
0.00.769.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.769.183 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.771 I 
0.00.803.833 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.849 I perplexity: tokenizing the input ..
0.00.810.282 I perplexity: tokenization took 6.429 ms
0.00.810.303 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.951.112 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.952.456 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.952.472 I llama_perf_context_print:        load time =     794.60 ms
0.00.952.473 I llama_perf_context_print: prompt eval time =     140.20 ms /   128 tokens (    1.10 ms per token,   912.99 tokens per second)
0.00.952.477 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.952.478 I llama_perf_context_print:       total time =     148.70 ms /   129 tokens
0.00.953.079 I ggml_metal_free: deallocating

real	0m0.966s
user	0m0.080s
sys	0m0.164s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4719 (956e0697)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101c07e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x101c08570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101c08b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101c090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101c09680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101c09c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x101c0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x101c0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x101c0ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x101c0b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x101c0b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x101c0bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x101c0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x101c0cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x101c0d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x101c0de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x101c0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x101c0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x101c0f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x101c0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x101c10290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101c109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101c110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101c11970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x101c12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x101c12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101c12960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101c135d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101c13b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101c13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x101c14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101c14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x101c14dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101c15300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x101c155c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101c15a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101c15f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101c163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101c16840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101c16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101c17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101c17620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101c17ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101c17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x101c18220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101c18830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x101c18e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101c19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x101c19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x101c1a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101c1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x101c1afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x101c1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x101c1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x101c1c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x101c1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x101c1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x101c1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x101c1d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x101c1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x101c1e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x101c1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x101c1e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x101c1ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x101c1f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101c1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101c1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101c200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x101c20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101c20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x101c20eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x101c21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101c217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x101c21d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x101c22290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101c227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x101c22d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x101c23280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101c237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101c23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x101c24270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101c247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101c24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101c25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101c257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101c25d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101c26250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101c267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101c26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101c27240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101c27790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x101c27ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x101c28230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101c28780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x101c28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x101c29220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101c29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x101c19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x101c29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x101c2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x101c2a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x101c2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x101c2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x101c2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x101c2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x101c2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x101c2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x101c2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x101c2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x101c2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x101c2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x101c2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x101c2e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x101c2ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x101c2f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x101c2f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x101c2fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101c2ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101c30460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101c30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x101c30da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x101c31240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101c316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101c31b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x101c32020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x101c324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101c32960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101c32e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x101c332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101c33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101c33be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101c34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101c34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101c349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101c34e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101c35300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101c357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101c35c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101c360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101c36580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101c36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101c36ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101c37360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101c37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101c37ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x101c38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x101c385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101c38a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x101c38f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101c393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101c39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x101c39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x101c3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x101c3a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x101c3aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x101c3af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x101c3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x101c3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x101c3bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x101c3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x101c3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101c3cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x101c3cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x101c3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x101c3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x101c3ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x101c3e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x101c3e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x101c3eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x101c3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x101c3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x101c3f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x101c3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x101c402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101c40760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101c40c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x101c410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x101c41540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101c419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x101c41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x101c42320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101c427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101c42c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101c43100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101c435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101c43a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101c43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101c44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101c44820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x101c44cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101c45160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101c45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101c45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101c45ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101c46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101c46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101c46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101c472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101c478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101c47ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x101c484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x101c48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x101c49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x101c49420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101c49a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x101c4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x101c4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x101c4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x101c4b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x101c4b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x101c4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x101c4c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x101c4c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x101c4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x101c4d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x101c4d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x101c4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x101c4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x101c4e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x101c4ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x101c4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x101c4f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x101c4fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101c502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101c50820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x101c50d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x101c512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101c51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x101c51d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x101c522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101c52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x101c52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x101c532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101c537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101c53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101c54290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101c547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101c54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101c55280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101c557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101c55d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x101c56270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101c567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101c56d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101c57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101c577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x101c57d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x101c58250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101c587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x101c58cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x101c59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101c59790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x101c59ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x101c5a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x101c5a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x101c5acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x101c5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x101c5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x101c5bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x101c5c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x101c5c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x101c5ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x101c5d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x101c5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x101c5dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x101c5e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x101c5e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x101c5ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x101c5f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x101c5f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101c5f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x101c5fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101c60300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101c607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101c60c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101c610e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x101c61580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101c61a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x101c61ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x101c62360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101c62800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x101c62ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101c631f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101c63910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x101c64030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101c64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x101c64e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x101c65130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101c65920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101c65be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101c661f0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
0.00.767.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.767.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101c65ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x101c47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101c47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101c48180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101c1b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101c1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x101c1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x101c49cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x101c12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x101c19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x101c19a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x101c1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x101c184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x101c1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x101c11610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x101c1d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x101c29ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x101c653f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x101c147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x101c14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x101c4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101c48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101c12c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101c12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x101c131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x101c66650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101c66910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101c66bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101c66e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101c67150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x101c67410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101c676d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x101c67990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101c67c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x101c67f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101c681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101c68490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101c68750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101c68a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101c68cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101c68f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101c69250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101c69510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101c697d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x101c69a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101c69d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x101c6a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101c6a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x101c6a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x101c6a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101c6ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x101c6add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x101c6b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x101c6b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x101c6b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x101c6b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x101c6bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x101c6be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x101c6c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x101c6c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x101c6c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x101c6c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x101c6cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x101c6ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x101c6d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101c6d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101c6d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101c6d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x101c6dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101c6df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x101c6e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x101c6e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101c6e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x101c6ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x101c6ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101c6efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x101c6f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x101c6f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101c6f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101c6fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x101c6fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101c70050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101c70310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101c705d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101c70890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101c70b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101c70e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101c710d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101c71390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101c71650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101c71910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x101c71bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x101c71e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101c72150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x101c72410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x101c726d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101c72990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x101c72c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x101c72f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x101c731d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x101c73490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x101c73750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x101c73a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x101c73cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x101c73f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x101c74250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x101c74510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x101c747d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x101c74a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x101c74d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x101c75010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x101c752d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x101c75590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x101c75850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x101c75b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x101c75dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x101c76090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101c76350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101c76610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101c768d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x101c76b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x101c76e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101c77110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101c773d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x101c77690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x101c77950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101c77c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101c77ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x101c78190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101c78450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101c78710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101c789d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101c78c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101c78f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101c79210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101c794d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101c79790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101c79a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101c79d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101c79fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101c7a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101c7a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101c7a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101c7aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101c7ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x101c7b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x101c7b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101c7b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x101c7b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101c7bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101c7be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x101c7c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x101c7c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x101c7c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x101c7c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x101c7cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x101c7ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x101c7d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x101c7d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x101c7d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x101c7d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101c7dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x101c7df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x101c7e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x101c7e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x101c7e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x101c7ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x101c7ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x101c7ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x101c7f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x101c7f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x101c7f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x101c7fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x101c7fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101c80010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101c802d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x101c80590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x101c80850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101c80b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x101c80dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x101c81090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101c81350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101c81610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101c818d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101c81b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101c81e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101c82110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101c823d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101c82690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x101c82950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101c82c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101c82ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101c83190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101c83450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101c83710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101c839d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101c83c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101c83f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101c84210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101c844d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x101c84790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x101c84a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x101c84d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x101c84fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101c85510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x101c85a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x101c85d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x101c861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x101c86650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x101c86af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x101c872a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x101c87560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x101c87820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x101c87c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x101c88100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x101c88570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x101c889e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x101c88e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x101c892c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x101c89730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x101c89ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x101c8a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x101c8a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101c8a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101c8ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x101c8b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x101c8b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101c8bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x101c8bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x101c8c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101c8c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x101c8cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x101c8d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101c8d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101c8d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101c8de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101c8e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101c8e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101c8eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101c8eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101c8f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x101c8f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101c8fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101c901b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101c90620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101c90a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x101c90f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x101c91370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101c917e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x101c91c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x101c920c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101c92530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x101c929a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x101c92e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x101c93280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x101c936f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x101c93b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x101c93fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x101c94440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x101c948b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x101c94d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x101c95190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x101c95600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x101c95a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x101c95ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x101c96350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x101c967c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x101c96c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x101c970a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x101c97510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101c97980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x101c97df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101c98260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101c986d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101c98b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101c98fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x101c99420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101c99890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x101c99d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x101c9a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101c9a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x101c9aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101c9aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101c9b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x101c9c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101c9c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x101c9ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x101c9d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101c9d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101c9dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101c9e210 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x101d04660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x101d04ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x101d04f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x101d053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x101d05820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x101d05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x101d06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x101d06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x101d069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x101d06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x101d072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x101d079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x101d08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x101d08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x101d094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x101d09be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x101d0a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x101d0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x101d0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x101d0b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x101d0bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x101d0c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x101d0cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x101d0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x101d0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x101d0ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x101d0e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x101d0e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x101d0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x101d0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x101d0f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x101d0f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x101d0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x101d0ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x101d10420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x101d10890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x101d10d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x101d11170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x101d115e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x101d11a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x101d11ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x101d12330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x101d127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x101d12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x101d13080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x101d134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x101d13960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x101d13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x101d14240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x101d146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x101d14b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x101d14f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x101d15400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x101d15870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x101d15ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x101d16150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x101d166c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x101d16bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x101d17030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x101d174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x101d17910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x101d17d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x101d181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x101d18660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x101d18ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x101d18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x101d193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x101d19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x101d19c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x101d1a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x101d1a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x101d1a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x101d1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x101d1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x101d1b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x101d1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x101d1c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x101d1c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x101d1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x101d1cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x101d1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x101d1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x101d1dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x101d1df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x101d1e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x101d1e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x101d1ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x101d1f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x101d1f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x101d1f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x101d1fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x101d202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x101d20710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x101d20b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x101d20ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x101d21460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x101d218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x101d21d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x101d221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x101d22620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x101d22a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x101d22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x101d23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x101d23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x101d23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x101d24330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x101d247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x101d24c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x101d25080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x101d254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x101d25960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x101d25dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x101d26240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x101d266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x101d26b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x101d26f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x101d27400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x101d27870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x101d27ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x101d28150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x101d285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x101d28a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x101d28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x101d29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x101d29780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x101d29bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x101d2a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x101d2a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x101d2a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x101d2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x101d2b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x101d2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x101d2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x101d2bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x101d2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x101d2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x101d2ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x101d2d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x101d2d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x101d2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x101d2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x101d2e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x101d2e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x101d2ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x101d2f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x101d2f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x101d2f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x101d2fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x101d30200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x101d30670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x101d30ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x101d30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x101d313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x101d31830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x101d31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x101d32110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x101d32580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x101d329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x101d32e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x101d332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x101d33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x101d33bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x101d34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x101d34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x101d34900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x101d34d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x101d351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x101d35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x101d35ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x101d35f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x101d363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x101d36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x101d36c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x101d370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x101d37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x101d379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x101d37e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x101d382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x101d38720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x101d38b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x101d39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x101d39470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x101d398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x101d39d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x101d3a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x101d3a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x101d3aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x101d3af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x101d3b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x101d3b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x101d3bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x101d3c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x101d3c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x101d3c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x101d3ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x101d3d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x101d3d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x101d3db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x101d3dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x101d3e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x101d3e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x101d3ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x101d3f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x101d3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x101d3fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x101d3fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x101d40360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x101d407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x101d40c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x101d410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x101d41c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x101d41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x101d421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x101d42620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x101d42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x101d42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x101d43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x101d437e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x101d43c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x101d440c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x101d44530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x101d449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x101d44e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x101d45280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x101d456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x101d45b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x101d45fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x101d46440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x101d468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x101d46d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x101d47190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x101d47600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x101d47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x101d47ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x101d48350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x101d487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x101d48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x101d490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x101d49510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x101d49980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x101d49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x101d4a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x101d4a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x101d4ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x101d4afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x101d4b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x101d4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x101d4bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x101d4c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x101d4c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x101d4ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x101d4cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x101d4d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x101d4d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x101d4dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x101d4e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x101d4e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x101d4e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x101d4edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x101d4f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x101d4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x101d4fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x101d4ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x101d50400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x101d50870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x101d50ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x101d51150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x101d515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x101d51a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x101d51ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x101d52310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x101d52780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x101d52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x101d53060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x101d534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x101d53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x101d53db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x101d54220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x101d54690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x101d54b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x101d54f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x101d553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x101d55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x101d562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x101d569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x101d57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x101d57820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x101d57ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x101d57f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x101d58550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x101d58b60 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.820s
user	0m0.279s
sys	0m0.326s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4719 (956e0697)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14fe0d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14fe0d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14fe0de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14fe0e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14fe0e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14fe0efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14fe0f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14fe0fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14fe100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14fe105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14fe10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14fe10fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14fe11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14fe12280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14fe12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14fe131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14fe138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14fe13ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14fe14710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14fe14ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14fe15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14fe15d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14fe16440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14fe16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14fe17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14fe176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14fe17cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14fe18940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14fe18e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14fe19140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14fe195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14fe198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14fe1a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14fe1a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14fe1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14fe1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14fe1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14fe1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14fe1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14fe1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14fe1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14fe1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14fe1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14fe1d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14fe1d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14fe1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14fe1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14fe1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14fe1f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14fe1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14fe1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14fe20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14fe20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14fe20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14fe21720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14fe21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14fe22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14fe22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14fe22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14fe23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14fe233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14fe23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14fe23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14fe241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14fe24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14fe24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14fe24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14fe25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14fe258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14fe25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14fe26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14fe266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14fe26b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14fe270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14fe27600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14fe27b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14fe280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14fe285f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14fe28b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14fe29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14fe295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14fe29b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14fe2a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14fe2a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14fe2ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14fe2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14fe2b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14fe2bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14fe2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14fe2c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14fe2cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14fe2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14fe2d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14fe2daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14fe2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14fe2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14fe2eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14fe1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14fe2ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14fe2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14fe2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14fe301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14fe306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14fe30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14fe31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14fe316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14fe31c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14fe32180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14fe326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14fe32c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14fe33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14fe336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14fe33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14fe340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14fe34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14fe349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14fe34e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14fe35330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14fe357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14fe35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14fe36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14fe365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14fe36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14fe36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14fe37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14fe37830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14fe37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14fe38170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14fe38610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14fe38ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14fe38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14fe393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14fe39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14fe39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14fe3a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14fe3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14fe3ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14fe3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14fe3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14fe3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14fe3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14fe3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14fe3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14fe3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14fe3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14fe3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14fe3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14fe3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14fe3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14fe3e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14fe3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14fe3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14fe3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14fe3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14fe3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14fe402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14fe40790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14fe40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14fe410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14fe41570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14fe41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14fe41eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14fe42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14fe427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14fe42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14fe43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14fe435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14fe43a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14fe43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14fe443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14fe44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14fe44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14fe45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14fe45630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14fe45ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14fe45f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14fe46410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14fe468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14fe46d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14fe471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14fe47690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14fe47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14fe47fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14fe48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14fe48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14fe48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14fe49250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14fe496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14fe49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14fe4a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14fe4a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14fe4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14fe4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14fe4b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14fe4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14fe4be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14fe4c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14fe4c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14fe4cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14fe4d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14fe4d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14fe4e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14fe4e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14fe4e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14fe4eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14fe4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14fe4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14fe50040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14fe504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14fe50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14fe51130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14fe51680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14fe51bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14fe52120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14fe52670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14fe52bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14fe53110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14fe53660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14fe53bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14fe54100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14fe54650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14fe54ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14fe550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14fe55640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14fe55b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14fe560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14fe56630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14fe56b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14fe570d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14fe57620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14fe57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14fe580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14fe58610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14fe58b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14fe590b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14fe59600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14fe59b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14fe5a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14fe5a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14fe5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14fe5b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14fe5b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14fe5bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14fe5c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14fe5c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14fe5cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14fe5d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14fe5d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14fe5db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14fe5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14fe5e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14fe5eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14fe5f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14fe5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14fe5faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14fe60040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14fe60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14fe60ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14fe61030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14fe61580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14fe61ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14fe62020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14fe62570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14fe62ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14fe63010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14fe63560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14fe63ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14fe63f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14fe643f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14fe64890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14fe64d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14fe651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14fe65670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14fe65b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14fe65fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14fe66450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14fe668f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14fe66d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14fe67230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14fe676d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14fe67b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14fe68010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14fe68560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14fe68c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14fe693a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14fe69ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14fe6a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14fe6a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14fe6ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14fe6af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14fe6b560 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
0.00.100.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14ff09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14ff097e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14ff09c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14ff0a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14ff0a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14ff0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14ff0ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14ff0b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14ff0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14ff0bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14ff0c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14ff0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14ff0d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14ff0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14ff0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14ff0e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14ff0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14ff0f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14ff0fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14ff10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14ff10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14ff114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14ff11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14ff12310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14ff12a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14ff12cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14ff12fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14ff13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14ff13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14ff13d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14ff14200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14ff14710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14ff14b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14ff14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14ff152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14ff15720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14ff15c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14ff16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14ff16680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14ff16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14ff17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14ff17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14ff17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14ff17f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14ff18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14ff188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14ff18d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14ff191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14ff19640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14ff19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14ff19f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14ff1a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14ff1a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14ff1ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14ff1b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14ff1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14ff1bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14ff1c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14ff1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14ff1ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14ff1d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14ff1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14ff1dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14ff1e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14ff1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14ff1e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14ff1ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14ff1f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14ff1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14ff1fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14ff200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14ff20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14ff20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14ff20f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14ff214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14ff21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14ff21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14ff224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14ff22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14ff22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14ff234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14ff23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14ff23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14ff244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14ff249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14ff24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14ff25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14ff259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14ff25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14ff26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14ff269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14ff26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14ff27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14ff279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14ff27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14ff28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14ff289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14ff28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14ff29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14ff299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14ff29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14ff2a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14ff2a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14ff2aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14ff2b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14ff2b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14ff2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14ff2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14ff2c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14ff2cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14ff2d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14ff2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14ff2deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14ff2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14ff2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14ff2ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14ff2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14ff2f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14ff2fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14ff2ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14ff303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14ff30850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14ff30cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14ff31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14ff31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14ff31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14ff31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14ff32410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14ff328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14ff32d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14ff331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14ff33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14ff33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14ff33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14ff34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14ff34910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14ff34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14ff35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14ff356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14ff35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14ff36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14ff364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14ff36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14ff36e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14ff372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14ff37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14ff37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14ff38090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14ff38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14ff389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14ff38e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14ff39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14ff397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14ff39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14ff3a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14ff3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14ff3aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14ff3aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14ff3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14ff3b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14ff3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14ff3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14ff3c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14ff3ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14ff3cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14ff3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14ff3d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14ff3dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14ff3e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14ff3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14ff3eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14ff3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14ff3f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14ff3f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14ff3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14ff40210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14ff406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14ff40b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14ff40ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14ff41490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14ff41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14ff41dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14ff42270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14ff42710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14ff42bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14ff43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14ff434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14ff43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14ff43e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14ff442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14ff44770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14ff44c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14ff450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14ff45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14ff45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14ff460a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14ff465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14ff468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14ff46ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14ff474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14ff47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14ff482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14ff48770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14ff48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14ff49040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14ff49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14ff49e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14ff4a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14ff4a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14ff4ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14ff4b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14ff4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14ff4be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14ff4c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14ff4c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14ff4ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14ff4d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14ff4d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14ff4de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14ff4e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14ff4e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14ff4ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14ff4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14ff4f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14ff4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14ff50380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14ff508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14ff50e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14ff51370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14ff518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14ff51e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14ff52360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14ff528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14ff52e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14ff53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14ff538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14ff53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14ff54340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14ff54890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14ff54de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14ff55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14ff55880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14ff55dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14ff56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14ff56870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14ff56dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14ff57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14ff57860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14ff57db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14ff58300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14ff58850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14ff58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14ff592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14ff59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14ff59d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14ff5a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14ff5a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14ff5ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14ff5b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14ff5b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14ff5bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14ff5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14ff5c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14ff5cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14ff5d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14ff5d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14ff5dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14ff5e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14ff5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14ff5eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14ff5efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14ff5f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ff5f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ff5fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ff60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ff606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ff60b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ff61030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ff614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ff61970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ff61e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ff622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ff62800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ff62f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ff63640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ff63d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ff64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ff64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ff64f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ff651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ff65800 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1510044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1510056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1510063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1510077e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151008300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151008ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1510092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1510099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15100a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15100a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15100af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15100b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15100be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15100c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15100cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15100d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15100dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15100dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15100e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15100e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15100e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15100ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15100f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15100f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15100fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15100fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1510102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151010730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151010ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151011010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151011480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1510118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151011d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1510121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151012640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151012ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151012f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151013390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151013800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151013c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1510140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151014550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1510149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151014e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1510152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151015710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151015b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151015ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151016560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151016a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151016ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151017340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1510177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151017c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151018090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151018500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151018970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151018de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151019250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1510196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151019b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151019fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15101a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15101a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15101acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15101b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15101b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15101ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15101beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15101c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15101c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15101cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15101d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15101d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15101d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15101ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15101e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15101e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15101eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15101ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15101f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15101f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15101fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151020140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1510205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151020a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151020e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151021300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151021770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151021be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x151022050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1510224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151022930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151022da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151023210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151023aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151023d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1510241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151024640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151024ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151024f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151025390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151025800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151025c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1510260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151026550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1510269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151026e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1510272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151027710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151027b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151027ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151028460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1510288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151028d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1510291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151029620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151029a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151029f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15102a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15102a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15102ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15102b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15102b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15102b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15102be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15102c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15102c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15102cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15102cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15102d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15102d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15102dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15102e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15102e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15102ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15102eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15102f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15102f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15102fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1510300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151030510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151030980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151030df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151031260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1510316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151031b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151031fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151032420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151032890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151032d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151033170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1510335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151033a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151033ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151034330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1510347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151034c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151035080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1510354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151035960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151035dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151036240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1510366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151036b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151036f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151037400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151037870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151037ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151038150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1510385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151038a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151038ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151039310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151039780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151039bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15103a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15103a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15103a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15103adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15103b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15103b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15103bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15103bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15103c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15103c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15103ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15103d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15103d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15103da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15103de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15103e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15103e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15103ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15103f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15103f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15103f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15103fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151040200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151040670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151040ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151040f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151041ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151041d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151042050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1510424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151042930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151042da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151043210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151043680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151043af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151043f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1510443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151044840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151044cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151045120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151045590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151045a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151045e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1510462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151046750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151046bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151047030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1510474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151047910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151047d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1510481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151048660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151048ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151048f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1510493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151049820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151049c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15104a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15104a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15104a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15104ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15104b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15104b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15104bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15104c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15104c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15104c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15104cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15104d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15104d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15104dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15104df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15104e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15104e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15104ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15104f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15104f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15104f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15104fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1510502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151050710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151050b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151050ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151051460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1510518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151051d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1510521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151052620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151052a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151052f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151053370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1510537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151053c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1510540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151054530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1510549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151054e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151055280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1510556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151056160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151056880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151056fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1510576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151057980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151057df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1510583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151058a00 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.946s
user	0m0.229s
sys	0m0.185s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.45 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.33 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.78 sec*proc (2 tests)

Total Test time (real) =   1.79 sec
        1.82 real         0.52 user         0.21 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.55 real         0.12 user         0.08 sys
```
