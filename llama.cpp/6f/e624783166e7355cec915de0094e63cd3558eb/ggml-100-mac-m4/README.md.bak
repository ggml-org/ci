### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    2.04 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.23 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.36 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.21 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.31 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.22 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  180.73 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.89 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   26.09 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.38 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.25 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 224.31 sec*proc (27 tests)

Total Test time (real) = 224.32 sec

real	3m44.347s
user	7m42.090s
sys	0m6.102s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.17 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.92 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.21 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.21 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.20 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.22 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.59 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.03 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.17 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.19 sec*proc (27 tests)

Total Test time (real) =  51.20 sec

real	0m51.209s
user	1m11.607s
sys	0m5.415s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.117 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.406 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.622 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.632 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.633 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.634 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.635 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.636 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.637 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.638 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.639 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.639 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.643 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.643 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.644 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.644 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.645 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.649 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.650 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.067 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.069 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.070 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.070 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.071 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.071 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.071 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.072 I llama_model_loader: - type  f32:  124 tensors
0.00.032.073 I llama_model_loader: - type  f16:   73 tensors
0.00.036.718 I llm_load_vocab: special tokens cache size = 5
0.00.039.130 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.039.134 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.039.135 I llm_load_print_meta: arch             = bert
0.00.039.135 I llm_load_print_meta: vocab type       = WPM
0.00.039.136 I llm_load_print_meta: n_vocab          = 30522
0.00.039.136 I llm_load_print_meta: n_merges         = 0
0.00.039.136 I llm_load_print_meta: vocab_only       = 0
0.00.039.137 I llm_load_print_meta: n_ctx_train      = 512
0.00.039.137 I llm_load_print_meta: n_embd           = 384
0.00.039.145 I llm_load_print_meta: n_layer          = 12
0.00.039.148 I llm_load_print_meta: n_head           = 12
0.00.039.149 I llm_load_print_meta: n_head_kv        = 12
0.00.039.175 I llm_load_print_meta: n_rot            = 32
0.00.039.176 I llm_load_print_meta: n_swa            = 0
0.00.039.176 I llm_load_print_meta: n_embd_head_k    = 32
0.00.039.177 I llm_load_print_meta: n_embd_head_v    = 32
0.00.039.178 I llm_load_print_meta: n_gqa            = 1
0.00.039.179 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.039.180 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.039.181 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.039.181 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.039.187 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.039.187 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.039.187 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.039.188 I llm_load_print_meta: n_ff             = 1536
0.00.039.189 I llm_load_print_meta: n_expert         = 0
0.00.039.189 I llm_load_print_meta: n_expert_used    = 0
0.00.039.189 I llm_load_print_meta: causal attn      = 0
0.00.039.190 I llm_load_print_meta: pooling type     = 2
0.00.039.190 I llm_load_print_meta: rope type        = 2
0.00.039.190 I llm_load_print_meta: rope scaling     = linear
0.00.039.191 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.039.192 I llm_load_print_meta: freq_scale_train = 1
0.00.039.192 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.039.192 I llm_load_print_meta: rope_finetuned   = unknown
0.00.039.192 I llm_load_print_meta: ssm_d_conv       = 0
0.00.039.193 I llm_load_print_meta: ssm_d_inner      = 0
0.00.039.193 I llm_load_print_meta: ssm_d_state      = 0
0.00.039.193 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.039.193 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.039.204 I llm_load_print_meta: model type       = 33M
0.00.039.205 I llm_load_print_meta: model ftype      = F16
0.00.039.206 I llm_load_print_meta: model params     = 33.21 M
0.00.039.206 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.039.207 I llm_load_print_meta: general.name     = Bge Small
0.00.039.207 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.039.208 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.039.208 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.039.209 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.039.209 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.039.209 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.039.210 I llm_load_print_meta: max token length = 21
0.00.041.212 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.041.212 I llm_load_tensors: offloading output layer to GPU
0.00.041.213 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.041.239 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.041.240 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.041.800 I llama_new_context_with_model: n_seq_max     = 1
0.00.041.802 I llama_new_context_with_model: n_ctx         = 512
0.00.041.802 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.041.802 I llama_new_context_with_model: n_batch       = 2048
0.00.041.802 I llama_new_context_with_model: n_ubatch      = 2048
0.00.041.803 I llama_new_context_with_model: flash_attn    = 0
0.00.041.803 I llama_new_context_with_model: freq_base     = 10000.0
0.00.041.804 I llama_new_context_with_model: freq_scale    = 1
0.00.041.804 I ggml_metal_init: allocating
0.00.041.809 I ggml_metal_init: found device: Apple M4
0.00.041.812 I ggml_metal_init: picking default device: Apple M4
0.00.042.654 I ggml_metal_init: using embedded metal library
0.00.059.333 I ggml_metal_init: GPU name:   Apple M4
0.00.059.337 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.338 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.338 I ggml_metal_init: simdgroup reduction   = true
0.00.059.339 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.339 I ggml_metal_init: has bfloat            = true
0.00.059.339 I ggml_metal_init: use bfloat            = true
0.00.059.340 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.964 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.074.967 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.074.968 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.075.867 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.075.869 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.075.869 I llama_new_context_with_model: graph nodes  = 429
0.00.075.870 I llama_new_context_with_model: graph splits = 2
0.00.075.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.082.523 I 
0.00.082.558 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.083.320 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.087.626 I llama_perf_context_print:        load time =      61.11 ms
0.00.087.628 I llama_perf_context_print: prompt eval time =       4.15 ms /     9 tokens (    0.46 ms per token,  2166.59 tokens per second)
0.00.087.629 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.087.629 I llama_perf_context_print:       total time =       5.10 ms /    10 tokens
0.00.087.857 I ggml_metal_free: deallocating

real	0m0.266s
user	0m0.055s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.351 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.505 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.510 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.510 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.510 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.511 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.511 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.512 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.512 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.513 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.513 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.514 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.516 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.516 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.519 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.519 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.519 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.520 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.520 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.785 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.786 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.787 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.787 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.787 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.788 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.788 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.788 I llama_model_loader: - type  f32:  124 tensors
0.00.014.789 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.361 I llm_load_vocab: special tokens cache size = 5
0.00.018.728 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.731 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.731 I llm_load_print_meta: arch             = bert
0.00.018.732 I llm_load_print_meta: vocab type       = WPM
0.00.018.732 I llm_load_print_meta: n_vocab          = 30522
0.00.018.732 I llm_load_print_meta: n_merges         = 0
0.00.018.732 I llm_load_print_meta: vocab_only       = 0
0.00.018.732 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.733 I llm_load_print_meta: n_embd           = 384
0.00.018.733 I llm_load_print_meta: n_layer          = 12
0.00.018.735 I llm_load_print_meta: n_head           = 12
0.00.018.736 I llm_load_print_meta: n_head_kv        = 12
0.00.018.742 I llm_load_print_meta: n_rot            = 32
0.00.018.742 I llm_load_print_meta: n_swa            = 0
0.00.018.742 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.743 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.743 I llm_load_print_meta: n_gqa            = 1
0.00.018.744 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.750 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.752 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.752 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.753 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.753 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.753 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.758 I llm_load_print_meta: n_ff             = 1536
0.00.018.758 I llm_load_print_meta: n_expert         = 0
0.00.018.758 I llm_load_print_meta: n_expert_used    = 0
0.00.018.758 I llm_load_print_meta: causal attn      = 0
0.00.018.758 I llm_load_print_meta: pooling type     = 2
0.00.018.758 I llm_load_print_meta: rope type        = 2
0.00.018.759 I llm_load_print_meta: rope scaling     = linear
0.00.018.759 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.759 I llm_load_print_meta: freq_scale_train = 1
0.00.018.759 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.760 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.760 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.760 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.760 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.765 I llm_load_print_meta: model type       = 33M
0.00.018.766 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.766 I llm_load_print_meta: model params     = 33.21 M
0.00.018.766 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.767 I llm_load_print_meta: general.name     = Bge Small
0.00.018.767 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.767 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.767 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.767 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.768 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.768 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.768 I llm_load_print_meta: max token length = 21
0.00.020.084 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.084 I llm_load_tensors: offloading output layer to GPU
0.00.020.084 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.092 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.093 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.438 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.439 I llama_new_context_with_model: n_ctx         = 512
0.00.020.439 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.439 I llama_new_context_with_model: n_batch       = 2048
0.00.020.439 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.439 I llama_new_context_with_model: flash_attn    = 0
0.00.020.440 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.440 I llama_new_context_with_model: freq_scale    = 1
0.00.020.440 I ggml_metal_init: allocating
0.00.020.443 I ggml_metal_init: found device: Apple M4
0.00.020.446 I ggml_metal_init: picking default device: Apple M4
0.00.020.994 I ggml_metal_init: using embedded metal library
0.00.024.311 I ggml_metal_init: GPU name:   Apple M4
0.00.024.313 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.313 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.314 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.314 I ggml_metal_init: simdgroup reduction   = true
0.00.024.314 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.314 I ggml_metal_init: has bfloat            = true
0.00.024.315 I ggml_metal_init: use bfloat            = true
0.00.024.315 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.316 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.714 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.716 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.718 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.373 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.374 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.374 I llama_new_context_with_model: graph nodes  = 429
0.00.035.375 I llama_new_context_with_model: graph splits = 2
0.00.035.388 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.874 I 
0.00.039.898 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.423 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.861 I llama_perf_context_print:        load time =      30.52 ms
0.00.044.862 I llama_perf_context_print: prompt eval time =       4.30 ms /     9 tokens (    0.48 ms per token,  2094.00 tokens per second)
0.00.044.863 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.863 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.045.059 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.127 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.202 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.736 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.742 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.743 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.744 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.745 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.745 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.746 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.747 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.748 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.749 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.751 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.754 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.755 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.755 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.667 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.792 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.666 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.668 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.669 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.669 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.670 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.670 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.670 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.671 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.671 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.672 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.672 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.673 I llama_model_loader: - type  f32:   41 tensors
0.00.048.673 I llama_model_loader: - type  f16:   29 tensors
0.00.066.654 W llm_load_vocab: empty token at index 5
0.00.071.122 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.385 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.411 I llm_load_vocab: special tokens cache size = 5
0.00.330.149 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.330.154 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.330.154 I llm_load_print_meta: arch             = jina-bert-v2
0.00.330.155 I llm_load_print_meta: vocab type       = BPE
0.00.330.155 I llm_load_print_meta: n_vocab          = 61056
0.00.330.155 I llm_load_print_meta: n_merges         = 39382
0.00.330.155 I llm_load_print_meta: vocab_only       = 0
0.00.330.156 I llm_load_print_meta: n_ctx_train      = 8192
0.00.330.156 I llm_load_print_meta: n_embd           = 384
0.00.330.156 I llm_load_print_meta: n_layer          = 4
0.00.330.162 I llm_load_print_meta: n_head           = 12
0.00.330.163 I llm_load_print_meta: n_head_kv        = 12
0.00.330.186 I llm_load_print_meta: n_rot            = 32
0.00.330.187 I llm_load_print_meta: n_swa            = 0
0.00.330.187 I llm_load_print_meta: n_embd_head_k    = 32
0.00.330.187 I llm_load_print_meta: n_embd_head_v    = 32
0.00.330.188 I llm_load_print_meta: n_gqa            = 1
0.00.330.188 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.330.189 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.330.191 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.330.191 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.330.191 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.330.192 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.330.192 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.330.193 I llm_load_print_meta: n_ff             = 1536
0.00.330.194 I llm_load_print_meta: n_expert         = 0
0.00.330.194 I llm_load_print_meta: n_expert_used    = 0
0.00.330.194 I llm_load_print_meta: causal attn      = 0
0.00.330.194 I llm_load_print_meta: pooling type     = -1
0.00.330.194 I llm_load_print_meta: rope type        = -1
0.00.330.195 I llm_load_print_meta: rope scaling     = linear
0.00.330.195 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.330.195 I llm_load_print_meta: freq_scale_train = 1
0.00.330.196 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.330.196 I llm_load_print_meta: rope_finetuned   = unknown
0.00.330.196 I llm_load_print_meta: ssm_d_conv       = 0
0.00.330.196 I llm_load_print_meta: ssm_d_inner      = 0
0.00.330.196 I llm_load_print_meta: ssm_d_state      = 0
0.00.330.196 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.330.197 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.330.217 I llm_load_print_meta: model type       = 33M
0.00.330.217 I llm_load_print_meta: model ftype      = F16
0.00.330.218 I llm_load_print_meta: model params     = 32.90 M
0.00.330.218 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.330.218 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.330.218 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.330.219 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.330.219 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.330.219 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.330.219 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.330.219 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.330.219 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.330.220 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.330.220 I llm_load_print_meta: max token length = 45
0.00.331.147 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.331.147 I llm_load_tensors: offloading output layer to GPU
0.00.331.148 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.331.172 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.172 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.331.942 I llama_new_context_with_model: n_seq_max     = 1
0.00.331.943 I llama_new_context_with_model: n_ctx         = 8192
0.00.331.943 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.331.943 I llama_new_context_with_model: n_batch       = 2048
0.00.331.944 I llama_new_context_with_model: n_ubatch      = 2048
0.00.331.944 I llama_new_context_with_model: flash_attn    = 0
0.00.331.944 I llama_new_context_with_model: freq_base     = 10000.0
0.00.331.944 I llama_new_context_with_model: freq_scale    = 1
0.00.331.945 I ggml_metal_init: allocating
0.00.331.948 I ggml_metal_init: found device: Apple M4
0.00.331.950 I ggml_metal_init: picking default device: Apple M4
0.00.332.663 I ggml_metal_init: using embedded metal library
0.00.335.621 I ggml_metal_init: GPU name:   Apple M4
0.00.335.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.622 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.623 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.623 I ggml_metal_init: simdgroup reduction   = true
0.00.335.623 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.623 I ggml_metal_init: has bfloat            = true
0.00.335.624 I ggml_metal_init: use bfloat            = true
0.00.335.624 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.394 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.347.395 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.396 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.348.041 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.348.042 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.348.042 I llama_new_context_with_model: graph nodes  = 154
0.00.348.042 I llama_new_context_with_model: graph splits = 2
0.00.348.060 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.358.926 I 
0.00.358.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.359.106 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.107 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.110 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.110 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.112 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.112 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.359.657 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.363.384 I llama_perf_context_print:        load time =     335.72 ms
0.00.363.385 I llama_perf_context_print: prompt eval time =       3.72 ms /    62 tokens (    0.06 ms per token, 16680.12 tokens per second)
0.00.363.389 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.363.389 I llama_perf_context_print:       total time =       4.46 ms /    63 tokens
0.00.363.634 I ggml_metal_free: deallocating

real	0m1.042s
user	0m0.336s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.216 I main: llama backend init
0.00.000.222 I main: load the model and apply lora adapter, if any
0.00.079.680 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.090.669 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.090.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.090.687 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.090.688 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.090.688 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.090.703 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.090.704 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.090.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.090.706 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.090.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.090.707 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.090.708 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.090.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.090.710 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.090.715 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.090.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.090.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.097.660 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.099.875 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.106.909 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.106.915 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.106.916 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.106.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.106.917 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.106.919 I llama_model_loader: - type  f32:  194 tensors
0.00.106.919 I llama_model_loader: - type  f16:   98 tensors
0.00.146.053 I llm_load_vocab: special tokens cache size = 25
0.00.153.841 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.153.845 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.153.845 I llm_load_print_meta: arch             = gptneox
0.00.153.845 I llm_load_print_meta: vocab type       = BPE
0.00.153.846 I llm_load_print_meta: n_vocab          = 50304
0.00.153.846 I llm_load_print_meta: n_merges         = 50009
0.00.153.846 I llm_load_print_meta: vocab_only       = 0
0.00.153.846 I llm_load_print_meta: n_ctx_train      = 2048
0.00.153.846 I llm_load_print_meta: n_embd           = 2048
0.00.153.846 I llm_load_print_meta: n_layer          = 24
0.00.153.850 I llm_load_print_meta: n_head           = 16
0.00.153.851 I llm_load_print_meta: n_head_kv        = 16
0.00.153.871 I llm_load_print_meta: n_rot            = 32
0.00.153.872 I llm_load_print_meta: n_swa            = 0
0.00.153.872 I llm_load_print_meta: n_embd_head_k    = 128
0.00.153.872 I llm_load_print_meta: n_embd_head_v    = 128
0.00.153.873 I llm_load_print_meta: n_gqa            = 1
0.00.153.873 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.153.874 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.153.874 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.153.875 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.153.875 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.153.875 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.153.875 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.153.876 I llm_load_print_meta: n_ff             = 8192
0.00.153.876 I llm_load_print_meta: n_expert         = 0
0.00.153.876 I llm_load_print_meta: n_expert_used    = 0
0.00.153.877 I llm_load_print_meta: causal attn      = 1
0.00.153.877 I llm_load_print_meta: pooling type     = 0
0.00.153.877 I llm_load_print_meta: rope type        = 2
0.00.153.877 I llm_load_print_meta: rope scaling     = linear
0.00.153.877 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.153.880 I llm_load_print_meta: freq_scale_train = 1
0.00.153.880 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.153.880 I llm_load_print_meta: rope_finetuned   = unknown
0.00.153.880 I llm_load_print_meta: ssm_d_conv       = 0
0.00.153.880 I llm_load_print_meta: ssm_d_inner      = 0
0.00.153.881 I llm_load_print_meta: ssm_d_state      = 0
0.00.153.881 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.153.881 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.153.890 I llm_load_print_meta: model type       = 1.4B
0.00.153.891 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.153.891 I llm_load_print_meta: model params     = 1.41 B
0.00.153.892 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.153.892 I llm_load_print_meta: general.name     = 1.4B
0.00.153.892 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.153.892 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.153.893 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.153.893 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.153.893 I llm_load_print_meta: LF token         = 128 ''
0.00.153.893 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.153.893 I llm_load_print_meta: max token length = 1024
0.00.156.596 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.156.596 I llm_load_tensors: offloading output layer to GPU
0.00.156.596 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.156.616 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.156.617 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.157.654 I llama_new_context_with_model: n_seq_max     = 1
0.00.157.655 I llama_new_context_with_model: n_ctx         = 2048
0.00.157.655 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.157.655 I llama_new_context_with_model: n_batch       = 2048
0.00.157.655 I llama_new_context_with_model: n_ubatch      = 512
0.00.157.656 I llama_new_context_with_model: flash_attn    = 0
0.00.157.656 I llama_new_context_with_model: freq_base     = 10000.0
0.00.157.656 I llama_new_context_with_model: freq_scale    = 1
0.00.157.657 I ggml_metal_init: allocating
0.00.157.660 I ggml_metal_init: found device: Apple M4
0.00.157.662 I ggml_metal_init: picking default device: Apple M4
0.00.158.305 I ggml_metal_init: using embedded metal library
0.00.168.546 I ggml_metal_init: GPU name:   Apple M4
0.00.168.548 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.168.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.168.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.168.549 I ggml_metal_init: simdgroup reduction   = true
0.00.168.550 I ggml_metal_init: simdgroup matrix mul. = true
0.00.168.550 I ggml_metal_init: has bfloat            = true
0.00.168.550 I ggml_metal_init: use bfloat            = true
0.00.168.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.168.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.216.732 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.216.738 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.216.757 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.217.757 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.217.759 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.217.759 I llama_new_context_with_model: graph nodes  = 967
0.00.217.759 I llama_new_context_with_model: graph splits = 2
0.00.217.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.295.673 I main: llama threadpool init, n_threads = 4
0.00.295.703 I 
0.00.295.736 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.295.737 I 
0.00.295.816 I sampler seed: 1234
0.00.295.820 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.295.846 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.295.847 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.295.848 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.149.264 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.02.149.265 I llama_perf_context_print:        load time =     215.98 ms
0.02.149.266 I llama_perf_context_print: prompt eval time =      43.90 ms /     7 tokens (    6.27 ms per token,   159.45 tokens per second)
0.02.149.266 I llama_perf_context_print:        eval time =    1806.44 ms /    63 runs   (   28.67 ms per token,    34.88 tokens per second)
0.02.149.267 I llama_perf_context_print:       total time =    1853.59 ms /    70 tokens
0.02.149.460 I ggml_metal_free: deallocating

real	0m2.459s
user	0m0.152s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.581 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.283 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.779 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.785 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.786 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.788 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.788 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.789 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.790 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.790 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.790 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.791 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.791 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.792 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.792 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.794 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.415 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.049.107 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.108 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.108 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.108 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.109 I llama_model_loader: - type  f32:  194 tensors
0.00.049.109 I llama_model_loader: - type  f16:   98 tensors
0.00.077.292 I llm_load_vocab: special tokens cache size = 25
0.00.083.637 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.640 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.640 I llm_load_print_meta: arch             = gptneox
0.00.083.641 I llm_load_print_meta: vocab type       = BPE
0.00.083.641 I llm_load_print_meta: n_vocab          = 50304
0.00.083.641 I llm_load_print_meta: n_merges         = 50009
0.00.083.641 I llm_load_print_meta: vocab_only       = 0
0.00.083.641 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.642 I llm_load_print_meta: n_embd           = 2048
0.00.083.642 I llm_load_print_meta: n_layer          = 24
0.00.083.645 I llm_load_print_meta: n_head           = 16
0.00.083.645 I llm_load_print_meta: n_head_kv        = 16
0.00.083.658 I llm_load_print_meta: n_rot            = 32
0.00.083.658 I llm_load_print_meta: n_swa            = 0
0.00.083.658 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.659 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.659 I llm_load_print_meta: n_gqa            = 1
0.00.083.660 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.660 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.661 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.661 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.661 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.661 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.662 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.662 I llm_load_print_meta: n_ff             = 8192
0.00.083.662 I llm_load_print_meta: n_expert         = 0
0.00.083.662 I llm_load_print_meta: n_expert_used    = 0
0.00.083.663 I llm_load_print_meta: causal attn      = 1
0.00.083.663 I llm_load_print_meta: pooling type     = 0
0.00.083.663 I llm_load_print_meta: rope type        = 2
0.00.083.663 I llm_load_print_meta: rope scaling     = linear
0.00.083.663 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.664 I llm_load_print_meta: freq_scale_train = 1
0.00.083.664 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.664 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.665 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.665 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.665 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.665 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.675 I llm_load_print_meta: model type       = 1.4B
0.00.083.676 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.676 I llm_load_print_meta: model params     = 1.41 B
0.00.083.677 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.677 I llm_load_print_meta: general.name     = 1.4B
0.00.083.677 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.677 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.677 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.678 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.678 I llm_load_print_meta: LF token         = 128 ''
0.00.083.678 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.678 I llm_load_print_meta: max token length = 1024
0.00.086.284 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.284 I llm_load_tensors: offloading output layer to GPU
0.00.086.284 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.295 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.296 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.087.428 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.429 I llama_new_context_with_model: n_ctx         = 128
0.00.087.429 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.087.429 I llama_new_context_with_model: n_batch       = 128
0.00.087.429 I llama_new_context_with_model: n_ubatch      = 128
0.00.087.429 I llama_new_context_with_model: flash_attn    = 0
0.00.087.430 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.430 I llama_new_context_with_model: freq_scale    = 1
0.00.087.430 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.431 I ggml_metal_init: allocating
0.00.087.434 I ggml_metal_init: found device: Apple M4
0.00.087.436 I ggml_metal_init: picking default device: Apple M4
0.00.088.020 I ggml_metal_init: using embedded metal library
0.00.090.550 I ggml_metal_init: GPU name:   Apple M4
0.00.090.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.552 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.553 I ggml_metal_init: simdgroup reduction   = true
0.00.090.553 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.553 I ggml_metal_init: has bfloat            = true
0.00.090.553 I ggml_metal_init: use bfloat            = true
0.00.090.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.909 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.914 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.927 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.904 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.905 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.906 I llama_new_context_with_model: graph nodes  = 967
0.00.101.906 I llama_new_context_with_model: graph splits = 2
0.00.101.918 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.171.368 I 
0.01.171.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.171.505 I perplexity: tokenizing the input ..
0.01.184.038 I perplexity: tokenization took 12.53 ms
0.01.184.059 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.304.695 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.306.683 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.306.724 I llama_perf_context_print:        load time =    1151.07 ms
0.01.306.726 I llama_perf_context_print: prompt eval time =     120.23 ms /   128 tokens (    0.94 ms per token,  1064.64 tokens per second)
0.01.306.727 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.306.732 I llama_perf_context_print:       total time =     135.36 ms /   129 tokens
0.01.307.404 I ggml_metal_free: deallocating

real	0m1.501s
user	0m0.122s
sys	0m0.227s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.652 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.496 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.503 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.509 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.510 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.511 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.511 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.512 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.513 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.513 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.513 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.513 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.514 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.514 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.516 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.516 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.576 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.576 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.577 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.578 I llama_model_loader: - type  f32:  194 tensors
0.00.037.578 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.106 I llm_load_vocab: special tokens cache size = 25
0.00.069.058 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.062 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.063 I llm_load_print_meta: arch             = gptneox
0.00.069.063 I llm_load_print_meta: vocab type       = BPE
0.00.069.063 I llm_load_print_meta: n_vocab          = 50304
0.00.069.063 I llm_load_print_meta: n_merges         = 50009
0.00.069.064 I llm_load_print_meta: vocab_only       = 0
0.00.069.064 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.064 I llm_load_print_meta: n_embd           = 2048
0.00.069.064 I llm_load_print_meta: n_layer          = 24
0.00.069.069 I llm_load_print_meta: n_head           = 16
0.00.069.069 I llm_load_print_meta: n_head_kv        = 16
0.00.069.086 I llm_load_print_meta: n_rot            = 32
0.00.069.086 I llm_load_print_meta: n_swa            = 0
0.00.069.086 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.087 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.087 I llm_load_print_meta: n_gqa            = 1
0.00.069.088 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.089 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.089 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.090 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.090 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.090 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.090 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.091 I llm_load_print_meta: n_ff             = 8192
0.00.069.091 I llm_load_print_meta: n_expert         = 0
0.00.069.091 I llm_load_print_meta: n_expert_used    = 0
0.00.069.091 I llm_load_print_meta: causal attn      = 1
0.00.069.092 I llm_load_print_meta: pooling type     = 0
0.00.069.092 I llm_load_print_meta: rope type        = 2
0.00.069.093 I llm_load_print_meta: rope scaling     = linear
0.00.069.094 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.094 I llm_load_print_meta: freq_scale_train = 1
0.00.069.094 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.094 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.094 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.095 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.095 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.095 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.095 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.105 I llm_load_print_meta: model type       = 1.4B
0.00.069.105 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.106 I llm_load_print_meta: model params     = 1.41 B
0.00.069.106 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.106 I llm_load_print_meta: general.name     = 1.4B
0.00.069.107 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.107 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.107 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.107 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.107 I llm_load_print_meta: LF token         = 128 ''
0.00.069.108 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.108 I llm_load_print_meta: max token length = 1024
0.00.071.752 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.752 I llm_load_tensors: offloading output layer to GPU
0.00.071.752 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.763 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.765 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.912 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.913 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.913 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.913 I llama_new_context_with_model: n_batch       = 2048
0.00.072.914 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.914 I llama_new_context_with_model: flash_attn    = 0
0.00.072.914 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.915 I llama_new_context_with_model: freq_scale    = 1
0.00.072.915 I ggml_metal_init: allocating
0.00.072.922 I ggml_metal_init: found device: Apple M4
0.00.072.925 I ggml_metal_init: picking default device: Apple M4
0.00.073.688 I ggml_metal_init: using embedded metal library
0.00.076.661 I ggml_metal_init: GPU name:   Apple M4
0.00.076.663 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.664 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.664 I ggml_metal_init: simdgroup reduction   = true
0.00.076.664 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.665 I ggml_metal_init: has bfloat            = true
0.00.076.665 I ggml_metal_init: use bfloat            = true
0.00.076.665 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.666 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.106 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.121 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.278 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.280 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.280 I llama_new_context_with_model: graph nodes  = 967
0.00.114.280 I llama_new_context_with_model: graph splits = 2
0.00.114.294 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.333.444 I main: llama threadpool init, n_threads = 4
0.01.333.478 I 
0.01.333.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.333.507 I 
0.01.333.742 I sampler seed: 1234
0.01.333.746 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.333.758 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.333.760 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.333.760 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.423.606 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53625.38 tokens per second)
0.02.423.607 I llama_perf_context_print:        load time =    1323.79 ms
0.02.423.608 I llama_perf_context_print: prompt eval time =      39.52 ms /     7 tokens (    5.65 ms per token,   177.11 tokens per second)
0.02.423.610 I llama_perf_context_print:        eval time =    1047.39 ms /    63 runs   (   16.63 ms per token,    60.15 tokens per second)
0.02.423.610 I llama_perf_context_print:       total time =    1090.17 ms /    70 tokens
0.02.423.818 I ggml_metal_free: deallocating

real	0m2.442s
user	0m0.118s
sys	0m0.246s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.123 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.769 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.886 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.892 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.894 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.895 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.895 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.895 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.896 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.897 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.898 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.898 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.899 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.902 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.902 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.903 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.905 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.906 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.906 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.903 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.608 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.719 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.725 I llama_model_loader: - type  f32:  194 tensors
0.00.034.726 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.944 I llm_load_vocab: special tokens cache size = 25
0.00.068.564 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.567 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.567 I llm_load_print_meta: arch             = gptneox
0.00.068.568 I llm_load_print_meta: vocab type       = BPE
0.00.068.568 I llm_load_print_meta: n_vocab          = 50304
0.00.068.568 I llm_load_print_meta: n_merges         = 50009
0.00.068.568 I llm_load_print_meta: vocab_only       = 0
0.00.068.569 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.569 I llm_load_print_meta: n_embd           = 2048
0.00.068.569 I llm_load_print_meta: n_layer          = 24
0.00.068.574 I llm_load_print_meta: n_head           = 16
0.00.068.575 I llm_load_print_meta: n_head_kv        = 16
0.00.068.587 I llm_load_print_meta: n_rot            = 32
0.00.068.588 I llm_load_print_meta: n_swa            = 0
0.00.068.588 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.588 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.589 I llm_load_print_meta: n_gqa            = 1
0.00.068.589 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.590 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.590 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.591 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.591 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.591 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.591 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.592 I llm_load_print_meta: n_ff             = 8192
0.00.068.592 I llm_load_print_meta: n_expert         = 0
0.00.068.592 I llm_load_print_meta: n_expert_used    = 0
0.00.068.593 I llm_load_print_meta: causal attn      = 1
0.00.068.593 I llm_load_print_meta: pooling type     = 0
0.00.068.593 I llm_load_print_meta: rope type        = 2
0.00.068.593 I llm_load_print_meta: rope scaling     = linear
0.00.068.593 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.593 I llm_load_print_meta: freq_scale_train = 1
0.00.068.594 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.594 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.594 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.594 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.594 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.595 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.596 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.605 I llm_load_print_meta: model type       = 1.4B
0.00.068.605 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.606 I llm_load_print_meta: model params     = 1.41 B
0.00.068.606 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.606 I llm_load_print_meta: general.name     = 1.4B
0.00.068.606 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.607 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.607 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.607 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.607 I llm_load_print_meta: LF token         = 128 ''
0.00.068.608 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.608 I llm_load_print_meta: max token length = 1024
0.00.070.450 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.450 I llm_load_tensors: offloading output layer to GPU
0.00.070.450 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.461 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.462 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.365 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.366 I llama_new_context_with_model: n_ctx         = 128
0.00.071.366 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.366 I llama_new_context_with_model: n_batch       = 128
0.00.071.366 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.366 I llama_new_context_with_model: flash_attn    = 0
0.00.071.367 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.367 I llama_new_context_with_model: freq_scale    = 1
0.00.071.367 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.368 I ggml_metal_init: allocating
0.00.071.371 I ggml_metal_init: found device: Apple M4
0.00.071.373 I ggml_metal_init: picking default device: Apple M4
0.00.072.072 I ggml_metal_init: using embedded metal library
0.00.075.019 I ggml_metal_init: GPU name:   Apple M4
0.00.075.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.022 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.022 I ggml_metal_init: simdgroup reduction   = true
0.00.075.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.022 I ggml_metal_init: has bfloat            = true
0.00.075.022 I ggml_metal_init: use bfloat            = true
0.00.075.023 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.433 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.086.436 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.086.452 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.493 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.087.494 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.087.495 I llama_new_context_with_model: graph nodes  = 967
0.00.087.495 I llama_new_context_with_model: graph splits = 2
0.00.087.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.872.606 I 
0.00.872.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.872.669 I perplexity: tokenizing the input ..
0.00.880.166 I perplexity: tokenization took 7.496 ms
0.00.880.180 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.004.254 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.005.596 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.005.612 I llama_perf_context_print:        load time =     860.83 ms
0.01.005.613 I llama_perf_context_print: prompt eval time =     123.85 ms /   128 tokens (    0.97 ms per token,  1033.51 tokens per second)
0.01.005.614 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.005.614 I llama_perf_context_print:       total time =     133.01 ms /   129 tokens
0.01.006.023 I ggml_metal_free: deallocating

real	0m1.025s
user	0m0.098s
sys	0m0.164s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.012.662 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.802 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.807 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.810 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.810 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.810 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.813 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.813 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.816 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.800 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.875 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.940 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.943 I llama_model_loader: - type  f32:  194 tensors
0.00.029.944 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.944 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.506 I llm_load_vocab: special tokens cache size = 25
0.00.057.535 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.537 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.538 I llm_load_print_meta: arch             = gptneox
0.00.057.538 I llm_load_print_meta: vocab type       = BPE
0.00.057.539 I llm_load_print_meta: n_vocab          = 50304
0.00.057.539 I llm_load_print_meta: n_merges         = 50009
0.00.057.539 I llm_load_print_meta: vocab_only       = 0
0.00.057.539 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.539 I llm_load_print_meta: n_embd           = 2048
0.00.057.540 I llm_load_print_meta: n_layer          = 24
0.00.057.545 I llm_load_print_meta: n_head           = 16
0.00.057.546 I llm_load_print_meta: n_head_kv        = 16
0.00.057.558 I llm_load_print_meta: n_rot            = 32
0.00.057.559 I llm_load_print_meta: n_swa            = 0
0.00.057.559 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.559 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.560 I llm_load_print_meta: n_gqa            = 1
0.00.057.561 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.561 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.562 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.562 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.562 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.562 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.563 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.563 I llm_load_print_meta: n_ff             = 8192
0.00.057.564 I llm_load_print_meta: n_expert         = 0
0.00.057.564 I llm_load_print_meta: n_expert_used    = 0
0.00.057.564 I llm_load_print_meta: causal attn      = 1
0.00.057.564 I llm_load_print_meta: pooling type     = 0
0.00.057.564 I llm_load_print_meta: rope type        = 2
0.00.057.565 I llm_load_print_meta: rope scaling     = linear
0.00.057.565 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.565 I llm_load_print_meta: freq_scale_train = 1
0.00.057.565 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.566 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.566 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.566 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.566 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.566 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.566 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.576 I llm_load_print_meta: model type       = 1.4B
0.00.057.577 I llm_load_print_meta: model ftype      = Q4_0
0.00.057.577 I llm_load_print_meta: model params     = 1.41 B
0.00.057.578 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.057.578 I llm_load_print_meta: general.name     = 1.4B
0.00.057.578 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.578 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.578 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.578 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.579 I llm_load_print_meta: LF token         = 128 ''
0.00.057.579 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.579 I llm_load_print_meta: max token length = 1024
0.00.059.836 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.836 I llm_load_tensors: offloading output layer to GPU
0.00.059.836 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.848 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.059.849 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.060.863 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.864 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.864 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.864 I llama_new_context_with_model: n_batch       = 2048
0.00.060.865 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.865 I llama_new_context_with_model: flash_attn    = 0
0.00.060.865 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.865 I llama_new_context_with_model: freq_scale    = 1
0.00.060.866 I ggml_metal_init: allocating
0.00.060.869 I ggml_metal_init: found device: Apple M4
0.00.060.871 I ggml_metal_init: picking default device: Apple M4
0.00.061.544 I ggml_metal_init: using embedded metal library
0.00.064.093 I ggml_metal_init: GPU name:   Apple M4
0.00.064.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.095 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.096 I ggml_metal_init: simdgroup reduction   = true
0.00.064.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.096 I ggml_metal_init: has bfloat            = true
0.00.064.096 I ggml_metal_init: use bfloat            = true
0.00.064.097 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.097 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.122 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.134 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.160 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.246 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.247 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.248 I llama_new_context_with_model: graph nodes  = 967
0.00.104.248 I llama_new_context_with_model: graph splits = 2
0.00.104.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.808 I main: llama threadpool init, n_threads = 4
0.00.650.856 I 
0.00.650.890 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.891 I 
0.00.651.125 I sampler seed: 1234
0.00.651.131 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.651.162 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.651.165 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.651.165 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.336.600 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61312.61 tokens per second)
0.01.336.602 I llama_perf_context_print:        load time =     638.14 ms
0.01.336.603 I llama_perf_context_print: prompt eval time =      43.71 ms /     7 tokens (    6.24 ms per token,   160.15 tokens per second)
0.01.336.603 I llama_perf_context_print:        eval time =     638.74 ms /    63 runs   (   10.14 ms per token,    98.63 tokens per second)
0.01.336.604 I llama_perf_context_print:       total time =     685.80 ms /    70 tokens
0.01.336.777 I ggml_metal_free: deallocating

real	0m1.355s
user	0m0.114s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.188 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.016 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.024 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.025 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.025 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.026 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.026 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.028 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.825 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.917 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.646 I llama_model_loader: - type  f32:  194 tensors
0.00.024.647 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.611 I llm_load_vocab: special tokens cache size = 25
0.00.050.548 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.550 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.551 I llm_load_print_meta: arch             = gptneox
0.00.050.551 I llm_load_print_meta: vocab type       = BPE
0.00.050.551 I llm_load_print_meta: n_vocab          = 50304
0.00.050.552 I llm_load_print_meta: n_merges         = 50009
0.00.050.552 I llm_load_print_meta: vocab_only       = 0
0.00.050.552 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.552 I llm_load_print_meta: n_embd           = 2048
0.00.050.552 I llm_load_print_meta: n_layer          = 24
0.00.050.555 I llm_load_print_meta: n_head           = 16
0.00.050.556 I llm_load_print_meta: n_head_kv        = 16
0.00.050.568 I llm_load_print_meta: n_rot            = 32
0.00.050.568 I llm_load_print_meta: n_swa            = 0
0.00.050.568 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.568 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.569 I llm_load_print_meta: n_gqa            = 1
0.00.050.570 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.571 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.571 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.572 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.572 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.572 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.572 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.573 I llm_load_print_meta: n_ff             = 8192
0.00.050.573 I llm_load_print_meta: n_expert         = 0
0.00.050.573 I llm_load_print_meta: n_expert_used    = 0
0.00.050.573 I llm_load_print_meta: causal attn      = 1
0.00.050.573 I llm_load_print_meta: pooling type     = 0
0.00.050.573 I llm_load_print_meta: rope type        = 2
0.00.050.574 I llm_load_print_meta: rope scaling     = linear
0.00.050.574 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.575 I llm_load_print_meta: freq_scale_train = 1
0.00.050.575 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.575 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.575 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.576 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.576 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.576 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.576 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.585 I llm_load_print_meta: model type       = 1.4B
0.00.050.585 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.586 I llm_load_print_meta: model params     = 1.41 B
0.00.050.586 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.586 I llm_load_print_meta: general.name     = 1.4B
0.00.050.587 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.588 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.588 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.588 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.589 I llm_load_print_meta: LF token         = 128 ''
0.00.050.589 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.589 I llm_load_print_meta: max token length = 1024
0.00.052.506 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.506 I llm_load_tensors: offloading output layer to GPU
0.00.052.506 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.517 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.518 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.405 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.405 I llama_new_context_with_model: n_ctx         = 128
0.00.053.406 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.406 I llama_new_context_with_model: n_batch       = 128
0.00.053.406 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.406 I llama_new_context_with_model: flash_attn    = 0
0.00.053.406 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.407 I llama_new_context_with_model: freq_scale    = 1
0.00.053.407 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.407 I ggml_metal_init: allocating
0.00.053.410 I ggml_metal_init: found device: Apple M4
0.00.053.412 I ggml_metal_init: picking default device: Apple M4
0.00.053.960 I ggml_metal_init: using embedded metal library
0.00.056.258 I ggml_metal_init: GPU name:   Apple M4
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.260 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.260 I ggml_metal_init: simdgroup reduction   = true
0.00.056.260 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.260 I ggml_metal_init: has bfloat            = true
0.00.056.260 I ggml_metal_init: use bfloat            = true
0.00.056.261 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.225 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.227 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.250 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.194 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.195 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.195 I llama_new_context_with_model: graph nodes  = 967
0.00.068.195 I llama_new_context_with_model: graph splits = 2
0.00.068.208 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.364 I 
0.00.607.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.415 I perplexity: tokenizing the input ..
0.00.614.834 I perplexity: tokenization took 7.416 ms
0.00.614.843 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.737.669 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.739.021 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.739.045 I llama_perf_context_print:        load time =     597.17 ms
0.00.739.047 I llama_perf_context_print: prompt eval time =     122.60 ms /   128 tokens (    0.96 ms per token,  1044.06 tokens per second)
0.00.739.048 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.049 I llama_perf_context_print:       total time =     131.68 ms /   129 tokens
0.00.739.565 I ggml_metal_free: deallocating

real	0m0.754s
user	0m0.078s
sys	0m0.104s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.709 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.713 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.715 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.715 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.715 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.716 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.719 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.720 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.720 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.721 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.721 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.632 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.521 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.521 I llama_model_loader: - type  f32:  194 tensors
0.00.026.521 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.522 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.562 I llm_load_vocab: special tokens cache size = 25
0.00.053.583 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.586 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.586 I llm_load_print_meta: arch             = gptneox
0.00.053.587 I llm_load_print_meta: vocab type       = BPE
0.00.053.587 I llm_load_print_meta: n_vocab          = 50304
0.00.053.587 I llm_load_print_meta: n_merges         = 50009
0.00.053.587 I llm_load_print_meta: vocab_only       = 0
0.00.053.587 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.587 I llm_load_print_meta: n_embd           = 2048
0.00.053.588 I llm_load_print_meta: n_layer          = 24
0.00.053.590 I llm_load_print_meta: n_head           = 16
0.00.053.591 I llm_load_print_meta: n_head_kv        = 16
0.00.053.598 I llm_load_print_meta: n_rot            = 32
0.00.053.598 I llm_load_print_meta: n_swa            = 0
0.00.053.599 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.599 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.599 I llm_load_print_meta: n_gqa            = 1
0.00.053.600 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.603 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.604 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.604 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.604 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.605 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.605 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.605 I llm_load_print_meta: n_ff             = 8192
0.00.053.606 I llm_load_print_meta: n_expert         = 0
0.00.053.606 I llm_load_print_meta: n_expert_used    = 0
0.00.053.609 I llm_load_print_meta: causal attn      = 1
0.00.053.610 I llm_load_print_meta: pooling type     = 0
0.00.053.610 I llm_load_print_meta: rope type        = 2
0.00.053.611 I llm_load_print_meta: rope scaling     = linear
0.00.053.611 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.612 I llm_load_print_meta: freq_scale_train = 1
0.00.053.612 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.613 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.613 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.613 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.613 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.613 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.613 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.618 I llm_load_print_meta: model type       = 1.4B
0.00.053.618 I llm_load_print_meta: model ftype      = Q4_1
0.00.053.618 I llm_load_print_meta: model params     = 1.41 B
0.00.053.619 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.053.619 I llm_load_print_meta: general.name     = 1.4B
0.00.053.619 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.619 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.619 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.619 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.620 I llm_load_print_meta: LF token         = 128 ''
0.00.053.620 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.620 I llm_load_print_meta: max token length = 1024
0.00.055.389 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.390 I llm_load_tensors: offloading output layer to GPU
0.00.055.390 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.395 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.396 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.056.289 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.290 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.290 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.290 I llama_new_context_with_model: n_batch       = 2048
0.00.056.290 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.290 I llama_new_context_with_model: flash_attn    = 0
0.00.056.291 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.291 I llama_new_context_with_model: freq_scale    = 1
0.00.056.291 I ggml_metal_init: allocating
0.00.056.298 I ggml_metal_init: found device: Apple M4
0.00.056.300 I ggml_metal_init: picking default device: Apple M4
0.00.056.865 I ggml_metal_init: using embedded metal library
0.00.059.163 I ggml_metal_init: GPU name:   Apple M4
0.00.059.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.165 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.166 I ggml_metal_init: simdgroup reduction   = true
0.00.059.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.166 I ggml_metal_init: has bfloat            = true
0.00.059.167 I ggml_metal_init: use bfloat            = true
0.00.059.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.168 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.506 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.513 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.533 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.479 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.480 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.480 I llama_new_context_with_model: graph nodes  = 967
0.00.089.480 I llama_new_context_with_model: graph splits = 2
0.00.089.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.831.137 I main: llama threadpool init, n_threads = 4
0.00.831.180 I 
0.00.831.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.831.217 I 
0.00.831.463 I sampler seed: 1234
0.00.831.467 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.831.509 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.831.509 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.831.509 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.566.625 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.566.625 I llama_perf_context_print:        load time =     822.26 ms
0.01.566.626 I llama_perf_context_print: prompt eval time =      46.23 ms /     7 tokens (    6.60 ms per token,   151.41 tokens per second)
0.01.566.627 I llama_perf_context_print:        eval time =     685.86 ms /    63 runs   (   10.89 ms per token,    91.86 tokens per second)
0.01.566.627 I llama_perf_context_print:       total time =     735.49 ms /    70 tokens
0.01.566.826 I ggml_metal_free: deallocating

real	0m1.583s
user	0m0.110s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.777 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.650 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.656 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.656 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.657 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.657 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.657 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.658 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.659 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.659 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.659 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.661 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.661 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.663 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.663 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.663 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.678 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.567 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.568 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.568 I llama_model_loader: - type  f32:  194 tensors
0.00.023.569 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.569 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.450 I llm_load_vocab: special tokens cache size = 25
0.00.050.336 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.339 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.339 I llm_load_print_meta: arch             = gptneox
0.00.050.340 I llm_load_print_meta: vocab type       = BPE
0.00.050.340 I llm_load_print_meta: n_vocab          = 50304
0.00.050.340 I llm_load_print_meta: n_merges         = 50009
0.00.050.340 I llm_load_print_meta: vocab_only       = 0
0.00.050.340 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.341 I llm_load_print_meta: n_embd           = 2048
0.00.050.341 I llm_load_print_meta: n_layer          = 24
0.00.050.344 I llm_load_print_meta: n_head           = 16
0.00.050.345 I llm_load_print_meta: n_head_kv        = 16
0.00.050.357 I llm_load_print_meta: n_rot            = 32
0.00.050.357 I llm_load_print_meta: n_swa            = 0
0.00.050.357 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.358 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.358 I llm_load_print_meta: n_gqa            = 1
0.00.050.359 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.360 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.360 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.361 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.361 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.361 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.361 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.362 I llm_load_print_meta: n_ff             = 8192
0.00.050.362 I llm_load_print_meta: n_expert         = 0
0.00.050.362 I llm_load_print_meta: n_expert_used    = 0
0.00.050.362 I llm_load_print_meta: causal attn      = 1
0.00.050.362 I llm_load_print_meta: pooling type     = 0
0.00.050.362 I llm_load_print_meta: rope type        = 2
0.00.050.365 I llm_load_print_meta: rope scaling     = linear
0.00.050.365 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.365 I llm_load_print_meta: freq_scale_train = 1
0.00.050.365 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.366 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.366 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.366 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.366 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.366 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.366 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.375 I llm_load_print_meta: model type       = 1.4B
0.00.050.377 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.377 I llm_load_print_meta: model params     = 1.41 B
0.00.050.378 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.378 I llm_load_print_meta: general.name     = 1.4B
0.00.050.378 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.378 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: LF token         = 128 ''
0.00.050.380 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.380 I llm_load_print_meta: max token length = 1024
0.00.052.329 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.329 I llm_load_tensors: offloading output layer to GPU
0.00.052.329 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.340 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.341 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.250 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.251 I llama_new_context_with_model: n_ctx         = 128
0.00.053.251 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.251 I llama_new_context_with_model: n_batch       = 128
0.00.053.251 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.251 I llama_new_context_with_model: flash_attn    = 0
0.00.053.252 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.252 I llama_new_context_with_model: freq_scale    = 1
0.00.053.252 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.253 I ggml_metal_init: allocating
0.00.053.258 I ggml_metal_init: found device: Apple M4
0.00.053.264 I ggml_metal_init: picking default device: Apple M4
0.00.053.798 I ggml_metal_init: using embedded metal library
0.00.056.114 I ggml_metal_init: GPU name:   Apple M4
0.00.056.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.116 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.116 I ggml_metal_init: simdgroup reduction   = true
0.00.056.117 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.117 I ggml_metal_init: has bfloat            = true
0.00.056.117 I ggml_metal_init: use bfloat            = true
0.00.056.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.710 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.716 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.732 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.593 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.594 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.594 I llama_new_context_with_model: graph nodes  = 967
0.00.067.594 I llama_new_context_with_model: graph splits = 2
0.00.067.606 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.515 I 
0.00.659.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.563 I perplexity: tokenizing the input ..
0.00.667.138 I perplexity: tokenization took 7.573 ms
0.00.667.154 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.950 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.791.286 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.791.300 I llama_perf_context_print:        load time =     650.73 ms
0.00.791.301 I llama_perf_context_print: prompt eval time =     122.57 ms /   128 tokens (    0.96 ms per token,  1044.30 tokens per second)
0.00.791.302 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.302 I llama_perf_context_print:       total time =     131.79 ms /   129 tokens
0.00.791.728 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.079s
sys	0m0.118s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.011.002 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.396 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.401 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.410 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.411 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.291 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.291 I llama_model_loader: - type  f32:  194 tensors
0.00.028.291 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.292 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.626 I llm_load_vocab: special tokens cache size = 25
0.00.054.683 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.686 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.686 I llm_load_print_meta: arch             = gptneox
0.00.054.686 I llm_load_print_meta: vocab type       = BPE
0.00.054.687 I llm_load_print_meta: n_vocab          = 50304
0.00.054.687 I llm_load_print_meta: n_merges         = 50009
0.00.054.687 I llm_load_print_meta: vocab_only       = 0
0.00.054.687 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.687 I llm_load_print_meta: n_embd           = 2048
0.00.054.687 I llm_load_print_meta: n_layer          = 24
0.00.054.690 I llm_load_print_meta: n_head           = 16
0.00.054.691 I llm_load_print_meta: n_head_kv        = 16
0.00.054.703 I llm_load_print_meta: n_rot            = 32
0.00.054.704 I llm_load_print_meta: n_swa            = 0
0.00.054.705 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.705 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.705 I llm_load_print_meta: n_gqa            = 1
0.00.054.706 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.707 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.707 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.708 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.708 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.708 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.708 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.709 I llm_load_print_meta: n_ff             = 8192
0.00.054.709 I llm_load_print_meta: n_expert         = 0
0.00.054.709 I llm_load_print_meta: n_expert_used    = 0
0.00.054.709 I llm_load_print_meta: causal attn      = 1
0.00.054.709 I llm_load_print_meta: pooling type     = 0
0.00.054.709 I llm_load_print_meta: rope type        = 2
0.00.054.710 I llm_load_print_meta: rope scaling     = linear
0.00.054.710 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.710 I llm_load_print_meta: freq_scale_train = 1
0.00.054.710 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.711 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.711 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.711 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.711 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.711 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.711 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.721 I llm_load_print_meta: model type       = 1.4B
0.00.054.721 I llm_load_print_meta: model ftype      = Q5_0
0.00.054.721 I llm_load_print_meta: model params     = 1.41 B
0.00.054.722 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.054.722 I llm_load_print_meta: general.name     = 1.4B
0.00.054.722 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.722 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.723 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.723 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.723 I llm_load_print_meta: LF token         = 128 ''
0.00.054.723 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.723 I llm_load_print_meta: max token length = 1024
0.00.056.681 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.681 I llm_load_tensors: offloading output layer to GPU
0.00.056.681 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.692 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.056.693 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.057.592 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.593 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.593 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.593 I llama_new_context_with_model: n_batch       = 2048
0.00.057.593 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.594 I llama_new_context_with_model: flash_attn    = 0
0.00.057.594 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.594 I llama_new_context_with_model: freq_scale    = 1
0.00.057.595 I ggml_metal_init: allocating
0.00.057.598 I ggml_metal_init: found device: Apple M4
0.00.057.600 I ggml_metal_init: picking default device: Apple M4
0.00.058.151 I ggml_metal_init: using embedded metal library
0.00.060.485 I ggml_metal_init: GPU name:   Apple M4
0.00.060.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.487 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.487 I ggml_metal_init: simdgroup reduction   = true
0.00.060.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.488 I ggml_metal_init: has bfloat            = true
0.00.060.488 I ggml_metal_init: use bfloat            = true
0.00.060.488 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.489 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.668 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.676 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.698 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.821 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.823 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.823 I llama_new_context_with_model: graph nodes  = 967
0.00.092.823 I llama_new_context_with_model: graph splits = 2
0.00.092.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.829.593 I main: llama threadpool init, n_threads = 4
0.00.829.631 I 
0.00.829.663 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.829.663 I 
0.00.829.880 I sampler seed: 1234
0.00.829.884 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.924 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.924 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.924 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.627.575 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.01.627.576 I llama_perf_context_print:        load time =     818.59 ms
0.01.627.577 I llama_perf_context_print: prompt eval time =      43.08 ms /     7 tokens (    6.15 ms per token,   162.47 tokens per second)
0.01.627.577 I llama_perf_context_print:        eval time =     751.48 ms /    63 runs   (   11.93 ms per token,    83.83 tokens per second)
0.01.627.578 I llama_perf_context_print:       total time =     797.98 ms /    70 tokens
0.01.627.751 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.110s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.596 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.342 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.344 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.347 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.347 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.159 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.235 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.069 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.069 I llama_model_loader: - type  f32:  194 tensors
0.00.024.070 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.070 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.139 I llm_load_vocab: special tokens cache size = 25
0.00.049.943 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.946 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.946 I llm_load_print_meta: arch             = gptneox
0.00.049.947 I llm_load_print_meta: vocab type       = BPE
0.00.049.947 I llm_load_print_meta: n_vocab          = 50304
0.00.049.947 I llm_load_print_meta: n_merges         = 50009
0.00.049.947 I llm_load_print_meta: vocab_only       = 0
0.00.049.947 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.947 I llm_load_print_meta: n_embd           = 2048
0.00.049.948 I llm_load_print_meta: n_layer          = 24
0.00.049.950 I llm_load_print_meta: n_head           = 16
0.00.049.951 I llm_load_print_meta: n_head_kv        = 16
0.00.049.963 I llm_load_print_meta: n_rot            = 32
0.00.049.963 I llm_load_print_meta: n_swa            = 0
0.00.049.964 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.964 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.964 I llm_load_print_meta: n_gqa            = 1
0.00.049.965 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.965 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.966 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.966 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.966 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.966 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.967 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.967 I llm_load_print_meta: n_ff             = 8192
0.00.049.968 I llm_load_print_meta: n_expert         = 0
0.00.049.968 I llm_load_print_meta: n_expert_used    = 0
0.00.049.970 I llm_load_print_meta: causal attn      = 1
0.00.049.970 I llm_load_print_meta: pooling type     = 0
0.00.049.970 I llm_load_print_meta: rope type        = 2
0.00.049.970 I llm_load_print_meta: rope scaling     = linear
0.00.049.970 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.972 I llm_load_print_meta: freq_scale_train = 1
0.00.049.972 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.972 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.972 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.972 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.972 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.972 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.972 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.982 I llm_load_print_meta: model type       = 1.4B
0.00.049.982 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.983 I llm_load_print_meta: model params     = 1.41 B
0.00.049.983 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.983 I llm_load_print_meta: general.name     = 1.4B
0.00.049.983 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: LF token         = 128 ''
0.00.049.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.986 I llm_load_print_meta: max token length = 1024
0.00.051.914 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.914 I llm_load_tensors: offloading output layer to GPU
0.00.051.915 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.925 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.926 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.802 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.803 I llama_new_context_with_model: n_ctx         = 128
0.00.052.803 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.803 I llama_new_context_with_model: n_batch       = 128
0.00.052.803 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.803 I llama_new_context_with_model: flash_attn    = 0
0.00.052.804 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.804 I llama_new_context_with_model: freq_scale    = 1
0.00.052.804 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.805 I ggml_metal_init: allocating
0.00.052.808 I ggml_metal_init: found device: Apple M4
0.00.052.810 I ggml_metal_init: picking default device: Apple M4
0.00.053.331 I ggml_metal_init: using embedded metal library
0.00.055.627 I ggml_metal_init: GPU name:   Apple M4
0.00.055.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.629 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.629 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.630 I ggml_metal_init: simdgroup reduction   = true
0.00.055.630 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.630 I ggml_metal_init: has bfloat            = true
0.00.055.630 I ggml_metal_init: use bfloat            = true
0.00.055.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.631 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.540 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.542 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.567 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.506 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.507 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.507 I llama_new_context_with_model: graph nodes  = 967
0.00.067.507 I llama_new_context_with_model: graph splits = 2
0.00.067.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.545 I 
0.00.708.591 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.612 I perplexity: tokenizing the input ..
0.00.716.137 I perplexity: tokenization took 7.523 ms
0.00.716.148 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.160 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.851.733 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.851.747 I llama_perf_context_print:        load time =     698.94 ms
0.00.851.748 I llama_perf_context_print: prompt eval time =     133.77 ms /   128 tokens (    1.05 ms per token,   956.84 tokens per second)
0.00.851.749 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.749 I llama_perf_context_print:       total time =     143.20 ms /   129 tokens
0.00.852.123 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.078s
sys	0m0.127s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.636 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.389 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.392 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.394 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.400 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.400 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.402 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.402 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.403 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.403 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.404 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.405 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.492 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.655 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.657 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.657 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.658 I llama_model_loader: - type  f32:  194 tensors
0.00.033.658 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.658 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.047 I llm_load_vocab: special tokens cache size = 25
0.00.060.970 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.973 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.973 I llm_load_print_meta: arch             = gptneox
0.00.060.974 I llm_load_print_meta: vocab type       = BPE
0.00.060.974 I llm_load_print_meta: n_vocab          = 50304
0.00.060.974 I llm_load_print_meta: n_merges         = 50009
0.00.060.974 I llm_load_print_meta: vocab_only       = 0
0.00.060.974 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.975 I llm_load_print_meta: n_embd           = 2048
0.00.060.975 I llm_load_print_meta: n_layer          = 24
0.00.060.978 I llm_load_print_meta: n_head           = 16
0.00.060.978 I llm_load_print_meta: n_head_kv        = 16
0.00.060.990 I llm_load_print_meta: n_rot            = 32
0.00.060.990 I llm_load_print_meta: n_swa            = 0
0.00.060.991 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.991 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.992 I llm_load_print_meta: n_gqa            = 1
0.00.060.992 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.993 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.993 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.994 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.994 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.994 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.994 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.995 I llm_load_print_meta: n_ff             = 8192
0.00.060.995 I llm_load_print_meta: n_expert         = 0
0.00.060.995 I llm_load_print_meta: n_expert_used    = 0
0.00.060.995 I llm_load_print_meta: causal attn      = 1
0.00.060.995 I llm_load_print_meta: pooling type     = 0
0.00.060.996 I llm_load_print_meta: rope type        = 2
0.00.060.996 I llm_load_print_meta: rope scaling     = linear
0.00.060.996 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.997 I llm_load_print_meta: freq_scale_train = 1
0.00.060.997 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.999 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.999 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.999 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.000 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.000 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.000 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.010 I llm_load_print_meta: model type       = 1.4B
0.00.061.010 I llm_load_print_meta: model ftype      = Q5_1
0.00.061.011 I llm_load_print_meta: model params     = 1.41 B
0.00.061.011 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.061.011 I llm_load_print_meta: general.name     = 1.4B
0.00.061.011 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.013 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.013 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.013 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.013 I llm_load_print_meta: LF token         = 128 ''
0.00.061.013 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.013 I llm_load_print_meta: max token length = 1024
0.00.063.055 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.056 I llm_load_tensors: offloading output layer to GPU
0.00.063.056 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.066 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.063.068 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.064.051 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.052 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.052 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.052 I llama_new_context_with_model: n_batch       = 2048
0.00.064.052 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.052 I llama_new_context_with_model: flash_attn    = 0
0.00.064.053 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.053 I llama_new_context_with_model: freq_scale    = 1
0.00.064.054 I ggml_metal_init: allocating
0.00.064.061 I ggml_metal_init: found device: Apple M4
0.00.064.063 I ggml_metal_init: picking default device: Apple M4
0.00.064.623 I ggml_metal_init: using embedded metal library
0.00.066.937 I ggml_metal_init: GPU name:   Apple M4
0.00.066.939 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.939 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.940 I ggml_metal_init: simdgroup reduction   = true
0.00.066.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.940 I ggml_metal_init: has bfloat            = true
0.00.066.940 I ggml_metal_init: use bfloat            = true
0.00.066.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.447 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.454 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.472 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.535 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.536 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.536 I llama_new_context_with_model: graph nodes  = 967
0.00.098.536 I llama_new_context_with_model: graph splits = 2
0.00.098.550 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.711 I main: llama threadpool init, n_threads = 4
0.00.818.747 I 
0.00.818.778 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.778 I 
0.00.819.002 I sampler seed: 1234
0.00.819.006 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.018 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.018 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.019 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.667.831 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.667.832 I llama_perf_context_print:        load time =     810.07 ms
0.01.667.833 I llama_perf_context_print: prompt eval time =      42.35 ms /     7 tokens (    6.05 ms per token,   165.29 tokens per second)
0.01.667.833 I llama_perf_context_print:        eval time =     803.46 ms /    63 runs   (   12.75 ms per token,    78.41 tokens per second)
0.01.667.837 I llama_perf_context_print:       total time =     849.12 ms /    70 tokens
0.01.668.027 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.111s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.717 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.509 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.511 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.511 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.512 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.512 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.512 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.513 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.513 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.514 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.514 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.514 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.515 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.515 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.517 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.518 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.518 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.345 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.384 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.228 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.229 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.229 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.230 I llama_model_loader: - type  f32:  194 tensors
0.00.023.230 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.230 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.712 I llm_load_vocab: special tokens cache size = 25
0.00.050.101 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.105 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.105 I llm_load_print_meta: arch             = gptneox
0.00.050.106 I llm_load_print_meta: vocab type       = BPE
0.00.050.106 I llm_load_print_meta: n_vocab          = 50304
0.00.050.106 I llm_load_print_meta: n_merges         = 50009
0.00.050.106 I llm_load_print_meta: vocab_only       = 0
0.00.050.106 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.108 I llm_load_print_meta: n_embd           = 2048
0.00.050.109 I llm_load_print_meta: n_layer          = 24
0.00.050.113 I llm_load_print_meta: n_head           = 16
0.00.050.113 I llm_load_print_meta: n_head_kv        = 16
0.00.050.127 I llm_load_print_meta: n_rot            = 32
0.00.050.128 I llm_load_print_meta: n_swa            = 0
0.00.050.128 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.128 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.129 I llm_load_print_meta: n_gqa            = 1
0.00.050.129 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.130 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.130 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.131 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.131 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.131 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.131 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.132 I llm_load_print_meta: n_ff             = 8192
0.00.050.132 I llm_load_print_meta: n_expert         = 0
0.00.050.132 I llm_load_print_meta: n_expert_used    = 0
0.00.050.132 I llm_load_print_meta: causal attn      = 1
0.00.050.132 I llm_load_print_meta: pooling type     = 0
0.00.050.132 I llm_load_print_meta: rope type        = 2
0.00.050.133 I llm_load_print_meta: rope scaling     = linear
0.00.050.133 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.133 I llm_load_print_meta: freq_scale_train = 1
0.00.050.133 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.134 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.134 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.134 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.134 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.134 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.134 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.144 I llm_load_print_meta: model type       = 1.4B
0.00.050.144 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.145 I llm_load_print_meta: model params     = 1.41 B
0.00.050.147 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.147 I llm_load_print_meta: general.name     = 1.4B
0.00.050.147 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.147 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.147 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.148 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.148 I llm_load_print_meta: LF token         = 128 ''
0.00.050.148 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.148 I llm_load_print_meta: max token length = 1024
0.00.052.164 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.164 I llm_load_tensors: offloading output layer to GPU
0.00.052.165 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.175 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.177 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.170 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.171 I llama_new_context_with_model: n_ctx         = 128
0.00.053.171 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.171 I llama_new_context_with_model: n_batch       = 128
0.00.053.172 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.172 I llama_new_context_with_model: flash_attn    = 0
0.00.053.172 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.172 I llama_new_context_with_model: freq_scale    = 1
0.00.053.172 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.173 I ggml_metal_init: allocating
0.00.053.177 I ggml_metal_init: found device: Apple M4
0.00.053.179 I ggml_metal_init: picking default device: Apple M4
0.00.053.746 I ggml_metal_init: using embedded metal library
0.00.056.298 I ggml_metal_init: GPU name:   Apple M4
0.00.056.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.301 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.302 I ggml_metal_init: simdgroup reduction   = true
0.00.056.302 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.302 I ggml_metal_init: has bfloat            = true
0.00.056.302 I ggml_metal_init: use bfloat            = true
0.00.056.303 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.304 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.688 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.693 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.712 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.572 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.573 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.573 I llama_new_context_with_model: graph nodes  = 967
0.00.067.573 I llama_new_context_with_model: graph splits = 2
0.00.067.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.866 I 
0.00.788.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.910 I perplexity: tokenizing the input ..
0.00.796.551 I perplexity: tokenization took 7.64 ms
0.00.796.563 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.930.495 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.931.849 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.931.866 I llama_perf_context_print:        load time =     780.14 ms
0.00.931.868 I llama_perf_context_print: prompt eval time =     133.71 ms /   128 tokens (    1.04 ms per token,   957.30 tokens per second)
0.00.931.868 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.931.869 I llama_perf_context_print:       total time =     143.00 ms /   129 tokens
0.00.932.333 I ggml_metal_free: deallocating

real	0m0.949s
user	0m0.079s
sys	0m0.126s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.013.538 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.020.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.246 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.246 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.247 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.247 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.248 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.249 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.249 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.250 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.250 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.250 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.252 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.320 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.433 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.435 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.435 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.435 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.436 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.436 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.029.437 I llama_model_loader: - type  f32:  194 tensors
0.00.029.437 I llama_model_loader: - type q2_K:   49 tensors
0.00.029.437 I llama_model_loader: - type q3_K:   48 tensors
0.00.029.437 I llama_model_loader: - type q6_K:    1 tensors
0.00.051.915 I llm_load_vocab: special tokens cache size = 25
0.00.057.722 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.724 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.725 I llm_load_print_meta: arch             = gptneox
0.00.057.725 I llm_load_print_meta: vocab type       = BPE
0.00.057.725 I llm_load_print_meta: n_vocab          = 50304
0.00.057.725 I llm_load_print_meta: n_merges         = 50009
0.00.057.726 I llm_load_print_meta: vocab_only       = 0
0.00.057.726 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.726 I llm_load_print_meta: n_embd           = 2048
0.00.057.726 I llm_load_print_meta: n_layer          = 24
0.00.057.729 I llm_load_print_meta: n_head           = 16
0.00.057.730 I llm_load_print_meta: n_head_kv        = 16
0.00.057.741 I llm_load_print_meta: n_rot            = 32
0.00.057.741 I llm_load_print_meta: n_swa            = 0
0.00.057.742 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.742 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.745 I llm_load_print_meta: n_gqa            = 1
0.00.057.746 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.746 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.747 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.747 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.747 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.747 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.747 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.748 I llm_load_print_meta: n_ff             = 8192
0.00.057.748 I llm_load_print_meta: n_expert         = 0
0.00.057.748 I llm_load_print_meta: n_expert_used    = 0
0.00.057.749 I llm_load_print_meta: causal attn      = 1
0.00.057.749 I llm_load_print_meta: pooling type     = 0
0.00.057.749 I llm_load_print_meta: rope type        = 2
0.00.057.749 I llm_load_print_meta: rope scaling     = linear
0.00.057.749 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.751 I llm_load_print_meta: freq_scale_train = 1
0.00.057.751 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.751 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.751 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.751 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.752 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.752 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.752 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.761 I llm_load_print_meta: model type       = 1.4B
0.00.057.762 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.057.762 I llm_load_print_meta: model params     = 1.41 B
0.00.057.763 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.057.763 I llm_load_print_meta: general.name     = 1.4B
0.00.057.763 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.763 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.763 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.763 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.764 I llm_load_print_meta: LF token         = 128 ''
0.00.057.764 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.764 I llm_load_print_meta: max token length = 1024
0.00.059.618 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.619 I llm_load_tensors: offloading output layer to GPU
0.00.059.619 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.629 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.059.630 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.060.523 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.524 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.524 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.524 I llama_new_context_with_model: n_batch       = 2048
0.00.060.524 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.524 I llama_new_context_with_model: flash_attn    = 0
0.00.060.525 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.525 I llama_new_context_with_model: freq_scale    = 1
0.00.060.525 I ggml_metal_init: allocating
0.00.060.528 I ggml_metal_init: found device: Apple M4
0.00.060.530 I ggml_metal_init: picking default device: Apple M4
0.00.061.069 I ggml_metal_init: using embedded metal library
0.00.063.364 I ggml_metal_init: GPU name:   Apple M4
0.00.063.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.366 I ggml_metal_init: simdgroup reduction   = true
0.00.063.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.367 I ggml_metal_init: has bfloat            = true
0.00.063.367 I ggml_metal_init: use bfloat            = true
0.00.063.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.697 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.707 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.729 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.810 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.811 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.812 I llama_new_context_with_model: graph nodes  = 967
0.00.094.812 I llama_new_context_with_model: graph splits = 2
0.00.094.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.300 I main: llama threadpool init, n_threads = 4
0.00.696.344 I 
0.00.696.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.379 I 
0.00.696.608 I sampler seed: 1234
0.00.696.613 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.653 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.655 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.655 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.377.376 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62943.26 tokens per second)
0.01.377.377 I llama_perf_context_print:        load time =     682.75 ms
0.01.377.378 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.32 tokens per second)
0.01.377.379 I llama_perf_context_print:        eval time =     642.02 ms /    63 runs   (   10.19 ms per token,    98.13 tokens per second)
0.01.377.379 I llama_perf_context_print:       total time =     681.08 ms /    70 tokens
0.01.377.575 I ggml_metal_free: deallocating

real	0m1.397s
user	0m0.112s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.325 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.548 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.560 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.561 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.562 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.485 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.654 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.943 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.945 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.946 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.946 I llama_model_loader: - type  f32:  194 tensors
0.00.033.947 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.947 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.947 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.466 I llm_load_vocab: special tokens cache size = 25
0.00.065.158 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.161 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.161 I llm_load_print_meta: arch             = gptneox
0.00.065.161 I llm_load_print_meta: vocab type       = BPE
0.00.065.162 I llm_load_print_meta: n_vocab          = 50304
0.00.065.162 I llm_load_print_meta: n_merges         = 50009
0.00.065.162 I llm_load_print_meta: vocab_only       = 0
0.00.065.162 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.162 I llm_load_print_meta: n_embd           = 2048
0.00.065.162 I llm_load_print_meta: n_layer          = 24
0.00.065.165 I llm_load_print_meta: n_head           = 16
0.00.065.166 I llm_load_print_meta: n_head_kv        = 16
0.00.065.177 I llm_load_print_meta: n_rot            = 32
0.00.065.178 I llm_load_print_meta: n_swa            = 0
0.00.065.179 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.179 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.179 I llm_load_print_meta: n_gqa            = 1
0.00.065.180 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.181 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.181 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.181 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.182 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.182 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.182 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.182 I llm_load_print_meta: n_ff             = 8192
0.00.065.183 I llm_load_print_meta: n_expert         = 0
0.00.065.183 I llm_load_print_meta: n_expert_used    = 0
0.00.065.183 I llm_load_print_meta: causal attn      = 1
0.00.065.183 I llm_load_print_meta: pooling type     = 0
0.00.065.183 I llm_load_print_meta: rope type        = 2
0.00.065.183 I llm_load_print_meta: rope scaling     = linear
0.00.065.184 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.184 I llm_load_print_meta: freq_scale_train = 1
0.00.065.184 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.185 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.185 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.185 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.185 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.185 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.185 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.194 I llm_load_print_meta: model type       = 1.4B
0.00.065.194 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.065.195 I llm_load_print_meta: model params     = 1.41 B
0.00.065.195 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.065.195 I llm_load_print_meta: general.name     = 1.4B
0.00.065.196 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.196 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.196 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.196 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.196 I llm_load_print_meta: LF token         = 128 ''
0.00.065.197 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.197 I llm_load_print_meta: max token length = 1024
0.00.066.810 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.810 I llm_load_tensors: offloading output layer to GPU
0.00.066.810 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.820 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.066.821 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.067.742 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.743 I llama_new_context_with_model: n_ctx         = 128
0.00.067.743 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.743 I llama_new_context_with_model: n_batch       = 128
0.00.067.743 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.743 I llama_new_context_with_model: flash_attn    = 0
0.00.067.744 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.744 I llama_new_context_with_model: freq_scale    = 1
0.00.067.745 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.745 I ggml_metal_init: allocating
0.00.067.751 I ggml_metal_init: found device: Apple M4
0.00.067.755 I ggml_metal_init: picking default device: Apple M4
0.00.068.355 I ggml_metal_init: using embedded metal library
0.00.071.062 I ggml_metal_init: GPU name:   Apple M4
0.00.071.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.065 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.065 I ggml_metal_init: simdgroup reduction   = true
0.00.071.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.066 I ggml_metal_init: has bfloat            = true
0.00.071.066 I ggml_metal_init: use bfloat            = true
0.00.071.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.962 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.967 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.985 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.163 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.164 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.165 I llama_new_context_with_model: graph nodes  = 967
0.00.084.165 I llama_new_context_with_model: graph splits = 2
0.00.084.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.523.791 I 
0.00.523.828 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.523.850 I perplexity: tokenizing the input ..
0.00.531.404 I perplexity: tokenization took 7.554 ms
0.00.531.414 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.663.788 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.665.114 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.665.130 I llama_perf_context_print:        load time =     509.46 ms
0.00.665.131 I llama_perf_context_print: prompt eval time =     132.13 ms /   128 tokens (    1.03 ms per token,   968.71 tokens per second)
0.00.665.132 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.133 I llama_perf_context_print:       total time =     141.34 ms /   129 tokens
0.00.665.597 I ggml_metal_free: deallocating

real	0m0.681s
user	0m0.087s
sys	0m0.088s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.142 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.217 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.030.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.229 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.230 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.231 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.232 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.232 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.233 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.237 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.237 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.238 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.239 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.240 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.356 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.498 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.668 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.668 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.669 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.039.669 I llama_model_loader: - type  f32:  194 tensors
0.00.039.670 I llama_model_loader: - type q3_K:   25 tensors
0.00.039.670 I llama_model_loader: - type q4_K:   71 tensors
0.00.039.670 I llama_model_loader: - type q5_K:    1 tensors
0.00.039.670 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.119 I llm_load_vocab: special tokens cache size = 25
0.00.070.688 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.691 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.691 I llm_load_print_meta: arch             = gptneox
0.00.070.692 I llm_load_print_meta: vocab type       = BPE
0.00.070.692 I llm_load_print_meta: n_vocab          = 50304
0.00.070.692 I llm_load_print_meta: n_merges         = 50009
0.00.070.692 I llm_load_print_meta: vocab_only       = 0
0.00.070.692 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.692 I llm_load_print_meta: n_embd           = 2048
0.00.070.692 I llm_load_print_meta: n_layer          = 24
0.00.070.695 I llm_load_print_meta: n_head           = 16
0.00.070.696 I llm_load_print_meta: n_head_kv        = 16
0.00.070.712 I llm_load_print_meta: n_rot            = 32
0.00.070.712 I llm_load_print_meta: n_swa            = 0
0.00.070.712 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.712 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.713 I llm_load_print_meta: n_gqa            = 1
0.00.070.713 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.714 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.714 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.715 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.715 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.715 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.715 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.716 I llm_load_print_meta: n_ff             = 8192
0.00.070.716 I llm_load_print_meta: n_expert         = 0
0.00.070.716 I llm_load_print_meta: n_expert_used    = 0
0.00.070.716 I llm_load_print_meta: causal attn      = 1
0.00.070.716 I llm_load_print_meta: pooling type     = 0
0.00.070.717 I llm_load_print_meta: rope type        = 2
0.00.070.717 I llm_load_print_meta: rope scaling     = linear
0.00.070.717 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.717 I llm_load_print_meta: freq_scale_train = 1
0.00.070.717 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.718 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.718 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.718 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.718 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.718 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.718 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.728 I llm_load_print_meta: model type       = 1.4B
0.00.070.728 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.070.729 I llm_load_print_meta: model params     = 1.41 B
0.00.070.729 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.070.731 I llm_load_print_meta: general.name     = 1.4B
0.00.070.731 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.731 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.731 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.731 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.731 I llm_load_print_meta: LF token         = 128 ''
0.00.070.732 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.733 I llm_load_print_meta: max token length = 1024
0.00.072.775 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.776 I llm_load_tensors: offloading output layer to GPU
0.00.072.776 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.786 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.072.787 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.073.754 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.755 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.755 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.755 I llama_new_context_with_model: n_batch       = 2048
0.00.073.755 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.755 I llama_new_context_with_model: flash_attn    = 0
0.00.073.756 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.756 I llama_new_context_with_model: freq_scale    = 1
0.00.073.757 I ggml_metal_init: allocating
0.00.073.763 I ggml_metal_init: found device: Apple M4
0.00.073.767 I ggml_metal_init: picking default device: Apple M4
0.00.074.338 I ggml_metal_init: using embedded metal library
0.00.076.680 I ggml_metal_init: GPU name:   Apple M4
0.00.076.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.682 I ggml_metal_init: simdgroup reduction   = true
0.00.076.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.682 I ggml_metal_init: has bfloat            = true
0.00.076.682 I ggml_metal_init: use bfloat            = true
0.00.076.683 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.683 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.681 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.690 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.707 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.709 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.710 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.711 I llama_new_context_with_model: graph nodes  = 967
0.00.107.711 I llama_new_context_with_model: graph splits = 2
0.00.107.725 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.250 I main: llama threadpool init, n_threads = 4
0.00.628.292 I 
0.00.628.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.324 I 
0.00.628.569 I sampler seed: 1234
0.00.628.573 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.628.610 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.628.612 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.628.612 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.375.090 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49859.55 tokens per second)
0.01.375.091 I llama_perf_context_print:        load time =     618.10 ms
0.01.375.092 I llama_perf_context_print: prompt eval time =      44.22 ms /     7 tokens (    6.32 ms per token,   158.31 tokens per second)
0.01.375.092 I llama_perf_context_print:        eval time =     699.81 ms /    63 runs   (   11.11 ms per token,    90.02 tokens per second)
0.01.375.093 I llama_perf_context_print:       total time =     746.84 ms /    70 tokens
0.01.375.297 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.114s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.580 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.852 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.857 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.857 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.857 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.857 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.858 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.858 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.859 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.859 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.860 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.860 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.861 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.862 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.862 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.732 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.564 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.566 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.567 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.567 I llama_model_loader: - type  f32:  194 tensors
0.00.027.568 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.568 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.568 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.568 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.505 I llm_load_vocab: special tokens cache size = 25
0.00.053.424 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.427 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.427 I llm_load_print_meta: arch             = gptneox
0.00.053.427 I llm_load_print_meta: vocab type       = BPE
0.00.053.428 I llm_load_print_meta: n_vocab          = 50304
0.00.053.428 I llm_load_print_meta: n_merges         = 50009
0.00.053.428 I llm_load_print_meta: vocab_only       = 0
0.00.053.428 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.428 I llm_load_print_meta: n_embd           = 2048
0.00.053.428 I llm_load_print_meta: n_layer          = 24
0.00.053.431 I llm_load_print_meta: n_head           = 16
0.00.053.432 I llm_load_print_meta: n_head_kv        = 16
0.00.053.444 I llm_load_print_meta: n_rot            = 32
0.00.053.444 I llm_load_print_meta: n_swa            = 0
0.00.053.444 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.444 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.445 I llm_load_print_meta: n_gqa            = 1
0.00.053.446 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.446 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.447 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.447 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.448 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.448 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.449 I llm_load_print_meta: n_ff             = 8192
0.00.053.449 I llm_load_print_meta: n_expert         = 0
0.00.053.449 I llm_load_print_meta: n_expert_used    = 0
0.00.053.449 I llm_load_print_meta: causal attn      = 1
0.00.053.449 I llm_load_print_meta: pooling type     = 0
0.00.053.449 I llm_load_print_meta: rope type        = 2
0.00.053.450 I llm_load_print_meta: rope scaling     = linear
0.00.053.450 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.450 I llm_load_print_meta: freq_scale_train = 1
0.00.053.450 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.450 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.451 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.451 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.451 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.451 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.451 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.460 I llm_load_print_meta: model type       = 1.4B
0.00.053.461 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.461 I llm_load_print_meta: model params     = 1.41 B
0.00.053.462 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.462 I llm_load_print_meta: general.name     = 1.4B
0.00.053.462 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.464 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.465 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.465 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.465 I llm_load_print_meta: LF token         = 128 ''
0.00.053.465 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.465 I llm_load_print_meta: max token length = 1024
0.00.055.312 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.312 I llm_load_tensors: offloading output layer to GPU
0.00.055.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.323 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.324 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.245 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.246 I llama_new_context_with_model: n_ctx         = 128
0.00.056.246 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.246 I llama_new_context_with_model: n_batch       = 128
0.00.056.246 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.246 I llama_new_context_with_model: flash_attn    = 0
0.00.056.247 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.247 I llama_new_context_with_model: freq_scale    = 1
0.00.056.247 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.248 I ggml_metal_init: allocating
0.00.056.251 I ggml_metal_init: found device: Apple M4
0.00.056.253 I ggml_metal_init: picking default device: Apple M4
0.00.056.777 I ggml_metal_init: using embedded metal library
0.00.059.073 I ggml_metal_init: GPU name:   Apple M4
0.00.059.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.076 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.076 I ggml_metal_init: simdgroup reduction   = true
0.00.059.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.076 I ggml_metal_init: has bfloat            = true
0.00.059.076 I ggml_metal_init: use bfloat            = true
0.00.059.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.756 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.769 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.690 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.691 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.692 I llama_new_context_with_model: graph nodes  = 967
0.00.070.692 I llama_new_context_with_model: graph splits = 2
0.00.070.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.971 I 
0.00.574.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.574.036 I perplexity: tokenizing the input ..
0.00.581.621 I perplexity: tokenization took 7.582 ms
0.00.581.631 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.713.787 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.715.112 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.715.129 I llama_perf_context_print:        load time =     565.39 ms
0.00.715.130 I llama_perf_context_print: prompt eval time =     131.93 ms /   128 tokens (    1.03 ms per token,   970.21 tokens per second)
0.00.715.131 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.131 I llama_perf_context_print:       total time =     141.16 ms /   129 tokens
0.00.715.633 I ggml_metal_free: deallocating

real	0m0.728s
user	0m0.079s
sys	0m0.109s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.024.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.568 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.569 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.573 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.537 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.033.538 I llama_model_loader: - type  f32:  194 tensors
0.00.033.539 I llama_model_loader: - type q4_K:   61 tensors
0.00.033.539 I llama_model_loader: - type q5_K:   24 tensors
0.00.033.539 I llama_model_loader: - type q6_K:   13 tensors
0.00.055.078 I llm_load_vocab: special tokens cache size = 25
0.00.061.056 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.058 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.059 I llm_load_print_meta: arch             = gptneox
0.00.061.059 I llm_load_print_meta: vocab type       = BPE
0.00.061.059 I llm_load_print_meta: n_vocab          = 50304
0.00.061.059 I llm_load_print_meta: n_merges         = 50009
0.00.061.059 I llm_load_print_meta: vocab_only       = 0
0.00.061.060 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.060 I llm_load_print_meta: n_embd           = 2048
0.00.061.060 I llm_load_print_meta: n_layer          = 24
0.00.061.063 I llm_load_print_meta: n_head           = 16
0.00.061.064 I llm_load_print_meta: n_head_kv        = 16
0.00.061.076 I llm_load_print_meta: n_rot            = 32
0.00.061.076 I llm_load_print_meta: n_swa            = 0
0.00.061.076 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.076 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.077 I llm_load_print_meta: n_gqa            = 1
0.00.061.078 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.078 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.079 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.079 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.079 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.080 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.080 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.082 I llm_load_print_meta: n_ff             = 8192
0.00.061.082 I llm_load_print_meta: n_expert         = 0
0.00.061.084 I llm_load_print_meta: n_expert_used    = 0
0.00.061.085 I llm_load_print_meta: causal attn      = 1
0.00.061.085 I llm_load_print_meta: pooling type     = 0
0.00.061.085 I llm_load_print_meta: rope type        = 2
0.00.061.085 I llm_load_print_meta: rope scaling     = linear
0.00.061.085 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.086 I llm_load_print_meta: freq_scale_train = 1
0.00.061.086 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.086 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.086 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.086 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.086 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.086 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.086 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.096 I llm_load_print_meta: model type       = 1.4B
0.00.061.097 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.061.097 I llm_load_print_meta: model params     = 1.41 B
0.00.061.098 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.061.098 I llm_load_print_meta: general.name     = 1.4B
0.00.061.098 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.100 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.100 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.100 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.100 I llm_load_print_meta: LF token         = 128 ''
0.00.061.101 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.101 I llm_load_print_meta: max token length = 1024
0.00.063.093 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.093 I llm_load_tensors: offloading output layer to GPU
0.00.063.094 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.104 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.063.105 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.064.055 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.055 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.055 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.056 I llama_new_context_with_model: n_batch       = 2048
0.00.064.056 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.056 I llama_new_context_with_model: flash_attn    = 0
0.00.064.056 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.057 I llama_new_context_with_model: freq_scale    = 1
0.00.064.057 I ggml_metal_init: allocating
0.00.064.061 I ggml_metal_init: found device: Apple M4
0.00.064.062 I ggml_metal_init: picking default device: Apple M4
0.00.064.615 I ggml_metal_init: using embedded metal library
0.00.066.969 I ggml_metal_init: GPU name:   Apple M4
0.00.066.970 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.972 I ggml_metal_init: simdgroup reduction   = true
0.00.066.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.973 I ggml_metal_init: has bfloat            = true
0.00.066.973 I ggml_metal_init: use bfloat            = true
0.00.066.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.322 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.331 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.361 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.425 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.426 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.426 I llama_new_context_with_model: graph nodes  = 967
0.00.099.427 I llama_new_context_with_model: graph splits = 2
0.00.099.441 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.711 I main: llama threadpool init, n_threads = 4
0.00.774.750 I 
0.00.774.784 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.786 I 
0.00.775.034 I sampler seed: 1234
0.00.775.039 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.050 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.052 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.052 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.534.834 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.534.835 I llama_perf_context_print:        load time =     765.96 ms
0.01.534.836 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.75 tokens per second)
0.01.534.836 I llama_perf_context_print:        eval time =     709.52 ms /    63 runs   (   11.26 ms per token,    88.79 tokens per second)
0.01.534.837 I llama_perf_context_print:       total time =     760.13 ms /    70 tokens
0.01.535.051 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.112s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.701 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.950 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.958 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.958 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.959 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.959 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.960 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.960 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.961 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.961 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.962 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.962 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.965 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.966 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.847 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.891 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.784 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.785 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.786 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.786 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.787 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.787 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.787 I llama_model_loader: - type  f32:  194 tensors
0.00.024.788 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.788 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.788 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.601 I llm_load_vocab: special tokens cache size = 25
0.00.051.625 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.628 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.628 I llm_load_print_meta: arch             = gptneox
0.00.051.629 I llm_load_print_meta: vocab type       = BPE
0.00.051.629 I llm_load_print_meta: n_vocab          = 50304
0.00.051.629 I llm_load_print_meta: n_merges         = 50009
0.00.051.629 I llm_load_print_meta: vocab_only       = 0
0.00.051.629 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.629 I llm_load_print_meta: n_embd           = 2048
0.00.051.630 I llm_load_print_meta: n_layer          = 24
0.00.051.633 I llm_load_print_meta: n_head           = 16
0.00.051.633 I llm_load_print_meta: n_head_kv        = 16
0.00.051.645 I llm_load_print_meta: n_rot            = 32
0.00.051.646 I llm_load_print_meta: n_swa            = 0
0.00.051.646 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.646 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.647 I llm_load_print_meta: n_gqa            = 1
0.00.051.648 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.648 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.649 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.649 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.650 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.650 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.650 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.650 I llm_load_print_meta: n_ff             = 8192
0.00.051.651 I llm_load_print_meta: n_expert         = 0
0.00.051.651 I llm_load_print_meta: n_expert_used    = 0
0.00.051.651 I llm_load_print_meta: causal attn      = 1
0.00.051.651 I llm_load_print_meta: pooling type     = 0
0.00.051.651 I llm_load_print_meta: rope type        = 2
0.00.051.651 I llm_load_print_meta: rope scaling     = linear
0.00.051.652 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.652 I llm_load_print_meta: freq_scale_train = 1
0.00.051.652 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.653 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.653 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.653 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.653 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.653 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.653 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.664 I llm_load_print_meta: model type       = 1.4B
0.00.051.665 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.665 I llm_load_print_meta: model params     = 1.41 B
0.00.051.665 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.666 I llm_load_print_meta: general.name     = 1.4B
0.00.051.666 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.666 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.666 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.666 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.667 I llm_load_print_meta: LF token         = 128 ''
0.00.051.667 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.667 I llm_load_print_meta: max token length = 1024
0.00.053.585 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.586 I llm_load_tensors: offloading output layer to GPU
0.00.053.586 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.596 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.597 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.534 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.535 I llama_new_context_with_model: n_ctx         = 128
0.00.054.535 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.535 I llama_new_context_with_model: n_batch       = 128
0.00.054.535 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.535 I llama_new_context_with_model: flash_attn    = 0
0.00.054.536 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.536 I llama_new_context_with_model: freq_scale    = 1
0.00.054.536 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.537 I ggml_metal_init: allocating
0.00.054.540 I ggml_metal_init: found device: Apple M4
0.00.054.542 I ggml_metal_init: picking default device: Apple M4
0.00.055.093 I ggml_metal_init: using embedded metal library
0.00.057.425 I ggml_metal_init: GPU name:   Apple M4
0.00.057.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.427 I ggml_metal_init: simdgroup reduction   = true
0.00.057.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.427 I ggml_metal_init: has bfloat            = true
0.00.057.427 I ggml_metal_init: use bfloat            = true
0.00.057.428 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.349 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.351 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.366 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.297 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.298 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.298 I llama_new_context_with_model: graph nodes  = 967
0.00.069.298 I llama_new_context_with_model: graph splits = 2
0.00.069.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.708 I 
0.00.607.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.758 I perplexity: tokenizing the input ..
0.00.615.545 I perplexity: tokenization took 7.786 ms
0.00.615.559 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.947 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.751.274 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.751.289 I llama_perf_context_print:        load time =     599.00 ms
0.00.751.290 I llama_perf_context_print: prompt eval time =     134.14 ms /   128 tokens (    1.05 ms per token,   954.23 tokens per second)
0.00.751.292 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.751.292 I llama_perf_context_print:       total time =     143.58 ms /   129 tokens
0.00.751.726 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.080s
sys	0m0.107s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.941 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.166 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.167 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.172 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.179 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.179 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.179 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.957 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.871 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.871 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.871 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.872 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.872 I llama_model_loader: - type  f32:  194 tensors
0.00.025.873 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.873 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.096 I llm_load_vocab: special tokens cache size = 25
0.00.052.106 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.109 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.110 I llm_load_print_meta: arch             = gptneox
0.00.052.110 I llm_load_print_meta: vocab type       = BPE
0.00.052.110 I llm_load_print_meta: n_vocab          = 50304
0.00.052.110 I llm_load_print_meta: n_merges         = 50009
0.00.052.110 I llm_load_print_meta: vocab_only       = 0
0.00.052.111 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.111 I llm_load_print_meta: n_embd           = 2048
0.00.052.111 I llm_load_print_meta: n_layer          = 24
0.00.052.114 I llm_load_print_meta: n_head           = 16
0.00.052.115 I llm_load_print_meta: n_head_kv        = 16
0.00.052.127 I llm_load_print_meta: n_rot            = 32
0.00.052.128 I llm_load_print_meta: n_swa            = 0
0.00.052.128 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.128 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.129 I llm_load_print_meta: n_gqa            = 1
0.00.052.130 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.130 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.131 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.131 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.132 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.132 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.132 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.132 I llm_load_print_meta: n_ff             = 8192
0.00.052.133 I llm_load_print_meta: n_expert         = 0
0.00.052.133 I llm_load_print_meta: n_expert_used    = 0
0.00.052.134 I llm_load_print_meta: causal attn      = 1
0.00.052.136 I llm_load_print_meta: pooling type     = 0
0.00.052.136 I llm_load_print_meta: rope type        = 2
0.00.052.136 I llm_load_print_meta: rope scaling     = linear
0.00.052.137 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.137 I llm_load_print_meta: freq_scale_train = 1
0.00.052.137 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.137 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.137 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.137 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.138 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.138 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.138 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.147 I llm_load_print_meta: model type       = 1.4B
0.00.052.148 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.148 I llm_load_print_meta: model params     = 1.41 B
0.00.052.148 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.149 I llm_load_print_meta: general.name     = 1.4B
0.00.052.149 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.149 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.149 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.149 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.149 I llm_load_print_meta: LF token         = 128 ''
0.00.052.150 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.150 I llm_load_print_meta: max token length = 1024
0.00.054.085 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.085 I llm_load_tensors: offloading output layer to GPU
0.00.054.085 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.096 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.097 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.004 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.005 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.005 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.006 I llama_new_context_with_model: n_batch       = 2048
0.00.055.006 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.006 I llama_new_context_with_model: flash_attn    = 0
0.00.055.006 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.007 I llama_new_context_with_model: freq_scale    = 1
0.00.055.007 I ggml_metal_init: allocating
0.00.055.013 I ggml_metal_init: found device: Apple M4
0.00.055.015 I ggml_metal_init: picking default device: Apple M4
0.00.055.561 I ggml_metal_init: using embedded metal library
0.00.057.915 I ggml_metal_init: GPU name:   Apple M4
0.00.057.916 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.918 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.919 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.919 I ggml_metal_init: simdgroup reduction   = true
0.00.057.919 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.919 I ggml_metal_init: has bfloat            = true
0.00.057.919 I ggml_metal_init: use bfloat            = true
0.00.057.920 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.681 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.690 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.707 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.707 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.709 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.709 I llama_new_context_with_model: graph nodes  = 967
0.00.087.709 I llama_new_context_with_model: graph splits = 2
0.00.087.722 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.137 I main: llama threadpool init, n_threads = 4
0.00.707.180 I 
0.00.707.227 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.229 I 
0.00.707.478 I sampler seed: 1234
0.00.707.483 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.525 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.526 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.526 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.556.053 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.556.054 I llama_perf_context_print:        load time =     696.19 ms
0.01.556.056 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.76 tokens per second)
0.01.556.057 I llama_perf_context_print:        eval time =     793.93 ms /    63 runs   (   12.60 ms per token,    79.35 tokens per second)
0.01.556.057 I llama_perf_context_print:       total time =     848.92 ms /    70 tokens
0.01.556.253 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.182 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.024.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.807 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.814 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.819 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.953 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.953 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.953 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.954 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.954 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.033.954 I llama_model_loader: - type  f32:  194 tensors
0.00.033.955 I llama_model_loader: - type q5_K:   61 tensors
0.00.033.955 I llama_model_loader: - type q6_K:   37 tensors
0.00.054.285 I llm_load_vocab: special tokens cache size = 25
0.00.060.164 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.167 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.167 I llm_load_print_meta: arch             = gptneox
0.00.060.167 I llm_load_print_meta: vocab type       = BPE
0.00.060.167 I llm_load_print_meta: n_vocab          = 50304
0.00.060.168 I llm_load_print_meta: n_merges         = 50009
0.00.060.168 I llm_load_print_meta: vocab_only       = 0
0.00.060.168 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.168 I llm_load_print_meta: n_embd           = 2048
0.00.060.168 I llm_load_print_meta: n_layer          = 24
0.00.060.171 I llm_load_print_meta: n_head           = 16
0.00.060.172 I llm_load_print_meta: n_head_kv        = 16
0.00.060.184 I llm_load_print_meta: n_rot            = 32
0.00.060.184 I llm_load_print_meta: n_swa            = 0
0.00.060.184 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.184 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.185 I llm_load_print_meta: n_gqa            = 1
0.00.060.186 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.187 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.187 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.187 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.188 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.188 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.188 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.188 I llm_load_print_meta: n_ff             = 8192
0.00.060.189 I llm_load_print_meta: n_expert         = 0
0.00.060.189 I llm_load_print_meta: n_expert_used    = 0
0.00.060.189 I llm_load_print_meta: causal attn      = 1
0.00.060.189 I llm_load_print_meta: pooling type     = 0
0.00.060.189 I llm_load_print_meta: rope type        = 2
0.00.060.189 I llm_load_print_meta: rope scaling     = linear
0.00.060.191 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.192 I llm_load_print_meta: freq_scale_train = 1
0.00.060.192 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.192 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.193 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.193 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.193 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.193 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.194 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.203 I llm_load_print_meta: model type       = 1.4B
0.00.060.203 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.060.204 I llm_load_print_meta: model params     = 1.41 B
0.00.060.205 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.060.205 I llm_load_print_meta: general.name     = 1.4B
0.00.060.205 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.205 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.205 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.206 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.206 I llm_load_print_meta: LF token         = 128 ''
0.00.060.206 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.206 I llm_load_print_meta: max token length = 1024
0.00.062.067 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.067 I llm_load_tensors: offloading output layer to GPU
0.00.062.068 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.078 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.062.079 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.062.960 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.961 I llama_new_context_with_model: n_ctx         = 128
0.00.062.961 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.961 I llama_new_context_with_model: n_batch       = 128
0.00.062.961 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.961 I llama_new_context_with_model: flash_attn    = 0
0.00.062.962 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.962 I llama_new_context_with_model: freq_scale    = 1
0.00.062.962 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.963 I ggml_metal_init: allocating
0.00.062.966 I ggml_metal_init: found device: Apple M4
0.00.062.968 I ggml_metal_init: picking default device: Apple M4
0.00.063.507 I ggml_metal_init: using embedded metal library
0.00.065.784 I ggml_metal_init: GPU name:   Apple M4
0.00.065.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.786 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.786 I ggml_metal_init: simdgroup reduction   = true
0.00.065.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.787 I ggml_metal_init: has bfloat            = true
0.00.065.787 I ggml_metal_init: use bfloat            = true
0.00.065.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.379 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.381 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.396 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.308 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.309 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.310 I llama_new_context_with_model: graph nodes  = 967
0.00.077.310 I llama_new_context_with_model: graph splits = 2
0.00.077.322 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.686 I 
0.00.677.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.732 I perplexity: tokenizing the input ..
0.00.685.233 I perplexity: tokenization took 7.499 ms
0.00.685.243 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.121 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.827.462 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.827.491 I llama_perf_context_print:        load time =     663.50 ms
0.00.827.495 I llama_perf_context_print: prompt eval time =     140.62 ms /   128 tokens (    1.10 ms per token,   910.22 tokens per second)
0.00.827.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.498 I llama_perf_context_print:       total time =     149.81 ms /   129 tokens
0.00.827.987 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.079s
sys	0m0.126s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.705 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.316 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.321 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.323 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.324 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.324 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.325 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.325 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.326 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.326 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.327 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.330 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.333 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.333 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.241 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.241 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.242 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.243 I llama_model_loader: - type  f32:  194 tensors
0.00.024.243 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.498 I llm_load_vocab: special tokens cache size = 25
0.00.050.582 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.585 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.586 I llm_load_print_meta: arch             = gptneox
0.00.050.586 I llm_load_print_meta: vocab type       = BPE
0.00.050.586 I llm_load_print_meta: n_vocab          = 50304
0.00.050.586 I llm_load_print_meta: n_merges         = 50009
0.00.050.587 I llm_load_print_meta: vocab_only       = 0
0.00.050.587 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.587 I llm_load_print_meta: n_embd           = 2048
0.00.050.587 I llm_load_print_meta: n_layer          = 24
0.00.050.590 I llm_load_print_meta: n_head           = 16
0.00.050.591 I llm_load_print_meta: n_head_kv        = 16
0.00.050.603 I llm_load_print_meta: n_rot            = 32
0.00.050.603 I llm_load_print_meta: n_swa            = 0
0.00.050.603 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.603 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.604 I llm_load_print_meta: n_gqa            = 1
0.00.050.605 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.605 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.606 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.606 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.609 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.609 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.609 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.609 I llm_load_print_meta: n_ff             = 8192
0.00.050.610 I llm_load_print_meta: n_expert         = 0
0.00.050.610 I llm_load_print_meta: n_expert_used    = 0
0.00.050.610 I llm_load_print_meta: causal attn      = 1
0.00.050.611 I llm_load_print_meta: pooling type     = 0
0.00.050.612 I llm_load_print_meta: rope type        = 2
0.00.050.612 I llm_load_print_meta: rope scaling     = linear
0.00.050.613 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.613 I llm_load_print_meta: freq_scale_train = 1
0.00.050.613 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.613 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.613 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.614 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.614 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.614 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.614 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.623 I llm_load_print_meta: model type       = 1.4B
0.00.050.624 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.624 I llm_load_print_meta: model params     = 1.41 B
0.00.050.624 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.624 I llm_load_print_meta: general.name     = 1.4B
0.00.050.625 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.625 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.625 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.625 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.625 I llm_load_print_meta: LF token         = 128 ''
0.00.050.626 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.626 I llm_load_print_meta: max token length = 1024
0.00.052.667 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.668 I llm_load_tensors: offloading output layer to GPU
0.00.052.668 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.678 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.679 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.669 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.669 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.670 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.670 I llama_new_context_with_model: n_batch       = 2048
0.00.053.670 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.670 I llama_new_context_with_model: flash_attn    = 0
0.00.053.671 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.671 I llama_new_context_with_model: freq_scale    = 1
0.00.053.671 I ggml_metal_init: allocating
0.00.053.675 I ggml_metal_init: found device: Apple M4
0.00.053.677 I ggml_metal_init: picking default device: Apple M4
0.00.054.238 I ggml_metal_init: using embedded metal library
0.00.056.534 I ggml_metal_init: GPU name:   Apple M4
0.00.056.536 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.536 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.537 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.537 I ggml_metal_init: simdgroup reduction   = true
0.00.056.538 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.539 I ggml_metal_init: has bfloat            = true
0.00.056.539 I ggml_metal_init: use bfloat            = true
0.00.056.539 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.540 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.574 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.579 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.598 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.624 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.625 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.626 I llama_new_context_with_model: graph nodes  = 967
0.00.088.626 I llama_new_context_with_model: graph splits = 2
0.00.088.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.032 I main: llama threadpool init, n_threads = 4
0.00.755.069 I 
0.00.755.112 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.113 I 
0.00.755.330 I sampler seed: 1234
0.00.755.335 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.346 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.346 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.346 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.645.478 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.645.478 I llama_perf_context_print:        load time =     746.32 ms
0.01.645.479 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.69 tokens per second)
0.01.645.480 I llama_perf_context_print:        eval time =     833.00 ms /    63 runs   (   13.22 ms per token,    75.63 tokens per second)
0.01.645.480 I llama_perf_context_print:       total time =     890.45 ms /    70 tokens
0.01.645.704 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4272 (6fe62478) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.744 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.044 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.051 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.051 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.051 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.052 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.052 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.053 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.053 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.053 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.053 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.054 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.055 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.056 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.056 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.841 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.874 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.864 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.864 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.865 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.865 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.865 I llama_model_loader: - type  f32:  194 tensors
0.00.025.866 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.148 I llm_load_vocab: special tokens cache size = 25
0.00.053.196 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.198 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.199 I llm_load_print_meta: arch             = gptneox
0.00.053.199 I llm_load_print_meta: vocab type       = BPE
0.00.053.199 I llm_load_print_meta: n_vocab          = 50304
0.00.053.200 I llm_load_print_meta: n_merges         = 50009
0.00.053.200 I llm_load_print_meta: vocab_only       = 0
0.00.053.200 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.200 I llm_load_print_meta: n_embd           = 2048
0.00.053.200 I llm_load_print_meta: n_layer          = 24
0.00.053.203 I llm_load_print_meta: n_head           = 16
0.00.053.204 I llm_load_print_meta: n_head_kv        = 16
0.00.053.216 I llm_load_print_meta: n_rot            = 32
0.00.053.216 I llm_load_print_meta: n_swa            = 0
0.00.053.216 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.216 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.217 I llm_load_print_meta: n_gqa            = 1
0.00.053.218 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.219 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.219 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.220 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.220 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.220 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.220 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.221 I llm_load_print_meta: n_ff             = 8192
0.00.053.221 I llm_load_print_meta: n_expert         = 0
0.00.053.221 I llm_load_print_meta: n_expert_used    = 0
0.00.053.221 I llm_load_print_meta: causal attn      = 1
0.00.053.221 I llm_load_print_meta: pooling type     = 0
0.00.053.221 I llm_load_print_meta: rope type        = 2
0.00.053.224 I llm_load_print_meta: rope scaling     = linear
0.00.053.224 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.224 I llm_load_print_meta: freq_scale_train = 1
0.00.053.224 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.224 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.225 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.225 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.225 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.225 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.225 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.235 I llm_load_print_meta: model type       = 1.4B
0.00.053.235 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.235 I llm_load_print_meta: model params     = 1.41 B
0.00.053.236 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.236 I llm_load_print_meta: general.name     = 1.4B
0.00.053.236 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.236 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.236 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.236 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.237 I llm_load_print_meta: LF token         = 128 ''
0.00.053.237 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.237 I llm_load_print_meta: max token length = 1024
0.00.055.206 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.206 I llm_load_tensors: offloading output layer to GPU
0.00.055.207 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.217 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.218 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.173 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.173 I llama_new_context_with_model: n_ctx         = 128
0.00.056.173 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.174 I llama_new_context_with_model: n_batch       = 128
0.00.056.174 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.174 I llama_new_context_with_model: flash_attn    = 0
0.00.056.174 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.175 I llama_new_context_with_model: freq_scale    = 1
0.00.056.175 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.175 I ggml_metal_init: allocating
0.00.056.179 I ggml_metal_init: found device: Apple M4
0.00.056.181 I ggml_metal_init: picking default device: Apple M4
0.00.056.727 I ggml_metal_init: using embedded metal library
0.00.059.013 I ggml_metal_init: GPU name:   Apple M4
0.00.059.014 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.015 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.015 I ggml_metal_init: simdgroup reduction   = true
0.00.059.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.015 I ggml_metal_init: has bfloat            = true
0.00.059.015 I ggml_metal_init: use bfloat            = true
0.00.059.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.810 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.812 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.826 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.752 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.753 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.754 I llama_new_context_with_model: graph nodes  = 967
0.00.070.754 I llama_new_context_with_model: graph splits = 2
0.00.070.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.081 I 
0.00.584.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.138 I perplexity: tokenizing the input ..
0.00.591.809 I perplexity: tokenization took 7.67 ms
0.00.591.820 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.302 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.732.624 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.732.655 I llama_perf_context_print:        load time =     575.33 ms
0.00.732.658 I llama_perf_context_print: prompt eval time =     139.25 ms /   128 tokens (    1.09 ms per token,   919.18 tokens per second)
0.00.732.659 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.659 I llama_perf_context_print:       total time =     148.58 ms /   129 tokens
0.00.733.298 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.080s
sys	0m0.103s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4272 (6fe62478)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15920b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15920be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15920c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15920c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15920cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15920d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15920dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15920e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15920e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15920eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15920f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15920f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x159210050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159210800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159211010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x159211730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x159211e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x159212570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x159212c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159213460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159213b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1592142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1592149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159215260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159215980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159215c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159216250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159216ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159217400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1592176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159217b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159217e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1592186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159218bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159218eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159219350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1592197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159219c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15921a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15921a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15921aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15921af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15921b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15921b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15921bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15921c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15921c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15921d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15921d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15921dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15921e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15921e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15921eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15921f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15921fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159220140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1592205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1592208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159220eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1592216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159221960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159221e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1592222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159222740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159222be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159223080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159223520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1592239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159223e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159224300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1592247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159224c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1592250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159225630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159225b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1592260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159226620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159226b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1592270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159227610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159227b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1592280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159228600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159228b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1592290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1592295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159229b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15922a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15922a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15922ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15922b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15922b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15922bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15922c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15922c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15922cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15922d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15921cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15922d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15922dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15922e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15922e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15922ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15922f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15922f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15922fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1592301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159230700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159230c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1592311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1592316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159231c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159232190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159232630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159232ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159232f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159233410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1592338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159233d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1592341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159234690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159234b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159234fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159235470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159235910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159235db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159236250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1592366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159236b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159237030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1592374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159237970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159237e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1592382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159238750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159238bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159239090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159239530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1592399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159239e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15923a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15923a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15923ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15923b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15923b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15923ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15923bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15923c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15923c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15923ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15923d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15923d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15923da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15923df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15923e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15923e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15923ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15923f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15923f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15923faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15923ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159240430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1592408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159240d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159241210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1592416b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159241b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159241ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159242490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159242930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159242dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159243270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159243710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159243bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159244050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1592444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159244990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159244e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1592452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159245770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159245c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1592460b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159246550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1592469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159246e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159247330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1592477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159247c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159248110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1592485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159248a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159248ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159249390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1592498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159249e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15924a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15924a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15924ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15924b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15924b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15924bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15924c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15924ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15924cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15924d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15924d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15924e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15924e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15924ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15924ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15924f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15924fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159250150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1592506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159250bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159251140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159251690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159251be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159252130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159252680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159252bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159253120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159253670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159253bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159254110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159254660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159254bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159255100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159255650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159255ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1592560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159256640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159256b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1592570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159257630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159257b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1592580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159258620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159258b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1592590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159259610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159259b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15925a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15925a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15925ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15925b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15925b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15925bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15925c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15925c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15925cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15925d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15925d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15925db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15925e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15925e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15925eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15925f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15925f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15925fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159260050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1592605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159260af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159261040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159261590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159261ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159262030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1592624d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159262970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159262e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1592632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159263750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159263bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159264090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159264530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1592649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159264e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159265310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1592657b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159265c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1592660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159266590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159266ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159267200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159267920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159268040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159268760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159268a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159269210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1592694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159269ae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.135.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159406300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159406770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x159406be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159407050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1594074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159407930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159407da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159408210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1594045d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x159404a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x159408680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159408b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1594096b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159409e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15940a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15940ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15940b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15940bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15940c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15940cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15940d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15940d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15940e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15940e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15940ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15940f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15940f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15940f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15940fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159410130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1594105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159410ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159410f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159411200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159411670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159411ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159411f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1594123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159412830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159412ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x159413110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x159413580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1594139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x159413e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1594142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159414740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x159414bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159415020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159415490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159415900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159415d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1594161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159416650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159416ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159416f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1594173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159417910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159417e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159418280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1594186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159418b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159418fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159419440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1594198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159419d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15941a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15941a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15941aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15941aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15941b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15941b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15941bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15941c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15941c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15941c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15941cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15941d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15941d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15941db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15941dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15941e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15941e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15941ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15941f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15941f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15941fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15941fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159420330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1594207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159420c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159421080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1594214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159421960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159421dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159422240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1594226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159422b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159422f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159423400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159423870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159423ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159424150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1594245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159424a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159424ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x159425310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159425780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159425bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159426060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1594264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x159426940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159426db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159427220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159427690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159427b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159427f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1594283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159428850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159428cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159429130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1594295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159429a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159429e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15942a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15942a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15942abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15942b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15942b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15942b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15942bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15942c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15942c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15942cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15942cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15942d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15942d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15942dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15942e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15942e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15942e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15942ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15942f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15942f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15942fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159430020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159430490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159430900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159430d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1594311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159431650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159431ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159431f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1594323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159432810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159432c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1594330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159433560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1594339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159433e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1594342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159434720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159434b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159435000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159435470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1594358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159435d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1594361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159436630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159436aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159436f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159437380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1594377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159437c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1594380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159438540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1594389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159438e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x159439290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159439700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159439b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159439fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15943a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15943a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15943ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15943b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15943b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15943ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15943bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15943c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15943c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15943cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15943d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15943d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15943d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15943de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15943e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15943e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15943eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15943efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15943f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15943f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15943fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159440180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1594405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159440a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159440ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159441340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1594418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159441d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1594421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159442d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159442fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159443280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1594436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159443b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159443fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159444440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1594448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x159444d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159445190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159445600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x159445a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159445ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159446350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1594467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159446c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1594470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159447510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159447980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159447df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159448260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1594486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159448b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159448fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159449420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159449890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159449d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15944a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15944a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15944aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15944aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15944b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15944b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15944bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15944c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15944c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15944c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15944cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15944d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15944d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15944db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15944df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15944e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15944e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15944ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15944f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15944f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15944fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15944fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159450310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159450780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159450bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159451060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1594514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159451940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159451db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x159452220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159452690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159452b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159452f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1594533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159453850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159453cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159454130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1594545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159454a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x159454e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1594552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159455760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159455bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159456040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1594564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159456920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159457390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159457ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1594581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1594588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159458bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159459020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159459620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159459c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15920cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15920d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15920d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15920db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15920df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15920e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15920e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15920ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15920f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15920f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15920fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159210000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1592108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159211070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159211850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x159211f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x159212630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x159212d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x159213410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159213d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159214480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159214b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159215260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159215950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159216040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1592164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159216920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159216d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159217200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159217670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159217ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159217f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1592183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x159218680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159218af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159218f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1592193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159219840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x159219cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15921a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15921a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15921aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15921ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15921b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15921b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15921bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15921c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15921c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15921c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15921cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15921d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15921d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15921dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15921df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15921e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159304950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159304c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159305080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1593054f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159305960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159305f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159306450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159306960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159306e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159307380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159307880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159307d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1593082a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1593087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159308cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1593091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1593096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159309be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15930a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15930a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15930ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15930b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15930b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15930bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15930c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15930c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15930cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15930d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15930da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15930e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15930e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15930ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15930f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15930f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15930fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1593102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159310870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159310e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1593113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1593119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159311f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159312530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159312af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1593130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159313670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159313c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1593141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1593147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159314d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159315330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1593158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159315eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159316470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x159316a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159316ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1593175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x159317b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159318130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1593186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159318c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x159319110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159319620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159319b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15931a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15931a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15931aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15931af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15931b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15931b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15931bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15931c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15931c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15931cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15931d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15931d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15931dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15931e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15931e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15931ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15931f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15931f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15931fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159320070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159320580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159320a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159320fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1593214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1593219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159321ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1593223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1593228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159322e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159323310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159323820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159323d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159324230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159324730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159324c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159325150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159325660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159325b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159326080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159326590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159326aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159326fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1593274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1593279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159327ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1593283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159328900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159328e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159329320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159329830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159329d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15932a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15932a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15932ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15932b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15932b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15932bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15932c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15932c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15932cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15932cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15932d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15932da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15932df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15932e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15932e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15932ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15932f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15932f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15932fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159330280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159330790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159330ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1593311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1593316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159331bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159332180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159332730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159332ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159333290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1593338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159333eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1593344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159334cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159335150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159335410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159335a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159336030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159336820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159336cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159337160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159337600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159337db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159338300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x159338850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159338da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1593392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x159339840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159339d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15933a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15933a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15933ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15933b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15933b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15933bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15933c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15933c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15933cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15933d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15933d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15933dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15933e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15933e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15933ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15933f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15933f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15933fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159340280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1593407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159340d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159341270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1593417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159341d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159342260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1593427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159342d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159343250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1593437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159343cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159344240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159344790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159344ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159345230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159345780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159345cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159346220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159346770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159346cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159347210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159347760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159347cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159348200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159348750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159348ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1593491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159349740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159349c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15934a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15934a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15934abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15934b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15934b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15934b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15934be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15934c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15934c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15934cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15934d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15934d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15934da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15934deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15934e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15934e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15934ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15934f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15934f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159350020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159350740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159350e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159351120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159351910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159351bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1593521e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.755s
user	0m0.282s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4272 (6fe62478)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15560b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15560bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15560c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15560c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15560cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15560d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15560d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15560df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15560e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15560e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15560eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15560f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15560fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155610680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155610e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1556115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155611cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1556123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155612b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1556132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155613a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155614120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155614840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1556150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155615800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155615ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1556160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155616d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155617280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155617540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1556179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155617ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155618530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155618a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155618d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1556191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155619670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155619b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155619fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15561a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15561a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15561ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15561b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15561b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15561b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15561bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15561c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15561ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15561d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15561daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15561e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15561e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15561ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15561f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15561fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15561ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155620460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155620720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155620d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155621520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1556217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155622120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1556225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155622a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155622f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1556233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155623840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155623ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155624180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155624620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155624ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155624f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1556254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155625a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155625f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1556264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1556269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155626f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155627490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1556279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155627f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155628480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1556289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155628f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155629470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1556299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155629f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15562a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15562a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15562af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15562b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15562b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15562bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15562c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15562c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15562cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15561cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15562d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15562db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15562e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15562e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15562eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15562f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15562f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15562fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155630030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155630580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155630ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155631020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155631570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155631ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155632010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1556324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155632950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155632df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155633290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155633730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155633bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155634070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155634510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1556349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155634e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1556352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155635790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155635c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1556360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155636570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155636a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155636eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155637350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1556377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155637c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155638130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1556385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155638a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155638f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1556393b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155639850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155639cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15563a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15563a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15563aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15563af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15563b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15563b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15563bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15563c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15563c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15563cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15563cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15563d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15563d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15563ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15563e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15563e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15563eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15563f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15563f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15563f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15563fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1556402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155640750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155640bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155641090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155641530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1556419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155641e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155642310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1556427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155642c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1556430f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155643590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155643a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155643ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155644370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155644810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155644cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155645150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1556455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155645a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155645f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1556463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155646870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155646d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1556471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155647650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155647af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155647f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155648430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1556488d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155648d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155649210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155649760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155649cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15564a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15564a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15564aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15564b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15564b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15564bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15564c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15564c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15564cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15564d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15564d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15564dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15564e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15564e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15564ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15564f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15564fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15564ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155650520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155650a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155650fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155651510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155651a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155651fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155652500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155652a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155652fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1556534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155653a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155653f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1556544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155654a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155654f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1556554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155655a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155655f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1556564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155656a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155656f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1556574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155657a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155657f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1556584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1556589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155658f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155659490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1556599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155659f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15565a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15565a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15565af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15565b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15565b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15565bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15565c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15565c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15565cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15565d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15565d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15565def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15565e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15565e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15565eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15565f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15565f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15565fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155660420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155660970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155660ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155661410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155661960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155661eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155662350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1556627f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155662c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155663130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1556635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155663a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155663f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1556643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155664850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155664cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155665190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155665630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155665ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155665f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155666410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155666960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155667080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1556677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155667ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1556685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1556688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155669090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155669350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155669960 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.095.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15570aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15570af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15570b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15570b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15570bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15570c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15570c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15570ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15570ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15570d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15570d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15570dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15570ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15570f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15570f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1557100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155710800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155710f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155711640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155711e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155712530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155712c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155713370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155713a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1557141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155714470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155714ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155715010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155715980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155715e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155716300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1557165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155716a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155716ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155717900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155717e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155718300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155718800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155718d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155719200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155719700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155719c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15571a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15571a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15571a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15571adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15571b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15571b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15571bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15571bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15571c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15571c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15571d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15571d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15571d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15571dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15571e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15571ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15571eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15571f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15571f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15571fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155720150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1557205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155720a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155720f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1557213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155721870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155721d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1557221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155722700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155722c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1557231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1557236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155723c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155724190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1557246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155724c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155725180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1557256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155725c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155726170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1557266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155726c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155727160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1557276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155727c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155728150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1557286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155728bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155729140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155729690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155729be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15572a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15572a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15572abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15572b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15572b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15572bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15572c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15572c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15572cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15572d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15572d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15572dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15572e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15572e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15572eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15572f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15572f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15572fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15572ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155730410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1557308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155730d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1557311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155731690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155731b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155731fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155732470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155732910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155732db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155733250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1557336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155733b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155734030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1557344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155734970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155734e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1557352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155735750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155735bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155736530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1557369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155736e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155737310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1557377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155737c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1557380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155738590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155738a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155738ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155739370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155739810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15573a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15573a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15573aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15573af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15573b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15573b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15573bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15573c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15573c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15573caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15573cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15573d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15573d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15573dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15573e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15573e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15573eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15573eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15573f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15573f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15573fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155740270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155740710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155740bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155741050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1557414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155741990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155741e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1557422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155742770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155742c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1557430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155743550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1557439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155743e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155744330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1557447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155744c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155745110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1557455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155745a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156a04430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156a048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156a04d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156a05180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156a055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156a05a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156a05ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156a06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156a067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156a06c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156a07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156a07500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156a07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156a07de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156a08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156a086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156a08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156a08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156a09410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156a09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156a0a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156a0a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156a0a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156a0adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156a0b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156a0b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156a0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156a0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156a0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156a0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156a0cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156a0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156a0d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156a0da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156a0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156a0e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156a0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156a0ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156a0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156a0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156a0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156a0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156a10240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156a106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156a10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156a10f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156a11400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156a11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156a11ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156a12150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156a125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156a12a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156a12ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156a13310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156a13780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156a13bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156a14060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156a144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156a14940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156a14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156a15220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156a15690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156a15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156a15f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156a163e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156a16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156a16cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156a17130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156a175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156a17a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156a17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156a182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156a18760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156a18bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156a19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156a194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156a19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156a19d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156a1a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156a1a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156a1aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156a1af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156a1b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156a1b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156a1bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156a1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156a1c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156a1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156a1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156a1d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156a1d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156a1dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156a1e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156a1ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156a1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156a1f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156a1fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156a202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156a20720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156a20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156a21330 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15570aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15570af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15570b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15570b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15570bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15570c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15570c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15570ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15570ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15570d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15570d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15570dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15570e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15570eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15570f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15570fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155710360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155710a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155711140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155711ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1557121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1557128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155712f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155713680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155713d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1557141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155714650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155714ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155714f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1557153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155715810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155715c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1557160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1557163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155716820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155716c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155717100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155717570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1557179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155717e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1557182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155718730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155718ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155719010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155719480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1557198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155719d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15571a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15571a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15571aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15571af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15571b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15571b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15571bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15571c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15571c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15571c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15571ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15571d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15571d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15571db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15571dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15571e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15571e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15571ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15571f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15571f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15571fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15571ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155720370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1557207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155720c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1557210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155721530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1557219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155721e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155722280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1557226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155722b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155722fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155723440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1557238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155723d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155724190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155724600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155724a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155724ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155725350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1557257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155725c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1557260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155726510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155726980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155726df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155727260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1557276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155727b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155727fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155728420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155728890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155728d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155729170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1557295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155729a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155729ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15572a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15572a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15572ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15572b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15572b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15572b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15572bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15572c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15572c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15572cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15572cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15572d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15572d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15572dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15572e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15572e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15572ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15572eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15572f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15572f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15572fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155730060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1557304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155730940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155730db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155731220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155731690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155731b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155731f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1557323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155732850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155732cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155733130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1557335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155733a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155733e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1557342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155734760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155734bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155735040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1557354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155735920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155735d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155736200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155736670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155736ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155736f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1557373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155737830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155737ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155738110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155738580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1557389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155738e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1557392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155739740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15573a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15573a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15573a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15573ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15573b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15573b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15573bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15573bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15573c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15573c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15573cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15573d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15573d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15573d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15573de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15573e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15573e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15573eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15573f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15573f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15573f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15573fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1557401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155740630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155740aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155740f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1557417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155741c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1557420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155742540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1557429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155742e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155743290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155743700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155743b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155744450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1557448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155744d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1557451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155745610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155745a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155745ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155746360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155746ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155746da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1557472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155747cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155747f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155748530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155748af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1557490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155749670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155749c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15574a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15574a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15574ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15574b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15574b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15574beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15574c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15574ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15574cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15574d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15574db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15574e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15574e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15574ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15574f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15574f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15574fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1557503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155750970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155750f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1557514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155751ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155752070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155752630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155752bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1557531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155753770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155753d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1557542f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1557548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155754e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155755430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1557559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155755fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155756570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155756b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1557570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1557576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155757c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155758230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1557587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155758db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155759370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155759930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155759ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15575a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15575aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15575b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15575b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15575bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15575c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15575c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15575cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15575d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15575d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15575da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15575df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15575e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15575e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15575ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15575f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15575f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15575fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15575ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155760410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155760960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155761080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1557617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155761ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1557625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1557628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155763090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155763350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155763960 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.944s
user	0m0.244s
sys	0m0.140s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.52 real         0.14 user         0.04 sys
```
