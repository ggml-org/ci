### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.41 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.39 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.14 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.24 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.19 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  172.90 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.93 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.88 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.22 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.19 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 215.32 sec*proc (27 tests)

Total Test time (real) = 215.33 sec

real	3m35.409s
user	7m24.190s
sys	0m5.539s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.26 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.91 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.20 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.43 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.28 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.02 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.10 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.04 sec*proc (27 tests)

Total Test time (real) =  50.06 sec

real	0m50.066s
user	1m10.263s
sys	0m5.050s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.074 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.627 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.293 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.018.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.302 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.018.304 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.306 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.018.307 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.018.307 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.018.309 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.018.309 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.018.310 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.018.310 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.018.311 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.018.314 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.018.314 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.018.315 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.018.315 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.018.316 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.018.316 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.018.317 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.022.750 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.023.998 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.000 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.001 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.001 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.002 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.024.002 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.002 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.024.003 I llama_model_loader: - type  f32:  124 tensors
0.00.024.004 I llama_model_loader: - type  f16:   73 tensors
0.00.028.463 I llm_load_vocab: special tokens cache size = 5
0.00.030.670 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.030.675 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.030.675 I llm_load_print_meta: arch             = bert
0.00.030.676 I llm_load_print_meta: vocab type       = WPM
0.00.030.676 I llm_load_print_meta: n_vocab          = 30522
0.00.030.676 I llm_load_print_meta: n_merges         = 0
0.00.030.676 I llm_load_print_meta: vocab_only       = 0
0.00.030.677 I llm_load_print_meta: n_ctx_train      = 512
0.00.030.677 I llm_load_print_meta: n_embd           = 384
0.00.030.677 I llm_load_print_meta: n_layer          = 12
0.00.030.681 I llm_load_print_meta: n_head           = 12
0.00.030.682 I llm_load_print_meta: n_head_kv        = 12
0.00.030.682 I llm_load_print_meta: n_rot            = 32
0.00.030.683 I llm_load_print_meta: n_swa            = 0
0.00.030.683 I llm_load_print_meta: n_embd_head_k    = 32
0.00.030.683 I llm_load_print_meta: n_embd_head_v    = 32
0.00.030.684 I llm_load_print_meta: n_gqa            = 1
0.00.030.685 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.030.685 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.030.686 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.030.687 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.030.687 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.030.687 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.030.687 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.030.688 I llm_load_print_meta: n_ff             = 1536
0.00.030.689 I llm_load_print_meta: n_expert         = 0
0.00.030.689 I llm_load_print_meta: n_expert_used    = 0
0.00.030.689 I llm_load_print_meta: causal attn      = 0
0.00.030.689 I llm_load_print_meta: pooling type     = 2
0.00.030.689 I llm_load_print_meta: rope type        = 2
0.00.030.692 I llm_load_print_meta: rope scaling     = linear
0.00.030.693 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.030.693 I llm_load_print_meta: freq_scale_train = 1
0.00.030.693 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.030.693 I llm_load_print_meta: rope_finetuned   = unknown
0.00.030.694 I llm_load_print_meta: ssm_d_conv       = 0
0.00.030.694 I llm_load_print_meta: ssm_d_inner      = 0
0.00.030.694 I llm_load_print_meta: ssm_d_state      = 0
0.00.030.694 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.030.694 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.030.709 I llm_load_print_meta: model type       = 33M
0.00.030.710 I llm_load_print_meta: model ftype      = F16
0.00.030.710 I llm_load_print_meta: model params     = 33.21 M
0.00.030.711 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.030.711 I llm_load_print_meta: general.name     = Bge Small
0.00.030.712 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.030.712 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.030.712 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.030.713 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.030.713 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.030.713 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.030.715 I llm_load_print_meta: max token length = 21
0.00.032.770 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.032.771 I llm_load_tensors: offloading output layer to GPU
0.00.032.772 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.032.796 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.032.797 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.033.349 I llama_new_context_with_model: n_seq_max     = 1
0.00.033.350 I llama_new_context_with_model: n_ctx         = 512
0.00.033.351 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.033.351 I llama_new_context_with_model: n_batch       = 2048
0.00.033.351 I llama_new_context_with_model: n_ubatch      = 2048
0.00.033.351 I llama_new_context_with_model: flash_attn    = 0
0.00.033.352 I llama_new_context_with_model: freq_base     = 10000.0
0.00.033.352 I llama_new_context_with_model: freq_scale    = 1
0.00.033.353 I ggml_metal_init: allocating
0.00.033.359 I ggml_metal_init: found device: Apple M4
0.00.033.362 I ggml_metal_init: picking default device: Apple M4
0.00.034.120 I ggml_metal_init: using embedded metal library
0.00.037.433 I ggml_metal_init: GPU name:   Apple M4
0.00.037.435 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.037.436 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.037.436 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.037.436 I ggml_metal_init: simdgroup reduction   = true
0.00.037.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.037.437 I ggml_metal_init: has bfloat            = true
0.00.037.437 I ggml_metal_init: use bfloat            = true
0.00.037.437 I ggml_metal_init: hasUnifiedMemory      = true
0.00.037.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.048.277 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.048.279 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.048.281 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.049.053 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.049.054 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.049.054 I llama_new_context_with_model: graph nodes  = 429
0.00.049.055 I llama_new_context_with_model: graph splits = 2
0.00.049.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.055.604 I 
0.00.055.625 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.056.296 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.061.095 I llama_perf_context_print:        load time =      40.97 ms
0.00.061.096 I llama_perf_context_print: prompt eval time =       4.65 ms /     9 tokens (    0.52 ms per token,  1935.48 tokens per second)
0.00.061.096 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.061.097 I llama_perf_context_print:       total time =       5.49 ms /    10 tokens
0.00.061.224 I ggml_metal_free: deallocating

real	0m0.243s
user	0m0.047s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.332 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.369 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.373 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.374 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.374 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.374 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.375 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.376 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.376 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.376 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.377 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.378 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.378 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.379 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.379 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.379 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.380 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.380 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.394 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.395 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.395 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.396 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.396 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.396 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.397 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.397 I llama_model_loader: - type  f32:  124 tensors
0.00.014.397 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.859 I llm_load_vocab: special tokens cache size = 5
0.00.018.117 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.120 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.120 I llm_load_print_meta: arch             = bert
0.00.018.120 I llm_load_print_meta: vocab type       = WPM
0.00.018.120 I llm_load_print_meta: n_vocab          = 30522
0.00.018.121 I llm_load_print_meta: n_merges         = 0
0.00.018.121 I llm_load_print_meta: vocab_only       = 0
0.00.018.121 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.121 I llm_load_print_meta: n_embd           = 384
0.00.018.121 I llm_load_print_meta: n_layer          = 12
0.00.018.123 I llm_load_print_meta: n_head           = 12
0.00.018.124 I llm_load_print_meta: n_head_kv        = 12
0.00.018.126 I llm_load_print_meta: n_rot            = 32
0.00.018.126 I llm_load_print_meta: n_swa            = 0
0.00.018.126 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.126 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.127 I llm_load_print_meta: n_gqa            = 1
0.00.018.127 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.131 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.131 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.132 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.132 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.132 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.132 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.133 I llm_load_print_meta: n_ff             = 1536
0.00.018.133 I llm_load_print_meta: n_expert         = 0
0.00.018.133 I llm_load_print_meta: n_expert_used    = 0
0.00.018.133 I llm_load_print_meta: causal attn      = 0
0.00.018.134 I llm_load_print_meta: pooling type     = 2
0.00.018.134 I llm_load_print_meta: rope type        = 2
0.00.018.134 I llm_load_print_meta: rope scaling     = linear
0.00.018.134 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.134 I llm_load_print_meta: freq_scale_train = 1
0.00.018.135 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.137 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.137 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.137 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.138 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.138 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.138 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.144 I llm_load_print_meta: model type       = 33M
0.00.018.144 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.144 I llm_load_print_meta: model params     = 33.21 M
0.00.018.145 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.145 I llm_load_print_meta: general.name     = Bge Small
0.00.018.146 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.146 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.146 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.147 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.147 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.148 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.148 I llm_load_print_meta: max token length = 21
0.00.019.433 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.433 I llm_load_tensors: offloading output layer to GPU
0.00.019.437 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.444 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.445 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.805 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.806 I llama_new_context_with_model: n_ctx         = 512
0.00.019.806 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.806 I llama_new_context_with_model: n_batch       = 2048
0.00.019.806 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.806 I llama_new_context_with_model: flash_attn    = 0
0.00.019.807 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.807 I llama_new_context_with_model: freq_scale    = 1
0.00.019.807 I ggml_metal_init: allocating
0.00.019.812 I ggml_metal_init: found device: Apple M4
0.00.019.814 I ggml_metal_init: picking default device: Apple M4
0.00.020.325 I ggml_metal_init: using embedded metal library
0.00.022.319 I ggml_metal_init: GPU name:   Apple M4
0.00.022.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.322 I ggml_metal_init: simdgroup reduction   = true
0.00.022.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.322 I ggml_metal_init: has bfloat            = true
0.00.022.322 I ggml_metal_init: use bfloat            = true
0.00.022.323 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.029.574 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.029.576 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.029.577 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.030.152 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.030.153 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.030.154 I llama_new_context_with_model: graph nodes  = 429
0.00.030.154 I llama_new_context_with_model: graph splits = 2
0.00.030.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.034.573 I 
0.00.034.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.035.108 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.039.345 I llama_perf_context_print:        load time =      25.24 ms
0.00.039.346 I llama_perf_context_print: prompt eval time =       4.10 ms /     9 tokens (    0.46 ms per token,  2196.73 tokens per second)
0.00.039.347 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.039.348 I llama_perf_context_print:       total time =       4.77 ms /    10 tokens
0.00.039.513 I ggml_metal_free: deallocating

real	0m0.052s
user	0m0.027s
sys	0m0.014s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.170 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.571 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.638 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.643 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.646 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.032.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.647 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.032.648 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.032.649 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.032.650 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.032.651 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.032.652 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.032.653 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.032.654 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.032.657 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.658 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.658 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.032.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.660 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.040.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.229 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.231 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.231 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.232 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.232 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.233 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.233 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.233 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.234 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.234 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.235 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.235 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.236 I llama_model_loader: - type  f32:   41 tensors
0.00.048.236 I llama_model_loader: - type  f16:   29 tensors
0.00.066.458 W llm_load_vocab: empty token at index 5
0.00.070.967 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.252 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.278 I llm_load_vocab: special tokens cache size = 5
0.00.313.152 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.313.157 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.313.158 I llm_load_print_meta: arch             = jina-bert-v2
0.00.313.158 I llm_load_print_meta: vocab type       = BPE
0.00.313.159 I llm_load_print_meta: n_vocab          = 61056
0.00.313.159 I llm_load_print_meta: n_merges         = 39382
0.00.313.159 I llm_load_print_meta: vocab_only       = 0
0.00.313.159 I llm_load_print_meta: n_ctx_train      = 8192
0.00.313.160 I llm_load_print_meta: n_embd           = 384
0.00.313.163 I llm_load_print_meta: n_layer          = 4
0.00.313.169 I llm_load_print_meta: n_head           = 12
0.00.313.170 I llm_load_print_meta: n_head_kv        = 12
0.00.313.170 I llm_load_print_meta: n_rot            = 32
0.00.313.170 I llm_load_print_meta: n_swa            = 0
0.00.313.172 I llm_load_print_meta: n_embd_head_k    = 32
0.00.313.172 I llm_load_print_meta: n_embd_head_v    = 32
0.00.313.172 I llm_load_print_meta: n_gqa            = 1
0.00.313.173 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.313.173 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.313.174 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.313.175 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.313.175 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.313.175 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.313.175 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.313.176 I llm_load_print_meta: n_ff             = 1536
0.00.313.176 I llm_load_print_meta: n_expert         = 0
0.00.313.176 I llm_load_print_meta: n_expert_used    = 0
0.00.313.176 I llm_load_print_meta: causal attn      = 0
0.00.313.176 I llm_load_print_meta: pooling type     = -1
0.00.313.176 I llm_load_print_meta: rope type        = -1
0.00.313.177 I llm_load_print_meta: rope scaling     = linear
0.00.313.182 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.313.183 I llm_load_print_meta: freq_scale_train = 1
0.00.313.183 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.313.183 I llm_load_print_meta: rope_finetuned   = unknown
0.00.313.185 I llm_load_print_meta: ssm_d_conv       = 0
0.00.313.185 I llm_load_print_meta: ssm_d_inner      = 0
0.00.313.185 I llm_load_print_meta: ssm_d_state      = 0
0.00.313.185 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.313.185 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.313.210 I llm_load_print_meta: model type       = 33M
0.00.313.211 I llm_load_print_meta: model ftype      = F16
0.00.313.212 I llm_load_print_meta: model params     = 32.90 M
0.00.313.212 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.313.212 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.313.212 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.313.213 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.313.213 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.313.213 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.313.213 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.313.213 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.313.213 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.313.214 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.313.214 I llm_load_print_meta: max token length = 45
0.00.314.421 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.314.421 I llm_load_tensors: offloading output layer to GPU
0.00.314.421 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.314.442 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.314.443 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.315.340 I llama_new_context_with_model: n_seq_max     = 1
0.00.315.341 I llama_new_context_with_model: n_ctx         = 8192
0.00.315.341 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.315.341 I llama_new_context_with_model: n_batch       = 2048
0.00.315.341 I llama_new_context_with_model: n_ubatch      = 2048
0.00.315.341 I llama_new_context_with_model: flash_attn    = 0
0.00.315.342 I llama_new_context_with_model: freq_base     = 10000.0
0.00.315.342 I llama_new_context_with_model: freq_scale    = 1
0.00.315.342 I ggml_metal_init: allocating
0.00.315.345 I ggml_metal_init: found device: Apple M4
0.00.315.347 I ggml_metal_init: picking default device: Apple M4
0.00.316.145 I ggml_metal_init: using embedded metal library
0.00.318.436 I ggml_metal_init: GPU name:   Apple M4
0.00.318.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.318.438 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.318.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.318.439 I ggml_metal_init: simdgroup reduction   = true
0.00.318.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.318.439 I ggml_metal_init: has bfloat            = true
0.00.318.439 I ggml_metal_init: use bfloat            = true
0.00.318.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.318.440 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.328.549 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.328.552 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.328.553 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.329.164 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.329.165 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.329.166 I llama_new_context_with_model: graph nodes  = 154
0.00.329.166 I llama_new_context_with_model: graph splits = 2
0.00.329.184 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.341.532 I 
0.00.341.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.341.811 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.341.811 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.341.814 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.341.814 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.341.818 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.341.818 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.342.321 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.346.045 I llama_perf_context_print:        load time =     319.95 ms
0.00.346.045 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16698.09 tokens per second)
0.00.346.047 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.346.047 I llama_perf_context_print:       total time =       4.51 ms /    63 tokens
0.00.346.281 I ggml_metal_free: deallocating

real	0m1.028s
user	0m0.321s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.162 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.307 I main: llama backend init
0.00.000.331 I main: load the model and apply lora adapter, if any
0.00.033.296 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.095 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.122 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.123 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.126 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.127 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.131 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.132 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.302 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.063.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.063.451 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.063.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.063.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.063.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.063.453 I llama_model_loader: - type  f32:  194 tensors
0.00.063.454 I llama_model_loader: - type  f16:   98 tensors
0.00.092.465 I llm_load_vocab: special tokens cache size = 25
0.00.099.346 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.350 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.350 I llm_load_print_meta: arch             = gptneox
0.00.099.350 I llm_load_print_meta: vocab type       = BPE
0.00.099.350 I llm_load_print_meta: n_vocab          = 50304
0.00.099.351 I llm_load_print_meta: n_merges         = 50009
0.00.099.351 I llm_load_print_meta: vocab_only       = 0
0.00.099.351 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.351 I llm_load_print_meta: n_embd           = 2048
0.00.099.351 I llm_load_print_meta: n_layer          = 24
0.00.099.353 I llm_load_print_meta: n_head           = 16
0.00.099.354 I llm_load_print_meta: n_head_kv        = 16
0.00.099.354 I llm_load_print_meta: n_rot            = 32
0.00.099.354 I llm_load_print_meta: n_swa            = 0
0.00.099.354 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.355 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.355 I llm_load_print_meta: n_gqa            = 1
0.00.099.356 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.356 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.361 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.361 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.361 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.361 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.361 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.362 I llm_load_print_meta: n_ff             = 8192
0.00.099.362 I llm_load_print_meta: n_expert         = 0
0.00.099.362 I llm_load_print_meta: n_expert_used    = 0
0.00.099.363 I llm_load_print_meta: causal attn      = 1
0.00.099.363 I llm_load_print_meta: pooling type     = 0
0.00.099.363 I llm_load_print_meta: rope type        = 2
0.00.099.363 I llm_load_print_meta: rope scaling     = linear
0.00.099.364 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.364 I llm_load_print_meta: freq_scale_train = 1
0.00.099.364 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.365 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.365 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.365 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.365 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.365 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.366 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.377 I llm_load_print_meta: model type       = 1.4B
0.00.099.378 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.379 I llm_load_print_meta: model params     = 1.41 B
0.00.099.380 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.380 I llm_load_print_meta: general.name     = 1.4B
0.00.099.380 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.381 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.381 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.381 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.381 I llm_load_print_meta: LF token         = 128 ''
0.00.099.381 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.382 I llm_load_print_meta: max token length = 1024
0.00.101.837 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.838 I llm_load_tensors: offloading output layer to GPU
0.00.101.838 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.855 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.856 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.813 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.813 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.813 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.814 I llama_new_context_with_model: n_batch       = 2048
0.00.102.814 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.814 I llama_new_context_with_model: flash_attn    = 0
0.00.102.814 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.815 I llama_new_context_with_model: freq_scale    = 1
0.00.102.815 I ggml_metal_init: allocating
0.00.102.818 I ggml_metal_init: found device: Apple M4
0.00.102.820 I ggml_metal_init: picking default device: Apple M4
0.00.103.391 I ggml_metal_init: using embedded metal library
0.00.116.026 I ggml_metal_init: GPU name:   Apple M4
0.00.116.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.028 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.029 I ggml_metal_init: simdgroup reduction   = true
0.00.116.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.029 I ggml_metal_init: has bfloat            = true
0.00.116.029 I ggml_metal_init: use bfloat            = true
0.00.116.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.150.154 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.150.158 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.150.182 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.151.035 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.151.036 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.151.037 I llama_new_context_with_model: graph nodes  = 967
0.00.151.037 I llama_new_context_with_model: graph splits = 2
0.00.151.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.224.837 I main: llama threadpool init, n_threads = 4
0.00.224.872 I 
0.00.224.891 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.224.891 I 
0.00.224.972 I sampler seed: 1234
0.00.224.977 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.225.001 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.225.003 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.225.003 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.082.403 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.02.082.404 I llama_perf_context_print:        load time =     191.53 ms
0.02.082.405 I llama_perf_context_print: prompt eval time =      37.47 ms /     7 tokens (    5.35 ms per token,   186.81 tokens per second)
0.02.082.406 I llama_perf_context_print:        eval time =    1816.95 ms /    63 runs   (   28.84 ms per token,    34.67 tokens per second)
0.02.082.406 I llama_perf_context_print:       total time =    1857.57 ms /    70 tokens
0.02.082.570 I ggml_metal_free: deallocating

real	0m2.396s
user	0m0.142s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.640 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.359 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.684 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.694 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.697 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.698 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.704 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.707 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.707 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.708 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.711 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.712 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.713 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.830 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.412 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.414 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.415 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.415 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.416 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.416 I llama_model_loader: - type  f32:  194 tensors
0.00.051.417 I llama_model_loader: - type  f16:   98 tensors
0.00.078.379 I llm_load_vocab: special tokens cache size = 25
0.00.084.889 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.892 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.892 I llm_load_print_meta: arch             = gptneox
0.00.084.892 I llm_load_print_meta: vocab type       = BPE
0.00.084.893 I llm_load_print_meta: n_vocab          = 50304
0.00.084.893 I llm_load_print_meta: n_merges         = 50009
0.00.084.893 I llm_load_print_meta: vocab_only       = 0
0.00.084.893 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.893 I llm_load_print_meta: n_embd           = 2048
0.00.084.893 I llm_load_print_meta: n_layer          = 24
0.00.084.896 I llm_load_print_meta: n_head           = 16
0.00.084.897 I llm_load_print_meta: n_head_kv        = 16
0.00.084.897 I llm_load_print_meta: n_rot            = 32
0.00.084.900 I llm_load_print_meta: n_swa            = 0
0.00.084.900 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.900 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.900 I llm_load_print_meta: n_gqa            = 1
0.00.084.901 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.902 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.903 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.905 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.905 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.905 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.906 I llm_load_print_meta: n_ff             = 8192
0.00.084.906 I llm_load_print_meta: n_expert         = 0
0.00.084.906 I llm_load_print_meta: n_expert_used    = 0
0.00.084.907 I llm_load_print_meta: causal attn      = 1
0.00.084.907 I llm_load_print_meta: pooling type     = 0
0.00.084.907 I llm_load_print_meta: rope type        = 2
0.00.084.907 I llm_load_print_meta: rope scaling     = linear
0.00.084.908 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.908 I llm_load_print_meta: freq_scale_train = 1
0.00.084.908 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.908 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.908 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.908 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.909 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.909 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.909 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.922 I llm_load_print_meta: model type       = 1.4B
0.00.084.922 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.922 I llm_load_print_meta: model params     = 1.41 B
0.00.084.923 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.923 I llm_load_print_meta: general.name     = 1.4B
0.00.084.923 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.924 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.924 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.924 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.924 I llm_load_print_meta: LF token         = 128 ''
0.00.084.924 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.924 I llm_load_print_meta: max token length = 1024
0.00.087.346 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.347 I llm_load_tensors: offloading output layer to GPU
0.00.087.347 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.357 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.358 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.290 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.291 I llama_new_context_with_model: n_ctx         = 128
0.00.088.291 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.291 I llama_new_context_with_model: n_batch       = 128
0.00.088.291 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.292 I llama_new_context_with_model: flash_attn    = 0
0.00.088.292 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.292 I llama_new_context_with_model: freq_scale    = 1
0.00.088.293 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.293 I ggml_metal_init: allocating
0.00.088.300 I ggml_metal_init: found device: Apple M4
0.00.088.303 I ggml_metal_init: picking default device: Apple M4
0.00.088.860 I ggml_metal_init: using embedded metal library
0.00.090.909 I ggml_metal_init: GPU name:   Apple M4
0.00.090.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.912 I ggml_metal_init: simdgroup reduction   = true
0.00.090.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.912 I ggml_metal_init: has bfloat            = true
0.00.090.912 I ggml_metal_init: use bfloat            = true
0.00.090.912 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.913 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.165 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.167 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.180 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.028 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.029 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.029 I llama_new_context_with_model: graph nodes  = 967
0.00.101.030 I llama_new_context_with_model: graph splits = 2
0.00.101.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.934.509 I 
0.00.934.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.934.551 I perplexity: tokenizing the input ..
0.00.948.297 I perplexity: tokenization took 13.74 ms
0.00.948.340 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.084.014 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.085.628 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.085.652 I llama_perf_context_print:        load time =     912.14 ms
0.01.085.653 I llama_perf_context_print: prompt eval time =     134.69 ms /   128 tokens (    1.05 ms per token,   950.32 tokens per second)
0.01.085.655 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.085.659 I llama_perf_context_print:       total time =     151.15 ms /   129 tokens
0.01.086.073 I ggml_metal_free: deallocating

real	0m1.347s
user	0m0.121s
sys	0m0.194s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.063 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.101 I main: load the model and apply lora adapter, if any
0.00.009.856 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.829 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.838 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.838 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.838 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.840 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.840 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.840 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.841 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.841 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.841 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.842 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.844 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.844 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.725 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.799 I llama_model_loader: - type  f32:  194 tensors
0.00.032.799 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.971 I llm_load_vocab: special tokens cache size = 25
0.00.059.983 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.988 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.988 I llm_load_print_meta: arch             = gptneox
0.00.059.989 I llm_load_print_meta: vocab type       = BPE
0.00.059.989 I llm_load_print_meta: n_vocab          = 50304
0.00.059.991 I llm_load_print_meta: n_merges         = 50009
0.00.059.991 I llm_load_print_meta: vocab_only       = 0
0.00.059.991 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.992 I llm_load_print_meta: n_embd           = 2048
0.00.059.992 I llm_load_print_meta: n_layer          = 24
0.00.059.996 I llm_load_print_meta: n_head           = 16
0.00.059.997 I llm_load_print_meta: n_head_kv        = 16
0.00.059.997 I llm_load_print_meta: n_rot            = 32
0.00.059.997 I llm_load_print_meta: n_swa            = 0
0.00.059.997 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.998 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.998 I llm_load_print_meta: n_gqa            = 1
0.00.059.999 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.000 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.000 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.001 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.001 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.001 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.002 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.002 I llm_load_print_meta: n_ff             = 8192
0.00.060.002 I llm_load_print_meta: n_expert         = 0
0.00.060.003 I llm_load_print_meta: n_expert_used    = 0
0.00.060.003 I llm_load_print_meta: causal attn      = 1
0.00.060.003 I llm_load_print_meta: pooling type     = 0
0.00.060.003 I llm_load_print_meta: rope type        = 2
0.00.060.004 I llm_load_print_meta: rope scaling     = linear
0.00.060.004 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.005 I llm_load_print_meta: freq_scale_train = 1
0.00.060.005 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.005 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.005 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.005 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.005 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.006 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.006 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.015 I llm_load_print_meta: model type       = 1.4B
0.00.060.015 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.016 I llm_load_print_meta: model params     = 1.41 B
0.00.060.016 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.016 I llm_load_print_meta: general.name     = 1.4B
0.00.060.017 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.017 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.017 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.017 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.017 I llm_load_print_meta: LF token         = 128 ''
0.00.060.018 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.018 I llm_load_print_meta: max token length = 1024
0.00.062.126 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.126 I llm_load_tensors: offloading output layer to GPU
0.00.062.126 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.132 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.132 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.149 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.150 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.150 I llama_new_context_with_model: n_batch       = 2048
0.00.063.150 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.150 I llama_new_context_with_model: flash_attn    = 0
0.00.063.151 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.151 I llama_new_context_with_model: freq_scale    = 1
0.00.063.151 I ggml_metal_init: allocating
0.00.063.162 I ggml_metal_init: found device: Apple M4
0.00.063.164 I ggml_metal_init: picking default device: Apple M4
0.00.063.890 I ggml_metal_init: using embedded metal library
0.00.066.111 I ggml_metal_init: GPU name:   Apple M4
0.00.066.113 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.114 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.114 I ggml_metal_init: simdgroup reduction   = true
0.00.066.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.115 I ggml_metal_init: has bfloat            = true
0.00.066.115 I ggml_metal_init: use bfloat            = true
0.00.066.115 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.116 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.983 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.996 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.021 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.157 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.159 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.159 I llama_new_context_with_model: graph nodes  = 967
0.00.103.160 I llama_new_context_with_model: graph splits = 2
0.00.103.184 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.426.577 I main: llama threadpool init, n_threads = 4
0.01.426.660 I 
0.01.426.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.01.426.712 I 
0.01.427.304 I sampler seed: 1234
0.01.427.311 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.427.341 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.427.343 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.427.343 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.521.795 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.02.521.795 I llama_perf_context_print:        load time =    1416.71 ms
0.02.521.797 I llama_perf_context_print: prompt eval time =      42.01 ms /     7 tokens (    6.00 ms per token,   166.65 tokens per second)
0.02.521.797 I llama_perf_context_print:        eval time =    1049.50 ms /    63 runs   (   16.66 ms per token,    60.03 tokens per second)
0.02.521.798 I llama_perf_context_print:       total time =    1095.22 ms /    70 tokens
0.02.521.979 I ggml_metal_free: deallocating

real	0m2.541s
user	0m0.121s
sys	0m0.256s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.123 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.914 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.320 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.326 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.334 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.335 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.338 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.339 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.343 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.343 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.344 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.767 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.307 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.208 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.210 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.211 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.211 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.212 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.213 I llama_model_loader: - type  f32:  194 tensors
0.00.032.213 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.584 I llm_load_vocab: special tokens cache size = 25
0.00.062.997 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.002 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.002 I llm_load_print_meta: arch             = gptneox
0.00.063.003 I llm_load_print_meta: vocab type       = BPE
0.00.063.005 I llm_load_print_meta: n_vocab          = 50304
0.00.063.005 I llm_load_print_meta: n_merges         = 50009
0.00.063.005 I llm_load_print_meta: vocab_only       = 0
0.00.063.005 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.006 I llm_load_print_meta: n_embd           = 2048
0.00.063.006 I llm_load_print_meta: n_layer          = 24
0.00.063.011 I llm_load_print_meta: n_head           = 16
0.00.063.012 I llm_load_print_meta: n_head_kv        = 16
0.00.063.012 I llm_load_print_meta: n_rot            = 32
0.00.063.012 I llm_load_print_meta: n_swa            = 0
0.00.063.013 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.013 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.015 I llm_load_print_meta: n_gqa            = 1
0.00.063.016 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.017 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.018 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.018 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.019 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.019 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.019 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.020 I llm_load_print_meta: n_ff             = 8192
0.00.063.020 I llm_load_print_meta: n_expert         = 0
0.00.063.020 I llm_load_print_meta: n_expert_used    = 0
0.00.063.021 I llm_load_print_meta: causal attn      = 1
0.00.063.022 I llm_load_print_meta: pooling type     = 0
0.00.063.022 I llm_load_print_meta: rope type        = 2
0.00.063.022 I llm_load_print_meta: rope scaling     = linear
0.00.063.022 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.023 I llm_load_print_meta: freq_scale_train = 1
0.00.063.023 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.023 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.023 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.023 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.023 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.024 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.024 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.037 I llm_load_print_meta: model type       = 1.4B
0.00.063.037 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.038 I llm_load_print_meta: model params     = 1.41 B
0.00.063.038 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.038 I llm_load_print_meta: general.name     = 1.4B
0.00.063.038 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.039 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.039 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.039 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.039 I llm_load_print_meta: LF token         = 128 ''
0.00.063.040 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.040 I llm_load_print_meta: max token length = 1024
0.00.064.838 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.838 I llm_load_tensors: offloading output layer to GPU
0.00.064.838 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.848 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.849 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.704 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.704 I llama_new_context_with_model: n_ctx         = 128
0.00.065.705 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.705 I llama_new_context_with_model: n_batch       = 128
0.00.065.705 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.705 I llama_new_context_with_model: flash_attn    = 0
0.00.065.706 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.706 I llama_new_context_with_model: freq_scale    = 1
0.00.065.706 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.707 I ggml_metal_init: allocating
0.00.065.710 I ggml_metal_init: found device: Apple M4
0.00.065.712 I ggml_metal_init: picking default device: Apple M4
0.00.066.343 I ggml_metal_init: using embedded metal library
0.00.068.425 I ggml_metal_init: GPU name:   Apple M4
0.00.068.426 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.427 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.428 I ggml_metal_init: simdgroup reduction   = true
0.00.068.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.428 I ggml_metal_init: has bfloat            = true
0.00.068.428 I ggml_metal_init: use bfloat            = true
0.00.068.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.429 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.397 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.078.400 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.078.417 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.387 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.388 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.388 I llama_new_context_with_model: graph nodes  = 967
0.00.079.388 I llama_new_context_with_model: graph splits = 2
0.00.079.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.901.317 I 
0.00.901.334 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.901.337 I perplexity: tokenizing the input ..
0.00.909.676 I perplexity: tokenization took 8.338 ms
0.00.909.694 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.031.703 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.032.949 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.032.965 I llama_perf_context_print:        load time =     889.40 ms
0.01.032.967 I llama_perf_context_print: prompt eval time =     121.78 ms /   128 tokens (    0.95 ms per token,  1051.12 tokens per second)
0.01.032.968 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.032.968 I llama_perf_context_print:       total time =     131.65 ms /   129 tokens
0.01.033.333 I ggml_metal_free: deallocating

real	0m1.051s
user	0m0.090s
sys	0m0.153s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.011.065 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.679 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.684 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.689 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.689 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.689 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.690 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.691 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.693 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.693 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.595 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.639 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.641 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.641 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.642 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.643 I llama_model_loader: - type  f32:  194 tensors
0.00.026.643 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.643 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.167 I llm_load_vocab: special tokens cache size = 25
0.00.053.282 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.286 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.286 I llm_load_print_meta: arch             = gptneox
0.00.053.287 I llm_load_print_meta: vocab type       = BPE
0.00.053.287 I llm_load_print_meta: n_vocab          = 50304
0.00.053.287 I llm_load_print_meta: n_merges         = 50009
0.00.053.287 I llm_load_print_meta: vocab_only       = 0
0.00.053.288 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.288 I llm_load_print_meta: n_embd           = 2048
0.00.053.288 I llm_load_print_meta: n_layer          = 24
0.00.053.293 I llm_load_print_meta: n_head           = 16
0.00.053.294 I llm_load_print_meta: n_head_kv        = 16
0.00.053.294 I llm_load_print_meta: n_rot            = 32
0.00.053.294 I llm_load_print_meta: n_swa            = 0
0.00.053.294 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.295 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.295 I llm_load_print_meta: n_gqa            = 1
0.00.053.296 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.297 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.297 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.298 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.298 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.298 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.298 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.299 I llm_load_print_meta: n_ff             = 8192
0.00.053.299 I llm_load_print_meta: n_expert         = 0
0.00.053.299 I llm_load_print_meta: n_expert_used    = 0
0.00.053.300 I llm_load_print_meta: causal attn      = 1
0.00.053.300 I llm_load_print_meta: pooling type     = 0
0.00.053.300 I llm_load_print_meta: rope type        = 2
0.00.053.300 I llm_load_print_meta: rope scaling     = linear
0.00.053.301 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.301 I llm_load_print_meta: freq_scale_train = 1
0.00.053.301 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.302 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.302 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.302 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.302 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.302 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.302 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.315 I llm_load_print_meta: model type       = 1.4B
0.00.053.316 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.316 I llm_load_print_meta: model params     = 1.41 B
0.00.053.317 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.317 I llm_load_print_meta: general.name     = 1.4B
0.00.053.317 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.317 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.317 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.318 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.319 I llm_load_print_meta: LF token         = 128 ''
0.00.053.319 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.319 I llm_load_print_meta: max token length = 1024
0.00.055.567 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.567 I llm_load_tensors: offloading output layer to GPU
0.00.055.567 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.578 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.579 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.559 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.560 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.560 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.561 I llama_new_context_with_model: n_batch       = 2048
0.00.056.561 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.561 I llama_new_context_with_model: flash_attn    = 0
0.00.056.561 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.562 I llama_new_context_with_model: freq_scale    = 1
0.00.056.562 I ggml_metal_init: allocating
0.00.056.565 I ggml_metal_init: found device: Apple M4
0.00.056.567 I ggml_metal_init: picking default device: Apple M4
0.00.057.250 I ggml_metal_init: using embedded metal library
0.00.059.367 I ggml_metal_init: GPU name:   Apple M4
0.00.059.368 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.369 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.370 I ggml_metal_init: simdgroup reduction   = true
0.00.059.370 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.370 I ggml_metal_init: has bfloat            = true
0.00.059.370 I ggml_metal_init: use bfloat            = true
0.00.059.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.371 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.259 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.273 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.311 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.458 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.460 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.460 I llama_new_context_with_model: graph nodes  = 967
0.00.093.461 I llama_new_context_with_model: graph splits = 2
0.00.093.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.883 I main: llama threadpool init, n_threads = 4
0.00.653.919 I 
0.00.653.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.653.937 I 
0.00.654.166 I sampler seed: 1234
0.00.654.170 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.654.201 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.654.203 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.654.203 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.334.382 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.334.382 I llama_perf_context_print:        load time =     642.81 ms
0.01.334.383 I llama_perf_context_print: prompt eval time =      36.62 ms /     7 tokens (    5.23 ms per token,   191.13 tokens per second)
0.01.334.384 I llama_perf_context_print:        eval time =     640.49 ms /    63 runs   (   10.17 ms per token,    98.36 tokens per second)
0.01.334.384 I llama_perf_context_print:       total time =     680.50 ms /    70 tokens
0.01.334.547 I ggml_metal_free: deallocating

real	0m1.352s
user	0m0.110s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.926 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.680 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.685 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.686 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.687 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.691 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.692 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.692 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.693 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.693 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.695 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.696 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.697 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.697 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.698 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.577 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.459 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.460 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.461 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.461 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.461 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.462 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.462 I llama_model_loader: - type  f32:  194 tensors
0.00.024.463 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.463 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.434 I llm_load_vocab: special tokens cache size = 25
0.00.050.309 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.312 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.312 I llm_load_print_meta: arch             = gptneox
0.00.050.313 I llm_load_print_meta: vocab type       = BPE
0.00.050.313 I llm_load_print_meta: n_vocab          = 50304
0.00.050.313 I llm_load_print_meta: n_merges         = 50009
0.00.050.313 I llm_load_print_meta: vocab_only       = 0
0.00.050.314 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.314 I llm_load_print_meta: n_embd           = 2048
0.00.050.314 I llm_load_print_meta: n_layer          = 24
0.00.050.317 I llm_load_print_meta: n_head           = 16
0.00.050.317 I llm_load_print_meta: n_head_kv        = 16
0.00.050.318 I llm_load_print_meta: n_rot            = 32
0.00.050.318 I llm_load_print_meta: n_swa            = 0
0.00.050.318 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.318 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.319 I llm_load_print_meta: n_gqa            = 1
0.00.050.320 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.320 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.321 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.321 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.321 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.323 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.323 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.323 I llm_load_print_meta: n_ff             = 8192
0.00.050.324 I llm_load_print_meta: n_expert         = 0
0.00.050.324 I llm_load_print_meta: n_expert_used    = 0
0.00.050.324 I llm_load_print_meta: causal attn      = 1
0.00.050.324 I llm_load_print_meta: pooling type     = 0
0.00.050.324 I llm_load_print_meta: rope type        = 2
0.00.050.325 I llm_load_print_meta: rope scaling     = linear
0.00.050.325 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.327 I llm_load_print_meta: freq_scale_train = 1
0.00.050.327 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.327 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.327 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.328 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.328 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.328 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.328 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.334 I llm_load_print_meta: model type       = 1.4B
0.00.050.335 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.335 I llm_load_print_meta: model params     = 1.41 B
0.00.050.335 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.335 I llm_load_print_meta: general.name     = 1.4B
0.00.050.336 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.336 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.336 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.336 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: LF token         = 128 ''
0.00.050.337 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: max token length = 1024
0.00.051.784 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.784 I llm_load_tensors: offloading output layer to GPU
0.00.051.784 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.794 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.795 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.608 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.609 I llama_new_context_with_model: n_ctx         = 128
0.00.052.610 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.610 I llama_new_context_with_model: n_batch       = 128
0.00.052.610 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.610 I llama_new_context_with_model: flash_attn    = 0
0.00.052.610 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.611 I llama_new_context_with_model: freq_scale    = 1
0.00.052.611 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.612 I ggml_metal_init: allocating
0.00.052.615 I ggml_metal_init: found device: Apple M4
0.00.052.617 I ggml_metal_init: picking default device: Apple M4
0.00.053.159 I ggml_metal_init: using embedded metal library
0.00.055.104 I ggml_metal_init: GPU name:   Apple M4
0.00.055.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.106 I ggml_metal_init: simdgroup reduction   = true
0.00.055.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.106 I ggml_metal_init: has bfloat            = true
0.00.055.107 I ggml_metal_init: use bfloat            = true
0.00.055.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.369 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.371 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.385 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.294 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.295 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.295 I llama_new_context_with_model: graph nodes  = 967
0.00.065.296 I llama_new_context_with_model: graph splits = 2
0.00.065.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.408 I 
0.00.608.431 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.608.439 I perplexity: tokenizing the input ..
0.00.616.084 I perplexity: tokenization took 7.642 ms
0.00.616.094 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.738.300 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.739.428 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.739.442 I llama_perf_context_print:        load time =     598.48 ms
0.00.739.443 I llama_perf_context_print: prompt eval time =     121.98 ms /   128 tokens (    0.95 ms per token,  1049.34 tokens per second)
0.00.739.445 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.445 I llama_perf_context_print:       total time =     131.04 ms /   129 tokens
0.00.739.861 I ggml_metal_free: deallocating

real	0m0.754s
user	0m0.076s
sys	0m0.120s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.655 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.116 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.117 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.127 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.021 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.893 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.895 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.895 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.895 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.895 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.896 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.896 I llama_model_loader: - type  f32:  194 tensors
0.00.024.897 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.897 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.086 I llm_load_vocab: special tokens cache size = 25
0.00.051.285 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.288 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.288 I llm_load_print_meta: arch             = gptneox
0.00.051.288 I llm_load_print_meta: vocab type       = BPE
0.00.051.289 I llm_load_print_meta: n_vocab          = 50304
0.00.051.289 I llm_load_print_meta: n_merges         = 50009
0.00.051.289 I llm_load_print_meta: vocab_only       = 0
0.00.051.289 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.289 I llm_load_print_meta: n_embd           = 2048
0.00.051.290 I llm_load_print_meta: n_layer          = 24
0.00.051.292 I llm_load_print_meta: n_head           = 16
0.00.051.293 I llm_load_print_meta: n_head_kv        = 16
0.00.051.293 I llm_load_print_meta: n_rot            = 32
0.00.051.293 I llm_load_print_meta: n_swa            = 0
0.00.051.295 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.296 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.296 I llm_load_print_meta: n_gqa            = 1
0.00.051.297 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.298 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.298 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.299 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.299 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.299 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.299 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.300 I llm_load_print_meta: n_ff             = 8192
0.00.051.300 I llm_load_print_meta: n_expert         = 0
0.00.051.300 I llm_load_print_meta: n_expert_used    = 0
0.00.051.301 I llm_load_print_meta: causal attn      = 1
0.00.051.303 I llm_load_print_meta: pooling type     = 0
0.00.051.303 I llm_load_print_meta: rope type        = 2
0.00.051.303 I llm_load_print_meta: rope scaling     = linear
0.00.051.303 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.304 I llm_load_print_meta: freq_scale_train = 1
0.00.051.304 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.304 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.304 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.304 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.304 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.305 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.305 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.316 I llm_load_print_meta: model type       = 1.4B
0.00.051.316 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.317 I llm_load_print_meta: model params     = 1.41 B
0.00.051.317 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.317 I llm_load_print_meta: general.name     = 1.4B
0.00.051.318 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.318 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.318 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.318 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.318 I llm_load_print_meta: LF token         = 128 ''
0.00.051.319 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.319 I llm_load_print_meta: max token length = 1024
0.00.052.872 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.873 I llm_load_tensors: offloading output layer to GPU
0.00.052.873 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.882 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.883 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.706 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.707 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.707 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.707 I llama_new_context_with_model: n_batch       = 2048
0.00.053.708 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.708 I llama_new_context_with_model: flash_attn    = 0
0.00.053.708 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.709 I llama_new_context_with_model: freq_scale    = 1
0.00.053.709 I ggml_metal_init: allocating
0.00.053.712 I ggml_metal_init: found device: Apple M4
0.00.053.714 I ggml_metal_init: picking default device: Apple M4
0.00.054.308 I ggml_metal_init: using embedded metal library
0.00.056.229 I ggml_metal_init: GPU name:   Apple M4
0.00.056.231 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.231 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.232 I ggml_metal_init: simdgroup reduction   = true
0.00.056.232 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.232 I ggml_metal_init: has bfloat            = true
0.00.056.233 I ggml_metal_init: use bfloat            = true
0.00.056.233 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.235 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.241 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.260 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.223 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.224 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.225 I llama_new_context_with_model: graph nodes  = 967
0.00.085.225 I llama_new_context_with_model: graph splits = 2
0.00.085.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.207 I main: llama threadpool init, n_threads = 4
0.00.646.241 I 
0.00.646.259 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.646.259 I 
0.00.646.405 I sampler seed: 1234
0.00.646.410 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.646.419 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.646.420 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.646.420 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.368.500 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.368.500 I llama_perf_context_print:        load time =     636.55 ms
0.01.368.501 I llama_perf_context_print: prompt eval time =      32.75 ms /     7 tokens (    4.68 ms per token,   213.77 tokens per second)
0.01.368.502 I llama_perf_context_print:        eval time =     686.41 ms /    63 runs   (   10.90 ms per token,    91.78 tokens per second)
0.01.368.502 I llama_perf_context_print:       total time =     722.29 ms /    70 tokens
0.01.368.676 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.561 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.349 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.349 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.350 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.351 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.351 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.356 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.212 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.155 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.156 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.157 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.157 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.157 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.158 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.158 I llama_model_loader: - type  f32:  194 tensors
0.00.023.158 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.158 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.209 I llm_load_vocab: special tokens cache size = 25
0.00.049.264 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.267 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.267 I llm_load_print_meta: arch             = gptneox
0.00.049.267 I llm_load_print_meta: vocab type       = BPE
0.00.049.267 I llm_load_print_meta: n_vocab          = 50304
0.00.049.268 I llm_load_print_meta: n_merges         = 50009
0.00.049.268 I llm_load_print_meta: vocab_only       = 0
0.00.049.268 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.268 I llm_load_print_meta: n_embd           = 2048
0.00.049.268 I llm_load_print_meta: n_layer          = 24
0.00.049.271 I llm_load_print_meta: n_head           = 16
0.00.049.271 I llm_load_print_meta: n_head_kv        = 16
0.00.049.272 I llm_load_print_meta: n_rot            = 32
0.00.049.272 I llm_load_print_meta: n_swa            = 0
0.00.049.272 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.272 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.273 I llm_load_print_meta: n_gqa            = 1
0.00.049.273 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.274 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.275 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.275 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.275 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.278 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.278 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.278 I llm_load_print_meta: n_ff             = 8192
0.00.049.279 I llm_load_print_meta: n_expert         = 0
0.00.049.279 I llm_load_print_meta: n_expert_used    = 0
0.00.049.279 I llm_load_print_meta: causal attn      = 1
0.00.049.279 I llm_load_print_meta: pooling type     = 0
0.00.049.279 I llm_load_print_meta: rope type        = 2
0.00.049.279 I llm_load_print_meta: rope scaling     = linear
0.00.049.280 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.280 I llm_load_print_meta: freq_scale_train = 1
0.00.049.282 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.282 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.282 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.282 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.282 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.282 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.283 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.294 I llm_load_print_meta: model type       = 1.4B
0.00.049.295 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.296 I llm_load_print_meta: model params     = 1.41 B
0.00.049.296 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.297 I llm_load_print_meta: general.name     = 1.4B
0.00.049.297 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.297 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.297 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.297 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.297 I llm_load_print_meta: LF token         = 128 ''
0.00.049.298 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.298 I llm_load_print_meta: max token length = 1024
0.00.051.303 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.303 I llm_load_tensors: offloading output layer to GPU
0.00.051.303 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.313 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.314 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.201 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.202 I llama_new_context_with_model: n_ctx         = 128
0.00.052.202 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.202 I llama_new_context_with_model: n_batch       = 128
0.00.052.202 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.202 I llama_new_context_with_model: flash_attn    = 0
0.00.052.203 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.203 I llama_new_context_with_model: freq_scale    = 1
0.00.052.203 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.204 I ggml_metal_init: allocating
0.00.052.209 I ggml_metal_init: found device: Apple M4
0.00.052.212 I ggml_metal_init: picking default device: Apple M4
0.00.052.744 I ggml_metal_init: using embedded metal library
0.00.054.664 I ggml_metal_init: GPU name:   Apple M4
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.666 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.667 I ggml_metal_init: simdgroup reduction   = true
0.00.054.667 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.667 I ggml_metal_init: has bfloat            = true
0.00.054.667 I ggml_metal_init: use bfloat            = true
0.00.054.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.569 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.571 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.587 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.500 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.501 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.501 I llama_new_context_with_model: graph nodes  = 967
0.00.064.501 I llama_new_context_with_model: graph splits = 2
0.00.064.514 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.465 I 
0.00.604.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.604.492 I perplexity: tokenizing the input ..
0.00.612.171 I perplexity: tokenization took 7.678 ms
0.00.612.182 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.925 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.736.164 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.736.178 I llama_perf_context_print:        load time =     595.90 ms
0.00.736.179 I llama_perf_context_print: prompt eval time =     122.52 ms /   128 tokens (    0.96 ms per token,  1044.74 tokens per second)
0.00.736.180 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.736.180 I llama_perf_context_print:       total time =     131.71 ms /   129 tokens
0.00.736.594 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.077s
sys	0m0.109s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.678 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.358 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.360 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.361 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.363 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.401 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.267 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.269 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.270 I llama_model_loader: - type  f32:  194 tensors
0.00.024.270 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.270 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.588 I llm_load_vocab: special tokens cache size = 25
0.00.050.647 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.649 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.650 I llm_load_print_meta: arch             = gptneox
0.00.050.650 I llm_load_print_meta: vocab type       = BPE
0.00.050.650 I llm_load_print_meta: n_vocab          = 50304
0.00.050.650 I llm_load_print_meta: n_merges         = 50009
0.00.050.651 I llm_load_print_meta: vocab_only       = 0
0.00.050.651 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.651 I llm_load_print_meta: n_embd           = 2048
0.00.050.651 I llm_load_print_meta: n_layer          = 24
0.00.050.653 I llm_load_print_meta: n_head           = 16
0.00.050.654 I llm_load_print_meta: n_head_kv        = 16
0.00.050.654 I llm_load_print_meta: n_rot            = 32
0.00.050.654 I llm_load_print_meta: n_swa            = 0
0.00.050.655 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.655 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.655 I llm_load_print_meta: n_gqa            = 1
0.00.050.656 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.657 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.657 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.658 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.658 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.658 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.660 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.661 I llm_load_print_meta: n_ff             = 8192
0.00.050.661 I llm_load_print_meta: n_expert         = 0
0.00.050.661 I llm_load_print_meta: n_expert_used    = 0
0.00.050.663 I llm_load_print_meta: causal attn      = 1
0.00.050.664 I llm_load_print_meta: pooling type     = 0
0.00.050.664 I llm_load_print_meta: rope type        = 2
0.00.050.665 I llm_load_print_meta: rope scaling     = linear
0.00.050.665 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.665 I llm_load_print_meta: freq_scale_train = 1
0.00.050.665 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.666 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.666 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.666 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.666 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.678 I llm_load_print_meta: model type       = 1.4B
0.00.050.679 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.679 I llm_load_print_meta: model params     = 1.41 B
0.00.050.680 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.680 I llm_load_print_meta: general.name     = 1.4B
0.00.050.680 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.680 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.681 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.681 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.681 I llm_load_print_meta: LF token         = 128 ''
0.00.050.681 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.682 I llm_load_print_meta: max token length = 1024
0.00.052.630 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.631 I llm_load_tensors: offloading output layer to GPU
0.00.052.631 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.641 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.642 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.583 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.584 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.584 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.584 I llama_new_context_with_model: n_batch       = 2048
0.00.053.584 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.584 I llama_new_context_with_model: flash_attn    = 0
0.00.053.585 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.585 I llama_new_context_with_model: freq_scale    = 1
0.00.053.585 I ggml_metal_init: allocating
0.00.053.592 I ggml_metal_init: found device: Apple M4
0.00.053.594 I ggml_metal_init: picking default device: Apple M4
0.00.054.150 I ggml_metal_init: using embedded metal library
0.00.056.054 I ggml_metal_init: GPU name:   Apple M4
0.00.056.056 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.057 I ggml_metal_init: simdgroup reduction   = true
0.00.056.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.057 I ggml_metal_init: has bfloat            = true
0.00.056.057 I ggml_metal_init: use bfloat            = true
0.00.056.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.154 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.164 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.195 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.161 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.162 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.163 I llama_new_context_with_model: graph nodes  = 967
0.00.084.163 I llama_new_context_with_model: graph splits = 2
0.00.084.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.111 I main: llama threadpool init, n_threads = 4
0.00.699.147 I 
0.00.699.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.699.166 I 
0.00.699.389 I sampler seed: 1234
0.00.699.393 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.404 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.404 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.406 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.490.142 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.490.143 I llama_perf_context_print:        load time =     690.43 ms
0.01.490.143 I llama_perf_context_print: prompt eval time =      40.47 ms /     7 tokens (    5.78 ms per token,   172.97 tokens per second)
0.01.490.144 I llama_perf_context_print:        eval time =     747.19 ms /    63 runs   (   11.86 ms per token,    84.32 tokens per second)
0.01.490.144 I llama_perf_context_print:       total time =     791.03 ms /    70 tokens
0.01.490.314 I ggml_metal_free: deallocating

real	0m1.507s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.991 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.989 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.993 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.994 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.995 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.996 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.000 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.001 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.001 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.001 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.002 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.004 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.005 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.008 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.008 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.871 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.971 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.793 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.795 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.796 I llama_model_loader: - type  f32:  194 tensors
0.00.024.796 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.797 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.738 I llm_load_vocab: special tokens cache size = 25
0.00.051.763 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.765 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.766 I llm_load_print_meta: arch             = gptneox
0.00.051.766 I llm_load_print_meta: vocab type       = BPE
0.00.051.766 I llm_load_print_meta: n_vocab          = 50304
0.00.051.767 I llm_load_print_meta: n_merges         = 50009
0.00.051.767 I llm_load_print_meta: vocab_only       = 0
0.00.051.767 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.767 I llm_load_print_meta: n_embd           = 2048
0.00.051.767 I llm_load_print_meta: n_layer          = 24
0.00.051.770 I llm_load_print_meta: n_head           = 16
0.00.051.771 I llm_load_print_meta: n_head_kv        = 16
0.00.051.771 I llm_load_print_meta: n_rot            = 32
0.00.051.772 I llm_load_print_meta: n_swa            = 0
0.00.051.772 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.772 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.773 I llm_load_print_meta: n_gqa            = 1
0.00.051.774 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.774 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.777 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.777 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.777 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.777 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.777 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.778 I llm_load_print_meta: n_ff             = 8192
0.00.051.778 I llm_load_print_meta: n_expert         = 0
0.00.051.778 I llm_load_print_meta: n_expert_used    = 0
0.00.051.779 I llm_load_print_meta: causal attn      = 1
0.00.051.779 I llm_load_print_meta: pooling type     = 0
0.00.051.779 I llm_load_print_meta: rope type        = 2
0.00.051.779 I llm_load_print_meta: rope scaling     = linear
0.00.051.781 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.782 I llm_load_print_meta: freq_scale_train = 1
0.00.051.782 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.783 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.783 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.783 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.783 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.783 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.783 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.798 I llm_load_print_meta: model type       = 1.4B
0.00.051.798 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.799 I llm_load_print_meta: model params     = 1.41 B
0.00.051.799 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.800 I llm_load_print_meta: general.name     = 1.4B
0.00.051.800 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.800 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.800 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.801 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.801 I llm_load_print_meta: LF token         = 128 ''
0.00.051.801 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.802 I llm_load_print_meta: max token length = 1024
0.00.053.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.826 I llm_load_tensors: offloading output layer to GPU
0.00.053.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.836 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.838 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.735 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.736 I llama_new_context_with_model: n_ctx         = 128
0.00.054.736 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.736 I llama_new_context_with_model: n_batch       = 128
0.00.054.737 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.737 I llama_new_context_with_model: flash_attn    = 0
0.00.054.737 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.737 I llama_new_context_with_model: freq_scale    = 1
0.00.054.738 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.738 I ggml_metal_init: allocating
0.00.054.744 I ggml_metal_init: found device: Apple M4
0.00.054.746 I ggml_metal_init: picking default device: Apple M4
0.00.055.277 I ggml_metal_init: using embedded metal library
0.00.057.188 I ggml_metal_init: GPU name:   Apple M4
0.00.057.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.190 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.191 I ggml_metal_init: simdgroup reduction   = true
0.00.057.191 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.191 I ggml_metal_init: has bfloat            = true
0.00.057.191 I ggml_metal_init: use bfloat            = true
0.00.057.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.192 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.243 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.246 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.264 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.114 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.115 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.115 I llama_new_context_with_model: graph nodes  = 967
0.00.067.115 I llama_new_context_with_model: graph splits = 2
0.00.067.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.812 I 
0.00.640.833 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.640.836 I perplexity: tokenizing the input ..
0.00.648.608 I perplexity: tokenization took 7.77 ms
0.00.648.622 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.783.755 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.785.077 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.785.097 I llama_perf_context_print:        load time =     630.82 ms
0.00.785.098 I llama_perf_context_print: prompt eval time =     134.89 ms /   128 tokens (    1.05 ms per token,   948.89 tokens per second)
0.00.785.099 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.099 I llama_perf_context_print:       total time =     144.29 ms /   129 tokens
0.00.785.508 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.077s
sys	0m0.125s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.918 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.567 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.572 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.573 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.574 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.574 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.575 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.575 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.575 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.575 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.576 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.576 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.578 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.579 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.579 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.485 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.585 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.477 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.479 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.479 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.479 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.479 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.480 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.480 I llama_model_loader: - type  f32:  194 tensors
0.00.025.481 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.481 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.914 I llm_load_vocab: special tokens cache size = 25
0.00.051.981 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.984 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.984 I llm_load_print_meta: arch             = gptneox
0.00.051.984 I llm_load_print_meta: vocab type       = BPE
0.00.051.985 I llm_load_print_meta: n_vocab          = 50304
0.00.051.985 I llm_load_print_meta: n_merges         = 50009
0.00.051.985 I llm_load_print_meta: vocab_only       = 0
0.00.051.985 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.985 I llm_load_print_meta: n_embd           = 2048
0.00.051.986 I llm_load_print_meta: n_layer          = 24
0.00.051.988 I llm_load_print_meta: n_head           = 16
0.00.051.989 I llm_load_print_meta: n_head_kv        = 16
0.00.051.989 I llm_load_print_meta: n_rot            = 32
0.00.051.989 I llm_load_print_meta: n_swa            = 0
0.00.051.990 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.990 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.991 I llm_load_print_meta: n_gqa            = 1
0.00.051.991 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.992 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.992 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.993 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.994 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.995 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.995 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.996 I llm_load_print_meta: n_ff             = 8192
0.00.051.996 I llm_load_print_meta: n_expert         = 0
0.00.051.996 I llm_load_print_meta: n_expert_used    = 0
0.00.051.998 I llm_load_print_meta: causal attn      = 1
0.00.051.999 I llm_load_print_meta: pooling type     = 0
0.00.051.999 I llm_load_print_meta: rope type        = 2
0.00.051.999 I llm_load_print_meta: rope scaling     = linear
0.00.052.000 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.000 I llm_load_print_meta: freq_scale_train = 1
0.00.052.000 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.000 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.000 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.000 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.001 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.001 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.001 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.013 I llm_load_print_meta: model type       = 1.4B
0.00.052.013 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.014 I llm_load_print_meta: model params     = 1.41 B
0.00.052.014 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.014 I llm_load_print_meta: general.name     = 1.4B
0.00.052.014 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.015 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.015 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.015 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.015 I llm_load_print_meta: LF token         = 128 ''
0.00.052.016 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.016 I llm_load_print_meta: max token length = 1024
0.00.054.019 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.019 I llm_load_tensors: offloading output layer to GPU
0.00.054.020 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.030 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.031 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.015 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.016 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.016 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.016 I llama_new_context_with_model: n_batch       = 2048
0.00.055.016 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.017 I llama_new_context_with_model: flash_attn    = 0
0.00.055.017 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.017 I llama_new_context_with_model: freq_scale    = 1
0.00.055.018 I ggml_metal_init: allocating
0.00.055.020 I ggml_metal_init: found device: Apple M4
0.00.055.022 I ggml_metal_init: picking default device: Apple M4
0.00.055.561 I ggml_metal_init: using embedded metal library
0.00.057.484 I ggml_metal_init: GPU name:   Apple M4
0.00.057.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.487 I ggml_metal_init: simdgroup reduction   = true
0.00.057.488 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.488 I ggml_metal_init: has bfloat            = true
0.00.057.489 I ggml_metal_init: use bfloat            = true
0.00.057.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.493 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.149 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.159 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.185 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.285 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.286 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.286 I llama_new_context_with_model: graph nodes  = 967
0.00.085.287 I llama_new_context_with_model: graph splits = 2
0.00.085.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.204 I main: llama threadpool init, n_threads = 4
0.00.773.237 I 
0.00.773.259 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.773.259 I 
0.00.773.476 I sampler seed: 1234
0.00.773.481 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.491 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.491 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.492 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.616.356 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.616.356 I llama_perf_context_print:        load time =     763.28 ms
0.01.616.357 I llama_perf_context_print: prompt eval time =      40.41 ms /     7 tokens (    5.77 ms per token,   173.22 tokens per second)
0.01.616.358 I llama_perf_context_print:        eval time =     799.30 ms /    63 runs   (   12.69 ms per token,    78.82 tokens per second)
0.01.616.359 I llama_perf_context_print:       total time =     843.15 ms /    70 tokens
0.01.616.525 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.109s
sys	0m0.184s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.842 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.706 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.707 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.472 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.322 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.323 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.323 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.324 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.324 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.324 I llama_model_loader: - type  f32:  194 tensors
0.00.023.325 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.325 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.588 I llm_load_vocab: special tokens cache size = 25
0.00.049.723 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.725 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.726 I llm_load_print_meta: arch             = gptneox
0.00.049.726 I llm_load_print_meta: vocab type       = BPE
0.00.049.726 I llm_load_print_meta: n_vocab          = 50304
0.00.049.726 I llm_load_print_meta: n_merges         = 50009
0.00.049.726 I llm_load_print_meta: vocab_only       = 0
0.00.049.727 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.727 I llm_load_print_meta: n_embd           = 2048
0.00.049.727 I llm_load_print_meta: n_layer          = 24
0.00.049.730 I llm_load_print_meta: n_head           = 16
0.00.049.731 I llm_load_print_meta: n_head_kv        = 16
0.00.049.731 I llm_load_print_meta: n_rot            = 32
0.00.049.731 I llm_load_print_meta: n_swa            = 0
0.00.049.731 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.731 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.732 I llm_load_print_meta: n_gqa            = 1
0.00.049.733 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.733 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.734 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.734 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.734 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.736 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.736 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.737 I llm_load_print_meta: n_ff             = 8192
0.00.049.737 I llm_load_print_meta: n_expert         = 0
0.00.049.738 I llm_load_print_meta: n_expert_used    = 0
0.00.049.738 I llm_load_print_meta: causal attn      = 1
0.00.049.739 I llm_load_print_meta: pooling type     = 0
0.00.049.739 I llm_load_print_meta: rope type        = 2
0.00.049.740 I llm_load_print_meta: rope scaling     = linear
0.00.049.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.740 I llm_load_print_meta: freq_scale_train = 1
0.00.049.740 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.741 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.741 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.741 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.741 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.741 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.741 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.753 I llm_load_print_meta: model type       = 1.4B
0.00.049.753 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.753 I llm_load_print_meta: model params     = 1.41 B
0.00.049.754 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.754 I llm_load_print_meta: general.name     = 1.4B
0.00.049.755 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.755 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.755 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.755 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.756 I llm_load_print_meta: LF token         = 128 ''
0.00.049.756 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.756 I llm_load_print_meta: max token length = 1024
0.00.051.312 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.313 I llm_load_tensors: offloading output layer to GPU
0.00.051.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.322 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.323 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.165 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.166 I llama_new_context_with_model: n_ctx         = 128
0.00.052.166 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.166 I llama_new_context_with_model: n_batch       = 128
0.00.052.166 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.166 I llama_new_context_with_model: flash_attn    = 0
0.00.052.167 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.167 I llama_new_context_with_model: freq_scale    = 1
0.00.052.167 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.168 I ggml_metal_init: allocating
0.00.052.173 I ggml_metal_init: found device: Apple M4
0.00.052.176 I ggml_metal_init: picking default device: Apple M4
0.00.052.702 I ggml_metal_init: using embedded metal library
0.00.054.592 I ggml_metal_init: GPU name:   Apple M4
0.00.054.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.594 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.594 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.594 I ggml_metal_init: simdgroup reduction   = true
0.00.054.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.594 I ggml_metal_init: has bfloat            = true
0.00.054.595 I ggml_metal_init: use bfloat            = true
0.00.054.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.835 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.844 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.858 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.745 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.746 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.746 I llama_new_context_with_model: graph nodes  = 967
0.00.064.747 I llama_new_context_with_model: graph splits = 2
0.00.064.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.364 I 
0.00.698.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.698.392 I perplexity: tokenizing the input ..
0.00.706.056 I perplexity: tokenization took 7.662 ms
0.00.706.066 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.657 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.841.825 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.841.838 I llama_perf_context_print:        load time =     689.52 ms
0.00.841.839 I llama_perf_context_print: prompt eval time =     134.37 ms /   128 tokens (    1.05 ms per token,   952.59 tokens per second)
0.00.841.840 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.840 I llama_perf_context_print:       total time =     143.48 ms /   129 tokens
0.00.842.122 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.077s
sys	0m0.145s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.746 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.446 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.446 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.447 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.450 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.450 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.210 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.282 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.049 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.051 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.051 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.051 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.051 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.052 I llama_model_loader: - type  f32:  194 tensors
0.00.024.052 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.052 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.053 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.136 I llm_load_vocab: special tokens cache size = 25
0.00.050.247 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.249 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.250 I llm_load_print_meta: arch             = gptneox
0.00.050.250 I llm_load_print_meta: vocab type       = BPE
0.00.050.250 I llm_load_print_meta: n_vocab          = 50304
0.00.050.251 I llm_load_print_meta: n_merges         = 50009
0.00.050.251 I llm_load_print_meta: vocab_only       = 0
0.00.050.251 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.251 I llm_load_print_meta: n_embd           = 2048
0.00.050.251 I llm_load_print_meta: n_layer          = 24
0.00.050.254 I llm_load_print_meta: n_head           = 16
0.00.050.254 I llm_load_print_meta: n_head_kv        = 16
0.00.050.254 I llm_load_print_meta: n_rot            = 32
0.00.050.255 I llm_load_print_meta: n_swa            = 0
0.00.050.255 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.255 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.256 I llm_load_print_meta: n_gqa            = 1
0.00.050.256 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.257 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.258 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.258 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.258 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.258 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.259 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.259 I llm_load_print_meta: n_ff             = 8192
0.00.050.259 I llm_load_print_meta: n_expert         = 0
0.00.050.260 I llm_load_print_meta: n_expert_used    = 0
0.00.050.260 I llm_load_print_meta: causal attn      = 1
0.00.050.260 I llm_load_print_meta: pooling type     = 0
0.00.050.260 I llm_load_print_meta: rope type        = 2
0.00.050.260 I llm_load_print_meta: rope scaling     = linear
0.00.050.261 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.261 I llm_load_print_meta: freq_scale_train = 1
0.00.050.261 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.261 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.262 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.262 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.262 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.262 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.262 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.266 I llm_load_print_meta: model type       = 1.4B
0.00.050.266 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.266 I llm_load_print_meta: model params     = 1.41 B
0.00.050.267 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.267 I llm_load_print_meta: general.name     = 1.4B
0.00.050.270 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.270 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.270 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.270 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.271 I llm_load_print_meta: LF token         = 128 ''
0.00.050.271 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.271 I llm_load_print_meta: max token length = 1024
0.00.051.776 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.777 I llm_load_tensors: offloading output layer to GPU
0.00.051.777 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.786 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.787 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.624 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.624 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.625 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.625 I llama_new_context_with_model: n_batch       = 2048
0.00.052.625 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.625 I llama_new_context_with_model: flash_attn    = 0
0.00.052.626 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.626 I llama_new_context_with_model: freq_scale    = 1
0.00.052.626 I ggml_metal_init: allocating
0.00.052.631 I ggml_metal_init: found device: Apple M4
0.00.052.634 I ggml_metal_init: picking default device: Apple M4
0.00.053.160 I ggml_metal_init: using embedded metal library
0.00.055.054 I ggml_metal_init: GPU name:   Apple M4
0.00.055.056 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.056 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.056 I ggml_metal_init: simdgroup reduction   = true
0.00.055.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.057 I ggml_metal_init: has bfloat            = true
0.00.055.057 I ggml_metal_init: use bfloat            = true
0.00.055.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.058 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.411 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.416 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.433 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.458 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.459 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.460 I llama_new_context_with_model: graph nodes  = 967
0.00.083.460 I llama_new_context_with_model: graph splits = 2
0.00.083.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.448.965 I main: llama threadpool init, n_threads = 4
0.00.449.005 I 
0.00.449.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.449.024 I 
0.00.449.247 I sampler seed: 1234
0.00.449.251 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.449.272 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.449.272 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.449.272 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.132.489 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66355.14 tokens per second)
0.01.132.490 I llama_perf_context_print:        load time =     439.22 ms
0.01.132.490 I llama_perf_context_print: prompt eval time =      39.35 ms /     7 tokens (    5.62 ms per token,   177.90 tokens per second)
0.01.132.491 I llama_perf_context_print:        eval time =     641.05 ms /    63 runs   (   10.18 ms per token,    98.28 tokens per second)
0.01.132.491 I llama_perf_context_print:       total time =     683.53 ms /    70 tokens
0.01.132.676 I ggml_metal_free: deallocating

real	0m1.153s
user	0m0.108s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.745 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.133 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.137 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.139 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.139 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.140 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.140 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.141 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.141 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.142 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.142 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.143 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.143 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.146 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.146 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.146 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.902 I llama_model_loader: - type  f32:  194 tensors
0.00.023.902 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.902 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.902 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.728 I llm_load_vocab: special tokens cache size = 25
0.00.049.722 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.725 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.726 I llm_load_print_meta: arch             = gptneox
0.00.049.726 I llm_load_print_meta: vocab type       = BPE
0.00.049.727 I llm_load_print_meta: n_vocab          = 50304
0.00.049.727 I llm_load_print_meta: n_merges         = 50009
0.00.049.727 I llm_load_print_meta: vocab_only       = 0
0.00.049.727 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.727 I llm_load_print_meta: n_embd           = 2048
0.00.049.727 I llm_load_print_meta: n_layer          = 24
0.00.049.730 I llm_load_print_meta: n_head           = 16
0.00.049.731 I llm_load_print_meta: n_head_kv        = 16
0.00.049.731 I llm_load_print_meta: n_rot            = 32
0.00.049.731 I llm_load_print_meta: n_swa            = 0
0.00.049.731 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.731 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.732 I llm_load_print_meta: n_gqa            = 1
0.00.049.733 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.733 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.734 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.734 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.734 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.737 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.737 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.738 I llm_load_print_meta: n_ff             = 8192
0.00.049.738 I llm_load_print_meta: n_expert         = 0
0.00.049.738 I llm_load_print_meta: n_expert_used    = 0
0.00.049.738 I llm_load_print_meta: causal attn      = 1
0.00.049.738 I llm_load_print_meta: pooling type     = 0
0.00.049.739 I llm_load_print_meta: rope type        = 2
0.00.049.739 I llm_load_print_meta: rope scaling     = linear
0.00.049.739 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.740 I llm_load_print_meta: freq_scale_train = 1
0.00.049.740 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.740 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.740 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.740 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.741 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.741 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.741 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.748 I llm_load_print_meta: model type       = 1.4B
0.00.049.748 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.748 I llm_load_print_meta: model params     = 1.41 B
0.00.049.749 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.749 I llm_load_print_meta: general.name     = 1.4B
0.00.049.749 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.749 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: LF token         = 128 ''
0.00.049.750 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.750 I llm_load_print_meta: max token length = 1024
0.00.051.471 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.472 I llm_load_tensors: offloading output layer to GPU
0.00.051.472 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.476 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.477 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.469 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.469 I llama_new_context_with_model: n_ctx         = 128
0.00.052.469 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.470 I llama_new_context_with_model: n_batch       = 128
0.00.052.470 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.470 I llama_new_context_with_model: flash_attn    = 0
0.00.052.470 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.471 I llama_new_context_with_model: freq_scale    = 1
0.00.052.471 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.471 I ggml_metal_init: allocating
0.00.052.476 I ggml_metal_init: found device: Apple M4
0.00.052.479 I ggml_metal_init: picking default device: Apple M4
0.00.052.985 I ggml_metal_init: using embedded metal library
0.00.055.084 I ggml_metal_init: GPU name:   Apple M4
0.00.055.085 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.085 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.086 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.086 I ggml_metal_init: simdgroup reduction   = true
0.00.055.086 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.086 I ggml_metal_init: has bfloat            = true
0.00.055.086 I ggml_metal_init: use bfloat            = true
0.00.055.087 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.089 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.994 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.996 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.011 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.835 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.836 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.837 I llama_new_context_with_model: graph nodes  = 967
0.00.064.837 I llama_new_context_with_model: graph splits = 2
0.00.064.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.383.823 I 
0.00.383.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.383.845 I perplexity: tokenizing the input ..
0.00.391.844 I perplexity: tokenization took 7.998 ms
0.00.391.857 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.524.410 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.525.651 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.525.669 I llama_perf_context_print:        load time =     374.07 ms
0.00.525.670 I llama_perf_context_print: prompt eval time =     132.33 ms /   128 tokens (    1.03 ms per token,   967.31 tokens per second)
0.00.525.670 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.525.671 I llama_perf_context_print:       total time =     141.85 ms /   129 tokens
0.00.525.992 I ggml_metal_free: deallocating

real	0m0.541s
user	0m0.076s
sys	0m0.079s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.394 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.082 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.084 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.084 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.085 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.085 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.086 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.086 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.087 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.087 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.088 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.088 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.092 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.179 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.082 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.084 I llama_model_loader: - type  f32:  194 tensors
0.00.025.084 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.085 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.085 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.085 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.192 I llm_load_vocab: special tokens cache size = 25
0.00.052.124 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.127 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.127 I llm_load_print_meta: arch             = gptneox
0.00.052.128 I llm_load_print_meta: vocab type       = BPE
0.00.052.128 I llm_load_print_meta: n_vocab          = 50304
0.00.052.128 I llm_load_print_meta: n_merges         = 50009
0.00.052.128 I llm_load_print_meta: vocab_only       = 0
0.00.052.128 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.129 I llm_load_print_meta: n_embd           = 2048
0.00.052.129 I llm_load_print_meta: n_layer          = 24
0.00.052.132 I llm_load_print_meta: n_head           = 16
0.00.052.133 I llm_load_print_meta: n_head_kv        = 16
0.00.052.133 I llm_load_print_meta: n_rot            = 32
0.00.052.133 I llm_load_print_meta: n_swa            = 0
0.00.052.134 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.134 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.134 I llm_load_print_meta: n_gqa            = 1
0.00.052.135 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.136 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.136 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.137 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.137 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.137 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.137 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.138 I llm_load_print_meta: n_ff             = 8192
0.00.052.139 I llm_load_print_meta: n_expert         = 0
0.00.052.142 I llm_load_print_meta: n_expert_used    = 0
0.00.052.142 I llm_load_print_meta: causal attn      = 1
0.00.052.142 I llm_load_print_meta: pooling type     = 0
0.00.052.143 I llm_load_print_meta: rope type        = 2
0.00.052.143 I llm_load_print_meta: rope scaling     = linear
0.00.052.143 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.143 I llm_load_print_meta: freq_scale_train = 1
0.00.052.144 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.144 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.144 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.144 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.144 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.144 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.145 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.156 I llm_load_print_meta: model type       = 1.4B
0.00.052.156 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.157 I llm_load_print_meta: model params     = 1.41 B
0.00.052.157 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.157 I llm_load_print_meta: general.name     = 1.4B
0.00.052.158 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.159 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.159 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.159 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.161 I llm_load_print_meta: LF token         = 128 ''
0.00.052.161 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.161 I llm_load_print_meta: max token length = 1024
0.00.054.175 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.175 I llm_load_tensors: offloading output layer to GPU
0.00.054.176 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.186 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.187 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.096 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.097 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.097 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.097 I llama_new_context_with_model: n_batch       = 2048
0.00.055.097 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.097 I llama_new_context_with_model: flash_attn    = 0
0.00.055.098 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.098 I llama_new_context_with_model: freq_scale    = 1
0.00.055.098 I ggml_metal_init: allocating
0.00.055.104 I ggml_metal_init: found device: Apple M4
0.00.055.106 I ggml_metal_init: picking default device: Apple M4
0.00.055.672 I ggml_metal_init: using embedded metal library
0.00.057.655 I ggml_metal_init: GPU name:   Apple M4
0.00.057.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.658 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.659 I ggml_metal_init: simdgroup reduction   = true
0.00.057.659 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.659 I ggml_metal_init: has bfloat            = true
0.00.057.659 I ggml_metal_init: use bfloat            = true
0.00.057.660 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.664 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.929 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.935 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.952 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.975 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.977 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.977 I llama_new_context_with_model: graph nodes  = 967
0.00.085.977 I llama_new_context_with_model: graph splits = 2
0.00.086.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.678 I main: llama threadpool init, n_threads = 4
0.00.543.718 I 
0.00.543.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.543.739 I 
0.00.543.981 I sampler seed: 1234
0.00.543.985 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.544.031 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.544.033 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.544.033 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.289.053 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.289.054 I llama_perf_context_print:        load time =     534.28 ms
0.01.289.055 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.84 tokens per second)
0.01.289.055 I llama_perf_context_print:        eval time =     706.23 ms /    63 runs   (   11.21 ms per token,    89.21 tokens per second)
0.01.289.055 I llama_perf_context_print:       total time =     745.38 ms /    70 tokens
0.01.289.217 I ggml_metal_free: deallocating

real	0m1.305s
user	0m0.110s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.006.726 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.012.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.012.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.012.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.012.791 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.012.792 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.012.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.012.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.012.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.012.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.012.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.012.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.012.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.012.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.012.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.016.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.017.712 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.021.618 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.021.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.021.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.021.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.021.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.021.620 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.021.621 I llama_model_loader: - type  f32:  194 tensors
0.00.021.621 I llama_model_loader: - type q3_K:   25 tensors
0.00.021.621 I llama_model_loader: - type q4_K:   71 tensors
0.00.021.621 I llama_model_loader: - type q5_K:    1 tensors
0.00.021.622 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.503 I llm_load_vocab: special tokens cache size = 25
0.00.048.626 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.632 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.632 I llm_load_print_meta: arch             = gptneox
0.00.048.633 I llm_load_print_meta: vocab type       = BPE
0.00.048.633 I llm_load_print_meta: n_vocab          = 50304
0.00.048.633 I llm_load_print_meta: n_merges         = 50009
0.00.048.633 I llm_load_print_meta: vocab_only       = 0
0.00.048.634 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.634 I llm_load_print_meta: n_embd           = 2048
0.00.048.634 I llm_load_print_meta: n_layer          = 24
0.00.048.637 I llm_load_print_meta: n_head           = 16
0.00.048.637 I llm_load_print_meta: n_head_kv        = 16
0.00.048.638 I llm_load_print_meta: n_rot            = 32
0.00.048.638 I llm_load_print_meta: n_swa            = 0
0.00.048.638 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.638 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.639 I llm_load_print_meta: n_gqa            = 1
0.00.048.640 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.641 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.641 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.641 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.642 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.642 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.642 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.643 I llm_load_print_meta: n_ff             = 8192
0.00.048.643 I llm_load_print_meta: n_expert         = 0
0.00.048.643 I llm_load_print_meta: n_expert_used    = 0
0.00.048.643 I llm_load_print_meta: causal attn      = 1
0.00.048.644 I llm_load_print_meta: pooling type     = 0
0.00.048.644 I llm_load_print_meta: rope type        = 2
0.00.048.644 I llm_load_print_meta: rope scaling     = linear
0.00.048.644 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.647 I llm_load_print_meta: freq_scale_train = 1
0.00.048.647 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.647 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.647 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.647 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.648 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.648 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.648 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.654 I llm_load_print_meta: model type       = 1.4B
0.00.048.655 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.655 I llm_load_print_meta: model params     = 1.41 B
0.00.048.655 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.656 I llm_load_print_meta: general.name     = 1.4B
0.00.048.656 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.656 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.656 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.656 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.657 I llm_load_print_meta: LF token         = 128 ''
0.00.048.657 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.658 I llm_load_print_meta: max token length = 1024
0.00.050.232 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.232 I llm_load_tensors: offloading output layer to GPU
0.00.050.232 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.237 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.238 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.225 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.226 I llama_new_context_with_model: n_ctx         = 128
0.00.051.226 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.226 I llama_new_context_with_model: n_batch       = 128
0.00.051.226 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.226 I llama_new_context_with_model: flash_attn    = 0
0.00.051.227 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.227 I llama_new_context_with_model: freq_scale    = 1
0.00.051.227 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.228 I ggml_metal_init: allocating
0.00.051.231 I ggml_metal_init: found device: Apple M4
0.00.051.232 I ggml_metal_init: picking default device: Apple M4
0.00.051.767 I ggml_metal_init: using embedded metal library
0.00.053.701 I ggml_metal_init: GPU name:   Apple M4
0.00.053.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.703 I ggml_metal_init: simdgroup reduction   = true
0.00.053.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.704 I ggml_metal_init: has bfloat            = true
0.00.053.704 I ggml_metal_init: use bfloat            = true
0.00.053.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.706 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.656 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.660 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.674 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.587 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.588 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.588 I llama_new_context_with_model: graph nodes  = 967
0.00.063.588 I llama_new_context_with_model: graph splits = 2
0.00.063.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.351 I 
0.00.506.369 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.506.371 I perplexity: tokenizing the input ..
0.00.513.934 I perplexity: tokenization took 7.561 ms
0.00.513.946 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.645.934 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.128 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.143 I llama_perf_context_print:        load time =     499.62 ms
0.00.647.143 I llama_perf_context_print: prompt eval time =     131.76 ms /   128 tokens (    1.03 ms per token,   971.48 tokens per second)
0.00.647.144 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.145 I llama_perf_context_print:       total time =     140.79 ms /   129 tokens
0.00.647.559 I ggml_metal_free: deallocating

real	0m0.661s
user	0m0.077s
sys	0m0.106s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.289 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.578 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.586 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.587 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.622 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.512 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.514 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.514 I llama_model_loader: - type  f32:  194 tensors
0.00.024.515 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.515 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.515 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.649 I llm_load_vocab: special tokens cache size = 25
0.00.050.755 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.758 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.758 I llm_load_print_meta: arch             = gptneox
0.00.050.758 I llm_load_print_meta: vocab type       = BPE
0.00.050.759 I llm_load_print_meta: n_vocab          = 50304
0.00.050.759 I llm_load_print_meta: n_merges         = 50009
0.00.050.759 I llm_load_print_meta: vocab_only       = 0
0.00.050.759 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.759 I llm_load_print_meta: n_embd           = 2048
0.00.050.759 I llm_load_print_meta: n_layer          = 24
0.00.050.762 I llm_load_print_meta: n_head           = 16
0.00.050.763 I llm_load_print_meta: n_head_kv        = 16
0.00.050.763 I llm_load_print_meta: n_rot            = 32
0.00.050.763 I llm_load_print_meta: n_swa            = 0
0.00.050.765 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.765 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.767 I llm_load_print_meta: n_gqa            = 1
0.00.050.768 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.769 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.770 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.770 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.770 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.771 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.771 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.771 I llm_load_print_meta: n_ff             = 8192
0.00.050.771 I llm_load_print_meta: n_expert         = 0
0.00.050.772 I llm_load_print_meta: n_expert_used    = 0
0.00.050.772 I llm_load_print_meta: causal attn      = 1
0.00.050.772 I llm_load_print_meta: pooling type     = 0
0.00.050.772 I llm_load_print_meta: rope type        = 2
0.00.050.772 I llm_load_print_meta: rope scaling     = linear
0.00.050.773 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.773 I llm_load_print_meta: freq_scale_train = 1
0.00.050.773 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.773 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.774 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.774 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.774 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.774 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.774 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.782 I llm_load_print_meta: model type       = 1.4B
0.00.050.782 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.782 I llm_load_print_meta: model params     = 1.41 B
0.00.050.783 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.783 I llm_load_print_meta: general.name     = 1.4B
0.00.050.783 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.783 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.783 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: LF token         = 128 ''
0.00.050.784 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: max token length = 1024
0.00.052.544 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.544 I llm_load_tensors: offloading output layer to GPU
0.00.052.544 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.549 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.550 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.468 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.469 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.469 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.469 I llama_new_context_with_model: n_batch       = 2048
0.00.053.469 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.470 I llama_new_context_with_model: flash_attn    = 0
0.00.053.470 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.470 I llama_new_context_with_model: freq_scale    = 1
0.00.053.471 I ggml_metal_init: allocating
0.00.053.476 I ggml_metal_init: found device: Apple M4
0.00.053.479 I ggml_metal_init: picking default device: Apple M4
0.00.054.051 I ggml_metal_init: using embedded metal library
0.00.055.989 I ggml_metal_init: GPU name:   Apple M4
0.00.055.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.991 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.992 I ggml_metal_init: simdgroup reduction   = true
0.00.055.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.992 I ggml_metal_init: has bfloat            = true
0.00.055.992 I ggml_metal_init: use bfloat            = true
0.00.055.993 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.010 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.016 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.034 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.009 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.010 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.011 I llama_new_context_with_model: graph nodes  = 967
0.00.084.011 I llama_new_context_with_model: graph splits = 2
0.00.084.034 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.617.173 I main: llama threadpool init, n_threads = 4
0.00.617.213 I 
0.00.617.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.617.231 I 
0.00.617.454 I sampler seed: 1234
0.00.617.458 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.617.467 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.617.467 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.617.468 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.373.305 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.01.373.305 I llama_perf_context_print:        load time =     607.88 ms
0.01.373.306 I llama_perf_context_print: prompt eval time =      36.16 ms /     7 tokens (    5.17 ms per token,   193.59 tokens per second)
0.01.373.307 I llama_perf_context_print:        eval time =     716.49 ms /    63 runs   (   11.37 ms per token,    87.93 tokens per second)
0.01.373.308 I llama_perf_context_print:       total time =     756.13 ms /    70 tokens
0.01.373.474 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.110s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.397 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.275 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.282 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.283 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.285 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.286 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.287 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.288 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.036 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.887 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.887 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.888 I llama_model_loader: - type  f32:  194 tensors
0.00.023.888 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.888 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.889 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.953 I llm_load_vocab: special tokens cache size = 25
0.00.049.927 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.929 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.930 I llm_load_print_meta: arch             = gptneox
0.00.049.930 I llm_load_print_meta: vocab type       = BPE
0.00.049.931 I llm_load_print_meta: n_vocab          = 50304
0.00.049.931 I llm_load_print_meta: n_merges         = 50009
0.00.049.931 I llm_load_print_meta: vocab_only       = 0
0.00.049.931 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.931 I llm_load_print_meta: n_embd           = 2048
0.00.049.931 I llm_load_print_meta: n_layer          = 24
0.00.049.935 I llm_load_print_meta: n_head           = 16
0.00.049.935 I llm_load_print_meta: n_head_kv        = 16
0.00.049.935 I llm_load_print_meta: n_rot            = 32
0.00.049.936 I llm_load_print_meta: n_swa            = 0
0.00.049.936 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.937 I llm_load_print_meta: n_gqa            = 1
0.00.049.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.939 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.939 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.939 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.939 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.939 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.940 I llm_load_print_meta: n_ff             = 8192
0.00.049.940 I llm_load_print_meta: n_expert         = 0
0.00.049.940 I llm_load_print_meta: n_expert_used    = 0
0.00.049.941 I llm_load_print_meta: causal attn      = 1
0.00.049.941 I llm_load_print_meta: pooling type     = 0
0.00.049.941 I llm_load_print_meta: rope type        = 2
0.00.049.941 I llm_load_print_meta: rope scaling     = linear
0.00.049.942 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.944 I llm_load_print_meta: freq_scale_train = 1
0.00.049.944 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.944 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.945 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.945 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.945 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.945 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.945 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.957 I llm_load_print_meta: model type       = 1.4B
0.00.049.957 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.957 I llm_load_print_meta: model params     = 1.41 B
0.00.049.958 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.958 I llm_load_print_meta: general.name     = 1.4B
0.00.049.958 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.959 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.959 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.959 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.959 I llm_load_print_meta: LF token         = 128 ''
0.00.049.960 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.960 I llm_load_print_meta: max token length = 1024
0.00.051.892 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.892 I llm_load_tensors: offloading output layer to GPU
0.00.051.892 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.902 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.903 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.840 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.840 I llama_new_context_with_model: n_ctx         = 128
0.00.052.841 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.841 I llama_new_context_with_model: n_batch       = 128
0.00.052.841 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.841 I llama_new_context_with_model: flash_attn    = 0
0.00.052.841 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.842 I llama_new_context_with_model: freq_scale    = 1
0.00.052.842 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.843 I ggml_metal_init: allocating
0.00.052.846 I ggml_metal_init: found device: Apple M4
0.00.052.847 I ggml_metal_init: picking default device: Apple M4
0.00.053.373 I ggml_metal_init: using embedded metal library
0.00.055.317 I ggml_metal_init: GPU name:   Apple M4
0.00.055.318 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.319 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.319 I ggml_metal_init: simdgroup reduction   = true
0.00.055.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.320 I ggml_metal_init: has bfloat            = true
0.00.055.320 I ggml_metal_init: use bfloat            = true
0.00.055.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.321 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.563 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.573 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.594 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.491 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.492 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.492 I llama_new_context_with_model: graph nodes  = 967
0.00.065.493 I llama_new_context_with_model: graph splits = 2
0.00.065.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.577.557 I 
0.00.577.585 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.577.589 I perplexity: tokenizing the input ..
0.00.585.399 I perplexity: tokenization took 7.809 ms
0.00.585.412 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.019 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.721.171 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.721.194 I llama_perf_context_print:        load time =     568.16 ms
0.00.721.195 I llama_perf_context_print: prompt eval time =     134.36 ms /   128 tokens (    1.05 ms per token,   952.64 tokens per second)
0.00.721.196 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.721.196 I llama_perf_context_print:       total time =     143.64 ms /   129 tokens
0.00.721.688 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.076s
sys	0m0.112s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.603 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.954 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.954 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.956 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.817 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.583 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.584 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.585 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.586 I llama_model_loader: - type  f32:  194 tensors
0.00.024.586 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.586 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.746 I llm_load_vocab: special tokens cache size = 25
0.00.050.841 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.843 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.843 I llm_load_print_meta: arch             = gptneox
0.00.050.844 I llm_load_print_meta: vocab type       = BPE
0.00.050.844 I llm_load_print_meta: n_vocab          = 50304
0.00.050.844 I llm_load_print_meta: n_merges         = 50009
0.00.050.844 I llm_load_print_meta: vocab_only       = 0
0.00.050.845 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.845 I llm_load_print_meta: n_embd           = 2048
0.00.050.845 I llm_load_print_meta: n_layer          = 24
0.00.050.848 I llm_load_print_meta: n_head           = 16
0.00.050.848 I llm_load_print_meta: n_head_kv        = 16
0.00.050.849 I llm_load_print_meta: n_rot            = 32
0.00.050.849 I llm_load_print_meta: n_swa            = 0
0.00.050.849 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.849 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.850 I llm_load_print_meta: n_gqa            = 1
0.00.050.851 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.852 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.853 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.853 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.854 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.854 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.854 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.854 I llm_load_print_meta: n_ff             = 8192
0.00.050.855 I llm_load_print_meta: n_expert         = 0
0.00.050.855 I llm_load_print_meta: n_expert_used    = 0
0.00.050.855 I llm_load_print_meta: causal attn      = 1
0.00.050.855 I llm_load_print_meta: pooling type     = 0
0.00.050.855 I llm_load_print_meta: rope type        = 2
0.00.050.856 I llm_load_print_meta: rope scaling     = linear
0.00.050.856 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.856 I llm_load_print_meta: freq_scale_train = 1
0.00.050.857 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.857 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.857 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.857 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.857 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.857 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.858 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.869 I llm_load_print_meta: model type       = 1.4B
0.00.050.870 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.872 I llm_load_print_meta: model params     = 1.41 B
0.00.050.872 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.872 I llm_load_print_meta: general.name     = 1.4B
0.00.050.873 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.873 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.873 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.873 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.873 I llm_load_print_meta: LF token         = 128 ''
0.00.050.874 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.874 I llm_load_print_meta: max token length = 1024
0.00.052.866 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.866 I llm_load_tensors: offloading output layer to GPU
0.00.052.867 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.877 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.878 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.855 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.856 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.856 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.857 I llama_new_context_with_model: n_batch       = 2048
0.00.053.857 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.857 I llama_new_context_with_model: flash_attn    = 0
0.00.053.857 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.858 I llama_new_context_with_model: freq_scale    = 1
0.00.053.858 I ggml_metal_init: allocating
0.00.053.863 I ggml_metal_init: found device: Apple M4
0.00.053.865 I ggml_metal_init: picking default device: Apple M4
0.00.054.431 I ggml_metal_init: using embedded metal library
0.00.056.329 I ggml_metal_init: GPU name:   Apple M4
0.00.056.330 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.330 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.331 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.331 I ggml_metal_init: simdgroup reduction   = true
0.00.056.331 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.331 I ggml_metal_init: has bfloat            = true
0.00.056.331 I ggml_metal_init: use bfloat            = true
0.00.056.332 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.989 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.994 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.013 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.997 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.998 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.998 I llama_new_context_with_model: graph nodes  = 967
0.00.083.999 I llama_new_context_with_model: graph splits = 2
0.00.084.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.518 I main: llama threadpool init, n_threads = 4
0.00.684.549 I 
0.00.684.571 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.684.571 I 
0.00.684.715 I sampler seed: 1234
0.00.684.719 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.735 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.736 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.736 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.527.720 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.527.721 I llama_perf_context_print:        load time =     674.91 ms
0.01.527.722 I llama_perf_context_print: prompt eval time =      38.58 ms /     7 tokens (    5.51 ms per token,   181.45 tokens per second)
0.01.527.722 I llama_perf_context_print:        eval time =     801.37 ms /    63 runs   (   12.72 ms per token,    78.61 tokens per second)
0.01.527.723 I llama_perf_context_print:       total time =     843.21 ms /    70 tokens
0.01.527.909 I ggml_metal_free: deallocating

real	0m1.544s
user	0m0.108s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.892 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.760 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.764 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.767 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.767 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.768 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.769 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.770 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.772 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.628 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.542 I llama_model_loader: - type  f32:  194 tensors
0.00.023.543 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.543 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.123 I llm_load_vocab: special tokens cache size = 25
0.00.050.307 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.309 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.309 I llm_load_print_meta: arch             = gptneox
0.00.050.310 I llm_load_print_meta: vocab type       = BPE
0.00.050.310 I llm_load_print_meta: n_vocab          = 50304
0.00.050.310 I llm_load_print_meta: n_merges         = 50009
0.00.050.311 I llm_load_print_meta: vocab_only       = 0
0.00.050.311 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.311 I llm_load_print_meta: n_embd           = 2048
0.00.050.311 I llm_load_print_meta: n_layer          = 24
0.00.050.313 I llm_load_print_meta: n_head           = 16
0.00.050.314 I llm_load_print_meta: n_head_kv        = 16
0.00.050.314 I llm_load_print_meta: n_rot            = 32
0.00.050.316 I llm_load_print_meta: n_swa            = 0
0.00.050.316 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.317 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.317 I llm_load_print_meta: n_gqa            = 1
0.00.050.318 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.319 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.319 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.320 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.320 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.320 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.320 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.321 I llm_load_print_meta: n_ff             = 8192
0.00.050.321 I llm_load_print_meta: n_expert         = 0
0.00.050.321 I llm_load_print_meta: n_expert_used    = 0
0.00.050.322 I llm_load_print_meta: causal attn      = 1
0.00.050.326 I llm_load_print_meta: pooling type     = 0
0.00.050.326 I llm_load_print_meta: rope type        = 2
0.00.050.326 I llm_load_print_meta: rope scaling     = linear
0.00.050.326 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.327 I llm_load_print_meta: freq_scale_train = 1
0.00.050.327 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.327 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.327 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.328 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.328 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.328 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.328 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.340 I llm_load_print_meta: model type       = 1.4B
0.00.050.341 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.341 I llm_load_print_meta: model params     = 1.41 B
0.00.050.342 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.342 I llm_load_print_meta: general.name     = 1.4B
0.00.050.342 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.342 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.343 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.344 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.344 I llm_load_print_meta: LF token         = 128 ''
0.00.050.344 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.344 I llm_load_print_meta: max token length = 1024
0.00.051.912 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.912 I llm_load_tensors: offloading output layer to GPU
0.00.051.913 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.922 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.923 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.767 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.768 I llama_new_context_with_model: n_ctx         = 128
0.00.052.768 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.768 I llama_new_context_with_model: n_batch       = 128
0.00.052.768 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.768 I llama_new_context_with_model: flash_attn    = 0
0.00.052.769 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.769 I llama_new_context_with_model: freq_scale    = 1
0.00.052.770 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.770 I ggml_metal_init: allocating
0.00.052.773 I ggml_metal_init: found device: Apple M4
0.00.052.775 I ggml_metal_init: picking default device: Apple M4
0.00.053.316 I ggml_metal_init: using embedded metal library
0.00.055.217 I ggml_metal_init: GPU name:   Apple M4
0.00.055.218 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.219 I ggml_metal_init: simdgroup reduction   = true
0.00.055.219 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.219 I ggml_metal_init: has bfloat            = true
0.00.055.219 I ggml_metal_init: use bfloat            = true
0.00.055.220 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.220 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.507 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.509 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.523 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.414 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.415 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.416 I llama_new_context_with_model: graph nodes  = 967
0.00.065.416 I llama_new_context_with_model: graph splits = 2
0.00.065.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.936 I 
0.00.652.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.652.963 I perplexity: tokenizing the input ..
0.00.660.977 I perplexity: tokenization took 8.013 ms
0.00.660.990 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.820 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.803.070 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.803.090 I llama_perf_context_print:        load time =     644.04 ms
0.00.803.092 I llama_perf_context_print: prompt eval time =     140.60 ms /   128 tokens (    1.10 ms per token,   910.35 tokens per second)
0.00.803.094 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.094 I llama_perf_context_print:       total time =     150.15 ms /   129 tokens
0.00.803.486 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.077s
sys	0m0.124s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.174 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.840 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.842 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.847 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.847 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.848 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.849 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.849 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.850 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.850 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.851 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.852 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.853 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.757 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.564 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.565 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.565 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.566 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.567 I llama_model_loader: - type  f32:  194 tensors
0.00.024.567 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.922 I llm_load_vocab: special tokens cache size = 25
0.00.050.959 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.961 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.962 I llm_load_print_meta: arch             = gptneox
0.00.050.962 I llm_load_print_meta: vocab type       = BPE
0.00.050.962 I llm_load_print_meta: n_vocab          = 50304
0.00.050.962 I llm_load_print_meta: n_merges         = 50009
0.00.050.962 I llm_load_print_meta: vocab_only       = 0
0.00.050.963 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.963 I llm_load_print_meta: n_embd           = 2048
0.00.050.963 I llm_load_print_meta: n_layer          = 24
0.00.050.966 I llm_load_print_meta: n_head           = 16
0.00.050.966 I llm_load_print_meta: n_head_kv        = 16
0.00.050.966 I llm_load_print_meta: n_rot            = 32
0.00.050.967 I llm_load_print_meta: n_swa            = 0
0.00.050.967 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.967 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.967 I llm_load_print_meta: n_gqa            = 1
0.00.050.968 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.969 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.969 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.970 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.970 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.973 I llm_load_print_meta: n_ff             = 8192
0.00.050.973 I llm_load_print_meta: n_expert         = 0
0.00.050.973 I llm_load_print_meta: n_expert_used    = 0
0.00.050.973 I llm_load_print_meta: causal attn      = 1
0.00.050.974 I llm_load_print_meta: pooling type     = 0
0.00.050.974 I llm_load_print_meta: rope type        = 2
0.00.050.974 I llm_load_print_meta: rope scaling     = linear
0.00.050.974 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.975 I llm_load_print_meta: freq_scale_train = 1
0.00.050.975 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.975 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.975 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.976 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.976 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.976 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.976 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.987 I llm_load_print_meta: model type       = 1.4B
0.00.050.987 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.987 I llm_load_print_meta: model params     = 1.41 B
0.00.050.988 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.988 I llm_load_print_meta: general.name     = 1.4B
0.00.050.988 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.988 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.989 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.990 I llm_load_print_meta: LF token         = 128 ''
0.00.050.990 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.990 I llm_load_print_meta: max token length = 1024
0.00.052.963 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.964 I llm_load_tensors: offloading output layer to GPU
0.00.052.964 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.974 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.975 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.904 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.904 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.904 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.905 I llama_new_context_with_model: n_batch       = 2048
0.00.053.905 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.905 I llama_new_context_with_model: flash_attn    = 0
0.00.053.905 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.906 I llama_new_context_with_model: freq_scale    = 1
0.00.053.906 I ggml_metal_init: allocating
0.00.053.912 I ggml_metal_init: found device: Apple M4
0.00.053.915 I ggml_metal_init: picking default device: Apple M4
0.00.054.446 I ggml_metal_init: using embedded metal library
0.00.056.386 I ggml_metal_init: GPU name:   Apple M4
0.00.056.388 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.388 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.389 I ggml_metal_init: simdgroup reduction   = true
0.00.056.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.389 I ggml_metal_init: has bfloat            = true
0.00.056.389 I ggml_metal_init: use bfloat            = true
0.00.056.390 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.390 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.666 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.674 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.693 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.741 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.742 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.743 I llama_new_context_with_model: graph nodes  = 967
0.00.084.743 I llama_new_context_with_model: graph splits = 2
0.00.084.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.053 I main: llama threadpool init, n_threads = 4
0.00.767.087 I 
0.00.767.113 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.767.113 I 
0.00.767.341 I sampler seed: 1234
0.00.767.345 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.365 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.365 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.365 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.640.459 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62610.23 tokens per second)
0.01.640.460 I llama_perf_context_print:        load time =     757.88 ms
0.01.640.461 I llama_perf_context_print: prompt eval time =      38.53 ms /     7 tokens (    5.50 ms per token,   181.68 tokens per second)
0.01.640.461 I llama_perf_context_print:        eval time =     831.73 ms /    63 runs   (   13.20 ms per token,    75.75 tokens per second)
0.01.640.462 I llama_perf_context_print:       total time =     873.41 ms /    70 tokens
0.01.640.644 I ggml_metal_free: deallocating

real	0m1.659s
user	0m0.109s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4176 (9a4b79bc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.498 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.502 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.504 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.504 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.505 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.509 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.509 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.285 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.286 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.287 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.287 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.288 I llama_model_loader: - type  f32:  194 tensors
0.00.024.288 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.177 I llm_load_vocab: special tokens cache size = 25
0.00.051.302 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.304 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.305 I llm_load_print_meta: arch             = gptneox
0.00.051.305 I llm_load_print_meta: vocab type       = BPE
0.00.051.305 I llm_load_print_meta: n_vocab          = 50304
0.00.051.306 I llm_load_print_meta: n_merges         = 50009
0.00.051.306 I llm_load_print_meta: vocab_only       = 0
0.00.051.306 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.306 I llm_load_print_meta: n_embd           = 2048
0.00.051.306 I llm_load_print_meta: n_layer          = 24
0.00.051.309 I llm_load_print_meta: n_head           = 16
0.00.051.309 I llm_load_print_meta: n_head_kv        = 16
0.00.051.310 I llm_load_print_meta: n_rot            = 32
0.00.051.310 I llm_load_print_meta: n_swa            = 0
0.00.051.310 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.310 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.311 I llm_load_print_meta: n_gqa            = 1
0.00.051.312 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.313 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.313 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.313 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.313 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.314 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.315 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.316 I llm_load_print_meta: n_ff             = 8192
0.00.051.316 I llm_load_print_meta: n_expert         = 0
0.00.051.318 I llm_load_print_meta: n_expert_used    = 0
0.00.051.318 I llm_load_print_meta: causal attn      = 1
0.00.051.318 I llm_load_print_meta: pooling type     = 0
0.00.051.318 I llm_load_print_meta: rope type        = 2
0.00.051.319 I llm_load_print_meta: rope scaling     = linear
0.00.051.319 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.321 I llm_load_print_meta: freq_scale_train = 1
0.00.051.322 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.323 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.323 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.323 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.323 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.323 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.323 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.335 I llm_load_print_meta: model type       = 1.4B
0.00.051.336 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.336 I llm_load_print_meta: model params     = 1.41 B
0.00.051.337 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.337 I llm_load_print_meta: general.name     = 1.4B
0.00.051.337 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.337 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.338 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.338 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.338 I llm_load_print_meta: LF token         = 128 ''
0.00.051.338 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.338 I llm_load_print_meta: max token length = 1024
0.00.053.453 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.453 I llm_load_tensors: offloading output layer to GPU
0.00.053.453 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.463 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.464 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.489 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.490 I llama_new_context_with_model: n_ctx         = 128
0.00.054.490 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.490 I llama_new_context_with_model: n_batch       = 128
0.00.054.490 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.491 I llama_new_context_with_model: flash_attn    = 0
0.00.054.491 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.491 I llama_new_context_with_model: freq_scale    = 1
0.00.054.492 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.492 I ggml_metal_init: allocating
0.00.054.498 I ggml_metal_init: found device: Apple M4
0.00.054.500 I ggml_metal_init: picking default device: Apple M4
0.00.055.076 I ggml_metal_init: using embedded metal library
0.00.057.041 I ggml_metal_init: GPU name:   Apple M4
0.00.057.043 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.043 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.043 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.044 I ggml_metal_init: simdgroup reduction   = true
0.00.057.044 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.044 I ggml_metal_init: has bfloat            = true
0.00.057.044 I ggml_metal_init: use bfloat            = true
0.00.057.046 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.050 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.281 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.285 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.301 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.192 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.193 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.194 I llama_new_context_with_model: graph nodes  = 967
0.00.067.194 I llama_new_context_with_model: graph splits = 2
0.00.067.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.160.808 I 
0.00.160.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.160.833 I perplexity: tokenizing the input ..
0.00.168.229 I perplexity: tokenization took 7.395 ms
0.00.168.244 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.307.838 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.309.041 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.309.055 I llama_perf_context_print:        load time =     151.11 ms
0.00.309.056 I llama_perf_context_print: prompt eval time =     139.33 ms /   128 tokens (    1.09 ms per token,   918.70 tokens per second)
0.00.309.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.309.057 I llama_perf_context_print:       total time =     148.25 ms /   129 tokens
0.00.309.438 I ggml_metal_free: deallocating

real	0m0.324s
user	0m0.077s
sys	0m0.043s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4176 (9a4b79bc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a3081b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a3088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a308e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a309440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a3099f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a309fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a30a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a30ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a30b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a30b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a30bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a30bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a30cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a30d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a30da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a30e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a30e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a30eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a30f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a30fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a310600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a310d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a311440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a311ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a312400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a3126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a312cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a313940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a313e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a314140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a3145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a3148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a315130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a315670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a315930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a315dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a316270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a316710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a316bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a317050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a3174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a317990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a317e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a3182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a318590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a318ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a3191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a319ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a31a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a31a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a31ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a31b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a31b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a31bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a31c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a31cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a31d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a31d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a31d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a31e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a31e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a31e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a31ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a31f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a31f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a31fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a31ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a320440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a3208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a320d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a321220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a3216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a321b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a322000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a3224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a322940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a322de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a323280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a323720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a323bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a324060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a324500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a3249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a324e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a3252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a325780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a325c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a3260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a326560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a326a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a326ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a327340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a3277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a327c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a328120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a3285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a328a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a3197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a3290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a329550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a3299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a329e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a32a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a32a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a32ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a32b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a32b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a32ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a32bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a32c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a32c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a32ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a32d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a32d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a32dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a32df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a32e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a32e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a32ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a32f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a32f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a32fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a32ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a330450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a3308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a330d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a331230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a3316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a331b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a332010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a3324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a332950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a332df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a333290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a333730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a333bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a334070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a334510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a3349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a334e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a3352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a335790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a335c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a3360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a336570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a336a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a336eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a337350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a3377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a337c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a338130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a3385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a338a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a338fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a339510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a339a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a339fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a33a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a33a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a33ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a33b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a33bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a33c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a33c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a33cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a33d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a33d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a33de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a33e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a33e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a33ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a33f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a33f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a33fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a340370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a3408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a340e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a341360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a3418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a341e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a342350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a3428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a342df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a343340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a343890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a343de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a344330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a344880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a344dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a345320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a345870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a345dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a346310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a346860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a346db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a347300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a347850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a347da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a3482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a348840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a348d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a3492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a349830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a349d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a34a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a34a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a34ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a34b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a34b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a34bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a34c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a34c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a34cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a34d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a34d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a34dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a34e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a34e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a34ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a34f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a34f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a34fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a350270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a3507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a350c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a351100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a3515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a351a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a351ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a352380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a352820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a352cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a353160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a353600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a353aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a353f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a3543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a354930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a355050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a355770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a355e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a3565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a356870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a356e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a357490 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.134.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a2055a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a205a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a205e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a208fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a209410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a209880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a209cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a20a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a20a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a20aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a20aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a20b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a20c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a20c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a20d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a20d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a20df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a20e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a20ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a20f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a20fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a2102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a2109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a2110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a211810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a211ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a211d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a212200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a212670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a212ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a212f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a213480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a2138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a213bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a214020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a214490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a214900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a214d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a2151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a215650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a215ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a215f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a2163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a216810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a216c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a2170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a217560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a2179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a217e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a2182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a218720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a218b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a219000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a219470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a2198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a219d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a347270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a3476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a347b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a347fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a348430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a3488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a348d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a349180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a3495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a349a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a349ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a34a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a34a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a34ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a34b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a34b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a34b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a34bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a34c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a34c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a34cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a34cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a34d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a34d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a34dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a34e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a34e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a34ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a34eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a34f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a34f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a34fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a350070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a3504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a350950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a350dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a351230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a3516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a351b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a351f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a3523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a352860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a352cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a353140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a3535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a353a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a353e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a354300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a354770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a354be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a355050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a3554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a355930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a355da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a356210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a356680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a356af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a356f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a3573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a309280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a309de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a308ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a309840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a307910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a3081b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a315660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a315ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a315f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a3163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a316820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a316c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a317100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a317570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a3179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a317e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a3182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a318730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a318ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a319010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a319480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a3198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a319d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a31a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a31a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a31aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a31af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a31b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a31b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a31bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a31c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a31c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a31c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a31ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a31d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a31d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a31db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a31dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a31e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a31e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a31ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a31f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a31f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a31fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a31ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a320370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a3207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a320c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a3210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a321530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a3219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a321e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a322700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a322b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a322fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a323450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a3238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a323d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a3241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a324610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a324a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a324ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a325360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a3257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a325c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a3260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a326520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a326990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a326e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a327270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a3276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a327b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a327fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a328430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a3288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a328d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a329180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a3295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a329a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a329ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a32a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a32a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a32ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a32b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a32b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a32b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a32bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a32c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a32c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a32cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a32cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a32d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a32d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a32dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a32e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a32e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a32ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a32eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a32f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a32f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a32fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a330070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a3304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a330950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a330dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a331230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a3316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a331b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a331f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a3323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a332860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a332cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a333140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a3335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a333a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a333e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a334300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a334770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a334be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a335050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a3354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a335930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a335da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a336490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a336b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a337270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a337960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a337dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a338240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a3386b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a209250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a2096c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a209b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a209fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a20a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a20a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a20acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a20b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a20b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a20ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a20beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a20c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a20cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a20d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a20dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a20e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a20eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a20f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a20f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a210220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a210910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a211000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a2116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a211de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a2124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a212940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a212db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a213220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a213690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a213b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a213f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a2143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a214850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a214b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a214f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a2153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a215860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a215cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a216140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a2165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a216a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a216e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a217300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a217770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a217be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a218050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a2184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a218930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a218da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a219210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a219680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a219af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a205140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a205400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a205870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a205ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a21a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a21a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a21abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a21b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a21b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a21bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a21c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a21c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a21cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a21cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a21d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a21d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a21dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a21e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a21e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a21eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a21efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a21f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a21f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a21fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a220250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a2206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a220b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a221030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a2214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a221970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a221e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a2222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a222750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a222bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a223090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a223530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a2239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a223e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a224310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a2247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a224c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a2250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a225590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a225a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a225ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a226370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a226810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a226cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a227150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a2275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a227a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a227f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a2283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a228870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a228d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a2291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a229650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a229af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a229f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a22a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a22a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a22ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a22b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a22b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a22bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a22bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a22c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a22c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a22cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a22d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a22d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a22dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a22e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a22e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a22e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a22ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a22f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a22f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a22fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a2300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a230550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a2309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a230e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a231330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a2317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a231c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a232110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a2325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a232a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a232ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a233390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a233830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a233cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a234170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a234610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a234ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a234f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a2353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a235890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a235d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a2361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a236720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a236c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a2371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a237710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a2379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a237fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a2385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a238c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a239210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a239820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a23a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a23a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a23a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a23adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a23b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a23baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a23c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a23c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a23cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a23d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a23d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a23dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a23e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a23e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a23eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a23f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a23f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a23fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a240000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a240550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a240aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a240ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a241540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a241a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a241fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a242530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a242a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a242fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a243520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a243a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a243fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a244510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a244a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a244fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a245500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a245a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a245fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a2464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a246a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a246f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a2474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a247a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a247f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a2484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a248a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a248f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a2494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a249a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a249f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a24a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a24aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a24af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a24b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a24b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a24bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a24c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a24c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a24cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a24d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a24d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a24df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a24e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a24e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a24ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a24f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a24f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a24fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a24ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a250420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a2508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a250d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a251200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a2516a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a251b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a252090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a2527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a252ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a2535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a253d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a253fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a2545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a254bf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.752s
user	0m0.288s
sys	0m0.301s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4176 (9a4b79bc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a80abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a80b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a80b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a80be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a80c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a80c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a80cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a80d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a80daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a80dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a80e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a80e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a80f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a80fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a8104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a810bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a811310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a811a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a812150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a812920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a813040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a813760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a813e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a814e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a815100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a815710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a816380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a8168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a816b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a817020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a8172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a817b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a8180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a818370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a818810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a818cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a819150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a8195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a819a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a819f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a81a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a81a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a81ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a81afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a81b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a81bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a81c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a81cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a81d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a81d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a81dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a81e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a81e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a81f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a81f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a81faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a81fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a820370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a820b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a820e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a8212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a821760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a821c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a8220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a822540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a8229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a822e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a823320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a8237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a823c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a824100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a824a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a824ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a825380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a825820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a825cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a826160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a826600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a826aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a826f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a8273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a827880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a827d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a8281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a828660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a828b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a828fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a829440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a8298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a829d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a82a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a82a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a82ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a82b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a82b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a81c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a82baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a82bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a82c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a82c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a82cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a82d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a82d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a82db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a82dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a82e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a82e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a82edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a82f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a82f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a82fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a830050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a8304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a830990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a830e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a8312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a831770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a831c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a8320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a832550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a8329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a832e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a833330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a8337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a833c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a834110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a8345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a834a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a834ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a835390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a835830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a835cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a836170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a836610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a836ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a836f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a8373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a837890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a837d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a8381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a838670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a838b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a838fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a839450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a8398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a839d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a83a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a83a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a83ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a83b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a83b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a83ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a83bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a83c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a83c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a83ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a83d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a83d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a83dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a83e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a83eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a83f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a83f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a83fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a8400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a840880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a840dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a841320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a841870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a842310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a842860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a842db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a843300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a843850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a843da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a8442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a844840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a844d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a8452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a845830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a845d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a8462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a846820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a846d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a8472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a847810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a847d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a8482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a848800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a848d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a8492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a8497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a849d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a84a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a84a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a84ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a84b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a84b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a84bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a84c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a84c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a84cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a84d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a84d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a84dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a84e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a84e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a84ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a84f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a84f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a84fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a850230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a850780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a850cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a851220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a851770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a851cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a852210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a852760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a852cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a853200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a8536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a853b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a853fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a854480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a854920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a854dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a855260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a855700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a855ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a856040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a8564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a856980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a856e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a857370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a857a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a8581b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a8588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a858ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a8592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a8598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a859ed0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.028 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139f0aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139f0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139f0b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139f0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139f0bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139f0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139f0c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139f0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139f0cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139f0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139f0d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139f0de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139f0e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139f0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139f0f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139f10050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139f10770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139f10e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139f115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139f11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139f124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139f12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139f132e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139f14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139f143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139f146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139f14b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139f14f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139f153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139f158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139f15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139f16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139f16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139f169a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139f16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139f17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139f17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139f18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139f18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139f18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139f19170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139f19670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139f19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139f19fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139f1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139f1a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139f1ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139f1b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139f1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139f1ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139f1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139f1c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139f1c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139f1cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139f1d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139f1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139f1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139f1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139f1ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139f1f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139f1f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139f1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139f200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139f20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139f20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139f20ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139f21340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139f217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139f21c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139f22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139f225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139f22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139f22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139f233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139f23840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139f23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139f24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139f24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139f24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139f24f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139f25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139f258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139f25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139f261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139f26680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139f26b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139f26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139f27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139f27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139f27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139f28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139f286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139f28b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139f29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139f294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139f29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139f29e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139f2a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139f2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139f2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139f2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139f2b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139f2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139f2be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139f2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139f2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139f2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139f2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139f2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139f2da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139f2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139f2e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139f2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139f2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139f2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139f2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139f2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139f2ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139f303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139f30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139f311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139f31640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139f31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139f31f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139f32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139f328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139f32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139f33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139f336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139f33b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139f33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139f34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139f34920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139f34dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139f35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139f35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139f35ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139f36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139f364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139f36980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139f36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139f372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139f37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139f37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139f380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139f38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139f389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139f38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139f39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139f39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139f39dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139f3a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139f3a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139f3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139f3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139f3b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139f3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139f3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139f3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139f3d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139f3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139f3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139f3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139f3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139f3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139f3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139f3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139f3fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139f40180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139f406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139f40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139f41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139f416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139f41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139f42160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139f426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139f42c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139f43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139f436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139f43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139f44140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139f44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139f44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139f45130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139f45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139f45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139f46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139f46670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139f46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139f47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139f47660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139f47bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139f48100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139f48650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139f48ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139f490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139f49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139f49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139f4a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139f4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139f4ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139f4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139f4b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139f4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139f4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139f4c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139f4cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139f4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139f4d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139f4db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139f4e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139f4e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139f4eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139f4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139f4f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139f4fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139f50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139f505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139f50b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139f51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139f51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139f519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139f51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139f522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139f52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139f52c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139f530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139f53570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139f53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139f53eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139f54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139f547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139f54c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139f551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139f56020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139f56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139f56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139f57120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139f57730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139f57d40 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x139f0aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x139f0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139f0b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139f0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139f0bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139f0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139f0c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139f0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139f0cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x139f0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x139f0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x139f0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x139f0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x139f0ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x139f0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x139f0fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x139f102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x139f109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x139f110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x139f11a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x139f12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x139f12810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139f12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139f135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139f13ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x139f14150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139f145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139f14a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139f14ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139f15310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x139f15780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x139f15bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139f16060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139f16320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139f16790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139f16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139f17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139f174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139f17950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139f17dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139f18230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x139f186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139f18b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x139f18f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x139f193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139f19860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x139f19cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x139f1a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139f1a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139f1aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139f1ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x139f1b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x139f1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x139f1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x139f1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x139f1c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x139f1c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x139f1cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139f1d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x139f1d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x139f1daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x139f1df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x139f1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x139f1e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x139f1ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139f1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139f1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139f1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x139f1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x139f202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139f20750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x139f20bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139f21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x139f214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x139f21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x139f21d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x139f221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x139f22660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x139f22ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x139f22f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139f233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x139f23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x139f23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x139f24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x139f24570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x139f249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x139f24e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x139f252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x139f25730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x139f25ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x139f26010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x139f26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x139f268f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x139f26d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x139f271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x139f27640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x139f27ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x139f27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x139f28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x139f28800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x139f28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x139f290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x139f29550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x139f299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x139f29e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x139f2a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x139f2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x139f2ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x139f2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x139f2b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x139f2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x139f2bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x139f2c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x139f2c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x139f2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x139f2cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x139f2d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x139f2d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x139f2dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x139f2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x139f2e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x139f2e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x139f2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x139f2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x139f2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x139f2fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x139f2ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x139f30440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x139f308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x139f30d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139f31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x139f31600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x139f31a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x139f31ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x139f32350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x139f327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x139f32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x139f330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x139f33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x139f33980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x139f33df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x139f34260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x139f346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x139f34b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x139f34fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x139f35420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x139f35890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x139f35d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x139f36170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x139f365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x139f36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x139f36ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x139f37330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x139f377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x139f37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x139f38080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x139f384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x139f38960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x139f38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x139f39240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x139f396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x139f39b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x139f39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x139f3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x139f3a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x139f3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x139f3b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x139f3b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x139f3bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x139f3c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x139f3c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x139f3ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x139f3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139f3d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x139f3d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x139f3dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x139f3e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x139f3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x139f3e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x139f3ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x139f3f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139f3f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x139f3fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139f3ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x139f40440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139f408b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x139f40d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139f41190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x139f41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139f41a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x139f41ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139f42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139f427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x139f42c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x139f430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x139f43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x139f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x139f43df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139f44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139f446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x139f44b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x139f44fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x139f45420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x139f45890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x139e04ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x139e04f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x139e05390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139e05800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x139e05c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139e060e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x139e06550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x139e069c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x139e06e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x139e072a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x139e07710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x139e07b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x139e07ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x139e08460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x139e088d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x139e08d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x139e091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x139e09620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x139e09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x139e09f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x139e0a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x139e0a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x139e0ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x139e0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x139e0b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x139e0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x139e0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x139e0c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x139e0c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x139e0cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x139e0cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x139e0d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x139e0d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x139e0dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x139e0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x139e0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x139e0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x139e0fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x139e10040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x139e10300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x139e10770 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.925s
user	0m0.239s
sys	0m0.143s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.53 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.10 sec*proc (2 tests)

Total Test time (real) =   1.11 sec
        1.13 real         0.71 user         0.04 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.24 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.51 sec
        0.51 real         0.14 user         0.04 sys
```
